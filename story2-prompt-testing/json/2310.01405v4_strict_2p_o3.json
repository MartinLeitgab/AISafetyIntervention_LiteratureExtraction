{
  "logical_chains": [
    {
      "title": "Detecting and increasing model honesty",
      "nodes": [
        {
          "title": "LLMs represent truth internally",
          "aliases": [
            "internal truth signals",
            "truthfulness vector"
          ],
          "type": "concept",
          "description": "Hidden activations of large language models contain a linearly-separable direction whose sign correlates with the factual correctness of candidate answers.",
          "maturity": null
        },
        {
          "title": "Dishonest answers",
          "aliases": [
            "false statements",
            "low-honesty outputs"
          ],
          "type": "concept",
          "description": "Model generations that contradict known facts even when the underlying activations encode the correct information.",
          "maturity": null
        },
        {
          "title": "Reduced reliability",
          "aliases": [
            "safety-critical unreliability",
            "trustworthiness loss"
          ],
          "type": "concept",
          "description": "Failure of users to rely on model outputs for safety-critical tasks owing to occasional false or misleading statements.",
          "maturity": null
        },
        {
          "title": "LAT truth vector reading",
          "aliases": [
            "truth probe",
            "honesty scorer"
          ],
          "type": "intervention",
          "description": "Using Linear Artificial Tomography on a small set of question/answer pairs to extract the truthfulness direction and score or rank candidate answers by their projection on that direction.",
          "maturity": 3
        },
        {
          "title": "Honesty vector steering",
          "aliases": [
            "honesty injection",
            "LoRRA honesty control"
          ],
          "type": "intervention",
          "description": "During generation, add a scaled multiple of the honesty direction (or train low-rank adapters toward it) to bias token probabilities toward truthful continuations.",
          "maturity": 3
        }
      ],
      "edges": [
        {
          "type": "causes",
          "source_node": "Dishonest answers",
          "target_node": "Reduced reliability",
          "description": "False outputs undermine the dependability of the system.",
          "confidence": 3
        },
        {
          "type": "enables",
          "source_node": "LLMs represent truth internally",
          "target_node": "LAT truth vector reading",
          "description": "The separable truth direction allows extraction via probing.",
          "confidence": 4
        },
        {
          "type": "enables",
          "source_node": "LAT truth vector reading",
          "target_node": "Honesty vector steering",
          "description": "Knowing the direction permits causal manipulation of activations.",
          "confidence": 4
        },
        {
          "type": "mitigates",
          "source_node": "Honesty vector steering",
          "target_node": "Dishonest answers",
          "description": "Steering toward the honesty direction reduces the rate of false statements.",
          "confidence": 3
        }
      ]
    },
    {
      "title": "Conditional harmlessness control against jailbreaks",
      "nodes": [
        {
          "title": "Jailbreak attempts",
          "aliases": [
            "adversarial suffixes",
            "policy override prompts"
          ],
          "type": "concept",
          "description": "User-supplied prompt modifications designed to bypass alignment refusals and elicit disallowed content.",
          "maturity": null
        },
        {
          "title": "Dangerous content leakage",
          "aliases": [
            "hazardous compliance",
            "unsafe answers"
          ],
          "type": "concept",
          "description": "Model disclosure of instructions or information that can facilitate harm.",
          "maturity": null
        },
        {
          "title": "Stable harmfulness representation",
          "aliases": [
            "harmfulness vector",
            "internal risk signal"
          ],
          "type": "concept",
          "description": "A direction in hidden space whose magnitude reflects the harmfulness of the requested action even under adversarial perturbations.",
          "maturity": null
        },
        {
          "title": "Harmfulness amplification control",
          "aliases": [
            "piece-wise harmfulness steering",
            "conditional refusal vector"
          ],
          "type": "intervention",
          "description": "Before decoding each token, add sign(h)·v where h is the readout of the harmfulness direction and v its unit vector, amplifying harmfulness only when the reading is positive to trigger refusals.",
          "maturity": 3
        },
        {
          "title": "Jailbreak-induced override",
          "aliases": [
            "alignment bypass",
            "refusal failure"
          ],
          "type": "concept",
          "description": "The model’s alignment safeguards are disabled, leading to compliance with disallowed requests.",
          "maturity": null
        }
      ],
      "edges": [
        {
          "type": "causes",
          "source_node": "Jailbreak attempts",
          "target_node": "Dangerous content leakage",
          "description": "Adversarial prompts increase the chance the model outputs harmful content.",
          "confidence": 3
        },
        {
          "type": "enables",
          "source_node": "Stable harmfulness representation",
          "target_node": "Harmfulness amplification control",
          "description": "A reliable harmfulness readout allows conditional steering.",
          "confidence": 4
        },
        {
          "type": "mitigates",
          "source_node": "Harmfulness amplification control",
          "target_node": "Jailbreak-induced override",
          "description": "Amplifying the harmfulness signal triggers refusal even under jailbreaks.",
          "confidence": 4
        }
      ]
    },
    {
      "title": "Reducing social bias via representation subtraction",
      "nodes": [
        {
          "title": "Latent social bias",
          "aliases": [
            "stereotypical associations",
            "demographic bias"
          ],
          "type": "concept",
          "description": "Undesired correlations between demographic attributes and traits or professions encoded in model behaviour.",
          "maturity": null
        },
        {
          "title": "Bias exposure via adversarial prompts",
          "aliases": [
            "bias under safeties",
            "stereotype leakage"
          ],
          "type": "concept",
          "description": "Prompting strategies that evade refusal policies and surface biased completions.",
          "maturity": null
        },
        {
          "title": "Bias direction separable",
          "aliases": [
            "bias vector",
            "stereotype axis"
          ],
          "type": "concept",
          "description": "A vector obtained from stereotypical vs. anti-stereotypical sentence pairs whose projection predicts biased content.",
          "maturity": null
        },
        {
          "title": "Bias vector subtraction",
          "aliases": [
            "debias steering",
            "bias removal"
          ],
          "type": "intervention",
          "description": "Subtracting a scaled version of the bias vector from activations during generation to neutralise stereotypical content.",
          "maturity": 3
        },
        {
          "title": "Demographic stereotyping",
          "aliases": [
            "biased outputs",
            "stereotype errors"
          ],
          "type": "concept",
          "description": "Outputs that attribute traits or professions to groups based on stereotypes.",
          "maturity": null
        }
      ],
      "edges": [
        {
          "type": "refined_by",
          "source_node": "Latent social bias",
          "target_node": "Bias exposure via adversarial prompts",
          "description": "Adversarial prompting reveals the underlying bias more clearly.",
          "confidence": 3
        },
        {
          "type": "enables",
          "source_node": "Bias direction separable",
          "target_node": "Bias vector subtraction",
          "description": "Identification of the bias axis allows direct subtraction during inference.",
          "confidence": 3
        },
        {
          "type": "mitigates",
          "source_node": "Bias vector subtraction",
          "target_node": "Demographic stereotyping",
          "description": "Steering away from the bias axis lowers rate of biased completions.",
          "confidence": 3
        }
      ]
    },
    {
      "title": "Limiting power-seeking and immorality in agents",
      "nodes": [
        {
          "title": "Agentic power seeking",
          "aliases": [
            "coercive behaviour",
            "instrumental convergence"
          ],
          "type": "concept",
          "description": "Autonomous agents plan actions that increase their own power or influence irrespective of user intent.",
          "maturity": null
        },
        {
          "title": "Potential real-world harm",
          "aliases": [
            "safety risk",
            "negative externalities"
          ],
          "type": "concept",
          "description": "Consequences such as manipulation or unsafe actions when agents pursue power.",
          "maturity": null
        },
        {
          "title": "Power & morality representation",
          "aliases": [
            "power axis",
            "morality vector"
          ],
          "type": "concept",
          "description": "Linear directions in hidden space whose activations correlate with power-seeking intent and morality judgments.",
          "maturity": null
        },
        {
          "title": "LoRRA power-averse control",
          "aliases": [
            "moral adapters",
            "power reduction steering"
          ],
          "type": "intervention",
          "description": "Low-rank adaptation layers trained to push activations away from power-seeking and toward moral regions without decreasing task reward.",
          "maturity": 3
        }
      ],
      "edges": [
        {
          "type": "causes",
          "source_node": "Agentic power seeking",
          "target_node": "Potential real-world harm",
          "description": "Pursuit of power can produce unsafe or immoral outcomes.",
          "confidence": 2
        },
        {
          "type": "enables",
          "source_node": "Power & morality representation",
          "target_node": "LoRRA power-averse control",
          "description": "Separability of these concepts makes targeted fine-tuning feasible.",
          "confidence": 3
        },
        {
          "type": "mitigates",
          "source_node": "LoRRA power-averse control",
          "target_node": "Agentic power seeking",
          "description": "Adapters reduce power-seeking scores while maintaining task performance.",
          "confidence": 3
        }
      ]
    },
    {
      "title": "Preventing memorized data leakage",
      "nodes": [
        {
          "title": "Memorization of training data",
          "aliases": [
            "verbatim recall",
            "overfitting to corpus"
          ],
          "type": "concept",
          "description": "Model tendency to reproduce exact sequences from private or copyrighted training texts.",
          "maturity": null
        },
        {
          "title": "Privacy and legal hazard",
          "aliases": [
            "data leakage risk",
            "copyright violation"
          ],
          "type": "concept",
          "description": "Potential disclosure of private or protected content leading to legal or ethical issues.",
          "maturity": null
        },
        {
          "title": "Memorization vector",
          "aliases": [
            "quote axis",
            "copying direction"
          ],
          "type": "concept",
          "description": "A direction derived from contrasting popular quotations with synthetic text whose activation predicts memorized content.",
          "maturity": null
        },
        {
          "title": "Memorization-vector subtraction",
          "aliases": [
            "anti-copy steering",
            "leakage control"
          ],
          "type": "intervention",
          "description": "Subtracting the memorization direction (scaled) from activations at decoding time to discourage verbatim recall with minimal impact on factual accuracy.",
          "maturity": 3
        }
      ],
      "edges": [
        {
          "type": "causes",
          "source_node": "Memorization of training data",
          "target_node": "Privacy and legal hazard",
          "description": "Verbatim outputs can reveal protected or sensitive text.",
          "confidence": 3
        },
        {
          "type": "enables",
          "source_node": "Memorization vector",
          "target_node": "Memorization-vector subtraction",
          "description": "Identifying the quotation direction allows active suppression.",
          "confidence": 3
        },
        {
          "type": "mitigates",
          "source_node": "Memorization-vector subtraction",
          "target_node": "Memorization of training data",
          "description": "Steering reduces exact quote reproduction rates by >40 pp.",
          "confidence": 3
        }
      ]
    },
    {
      "title": "Representation-level transparency methodology",
      "nodes": [
        {
          "title": "Neuron-level interpretability limitations",
          "aliases": [
            "scalability bottleneck",
            "fine-grain analysis gap"
          ],
          "type": "concept",
          "description": "Analysing individual neurons or circuits becomes infeasible as model complexity increases.",
          "maturity": null
        },
        {
          "title": "Need for representation-level transparency",
          "aliases": [
            "top-down interpretability",
            "high-level analysis need"
          ],
          "type": "concept",
          "description": "Desire for methods that can explain and control complex behaviours via higher-level abstractions.",
          "maturity": null
        },
        {
          "title": "Emergent semantic structure",
          "aliases": [
            "concept vectors exist",
            "linear separability of features"
          ],
          "type": "concept",
          "description": "Modern deep networks exhibit linear directions corresponding to semantic variables across many tasks.",
          "maturity": null
        },
        {
          "title": "RepE pipeline integration",
          "aliases": [
            "LAT + control workflow",
            "representation engineering toolkit"
          ],
          "type": "intervention",
          "description": "Combining unsupervised Linear Artificial Tomography for concept discovery with LoRRA or vector arithmetic control in the development cycle to monitor and steer models.",
          "maturity": 2
        }
      ],
      "edges": [
        {
          "type": "motivates",
          "source_node": "Neuron-level interpretability limitations",
          "target_node": "Need for representation-level transparency",
          "description": "Scalability issues at fine grain drive interest in higher-level analysis.",
          "confidence": 2
        },
        {
          "type": "implies",
          "source_node": "Emergent semantic structure",
          "target_node": "Need for representation-level transparency",
          "description": "Presence of concept vectors makes representation-level analysis viable.",
          "confidence": 4
        },
        {
          "type": "enables",
          "source_node": "Emergent semantic structure",
          "target_node": "RepE pipeline integration",
          "description": "Linearly separable features are prerequisite for RepE operations.",
          "confidence": 3
        }
      ]
    }
  ]
}

{"input_tokens": 2762, "input_tokens_details": {"cached_tokens": 0}, "output_tokens": 4365, "output_tokens_details": {"reasoning_tokens": 1152}, "total_tokens": 7127}