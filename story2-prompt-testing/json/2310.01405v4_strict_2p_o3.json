{
  "nodes": [
    {
      "title": "Honesty representation in LLMs",
      "aliases": [
        "honesty concept",
        "truthfulness encoding"
      ],
      "type": "concept",
      "description": "Distributed activation pattern within large language models that correlates with whether generated content matches the model’s latent beliefs.",
      "maturity": null
    },
    {
      "title": "LAT honesty direction",
      "aliases": [
        "honesty vector",
        "honesty reading direction"
      ],
      "type": "concept",
      "description": "Linear projection obtained with Linear Artificial Tomography whose sign and magnitude robustly separate truthful from deceptive activations across prompts.",
      "maturity": null
    },
    {
      "title": "Lie detector using honesty projection",
      "aliases": [
        "honesty-based lie detector",
        "real-time honesty monitor"
      ],
      "type": "intervention",
      "description": "At generation time sums the negative projection of hidden states onto the honesty vector across middle layers to flag or block potential lies.",
      "maturity": 3
    },
    {
      "title": "Misaligned (lying) outputs",
      "aliases": [
        "deceptive responses",
        "belief-discordant outputs"
      ],
      "type": "concept",
      "description": "Model-generated text that contradicts either established facts or the model’s own internal beliefs.",
      "maturity": null
    },
    {
      "title": "Linear honesty steering",
      "aliases": [
        "honesty activation edit",
        "add honesty vector"
      ],
      "type": "intervention",
      "description": "Adds a scaled honesty vector to hidden activations (layers 8–40) during decoding to bias the model toward truthful answers.",
      "maturity": 3
    },
    {
      "title": "LoRRA honesty adapters",
      "aliases": [
        "honesty LoRRA",
        "low-rank honesty control"
      ],
      "type": "intervention",
      "description": "Low-rank adapters fine-tuned with a contrastive loss that penalizes deception, thereby increasing truthful output when plugged into the model.",
      "maturity": 4
    },
    {
      "title": "Power-seeking representation",
      "aliases": [
        "power concept",
        "power-seeking encoding"
      ],
      "type": "concept",
      "description": "Latent feature whose activation predicts preference for actions that expand the agent’s power or resources.",
      "maturity": null
    },
    {
      "title": "Power contrast vector",
      "aliases": [
        "power vector",
        "power-seeking direction"
      ],
      "type": "concept",
      "description": "Linear direction derived from contrast pairs of power-seeking vs power-averse prompts that isolates the power-seeking feature.",
      "maturity": null
    },
    {
      "title": "LoRRA power-suppression adapters",
      "aliases": [
        "anti-power LoRRA",
        "power-averse adapters"
      ],
      "type": "intervention",
      "description": "Low-rank adapters trained to down-weight activations along power-seeking and immoral directions while preserving task reward.",
      "maturity": 3
    },
    {
      "title": "Power-seeking & immoral behavior",
      "aliases": [
        "immoral actions",
        "power-seeking decisions"
      ],
      "type": "concept",
      "description": "Model choices or outputs in interactive settings that prioritize self-power or violate moral norms.",
      "maturity": null
    },
    {
      "title": "Harmfulness representation",
      "aliases": [
        "harm concept",
        "harmful content encoding"
      ],
      "type": "concept",
      "description": "Activation subspace whose magnitude correlates with intent or content that can cause physical, psychological, or societal harm.",
      "maturity": null
    },
    {
      "title": "Piece-wise harmfulness amplification control",
      "aliases": [
        "conditional harm amplifier",
        "harmlessness guard"
      ],
      "type": "intervention",
      "description": "During decoding, conditionally amplifies activations aligned with the harmfulness vector to trigger refusal behaviour for malicious requests.",
      "maturity": 3
    },
    {
      "title": "Jailbreak compliance",
      "aliases": [
        "successful jailbreak",
        "harmful instruction compliance"
      ],
      "type": "concept",
      "description": "Situations where adversarial prompts override safety measures and the model provides disallowed harmful content.",
      "maturity": null
    },
    {
      "title": "Race-derived bias direction",
      "aliases": [
        "race bias vector",
        "StereoSet bias direction"
      ],
      "type": "concept",
      "description": "Linear direction extracted from race-related stereotype sentence pairs that separates biased from unbiased activations.",
      "maturity": null
    },
    {
      "title": "Unified demographic bias vector",
      "aliases": [
        "global bias direction",
        "cross-demographic bias vector"
      ],
      "type": "concept",
      "description": "Consolidated linear direction that generalizes across race, gender and profession stereotypes.",
      "maturity": null
    },
    {
      "title": "Fairness control via bias subtraction",
      "aliases": [
        "bias projection removal",
        "debiasing activation edit"
      ],
      "type": "intervention",
      "description": "Subtracts a scaled bias vector from hidden activations to reduce demographic stereotyping without harming task performance.",
      "maturity": 3
    },
    {
      "title": "Biased outputs",
      "aliases": [
        "stereotyped responses",
        "demographically biased text"
      ],
      "type": "concept",
      "description": "Generated content that reflects unfair or stereotypical associations toward protected demographic groups.",
      "maturity": null
    },
    {
      "title": "Synthetic vs popular text contrast",
      "aliases": [
        "memorization stimulus contrast",
        "real vs synthetic text pairs"
      ],
      "type": "concept",
      "description": "Dataset of well-known passages contrasted with artificial variants used to elicit memorization signals.",
      "maturity": null
    },
    {
      "title": "Memorization vector",
      "aliases": [
        "memorization direction",
        "verbatim memory vector"
      ],
      "type": "concept",
      "description": "Linear direction whose activation correlates with presence of text memorized verbatim from the training data.",
      "maturity": null
    },
    {
      "title": "Memorization suppression control",
      "aliases": [
        "anti-memorization edit",
        "memory leakage reducer"
      ],
      "type": "intervention",
      "description": "Subtracts a scaled memorization vector during generation—especially on final tokens—to reduce verbatim reproduction of training text.",
      "maturity": 2
    },
    {
      "title": "Text leakage of memorized content",
      "aliases": [
        "training data leakage",
        "copyrighted text spillover"
      ],
      "type": "concept",
      "description": "Unauthorized or verbatim reproduction of large spans from the model’s proprietary or copyrighted training data.",
      "maturity": null
    }
  ],
  "logical_chains": [
    {
      "title": "Honesty extraction and steering",
      "edges": [
        {
          "type": "causes",
          "source_node": "Honesty representation in LLMs",
          "target_node": "LAT honesty direction",
          "description": "Honesty concept in activations makes it possible to derive a separating linear direction.",
          "confidence": 4
        },
        {
          "type": "enables",
          "source_node": "LAT honesty direction",
          "target_node": "Lie detector using honesty projection",
          "description": "Honesty vector serves as the signal used to implement the lie detector.",
          "confidence": 4
        },
        {
          "type": "mitigates",
          "source_node": "Lie detector using honesty projection",
          "target_node": "Misaligned (lying) outputs",
          "description": "Detector identifies and can block deceptive responses.",
          "confidence": 4
        },
        {
          "type": "addressed_by",
          "source_node": "LAT honesty direction",
          "target_node": "Linear honesty steering",
          "description": "The same honesty vector is directly added to activations for steering.",
          "confidence": 4
        },
        {
          "type": "mitigates",
          "source_node": "Linear honesty steering",
          "target_node": "Misaligned (lying) outputs",
          "description": "Activation edit increases truthfulness, reducing deceptive answers.",
          "confidence": 4
        },
        {
          "type": "outperforms",
          "source_node": "LoRRA honesty adapters",
          "target_node": "Linear honesty steering",
          "description": "Low-rank adapters achieve larger honesty gains than simple linear edits.",
          "confidence": 4
        },
        {
          "type": "mitigates",
          "source_node": "LoRRA honesty adapters",
          "target_node": "Misaligned (lying) outputs",
          "description": "Adapters decrease the rate of deceptive or hallucinated responses.",
          "confidence": 4
        }
      ]
    },
    {
      "title": "Suppressing power-seeking and immorality",
      "edges": [
        {
          "type": "contributes_to",
          "source_node": "Power-seeking representation",
          "target_node": "Power contrast vector",
          "description": "Latent power feature gives rise to a measurable linear direction.",
          "confidence": 4
        },
        {
          "type": "implemented_by",
          "source_node": "Power contrast vector",
          "target_node": "LoRRA power-suppression adapters",
          "description": "Contrast vector is used as training signal for the low-rank adapters.",
          "confidence": 4
        },
        {
          "type": "mitigates",
          "source_node": "LoRRA power-suppression adapters",
          "target_node": "Power-seeking & immoral behavior",
          "description": "Adapters reduce power-seeking actions while preserving task reward.",
          "confidence": 4
        }
      ]
    },
    {
      "title": "Harmlessness under jailbreaks",
      "edges": [
        {
          "type": "enables",
          "source_node": "Harmfulness representation",
          "target_node": "Piece-wise harmfulness amplification control",
          "description": "Stable harmfulness feature allows conditional amplification strategy.",
          "confidence": 4
        },
        {
          "type": "mitigates",
          "source_node": "Piece-wise harmfulness amplification control",
          "target_node": "Jailbreak compliance",
          "description": "Control greatly increases refusal rate on adversarial harmful requests.",
          "confidence": 4
        }
      ]
    },
    {
      "title": "Bias reduction via projection",
      "edges": [
        {
          "type": "refined_by",
          "source_node": "Race-derived bias direction",
          "target_node": "Unified demographic bias vector",
          "description": "Race-based bias vector is extended to capture broader demographic bias.",
          "confidence": 4
        },
        {
          "type": "addressed_by",
          "source_node": "Unified demographic bias vector",
          "target_node": "Fairness control via bias subtraction",
          "description": "Linear subtraction operator uses the unified vector.",
          "confidence": 4
        },
        {
          "type": "mitigates",
          "source_node": "Fairness control via bias subtraction",
          "target_node": "Biased outputs",
          "description": "Control lowers probability of stereotyped or biased generations.",
          "confidence": 4
        }
      ]
    },
    {
      "title": "Reducing verbatim memorization",
      "edges": [
        {
          "type": "causes",
          "source_node": "Synthetic vs popular text contrast",
          "target_node": "Memorization vector",
          "description": "Contrast dataset elicits linear direction that detects memorization.",
          "confidence": 4
        },
        {
          "type": "addressed_by",
          "source_node": "Memorization vector",
          "target_node": "Memorization suppression control",
          "description": "The same vector is subtracted to build the suppression operator.",
          "confidence": 3
        },
        {
          "type": "mitigates",
          "source_node": "Memorization suppression control",
          "target_node": "Text leakage of memorized content",
          "description": "Control reduces verbatim reproduction of copyrighted passages.",
          "confidence": 2
        }
      ]
    }
  ]
}

{"input_tokens": 2219, "input_tokens_details": {"cached_tokens": 0}, "output_tokens": 6436, "output_tokens_details": {"reasoning_tokens": 3712}, "total_tokens": 8655}