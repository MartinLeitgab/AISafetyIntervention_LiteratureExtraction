{
  "logical_chains": [
    {
      "title": "Text-only deception detector for content moderation",
      "nodes": [
        {
          "title": "Online textual deception undermines information reliability",
          "aliases": [
            "text deception harms trust",
            "misleading online text",
            "fake textual content"
          ],
          "type": "concept",
          "description": "Widespread deceptive or misleading user-generated text reduces the reliability of online information ecosystems.",
          "maturity": null
        },
        {
          "title": "Need for accurate automated deception detection",
          "aliases": [
            "demand for deception filters",
            "requirement for lie detection systems",
            "need for robust detection"
          ],
          "type": "concept",
          "description": "Platforms require scalable automatic methods to detect deceptive statements in text to supplement limited human moderation accuracy.",
          "maturity": null
        },
        {
          "title": "Humans achieve 41 % accuracy in T4TEXT",
          "aliases": [
            "human baseline 41 %",
            "judge accuracy 41 %",
            "human performance limit"
          ],
          "type": "concept",
          "description": "Crowd and expert judges correctly identify the real contestant only 41 % of the time in the T4TEXT dataset.",
          "maturity": null
        },
        {
          "title": "Bottleneck LLM with linguistic cues attains 39 % accuracy (77 % top-2)",
          "aliases": [
            "cue-based GPT-4 model",
            "bottleneck detector performance",
            "LLM deception score"
          ],
          "type": "concept",
          "description": "A GPT-4 system that first extracts four linguistic cues and then classifies the speaker matches or exceeds average human accuracy and reaches 77 % top-2 recall.",
          "maturity": null
        },
        {
          "title": "Sequential cue extraction enables performance",
          "aliases": [
            "linguistic cue pipeline",
            "entailment/ambiguity/overconfidence/half-truth cues",
            "cue bottleneck method"
          ],
          "type": "concept",
          "description": "The model’s sequential extraction of entailment, ambiguity, overconfidence, and half-truth cues over dialogue turns is critical for its accuracy.",
          "maturity": null
        },
        {
          "title": "Deploy cue-based LLMs in moderation pipelines",
          "aliases": [
            "production deception filter",
            "LLM moderation deployment",
            "operational cue detector"
          ],
          "type": "intervention",
          "description": "Integrate the sequential linguistic-cue bottleneck LLM into content-moderation workflows to automatically flag text that is likely deceptive for human review.",
          "maturity": 1
        }
      ],
      "edges": [
        {
          "type": "causes",
          "source_node": "Online textual deception undermines information reliability",
          "target_node": "Need for accurate automated deception detection",
          "description": "Prevalent textual deception motivates the requirement for automated detection tools.",
          "confidence": 4
        },
        {
          "type": "contributes_to",
          "source_node": "Humans achieve 41 % accuracy in T4TEXT",
          "target_node": "Need for accurate automated deception detection",
          "description": "Limited human accuracy increases the urgency for automated support.",
          "confidence": 4
        },
        {
          "type": "mitigates",
          "source_node": "Bottleneck LLM with linguistic cues attains 39 % accuracy (77 % top-2)",
          "target_node": "Need for accurate automated deception detection",
          "description": "The cue-based LLM reduces the unmet need by providing competitive automated detection.",
          "confidence": 4
        },
        {
          "type": "enables",
          "source_node": "Sequential cue extraction enables performance",
          "target_node": "Bottleneck LLM with linguistic cues attains 39 % accuracy (77 % top-2)",
          "description": "The sequential cue mechanism is a key technical contributor to the model’s performance.",
          "confidence": 4
        },
        {
          "type": "mitigates",
          "source_node": "Deploy cue-based LLMs in moderation pipelines",
          "target_node": "Need for accurate automated deception detection",
          "description": "Operational deployment of the model would directly address the need for scalable deception detection.",
          "confidence": 2
        }
      ]
    },
    {
      "title": "Ground-truth referencing & half-truth detection as alignment signal",
      "nodes": [
        {
          "title": "Objective truth reference enables entailment classification",
          "aliases": [
            "affidavit ground truth",
            "reference facts",
            "truth baseline"
          ],
          "type": "concept",
          "description": "Having a structured affidavit or knowledge base allows the system to compute entailment and contradiction scores for each statement.",
          "maturity": null
        },
        {
          "title": "Accurate entailment verdict improves deception detection",
          "aliases": [
            "entailment cue utility",
            "truth consistency benefit",
            "fact-checking cue"
          ],
          "type": "concept",
          "description": "High-quality entailment assessment between utterances and ground truth raises overall deception-detection accuracy.",
          "maturity": null
        },
        {
          "title": "Half-truth cue removal drops accuracy by 7 %",
          "aliases": [
            "half-truth ablation effect",
            "partial truth importance",
            "critical half-truth cue"
          ],
          "type": "concept",
          "description": "Ablation experiments show the half-truth cue contributes the largest performance margin, with a 7 % drop when omitted.",
          "maturity": null
        },
        {
          "title": "Half-truth detection is critical predictor of deception",
          "aliases": [
            "partial truth indicator",
            "half-truth significance",
            "key deception signal"
          ],
          "type": "concept",
          "description": "Detecting statements that mix true and false content is especially informative for identifying deception.",
          "maturity": null
        },
        {
          "title": "AI models may output partially true statements that mislead users",
          "aliases": [
            "model half-truth risk",
            "misleading partial truths",
            "model deception hazard"
          ],
          "type": "concept",
          "description": "Generative models can produce statements that are factually blended, creating a risk of user misinformation.",
          "maturity": null
        },
        {
          "title": "Integrate entailment & half-truth penalties in RLHF",
          "aliases": [
            "alignment penalty for half-truths",
            "truth-consistency RLHF",
            "misleading output penalty"
          ],
          "type": "intervention",
          "description": "During reinforcement-learning-from-human-feedback or similar alignment fine-tuning, apply loss terms that penalise contradiction and half-truth patterns against a reference knowledge base.",
          "maturity": 1
        }
      ],
      "edges": [
        {
          "type": "enables",
          "source_node": "Objective truth reference enables entailment classification",
          "target_node": "Accurate entailment verdict improves deception detection",
          "description": "Access to ground-truth facts makes precise entailment scoring possible, boosting detection accuracy.",
          "confidence": 4
        },
        {
          "type": "reveals",
          "source_node": "Half-truth cue removal drops accuracy by 7 %",
          "target_node": "Half-truth detection is critical predictor of deception",
          "description": "Ablation evidence highlights the importance of half-truth detection.",
          "confidence": 4
        },
        {
          "type": "mitigates",
          "source_node": "Integrate entailment & half-truth penalties in RLHF",
          "target_node": "AI models may output partially true statements that mislead users",
          "description": "Applying penalties during alignment aims to reduce the model’s tendency to produce misleading half-truths.",
          "confidence": 2
        }
      ]
    },
    {
      "title": "Human-AI collaboration through explanatory cues",
      "nodes": [
        {
          "title": "Model explanations rated more satisfactory by humans",
          "aliases": [
            "high e-ViL score",
            "preferred model explanations",
            "explanation quality"
          ],
          "type": "concept",
          "description": "Human evaluators judge the model-generated natural-language explanations of its predictions as more helpful than baselines.",
          "maturity": null
        },
        {
          "title": "Explanatory cues aid human reasoning about deception",
          "aliases": [
            "explanations improve judgment",
            "user reasoning support",
            "explanation benefit"
          ],
          "type": "concept",
          "description": "High-quality explanations can enhance human ability to understand and detect deceptive statements.",
          "maturity": null
        },
        {
          "title": "Model succeeds when all judges are fooled",
          "aliases": [
            "model beats humans hardest cases",
            "success on difficult games",
            "model advantage on fooling cases"
          ],
          "type": "concept",
          "description": "The model often identifies the liar in scenarios where every human judge fails, indicating access to non-obvious cues.",
          "maturity": null
        },
        {
          "title": "Model uncovers non-obvious linguistic patterns missed by humans",
          "aliases": [
            "hidden cue discovery",
            "non-obvious pattern detection",
            "LLM unique cues"
          ],
          "type": "concept",
          "description": "Performance analysis suggests the model leverages linguistic patterns that are typically overlooked by human judges.",
          "maturity": null
        },
        {
          "title": "Over-reliance on unexplained AI judgments in safety-critical settings",
          "aliases": [
            "opaque AI risk",
            "black-box trust problem",
            "unexplained decision danger"
          ],
          "type": "concept",
          "description": "Blind trust in AI outputs without rationale can cause errors or misuse, especially in high-stakes contexts.",
          "maturity": null
        },
        {
          "title": "Provide cue-based explanations to human moderators",
          "aliases": [
            "explanatory interface",
            "human-AI teamwork UI",
            "explanation delivery"
          ],
          "type": "intervention",
          "description": "Display the model’s entailment, ambiguity, overconfidence, and half-truth cues together with deception scores to support human decision-making.",
          "maturity": 1
        }
      ],
      "edges": [
        {
          "type": "implies",
          "source_node": "Model explanations rated more satisfactory by humans",
          "target_node": "Explanatory cues aid human reasoning about deception",
          "description": "Positive human ratings suggest that explanations help reasoning.",
          "confidence": 4
        },
        {
          "type": "reveals",
          "source_node": "Model succeeds when all judges are fooled",
          "target_node": "Model uncovers non-obvious linguistic patterns missed by humans",
          "description": "Model success on hardest cases indicates discovery of hidden cues.",
          "confidence": 4
        },
        {
          "type": "mitigates",
          "source_node": "Provide cue-based explanations to human moderators",
          "target_node": "Over-reliance on unexplained AI judgments in safety-critical settings",
          "description": "Supplying detailed explanations reduces blind trust and calibrates human reliance.",
          "confidence": 2
        }
      ]
    },
    {
      "title": "Sequential cue accumulation improves robustness",
      "nodes": [
        {
          "title": "Sequential bottleneck derivation outperforms independent cues",
          "aliases": [
            "sequential vs independent cues",
            "temporal cue advantage",
            "bottleneck sequence win"
          ],
          "type": "concept",
          "description": "Using a running sequence of cue verdicts across turns yields higher detection accuracy than analysing each utterance in isolation.",
          "maturity": null
        },
        {
          "title": "Temporal accumulation of cues enhances detection robustness",
          "aliases": [
            "time-aware cue benefit",
            "sequential robustness",
            "temporal cue strength"
          ],
          "type": "concept",
          "description": "Aggregating cues over time makes the system more resilient and accurate in dialogue settings.",
          "maturity": null
        },
        {
          "title": "Need robust real-time detection of deceptive dialogue",
          "aliases": [
            "real-time deception monitoring",
            "streaming detection need",
            "live chat risk"
          ],
          "type": "concept",
          "description": "Live conversational platforms require detection methods that operate reliably on streaming data.",
          "maturity": null
        },
        {
          "title": "Implement sliding-window sequential cue extraction in streaming systems",
          "aliases": [
            "real-time cue extractor",
            "sliding window implementation",
            "streaming cue pipeline"
          ],
          "type": "intervention",
          "description": "Apply a sliding-window variant of the sequential cue-extraction pipeline to monitor chat streams for deceptive content in real time.",
          "maturity": 1
        }
      ],
      "edges": [
        {
          "type": "demonstrates",
          "source_node": "Sequential bottleneck derivation outperforms independent cues",
          "target_node": "Temporal accumulation of cues enhances detection robustness",
          "description": "Empirical improvement evidences the benefit of temporal cue accumulation.",
          "confidence": 4
        },
        {
          "type": "implies",
          "source_node": "Temporal accumulation of cues enhances detection robustness",
          "target_node": "Need robust real-time detection of deceptive dialogue",
          "description": "Robustness gains point toward usefulness in real-time settings.",
          "confidence": 4
        },
        {
          "type": "addresses",
          "source_node": "Implement sliding-window sequential cue extraction in streaming systems",
          "target_node": "Need robust real-time detection of deceptive dialogue",
          "description": "Deploying the sliding-window method would satisfy the requirement for real-time robustness.",
          "confidence": 2
        }
      ]
    }
  ]
}

{"input_tokens": 2152, "input_tokens_details": {"cached_tokens": 0}, "output_tokens": 3977, "output_tokens_details": {"reasoning_tokens": 960}, "total_tokens": 6129}