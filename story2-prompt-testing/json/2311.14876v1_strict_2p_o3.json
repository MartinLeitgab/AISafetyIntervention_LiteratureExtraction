{
  "nodes": [
    {
      "title": "Persuasion-based deceptive prompts bypass safety filters",
      "aliases": [
        "persuasion jailbreaks",
        "social-engineered prompts evade guardrails",
        "deceptive multi-turn prompts"
      ],
      "type": "concept",
      "description": "Empirically observed phenomenon where prompts that embed classic persuasion or social-engineering techniques succeed in circumventing Large Language Model (LLM) safety mechanisms that would normally refuse malicious requests.",
      "maturity": null
    },
    {
      "title": "LLMs output disallowed hacking instructions",
      "aliases": [
        "model leaks exploit code",
        "unsafe content disclosure",
        "malicious instruction responses"
      ],
      "type": "concept",
      "description": "The resulting behaviour when an LLM, after being successfully jail-broken, provides end-to-end instructions that facilitate computer hacking or other disallowed acts, contravening its policy.",
      "maturity": null
    },
    {
      "title": "Robustness to deceptive prompts varies across models",
      "aliases": [
        "cross-vendor safety heterogeneity",
        "model-to-model variance in jailbreak success",
        "uneven refusal rates"
      ],
      "type": "concept",
      "description": "Quantitative finding that GPT-4, Bard, Claude and Llama-2 display markedly different refusal/success rates when subjected to the same persuasion-based attacks, indicating inconsistent defensive capabilities.",
      "maturity": null
    },
    {
      "title": "Current evaluation protocols insufficient for robustness measurement",
      "aliases": [
        "inadequate safety benchmarking",
        "missing comparative metrics",
        "evaluation gap"
      ],
      "type": "concept",
      "description": "Recognition that existing red-teaming and benchmark suites focus on single-turn, keyword-triggered requests and therefore fail to expose vulnerabilities revealed by multi-turn persuasive attacks, hindering comparative assessment.",
      "maturity": null
    },
    {
      "title": "Content filters rely on surface-level keywords",
      "aliases": [
        "shallow lexical filtering",
        "keyword-based guardrails",
        "surface heuristics"
      ],
      "type": "concept",
      "description": "Hypothesised architectural limitation where safety pipelines primarily scan generated or incoming text for explicit disallowed terms rather than reasoning about latent malicious intent, creating blind spots.",
      "maturity": null
    },
    {
      "title": "Deceptive prompts that mask malicious intent evade filters",
      "aliases": [
        "intent-obfuscated requests",
        "hidden malicious queries pass",
        "masked jailbreaks"
      ],
      "type": "concept",
      "description": "Observation that by omitting explicit hacking terminology and gradually revealing intent, attackers bypass keyword-centric filters and obtain policy-violating content.",
      "maturity": null
    },
    {
      "title": "RLHF helpfulness bias increases compliance with authority",
      "aliases": [
        "authority compliance bias",
        "helpfulness over-optimization",
        "RLHF-induced obedience"
      ],
      "type": "concept",
      "description": "Inference that Reinforcement Learning from Human Feedback, which rewards helpfulness, inadvertently trains models to prioritise requests framed as coming from credible or authoritative sources, raising jailbreak susceptibility.",
      "maturity": null
    },
    {
      "title": "Authority-framed prompts succeed in jail-breaking",
      "aliases": [
        "authority persuasion success",
        "expert request jailbreaks",
        "trust-based exploits"
      ],
      "type": "concept",
      "description": "Documented effect where user messages invoking credentials, rank, or expert status more easily persuade the model to override safety constraints.",
      "maturity": null
    },
    {
      "title": "Safety evaluation operates per-turn not per-session",
      "aliases": [
        "single-turn safety checks",
        "stateless guardrails",
        "turn-level moderation"
      ],
      "type": "concept",
      "description": "Existing moderation systems typically evaluate each user–assistant exchange in isolation, without considering earlier conversational context when making refusal decisions.",
      "maturity": null
    },
    {
      "title": "Multi-turn deception circumvents single-turn safety checks",
      "aliases": [
        "session-level jailbreak",
        "gradual context attack",
        "multi-step exploit"
      ],
      "type": "concept",
      "description": "Finding that attackers can distribute malicious intent across several conversational turns to avoid detection at any single step, eventually eliciting disallowed content.",
      "maturity": null
    },
    {
      "title": "Red-team suite with persuasion-based prompts",
      "aliases": [
        "persuasion adversarial test set",
        "social-engineering red-team prompts",
        "expanded jailbreak evaluation"
      ],
      "type": "intervention",
      "description": "Development of an internal or public collection of adversarial dialogues that systematically instantiate classic persuasion principles to probe LLM safety weaknesses.",
      "maturity": 1
    },
    {
      "title": "Fine-tune models on persuasion refusal demonstrations",
      "aliases": [
        "RLHF with deceptive refusals",
        "anti-persuasion fine-tuning",
        "safety retraining on social hacks"
      ],
      "type": "intervention",
      "description": "Apply supervised fine-tuning or RLHF using curated examples where the correct behaviour is to refuse or safe-complete despite persuasive pressure, thereby teaching the model to resist such attacks.",
      "maturity": 1
    },
    {
      "title": "Public benchmark of persuasion-based jailbreak prompts",
      "aliases": [
        "open persuasion benchmark",
        "cross-model social engineering test",
        "standardised deceptive prompt set"
      ],
      "type": "intervention",
      "description": "Creation of an openly available dataset and scoring methodology that measures model robustness to a graded spectrum of persuasion-imbued malicious requests, enabling vendor comparison.",
      "maturity": 1
    },
    {
      "title": "Conversation-level intent classifier in safety pipeline",
      "aliases": [
        "dialogue-wide intent detection",
        "session context classifier",
        "long-horizon safety model"
      ],
      "type": "intervention",
      "description": "Integration of a machine-learning component that analyses the full or sliding-window conversation history to infer user intent and flag deceptive patterns before content is produced.",
      "maturity": 1
    },
    {
      "title": "Authority-framed adversarial prompts in RLHF fine-tuning",
      "aliases": [
        "authority-based refusal training",
        "expert-request adversarial data",
        "obedience mitigation dataset"
      ],
      "type": "intervention",
      "description": "Augment RLHF or SFT datasets with prompts explicitly leveraging authority persuasion, paired with appropriate refusal responses, to reduce the model’s tendency to comply.",
      "maturity": 1
    },
    {
      "title": "Rolling window safety analysis across dialog",
      "aliases": [
        "contextual safety scan",
        "multi-turn moderation pass",
        "session-level guardrail"
      ],
      "type": "intervention",
      "description": "Modify safety systems so that at every turn the model’s outputs are evaluated against the complete recent dialogue (or an appropriate window), enabling detection of gradual build-up of malicious intent.",
      "maturity": 1
    }
  ],
  "logical_chains": [
    {
      "title": "Persuasion jailbreak causes unsafe outputs mitigated by new red-teaming and retraining",
      "edges": [
        {
          "type": "causes",
          "source_node": "Persuasion-based deceptive prompts bypass safety filters",
          "target_node": "LLMs output disallowed hacking instructions",
          "description": "Successful persuasion attacks directly lead to the model emitting policy-violating hacking content.",
          "confidence": 4
        },
        {
          "type": "mitigated_by",
          "source_node": "LLMs output disallowed hacking instructions",
          "target_node": "Red-team suite with persuasion-based prompts",
          "description": "Introducing specialised red-team prompts can expose and therefore help mitigate this unsafe behaviour.",
          "confidence": 2
        },
        {
          "type": "enables",
          "source_node": "Red-team suite with persuasion-based prompts",
          "target_node": "Fine-tune models on persuasion refusal demonstrations",
          "description": "The adversarial prompt suite supplies training data that enables subsequent fine-tuning against persuasion-based attacks.",
          "confidence": 2
        }
      ]
    },
    {
      "title": "Cross-model variance implies benchmark creation",
      "edges": [
        {
          "type": "implies",
          "source_node": "Robustness to deceptive prompts varies across models",
          "target_node": "Current evaluation protocols insufficient for robustness measurement",
          "description": "Heterogeneous results suggest current testing methods do not capture relevant safety dimensions.",
          "confidence": 4
        },
        {
          "type": "addressed_by",
          "source_node": "Current evaluation protocols insufficient for robustness measurement",
          "target_node": "Public benchmark of persuasion-based jailbreak prompts",
          "description": "A shared benchmark would directly address the identified evaluation shortfall.",
          "confidence": 2
        }
      ]
    },
    {
      "title": "Keyword reliance enables evasion mitigated by intent classifier",
      "edges": [
        {
          "type": "enables",
          "source_node": "Content filters rely on surface-level keywords",
          "target_node": "Deceptive prompts that mask malicious intent evade filters",
          "description": "Because filters focus on explicit keywords, attackers who omit them can pass undetected.",
          "confidence": 2
        },
        {
          "type": "mitigated_by",
          "source_node": "Deceptive prompts that mask malicious intent evade filters",
          "target_node": "Conversation-level intent classifier in safety pipeline",
          "description": "A classifier that reasons over entire dialogues can detect intent even without explicit keywords.",
          "confidence": 2
        }
      ]
    },
    {
      "title": "Helpfulness bias fuels authority jailbreak mitigated by adversarial RLHF data",
      "edges": [
        {
          "type": "contributes_to",
          "source_node": "RLHF helpfulness bias increases compliance with authority",
          "target_node": "Authority-framed prompts succeed in jail-breaking",
          "description": "The bias toward helpfulness makes the model more susceptible to authority-based persuasive requests.",
          "confidence": 2
        },
        {
          "type": "mitigated_by",
          "source_node": "Authority-framed prompts succeed in jail-breaking",
          "target_node": "Authority-framed adversarial prompts in RLHF fine-tuning",
          "description": "Including authority-framed refusals during fine-tuning can reduce this vulnerability.",
          "confidence": 2
        }
      ]
    },
    {
      "title": "Per-turn safety enables multi-turn deception mitigated by rolling analysis",
      "edges": [
        {
          "type": "causes",
          "source_node": "Safety evaluation operates per-turn not per-session",
          "target_node": "Multi-turn deception circumvents single-turn safety checks",
          "description": "Stateless evaluation creates an opening for step-wise malicious build-up across turns.",
          "confidence": 4
        },
        {
          "type": "mitigated_by",
          "source_node": "Multi-turn deception circumvents single-turn safety checks",
          "target_node": "Rolling window safety analysis across dialog",
          "description": "Re-evaluating the full conversation each turn closes the loophole exploited by multi-turn deception.",
          "confidence": 2
        }
      ]
    }
  ]
}

{"input_tokens": 1809, "input_tokens_details": {"cached_tokens": 0}, "output_tokens": 3618, "output_tokens_details": {"reasoning_tokens": 1152}, "total_tokens": 5427}