Summary
Data source overview: The paper “Should Robots be Obedient?” analyses when an autonomous robot should follow or ignore human orders. It formalises the human-robot interaction as a supervision POMDP, proves an autonomy-obedience trade-off, studies several inverse-reinforcement-learning (IRL) estimators, and shows that a maximum-likelihood (MLE) estimator is both more obedient initially and more robust to model misspecification. It proposes practical mechanisms—adding redundant reward features, an initial burn-in obedience period with policy mixing, and a detection rule based on first-order obedience—to keep systems on the safe side.

EXTRACTION CONFIDENCE[87]
Inference strategy justification: Where explicit intervention descriptions were missing, I inferred reasonable AI-safety-relevant actions directly implied by design rationales and validated by theorems/experiments (confidence 2 edges). All assumptions/inferences are marked with lower edge-confidence values.
Extraction completeness explanation: Every theorem, lemma or simulation that forms part of a causal chain from risk to intervention is represented as a node; all reasoning paths inter-connect into a single fabric; no isolated nodes remain.
Key limitations: • The source is theoretical; validation evidence is limited to proofs and small-scale simulations. • Some intervention details (e.g., exact hyper-parameters for IRL) are not provided and had to be abstracted. • The fabric assumes IRL-based systems; applicability to very different architectures may require extension.

JSON knowledge fabric (nodes and edges)


Improvements to instruction set: Clarify whether multiple validation–>intervention edges are required or if a single strongest is preferred; provide guidance on handling overlapping interventions (e.g., “prefer MLE” vs “fine-tune with constrained MLE”) to reduce potential node duplication; add explicit examples for combining theoretical and simulation evidence into a single validation node when they support the same claim.