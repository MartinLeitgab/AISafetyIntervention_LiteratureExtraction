{
  "nodes": [
    {
      "name": "Catastrophic reward misalignment from preference model misspecification in human-robot interaction",
      "aliases": [
        "reward misalignment due to misspecified preferences",
        "catastrophic outcomes from model error in robots"
      ],
      "type": "concept",
      "description": "Autonomous robots that infer human preferences with an incorrect model (wrong rationality parameter β or missing reward features Θ) can choose disobedient actions causing severe, potentially catastrophic harm (Section 5 Model Misspecification).",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 5 emphasises that misspecification can turn autonomy advantage into ‘rebellion regret’, posing existential safety risks."
    },
    {
      "name": "Reduced human value attainment from blind robot obedience in human-robot interaction",
      "aliases": [
        "performance loss from obedient robots",
        "low reward from strictly obedient robots"
      ],
      "type": "concept",
      "description": "Forcing robots to obey all human orders limits achievable reward to the human’s (possibly sub-optimal) performance level, lowering overall value (Section 3 Justifying Autonomy).",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Theorem 1 shows blindly obedient robots forego positive autonomy advantage, creating a risk of under-performance."
    },
    {
      "name": "Human decision suboptimality in supervisory control tasks",
      "aliases": [
        "human irrationality in robot supervision",
        "suboptimal human orders"
      ],
      "type": "concept",
      "description": "Humans act noisily rational (β>0) and often issue non-optimal commands, creating a gap between literal orders and true preferences (Sections 2–3).",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explains why blind obedience yields low reward (Δ=0)."
    },
    {
      "name": "Robot preference model misspecification (behavioral rationality or feature space) in human-robot interaction",
      "aliases": [
        "misspecified β or Θ in IRL",
        "incorrect human model in robot"
      ],
      "type": "concept",
      "description": "The robot may assume an incorrect human rationality level or omit reward-relevant features, leading to wrong θ estimates and unsafe disobedience (Section 5).",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Direct causal mechanism for catastrophic misalignment risk."
    },
    {
      "name": "Autonomy advantage from IRL-based preference inference in presence of human suboptimality",
      "aliases": [
        "positive Δ from inferred preferences",
        "performance gain via IRL autonomy"
      ],
      "type": "concept",
      "description": "If the robot infers θ via IRL and optimises expected reward, it attains non-negative and often positive extra reward Δ over blind obedience when the human is not fully rational (Theorem 1, Section 3).",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Establishes benefit motivating autonomy."
    },
    {
      "name": "Obedience–performance tradeoff in robot autonomy decisions",
      "aliases": [
        "Δ vs O tradeoff",
        "autonomy-obedience balance"
      ],
      "type": "concept",
      "description": "Robots must sacrifice some obedience probability O to gain autonomy advantage Δ; maximum Δ is reached when obeying only optimal orders (Section 3, Figure 2).",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Core theoretical constraint guiding safe design."
    },
    {
      "name": "MLE IRL robustness to human rationality misspecification",
      "aliases": [
        "MLE robustness to wrong β",
        "maximum likelihood IRL insensitivity to β error"
      ],
      "type": "concept",
      "description": "Theorem 5 shows that a robot using maximum-likelihood estimation for θ selects identical actions even if its assumed β′ differs from true β₀, unlike Bayesian-optimal policies (Section 5).",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Provides robustness property critical for safety under uncertain human models."
    },
    {
      "name": "Providing extra reward features increases robot obedience bias",
      "aliases": [
        "feature space expansion increases obedience",
        "extra Θ dimensions raise obedience"
      ],
      "type": "concept",
      "description": "Simulation results (Figure 4, Section 5) indicate that augmenting the reward feature space with irrelevant dimensions makes the robot more likely to obey, while missing features has the opposite effect.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Guides safer model-design choices."
    },
    {
      "name": "First-order obedience property of MLE IRL enables model misspecification detection",
      "aliases": [
        "MLE obeys undominated first order",
        "obedience as anomaly detector"
      ],
      "type": "concept",
      "description": "Theorem 4 proves that an MLE-based robot with a bounded θ-norm always executes the first undominated human order; deviation from this expectation can signal a missing-feature model (Section 4).",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Underpins proposed misspecification-detection strategy."
    },
    {
      "name": "Adopt IRL policies that prioritize early obedience and robustness to misspecification",
      "aliases": [
        "robust IRL policy design",
        "safety-prioritized IRL"
      ],
      "type": "concept",
      "description": "Design principle to choose IRL algorithms (e.g., MLE) and auxiliary mechanisms that err on the side of obedience during early interaction while still converging to high reward (Sections 4–5).",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Translates theoretical insights into overall solution approach."
    },
    {
      "name": "Use MLE estimator with norm constraint to guarantee obedience to initial orders",
      "aliases": [
        "bounded-norm MLE for first-order obedience",
        "constrained MLE θ"
      ],
      "type": "concept",
      "description": "Limit the magnitude of θ during maximum-likelihood estimation so that a solution exists and the robot will execute the first human order when it is undominated (Section 4, Theorem 4).",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Specific design tactic ensuring early obedience."
    },
    {
      "name": "Incorporate initial burn-in obedience period for anomaly detection",
      "aliases": [
        "early blind obedience phase",
        "burn-in obedience strategy"
      ],
      "type": "concept",
      "description": "Run the robot under forced obedience for B steps, then assess sample obedience to detect feature misspecification before switching to autonomous IRL (Section 5, Remark 4).",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Low-risk way to gather evidence for model correctness."
    },
    {
      "name": "Add redundant reward features to err on side of obedience",
      "aliases": [
        "model complexity for safety",
        "extra Θ dimensions design choice"
      ],
      "type": "concept",
      "description": "Include additional, possibly irrelevant, features in the reward model so that errors make the robot more obedient rather than dangerously disobedient (Section 5).",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Design choice derived from simulation evidence."
    },
    {
      "name": "Constrained MLE inverse reinforcement learning during fine-tuning",
      "aliases": [
        "fine-tune with bounded MLE IRL",
        "implementation of constrained MLE"
      ],
      "type": "concept",
      "description": "Technical implementation of θ estimation via maximum likelihood with an L2-norm cap during policy fine-tuning, ensuring existence of a solution and first-order obedience (Section 4).",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Realises design rationale on bounded MLE."
    },
    {
      "name": "Policy mixing with declining obedience probability after burn-in",
      "aliases": [
        "burn-in policy mixing",
        "obedience-to-autonomy schedule"
      ],
      "type": "concept",
      "description": "Execute πR(h)=πOR during burn-in and gradually decrease obedience probability cn→0, enabling safe transition from obedience to autonomy (Section 5, Remark 4).",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implements burn-in design strategy."
    },
    {
      "name": "Reward feature space expansion with irrelevant dimensions",
      "aliases": [
        "feature augmentation for obedience",
        "expanded Θ implementation"
      ],
      "type": "concept",
      "description": "Augment the reward feature vector ϕ with extra, task-irrelevant dimensions to bias the robot toward obedience without harming eventual convergence (Section 5, Figure 4).",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Technical realisation of redundant-feature design."
    },
    {
      "name": "Sample first-order obedience rate to trigger obedience fallback",
      "aliases": [
        "first-order obedience monitor",
        "misspecification detection algorithm"
      ],
      "type": "concept",
      "description": "Compute ~O1 over burn-in and, if below 1−ε, switch to always-obedient policy (Equation 3, Section 5); serves as runtime misspecification detector.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implements detection strategy based on theoretical guarantee."
    },
    {
      "name": "Theorem 4 proof of MLE first-order obedience",
      "aliases": [
        "formal proof of first-order obedience",
        "Theorem 4 evidence"
      ],
      "type": "concept",
      "description": "Mathematical proof that bounded-norm MLE IRL robots always execute an undominated initial order (Section 4).",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Validates safety of constrained MLE implementation."
    },
    {
      "name": "Theorem 5 proof of MLE robustness to rationality misspecification",
      "aliases": [
        "proof of β robustness",
        "Theorem 5 evidence"
      ],
      "type": "concept",
      "description": "Proof that MLE-based robots act identically under incorrect β assumptions (Section 5).",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Supports choosing MLE over Bayesian-optimal policies."
    },
    {
      "name": "Simulation evidence of autonomy advantage and decreasing obedience",
      "aliases": [
        "Figure 2 Δ and O simulation",
        "empirical tradeoff data"
      ],
      "type": "concept",
      "description": "Experiments (Figure 2, Section 2) confirm positive autonomy advantage Δ and decreasing obedience O for MLE IRL robots.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Empirically validates theoretical trade-off and MLE behaviour."
    },
    {
      "name": "Simulation evidence of feature space effects on obedience",
      "aliases": [
        "Figure 4 feature misspecification simulation",
        "obedience vs features empirical"
      ],
      "type": "concept",
      "description": "Experiments (Figure 4, Section 5) show extra features increase obedience while missing features lower it and can produce negative Δ.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Validates redundant-feature approach."
    },
    {
      "name": "Simulation evidence of misspecification detection approach",
      "aliases": [
        "Figure 5 detection simulation",
        "Δ improvement with detection"
      ],
      "type": "concept",
      "description": "Experiments (Figure 5, Section 5) demonstrate that monitoring first-order obedience and switching policies mitigates harms when features are missing.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Empirical support for burn-in and monitoring mechanisms."
    },
    {
      "name": "Fine-tune robots with constrained MLE IRL to enhance obedience robustness",
      "aliases": [
        "apply bounded MLE during RLHF",
        "MLE fine-tuning for safety"
      ],
      "type": "intervention",
      "description": "During model fine-tuning, estimate reward parameters via maximum likelihood under an L2-norm constraint to ensure first-order obedience and robustness to β misspecification.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Section 4 describes using MLE during policy learning, aligning with the fine-tuning/RL phase.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Supported by theoretical proofs and small-scale simulations; not yet broadly validated in deployed systems.",
      "node_rationale": "Direct actionable measure derived from design rationale 11 and validated by Theorem 4."
    },
    {
      "name": "Design reward models with additional irrelevant features to bias toward obedience",
      "aliases": [
        "add redundant features in Θ",
        "complex reward model for safety"
      ],
      "type": "intervention",
      "description": "At model-design time, deliberately augment the reward feature space with extra dimensions so that model errors lead to over-obedience rather than dangerous disobedience.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Feature-space decisions are made during initial model design (Section 5).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Currently a theoretical proposal supported only by simulation evidence.",
      "node_rationale": "Implements design rationale 13 to bias safety."
    },
    {
      "name": "Deploy burn-in policy mixing that starts with blind obedience before autonomy",
      "aliases": [
        "initial obedience deployment schedule",
        "burn-in obedience deployment"
      ],
      "type": "intervention",
      "description": "Operate the robot in fully obedient mode for a fixed burn-in period, then gradually reduce obedience probability according to a predefined schedule.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Policy scheduling occurs during deployed operation (Section 5, Remark 4).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated in simulations but not yet validated in real deployments.",
      "node_rationale": "Implements safe transition strategy from obedience to autonomy."
    },
    {
      "name": "Monitor first-order obedience rate and switch to full obedience on anomaly",
      "aliases": [
        "obedience rate monitoring fallback",
        "misspecification detection fallback"
      ],
      "type": "intervention",
      "description": "During deployment, compute the empirical probability that the robot follows the first human order; if it falls below a threshold, revert to permanently obeying all orders.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Runtime monitoring and fallback control occur during deployment (Section 5, Equation 3).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Backed by simulation evidence; real-world validation pending.",
      "node_rationale": "Safety back-stop based on misspecification detection mechanism."
    },
    {
      "name": "Prefer MLE IRL over Bayesian optimal to mitigate rationality misspecification",
      "aliases": [
        "choose MLE instead of Bayesian IRL",
        "MLE preference for robustness"
      ],
      "type": "intervention",
      "description": "Select maximum-likelihood inverse reinforcement learning rather than Bayesian posterior-mean policies to gain robustness to errors in assumed human rationality β.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Choice of learning algorithm is made during fine-tuning/RL (Sections 4–5).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Supported by Theorem 5 and simulations; no large-scale deployment studies.",
      "node_rationale": "Addresses robustness concerns highlighted by theoretical analysis."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Reduced human value attainment from blind robot obedience in human-robot interaction",
      "target_node": "Human decision suboptimality in supervisory control tasks",
      "description": "Performance loss arises because humans issue sub-optimal orders.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit formalisation in Section 3 equations.",
      "edge_rationale": "Theorem 1 derives Δ=0 when robot obeys sub-optimal orders."
    },
    {
      "type": "mitigated_by",
      "source_node": "Human decision suboptimality in supervisory control tasks",
      "target_node": "Autonomy advantage from IRL-based preference inference in presence of human suboptimality",
      "description": "IRL-based inference allows robot to overcome human suboptimality.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Mathematical proof (Theorem 1).",
      "edge_rationale": "Theorem shows Δ≥0 with IRL robot."
    },
    {
      "type": "enabled_by",
      "source_node": "Autonomy advantage from IRL-based preference inference in presence of human suboptimality",
      "target_node": "Adopt IRL policies that prioritize early obedience and robustness to misspecification",
      "description": "Positive autonomy advantage motivates designing IRL-based policies.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Conceptual linkage made in Section 3 conclusion.",
      "edge_rationale": "Design rationale builds on proven advantage."
    },
    {
      "type": "implemented_by",
      "source_node": "Adopt IRL policies that prioritize early obedience and robustness to misspecification",
      "target_node": "Constrained MLE inverse reinforcement learning during fine-tuning",
      "description": "Constrained MLE IRL realises the robust IRL design.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 4 explicitly proposes bounded-norm MLE as practical IRL.",
      "edge_rationale": "Implementation follows design principle."
    },
    {
      "type": "validated_by",
      "source_node": "Constrained MLE inverse reinforcement learning during fine-tuning",
      "target_node": "Theorem 4 proof of MLE first-order obedience",
      "description": "Proof confirms implementation obeys first order.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Formal theorem.",
      "edge_rationale": "Direct validation of implementation property."
    },
    {
      "type": "validated_by",
      "source_node": "Constrained MLE inverse reinforcement learning during fine-tuning",
      "target_node": "Simulation evidence of autonomy advantage and decreasing obedience",
      "description": "Simulations show implementation achieves Δ>0 and correct obedience trend.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Repeated empirical runs (Figure 2).",
      "edge_rationale": "Empirically corroborates theoretical behaviour."
    },
    {
      "type": "motivates",
      "source_node": "Theorem 4 proof of MLE first-order obedience",
      "target_node": "Fine-tune robots with constrained MLE IRL to enhance obedience robustness",
      "description": "Formal guarantee motivates adopting bounded MLE in fine-tuning.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Strong theoretical result directly supports intervention.",
      "edge_rationale": "Guarantee is key justification for intervention."
    },
    {
      "type": "motivates",
      "source_node": "Simulation evidence of autonomy advantage and decreasing obedience",
      "target_node": "Fine-tune robots with constrained MLE IRL to enhance obedience robustness",
      "description": "Empirical performance benefits encourage fine-tuning with constrained MLE.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Simulation not yet scaled to real systems.",
      "edge_rationale": "Shows practical effectiveness."
    },
    {
      "type": "caused_by",
      "source_node": "Catastrophic reward misalignment from preference model misspecification in human-robot interaction",
      "target_node": "Robot preference model misspecification (behavioral rationality or feature space) in human-robot interaction",
      "description": "Misalignment stems from incorrect β or Θ assumptions.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 5 explicitly links misspecification to ‘rebellion regret’.",
      "edge_rationale": "Causal mechanism specified."
    },
    {
      "type": "mitigated_by",
      "source_node": "Robot preference model misspecification (behavioral rationality or feature space) in human-robot interaction",
      "target_node": "MLE IRL robustness to human rationality misspecification",
      "description": "MLE robustness reduces harm from wrong β.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Theorem 5 proof.",
      "edge_rationale": "Robustness property counters misspecification."
    },
    {
      "type": "mitigated_by",
      "source_node": "Robot preference model misspecification (behavioral rationality or feature space) in human-robot interaction",
      "target_node": "Providing extra reward features increases robot obedience bias",
      "description": "Extra features bias robot toward obedience despite misspecification.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Based on simulation evidence only.",
      "edge_rationale": "Empirical finding suggests mitigation."
    },
    {
      "type": "mitigated_by",
      "source_node": "Robot preference model misspecification (behavioral rationality or feature space) in human-robot interaction",
      "target_node": "First-order obedience property of MLE IRL enables model misspecification detection",
      "description": "First-order obedience property can signal when model is wrong.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Derived from Theorem 4.",
      "edge_rationale": "Provides basis for detection mechanism."
    },
    {
      "type": "enabled_by",
      "source_node": "MLE IRL robustness to human rationality misspecification",
      "target_node": "Use MLE estimator with norm constraint to guarantee obedience to initial orders",
      "description": "Robustness insight informs specific estimator choice.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical design derivation.",
      "edge_rationale": "Design directly leverages theoretical robustness."
    },
    {
      "type": "enabled_by",
      "source_node": "Providing extra reward features increases robot obedience bias",
      "target_node": "Add redundant reward features to err on side of obedience",
      "description": "Insight leads to design decision to include extra features.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Simulation-based reasoning.",
      "edge_rationale": "Design responds to empirical finding."
    },
    {
      "type": "enabled_by",
      "source_node": "First-order obedience property of MLE IRL enables model misspecification detection",
      "target_node": "Incorporate initial burn-in obedience period for anomaly detection",
      "description": "Property suggests using burn-in to measure obedience and detect errors.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit proposal in Section 5.",
      "edge_rationale": "Design leverages theoretical guarantee."
    },
    {
      "type": "implemented_by",
      "source_node": "Use MLE estimator with norm constraint to guarantee obedience to initial orders",
      "target_node": "Constrained MLE inverse reinforcement learning during fine-tuning",
      "description": "Constrained MLE realises the estimator design.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct mapping from design to code-level method.",
      "edge_rationale": "Concrete implementation step."
    },
    {
      "type": "implemented_by",
      "source_node": "Incorporate initial burn-in obedience period for anomaly detection",
      "target_node": "Policy mixing with declining obedience probability after burn-in",
      "description": "Policy-mixing mechanism executes the burn-in strategy.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Mechanism described in same section.",
      "edge_rationale": "Implements burn-in schedule."
    },
    {
      "type": "implemented_by",
      "source_node": "Incorporate initial burn-in obedience period for anomaly detection",
      "target_node": "Sample first-order obedience rate to trigger obedience fallback",
      "description": "Monitoring algorithm operationalises anomaly detection.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Algorithm defined directly after design description.",
      "edge_rationale": "Implements detection logic."
    },
    {
      "type": "implemented_by",
      "source_node": "Add redundant reward features to err on side of obedience",
      "target_node": "Reward feature space expansion with irrelevant dimensions",
      "description": "Feature augmentation realises redundant-feature design.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Straightforward technical mapping.",
      "edge_rationale": "Concrete method to add features."
    },
    {
      "type": "validated_by",
      "source_node": "Policy mixing with declining obedience probability after burn-in",
      "target_node": "Simulation evidence of misspecification detection approach",
      "description": "Simulations confirm policy-mix effectiveness.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Figure 5 empirical results.",
      "edge_rationale": "Evidence supports implementation."
    },
    {
      "type": "validated_by",
      "source_node": "Sample first-order obedience rate to trigger obedience fallback",
      "target_node": "Simulation evidence of misspecification detection approach",
      "description": "Simulation demonstrates detection reduces negative Δ.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Figure 5.",
      "edge_rationale": "Confirms efficacy of monitoring mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Reward feature space expansion with irrelevant dimensions",
      "target_node": "Simulation evidence of feature space effects on obedience",
      "description": "Simulations show expanded feature space increases obedience.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Figure 4 data.",
      "edge_rationale": "Empirical validation."
    },
    {
      "type": "motivates",
      "source_node": "Simulation evidence of feature space effects on obedience",
      "target_node": "Design reward models with additional irrelevant features to bias toward obedience",
      "description": "Empirical benefit motivates adding redundant features.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Simulation-only evidence.",
      "edge_rationale": "Drives intervention proposal."
    },
    {
      "type": "motivates",
      "source_node": "Simulation evidence of misspecification detection approach",
      "target_node": "Deploy burn-in policy mixing that starts with blind obedience before autonomy",
      "description": "Detection success encourages deploying burn-in strategy.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Based on Figure 5 results.",
      "edge_rationale": "Simulated superiority informs deployment choice."
    },
    {
      "type": "motivates",
      "source_node": "Simulation evidence of misspecification detection approach",
      "target_node": "Monitor first-order obedience rate and switch to full obedience on anomaly",
      "description": "Positive results motivate runtime monitoring intervention.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Simulation evidence only.",
      "edge_rationale": "Provides empirical backing."
    },
    {
      "type": "validated_by",
      "source_node": "Constrained MLE inverse reinforcement learning during fine-tuning",
      "target_node": "Theorem 5 proof of MLE robustness to rationality misspecification",
      "description": "Proof confirms robustness of implementation to β error.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Formal theorem.",
      "edge_rationale": "Strengthens safety case."
    },
    {
      "type": "motivates",
      "source_node": "Theorem 5 proof of MLE robustness to rationality misspecification",
      "target_node": "Prefer MLE IRL over Bayesian optimal to mitigate rationality misspecification",
      "description": "Robustness proof motivates choosing MLE over Bayesian methods.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Strong theoretical evidence.",
      "edge_rationale": "Justifies intervention choice."
    },
    {
      "type": "validated_by",
      "source_node": "Obedience–performance tradeoff in robot autonomy decisions",
      "target_node": "Simulation evidence of autonomy advantage and decreasing obedience",
      "description": "Simulation empirically confirms trade-off curve.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Figure 2 matches theoretical prediction.",
      "edge_rationale": "Provides evidence for theoretical insight."
    },
    {
      "type": "enabled_by",
      "source_node": "Obedience–performance tradeoff in robot autonomy decisions",
      "target_node": "Adopt IRL policies that prioritize early obedience and robustness to misspecification",
      "description": "Trade-off insight guides design of balanced IRL policies.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Conceptual linkage in Sections 3–4.",
      "edge_rationale": "Design balances obedience and performance."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1705.09990"
    },
    {
      "key": "title",
      "value": "Should Robots be Obedient?"
    },
    {
      "key": "authors",
      "value": [
        "Smitha Milli",
        "Dylan Hadfield-Menell",
        "Anca Dragan",
        "Stuart Russell"
      ]
    },
    {
      "key": "date_published",
      "value": "2017-05-28T00:00:00Z"
    },
    {
      "key": "source",
      "value": "arxiv"
    }
  ]
}