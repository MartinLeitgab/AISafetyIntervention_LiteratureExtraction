{
  "nodes": [
    {
      "name": "Erroneous or harmful translations from adversarial perturbations in neural machine translation systems",
      "aliases": [
        "mistranslations from adversarial noise in NMT",
        "adversarially induced harmful MT errors"
      ],
      "type": "concept",
      "description": "Real-world incidents (e.g., Facebook ‘good morning’ → ‘attack them’) show that tiny character changes can trigger dangerous mistranslations, posing user-safety and reputational risks for MT deployment.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in 1 Introduction as the top-level safety risk motivating the study."
    },
    {
      "name": "Character-level vulnerability to small input perturbations in NMT",
      "aliases": [
        "over-sensitivity of char-level NMT",
        "fragility of NMT to typos"
      ],
      "type": "concept",
      "description": "State-of-the-art character/sub-word NMT models suffer large BLEU drops when single characters are flipped, inserted, deleted or swapped, indicating brittle internal representations.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed throughout 1 Introduction and 2 Related Work as the mechanism that produces the observed risk."
    },
    {
      "name": "Ineffectiveness of black-box adversarial testing in revealing worst-case translation errors",
      "aliases": [
        "limitations of black-box attacks for NMT",
        "black-box heuristic attack insufficiency"
      ],
      "type": "concept",
      "description": "Previous NLP adversarial work relied on random or heuristic input noise without gradient access, often missing the worst-case failures characterising real-world incidents.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined in 1 Introduction and contrasted with white-box approaches in 3 White-Box Adversarial Examples."
    },
    {
      "name": "Gradient-based white-box attacks approximate worst-case input perturbations in NMT",
      "aliases": [
        "white-box gradient attacks capture worst-case",
        "gradient-based vulnerability estimation"
      ],
      "type": "concept",
      "description": "Using model gradients with respect to one-hot inputs enables ranking of character edits that maximise translation loss, yielding significantly stronger untargeted, controlled and targeted attacks.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Formalised in Section 3 with Eq.(1) and Figure 1 validating correlation between gradient estimate and true loss."
    },
    {
      "name": "Adversarial training on stronger attacks yields robustness to multiple noise sources",
      "aliases": [
        "robustness via adversarial training on worst-case examples",
        "white-box adversarial training improves generalization"
      ],
      "type": "concept",
      "description": "Training on gradient-based adversarial examples (alone or combined with other noise) can harden models, improving BLEU on unseen natural, keyboard and random noise without overfitting.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivated in 6 Robustness to Adversarial Examples, theorising why worst-case training generalises."
    },
    {
      "name": "Use gradient-informed text edit operations to craft adversarial examples",
      "aliases": [
        "gradient-driven character edits",
        "HotFlip extension rationale"
      ],
      "type": "concept",
      "description": "Design applies gradient ranking to select flip, insert, delete or swap actions that most increase loss, enabling efficient search (greedy, beam, one-shot).",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in 3 White-Box Adversarial Examples, Sections 3.2-3.4."
    },
    {
      "name": "Integrate white-box adversarial examples into training to harden models",
      "aliases": [
        "incorporating white-box noise in training",
        "adversarial training strategy"
      ],
      "type": "concept",
      "description": "Model parameters are updated on both clean and gradient-generated adversarial inputs each epoch, following one-shot HotFlip operations, with the goal of reducing future attack success.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 6.2 describes the adversarial-training protocol labelled FIDS-W."
    },
    {
      "name": "Combine white-box and black-box noise generation in ensemble training for broader robustness",
      "aliases": [
        "ensemble adversarial data augmentation",
        "hybrid noise training rationale"
      ],
      "type": "concept",
      "description": "An ensemble curriculum feeds both gradient-based and heuristic (Rand/Key/Nat) noisy sentences during fine-tuning to cover diverse threat models.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in 6.2 White-Box and Ensemble Methods."
    },
    {
      "name": "Flip/Insert/Delete/Swap character operations guided by loss gradients in HotFlip extension",
      "aliases": [
        "FIDS gradient operations",
        "HotFlip FIDS"
      ],
      "type": "concept",
      "description": "Implementation realises four atomic edits represented as input-space vectors; directional derivatives pick the most damaging operation per word.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mechanics presented in Section 3.2 Derivatives of Operations."
    },
    {
      "name": "One-shot gradient-based adversarial example generation for efficient adversarial training",
      "aliases": [
        "one-shot white-box attack",
        "parallel word flips"
      ],
      "type": "concept",
      "description": "All words in a sentence are perturbed simultaneously using their locally best edits after a single forward–backward pass, reducing training slowdown to ×3.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Efficiency gains highlighted in 3.4 Multiple Changes and revisited in 6 Robustness."
    },
    {
      "name": "Ensemble training pipeline mixing clean, white-box, and black-box noisy inputs",
      "aliases": [
        "ensemble adversarial training process",
        "mixed-noise training pipeline"
      ],
      "type": "concept",
      "description": "Training batches are split 50/50 between FIDS-W gradient attacks and Rand/Key/Nat noise, broadening exposure without degrading clean-data BLEU.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Operational details given in 6.2 White-Box and Ensemble Methods."
    },
    {
      "name": "White-box attacks reduce BLEU more than black-box attacks in untargeted, controlled, targeted scenarios on IWSLT datasets",
      "aliases": [
        "white-box attack effectiveness metrics",
        "BLEU drop with gradient attacks"
      ],
      "type": "concept",
      "description": "Tables 3-4 and Figures 2-3 show gradient attacks cutting BLEU to ~4–6 compared with ~7-10 for black-box, and achieving far higher success in muting/pushing words.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Empirical comparison presented in Section 5 Analysis of Adversaries."
    },
    {
      "name": "Adversarial training with white-box examples improves BLEU on noisy test sets versus baselines",
      "aliases": [
        "white-box training robustness results",
        "FIDS-W robustness evidence"
      ],
      "type": "concept",
      "description": "FIDS-W models gain 5-10 BLEU over vanilla on Nat/Key noise and maintain clean performance (Table 6), evidencing robustness from white-box training.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Quantitative results in 6.3 Discussion."
    },
    {
      "name": "Ensemble adversarial training achieves highest average BLEU across noise types",
      "aliases": [
        "ensemble model best average BLEU",
        "hybrid training validation"
      ],
      "type": "concept",
      "description": "The Ensemble model tops Table 6 in 7 of 9 noisy-set comparisons, averaging 32.30, 27.82, 19.99 BLEU for FR, DE, CS respectively.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Performance highlighted in Section 6.3 Discussion."
    },
    {
      "name": "Apply gradient-based one-shot adversarial training with FIDS operations during NMT fine-tuning to improve robustness",
      "aliases": [
        "FIDS-W adversarial training",
        "white-box adversarial fine-tuning"
      ],
      "type": "intervention",
      "description": "During fine-tuning, generate one-shot FIDS gradient attacks for each mini-batch and train on both clean and adversarial inputs to harden the translation model.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Occurs during fine-tuning phase described in Section 6.2 White-Box Methods.",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Validated on IWSLT datasets with systematic BLEU evaluation (Table 6) – prototype robustness evidence.",
      "node_rationale": "Central defence proposed; enables mitigation of risk by increasing robustness."
    },
    {
      "name": "Employ ensemble adversarial training mixing white-box and black-box noise sources during NMT fine-tuning",
      "aliases": [
        "ensemble adversarial fine-tuning",
        "hybrid noise training"
      ],
      "type": "intervention",
      "description": "Alternate gradient-based FIDS attacks with Rand/Key/Nat noise at a 50/50 ratio while training, yielding broad-spectrum robustness.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Implemented during fine-tuning as detailed in Section 6.2 Ensemble.",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Empirically shown to outperform all baselines (Table 6) but not yet deployed in production systems.",
      "node_rationale": "Provides the best validated mitigation path against diverse adversaries."
    },
    {
      "name": "Conduct gradient-based controlled and targeted attack evaluation in pre-deployment testing to identify translation vulnerabilities",
      "aliases": [
        "white-box attack evaluation testing",
        "controlled/targeted attack audit"
      ],
      "type": "intervention",
      "description": "Before release, run the white-box controlled/targeted attack suite (beam-search FIDS) to measure success-rate and BLEU degradation, flagging critical failures.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Used as a safety audit prior to deployment; corresponds to evaluation stage implied in Sections 5.2-5.3.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated experimentally but not integrated into standard CI pipelines yet.",
      "node_rationale": "Addresses testing gap identified by black-box inadequacy, preventing undetected vulnerabilities."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Erroneous or harmful translations from adversarial perturbations in neural machine translation systems",
      "target_node": "Character-level vulnerability to small input perturbations in NMT",
      "description": "Unsafe translations occur because the model’s output distribution shifts drastically when a few characters are perturbed.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Multiple empirical examples and BLEU drops in Table 3 support direct causation (Sections 1 & 5.1).",
      "edge_rationale": "Links top-level risk to its mechanistic cause."
    },
    {
      "type": "caused_by",
      "source_node": "Erroneous or harmful translations from adversarial perturbations in neural machine translation systems",
      "target_node": "Ineffectiveness of black-box adversarial testing in revealing worst-case translation errors",
      "description": "When only weak black-box testing is used, dangerous vulnerabilities remain, allowing harmful outputs in the field.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Connection is argued conceptually in 1 Introduction but not quantitatively measured.",
      "edge_rationale": "Shows auxiliary pathway by which risk persists."
    },
    {
      "type": "addressed_by",
      "source_node": "Character-level vulnerability to small input perturbations in NMT",
      "target_node": "Gradient-based white-box attacks approximate worst-case input perturbations in NMT",
      "description": "White-box analysis directly tackles the fragility by surfacing the worst-case edits that exploit the vulnerability.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoning explained in Sections 3 & 5 with supportive metrics.",
      "edge_rationale": "Connects problem mechanism to proposed analytical insight."
    },
    {
      "type": "addressed_by",
      "source_node": "Ineffectiveness of black-box adversarial testing in revealing worst-case translation errors",
      "target_node": "Gradient-based white-box attacks approximate worst-case input perturbations in NMT",
      "description": "White-box gradient methods overcome the limitations of heuristic black-box attacks by directly optimizing loss.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 5 compares success rates; moderate empirical backing.",
      "edge_rationale": "Justifies need for gradient access."
    },
    {
      "type": "specified_by",
      "source_node": "Gradient-based white-box attacks approximate worst-case input perturbations in NMT",
      "target_node": "Use gradient-informed text edit operations to craft adversarial examples",
      "description": "The theoretical claim is operationalised through a concrete design of gradient-ranked character edits.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Design directly follows theory in Section 3.",
      "edge_rationale": "Transitions from insight to design."
    },
    {
      "type": "implemented_by",
      "source_node": "Use gradient-informed text edit operations to craft adversarial examples",
      "target_node": "Flip/Insert/Delete/Swap character operations guided by loss gradients in HotFlip extension",
      "description": "The design is realised with the FIDS HotFlip extension that performs the ranked edits.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Detailed algorithm and vector definitions in Section 3.2.",
      "edge_rationale": "Design → concrete mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Flip/Insert/Delete/Swap character operations guided by loss gradients in HotFlip extension",
      "target_node": "White-box attacks reduce BLEU more than black-box attacks in untargeted, controlled, targeted scenarios on IWSLT datasets",
      "description": "Experiments show FIDS edits cause larger BLEU drops than black-box edits, validating the mechanism’s effectiveness.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Strong empirical results in Tables 3-4 and Figures 2-3.",
      "edge_rationale": "Mechanism performance evidence."
    },
    {
      "type": "mitigated_by",
      "source_node": "Character-level vulnerability to small input perturbations in NMT",
      "target_node": "Adversarial training on stronger attacks yields robustness to multiple noise sources",
      "description": "Training on worst-case examples reduces the model’s sensitivity to small perturbations.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoned in Section 6; empirical support via BLEU improvements.",
      "edge_rationale": "Shows how theory mitigates the vulnerability."
    },
    {
      "type": "specified_by",
      "source_node": "Adversarial training on stronger attacks yields robustness to multiple noise sources",
      "target_node": "Integrate white-box adversarial examples into training to harden models",
      "description": "The insight is translated into a concrete training strategy using white-box examples.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Strategy outlined directly after theoretical claim in 6 Robustness.",
      "edge_rationale": "Theory → design."
    },
    {
      "type": "implemented_by",
      "source_node": "Integrate white-box adversarial examples into training to harden models",
      "target_node": "One-shot gradient-based adversarial example generation for efficient adversarial training",
      "description": "Efficiency-oriented one-shot generation realises the training design in practice.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Implementation details and speedups quantified in Sections 3.4 & 6.2.",
      "edge_rationale": "Design to mechanism link."
    },
    {
      "type": "validated_by",
      "source_node": "One-shot gradient-based adversarial example generation for efficient adversarial training",
      "target_node": "Adversarial training with white-box examples improves BLEU on noisy test sets versus baselines",
      "description": "BLEU improvements across Nat/Key/Rand confirm that the one-shot mechanism is effective for robustness.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Table 6 shows systematic gains; strong evidence.",
      "edge_rationale": "Mechanism validation."
    },
    {
      "type": "motivates",
      "source_node": "White-box attacks reduce BLEU more than black-box attacks in untargeted, controlled, targeted scenarios on IWSLT datasets",
      "target_node": "Conduct gradient-based controlled and targeted attack evaluation in pre-deployment testing to identify translation vulnerabilities",
      "description": "Demonstrated attack potency motivates adopting these evaluations in safety audits.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical extension from Section 5 results; not yet deployed.",
      "edge_rationale": "Evidence drives testing intervention."
    },
    {
      "type": "motivates",
      "source_node": "Adversarial training with white-box examples improves BLEU on noisy test sets versus baselines",
      "target_node": "Apply gradient-based one-shot adversarial training with FIDS operations during NMT fine-tuning to improve robustness",
      "description": "Observed robustness gains support adopting one-shot FIDS adversarial training.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct quantitative support in Table 6.",
      "edge_rationale": "Evidence to intervention."
    },
    {
      "type": "specified_by",
      "source_node": "Adversarial training on stronger attacks yields robustness to multiple noise sources",
      "target_node": "Combine white-box and black-box noise generation in ensemble training for broader robustness",
      "description": "The insight suggests mixing multiple noise types for greater coverage.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Design explained following theory in 6.2.",
      "edge_rationale": "Theory to design."
    },
    {
      "type": "implemented_by",
      "source_node": "Combine white-box and black-box noise generation in ensemble training for broader robustness",
      "target_node": "Ensemble training pipeline mixing clean, white-box, and black-box noisy inputs",
      "description": "Implementation realises the design via a 50/50 data split during fine-tuning.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Implementation specifics detailed in Section 6.2.",
      "edge_rationale": "Design to mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Ensemble training pipeline mixing clean, white-box, and black-box noisy inputs",
      "target_node": "Ensemble adversarial training achieves highest average BLEU across noise types",
      "description": "Experiments show ensemble pipeline yields best BLEU averages across all noise categories.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Table 6 provides strong empirical support.",
      "edge_rationale": "Mechanism validation."
    },
    {
      "type": "motivates",
      "source_node": "Ensemble adversarial training achieves highest average BLEU across noise types",
      "target_node": "Employ ensemble adversarial training mixing white-box and black-box noise sources during NMT fine-tuning",
      "description": "Superior robustness metrics endorse the ensemble training intervention.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "High BLEU scores directly encourage adoption (Section 6.3).",
      "edge_rationale": "Evidence to intervention."
    }
  ],
  "meta": [
    {
      "key": "authors",
      "value": [
        "Javid Ebrahimi",
        "Daniel Lowd",
        "Dejing Dou"
      ]
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1806.09030"
    },
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "title",
      "value": "On Adversarial Examples for Character-Level Neural Machine Translation"
    },
    {
      "key": "date_published",
      "value": "2018-06-23T00:00:00Z"
    }
  ]
}