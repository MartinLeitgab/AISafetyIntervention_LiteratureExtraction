{
  "nodes": [
    {
      "name": "Unnatural behaviors from unsupervised option discovery in high-dimensional control",
      "aliases": [
        "unnatural emergent behaviors in variational option discovery",
        "non-humanlike skills from unsupervised RL"
      ],
      "type": "concept",
      "description": "In complex systems such as the ‘Toddler’ humanoid, variational option discovery produced highly unnatural, non-humanlike gaits, indicating a mis-alignment between the information-theoretic objective and human priors on useful behavior.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section “Hand and Toddler Environments” notes poor, unnatural behaviors learned in Toddler."
    },
    {
      "name": "Training instability with large context sets in variational option discovery",
      "aliases": [
        "optimization instability for large K",
        "slow convergence with many contexts"
      ],
      "type": "concept",
      "description": "When the number of contexts (K) is large and sampled uniformly, gradients become high-variance and learning stagnates, as shown in HalfCheetah experiments.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Figures 2a and discussion in “Exploring Optimization Techniques”."
    },
    {
      "name": "Trivial context signalling through actions in variational option discovery",
      "aliases": [
        "context encoded directly in actions",
        "action-based signalling problem"
      ],
      "type": "concept",
      "description": "If the decoder observes raw actions, an agent can communicate context directly via action sequences without affecting the environment, defeating the aim of learning meaningful skills.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Decoder design discussion in Section 3.2 VALOR."
    },
    {
      "name": "Limited downstream task utility of learned options",
      "aliases": [
        "poor transfer of unsupervised skills",
        "ineffective options for downstream control"
      ],
      "type": "concept",
      "description": "Unsupervisedly learned options might not accelerate or even match task-specific learning unless they are sufficiently expressive.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivated in Introduction; empirically examined in “Downstream Tasks”."
    },
    {
      "name": "Information-theoretic objectives maximize mutual information irrespective of human priors",
      "aliases": [
        "objective mis-alignment with human priors",
        "mutual information objective limitation"
      ],
      "type": "concept",
      "description": "Purely maximizing I(context;trajectory) leads to behaviors that are distinguishable but not necessarily meaningful or safe by human standards.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussion in Conclusions and Toddler results."
    },
    {
      "name": "Uniform context distribution yields sparse learning signal as K grows",
      "aliases": [
        "low-signal uniform sampling problem",
        "context sparsity issue"
      ],
      "type": "concept",
      "description": "Sampling evenly from many contexts means each context is rarely updated, producing noisy gradients and slowing learning.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Observed in Fig 2a—performance degrades with K=64 under uniform sampling."
    },
    {
      "name": "Decoder observability of actions allows direct context encoding",
      "aliases": [
        "action visibility causes signalling",
        "decoder sees actions"
      ],
      "type": "concept",
      "description": "When the decoder can read action vectors, the easiest route to maximise mutual information is to emit context-specific action codes without engaging the environment.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Justification for VALOR’s ‘decoder never sees actions’ bullet."
    },
    {
      "name": "Learned behaviors may not span task-relevant action space",
      "aliases": [
        "insufficient option expressivity",
        "option set coverage issue"
      ],
      "type": "concept",
      "description": "Even diverse options can still omit actions needed by a downstream task, limiting hierarchical RL gains.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Ant-Maze hierarchy results subsection."
    },
    {
      "name": "Using trajectory-level mutual information with state-only decoder encourages environmental interaction",
      "aliases": [
        "trajectory MI with state-only decoding",
        "trajectory-centric state decoder insight"
      ],
      "type": "concept",
      "description": "By decoding contexts from sparse state observations of an entire trajectory while hiding actions, the objective rewards modes that create distinguishable state dynamics rather than static poses or action codes.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Reasoning in Section 3.2 VALOR."
    },
    {
      "name": "Gradually increasing context count when decoder accuracy is high maintains training signal",
      "aliases": [
        "context curriculum insight",
        "progressive K increase idea"
      ],
      "type": "concept",
      "description": "Expanding the context set only after the agent reliably distinguishes current contexts keeps per-context data density high, stabilising optimisation.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Derived from curriculum approach in Section 3.3."
    },
    {
      "name": "Embedding vectors for contexts enhance representation capacity and learning stability",
      "aliases": [
        "context embeddings insight",
        "learned context representations"
      ],
      "type": "concept",
      "description": "Replacing one-hot context input with trainable embeddings lets the policy capture relational structure among contexts, improving gradient quality.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Exploring optimisation techniques – embeddings consistently help."
    },
    {
      "name": "Fixed unsupervised option layer can provide expressive action abstractions equivalent to primitive actions",
      "aliases": [
        "pretrained options utility",
        "unsupervised lower layer expressivity"
      ],
      "type": "concept",
      "description": "A frozen policy trained with unsupervised option discovery matched the performance of training a hierarchy from scratch on Ant-Maze, indicating sufficient expressiveness.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section “Downstream Tasks”."
    },
    {
      "name": "Design VALOR algorithm with trajectory-based decoder excluding actions",
      "aliases": [
        "VALOR design rationale",
        "trajectory decoder without actions"
      ],
      "type": "concept",
      "description": "VALOR extends prior methods by decoding from sparse state deltas over the whole trajectory and hiding actions, aiming for dynamical skill discovery.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.2 VALOR."
    },
    {
      "name": "Introduce context count curriculum based on decoder performance threshold",
      "aliases": [
        "curriculum design rationale",
        "adaptive K growth"
      ],
      "type": "concept",
      "description": "The curriculum inflates K once average decoder probability exceeds ≈0.86, mitigating sparse-signal problems for large context spaces.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.3 Curriculum Approach."
    },
    {
      "name": "Implement learnable context embeddings as policy inputs",
      "aliases": [
        "context embeddings design rationale"
      ],
      "type": "concept",
      "description": "Instead of one-hot vectors, each discrete context ID is mapped to a trainable embedding vector concatenated to state inputs.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Exploring optimisation techniques."
    },
    {
      "name": "Use hierarchical RL with fixed pre-trained option policy for downstream tasks",
      "aliases": [
        "hierarchy design rationale with fixed lower layer"
      ],
      "type": "concept",
      "description": "A two-level architecture where only the upper policy is trained on the downstream task leverages reusable unsupervised options.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Downstream Tasks section."
    },
    {
      "name": "Bidirectional LSTM decoder over sparse state deltas",
      "aliases": [
        "VALOR decoder implementation",
        "trajectory LSTM decoder"
      ],
      "type": "concept",
      "description": "The decoder processes 11 equally-spaced state transitions via a bidirectional LSTM to classify context, enforcing sensitivity to whole-trajectory dynamics.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Figure 1 and Section 3.2."
    },
    {
      "name": "Curriculum update rule K←min(int(1.5×K+1), Kmax)",
      "aliases": [
        "context curriculum implementation rule"
      ],
      "type": "concept",
      "description": "The number of contexts is multiplied by 1.5 and incremented by 1 when decoder accuracy crosses threshold, capped at Kmax.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Equation 5."
    },
    {
      "name": "Embedding lookup table for context IDs",
      "aliases": [
        "context embedding implementation"
      ],
      "type": "concept",
      "description": "A learnable matrix maps each discrete context to a continuous vector fed into the policy network, replacing one-hot encoding.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation details in Exploring Optimisation Techniques."
    },
    {
      "name": "Two-level policy with fixed VALOR lower layer",
      "aliases": [
        "hierarchical policy implementation"
      ],
      "type": "concept",
      "description": "Upper-level A2C outputs continuous contexts to a frozen VALOR policy, enabling compositional control without updating the lower layer.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Ant-Maze experiment."
    },
    {
      "name": "VALOR produces naturalistic hand and locomotion behaviors",
      "aliases": [
        "VALOR qualitative evidence",
        "naturalistic behavior observation"
      ],
      "type": "concept",
      "description": "Experiments show VALOR discovers finger flexing and dynamic locomotion modes distinct from VIC/DIAYN, indicating success of trajectory-level decoding.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Figures 3 & 4, Hand environment."
    },
    {
      "name": "Curriculum yields faster convergence and hundreds of modes",
      "aliases": [
        "curriculum performance evidence"
      ],
      "type": "concept",
      "description": "Across environments, curriculum agents reach decoder mastery more quickly and learn up to 1024 distinguishable behaviors.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Figures 2b, 4e."
    },
    {
      "name": "Context embeddings improve speed and stability for large K",
      "aliases": [
        "embedding performance evidence"
      ],
      "type": "concept",
      "description": "With K=64, policies using embeddings show steeper learning curves and lower variance than one-hot baselines.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Figure 2a."
    },
    {
      "name": "Fixed VALOR lower-level matches scratch hierarchy in Ant-Maze",
      "aliases": [
        "hierarchy downstream evidence"
      ],
      "type": "concept",
      "description": "Return curves show VALOR-based hierarchy equals the performance of training both layers from scratch and surpasses random-network baseline.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Figure 4h."
    },
    {
      "name": "Train unsupervised skills with VALOR using trajectory-based state-only decoder",
      "aliases": [
        "apply VALOR pretraining",
        "VALOR skill discovery intervention"
      ],
      "type": "intervention",
      "description": "During unsupervised pre-training, use VALOR’s bidirectional LSTM decoder over sparse state deltas with no access to actions to learn diverse dynamical options.",
      "concept_category": null,
      "intervention_lifecycle": "2",
      "intervention_lifecycle_rationale": "Pre-training stage of agent skill acquisition (Section 3.2).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Validated in controlled simulators but not yet large-scale deployment.",
      "node_rationale": "Primary algorithmic contribution."
    },
    {
      "name": "Apply context-count curriculum during variational option discovery training",
      "aliases": [
        "use curriculum for K",
        "adaptive context expansion intervention"
      ],
      "type": "intervention",
      "description": "Begin with small K and update using K←min(int(1.5×K+1),Kmax) whenever decoder accuracy exceeds ≈0.86 to stabilise learning.",
      "concept_category": null,
      "intervention_lifecycle": "2",
      "intervention_lifecycle_rationale": "Technique is part of unsupervised option-discovery training pipeline (Section 3.3).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Tested experimentally across several environments.",
      "node_rationale": "Addresses instability with large context sets."
    },
    {
      "name": "Provide learnable context embeddings in policy networks for option discovery",
      "aliases": [
        "context embedding intervention"
      ],
      "type": "intervention",
      "description": "Replace one-hot context vectors with trainable embedding lookup passed to the policy to improve representation learning for large K.",
      "concept_category": null,
      "intervention_lifecycle": "2",
      "intervention_lifecycle_rationale": "Implemented during unsupervised option discovery (Exploring Optimisation Techniques).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Early-stage empirical support in simulators.",
      "node_rationale": "Mitigates gradient sparsity issues."
    },
    {
      "name": "Use fixed pre-trained VALOR policy as lower-level in hierarchical RL for downstream tasks",
      "aliases": [
        "deploy VALOR options in hierarchy"
      ],
      "type": "intervention",
      "description": "Freeze a VALOR-trained option policy and train a separate upper policy (e.g., A2C) that selects contexts, enabling reuse of unsupervised skills.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Fine-tuning/ RL stage for specific downstream task (Section Downstream Tasks).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Proof-of-concept in Ant-Maze only.",
      "node_rationale": "Addresses limited downstream utility risk."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Unnatural behaviors from unsupervised option discovery in high-dimensional control",
      "target_node": "Information-theoretic objectives maximize mutual information irrespective of human priors",
      "description": "Objective rewards distinguishability, not human-preferred natural motions.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Qualitative evidence from Toddler experiments.",
      "edge_rationale": "Conclusions describe mismatch between information-theoretic objective and human priors."
    },
    {
      "type": "caused_by",
      "source_node": "Training instability with large context sets in variational option discovery",
      "target_node": "Uniform context distribution yields sparse learning signal as K grows",
      "description": "Uniformly sampling many contexts reduces per-context updates, leading to noisy gradients.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Empirical curves showing degraded learning at K=64.",
      "edge_rationale": "Figure 2a discussion."
    },
    {
      "type": "caused_by",
      "source_node": "Trivial context signalling through actions in variational option discovery",
      "target_node": "Decoder observability of actions allows direct context encoding",
      "description": "If the decoder sees actions, agent can encode context directly without environment interaction.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical reasoning in Section 3.2.",
      "edge_rationale": "VALOR design motivation."
    },
    {
      "type": "caused_by",
      "source_node": "Limited downstream task utility of learned options",
      "target_node": "Learned behaviors may not span task-relevant action space",
      "description": "Options may fail to cover behaviors needed by the downstream task.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Ant-Maze comparison commentary.",
      "edge_rationale": "Section Downstream Tasks."
    },
    {
      "type": "mitigated_by",
      "source_node": "Information-theoretic objectives maximize mutual information irrespective of human priors",
      "target_node": "Using trajectory-level mutual information with state-only decoder encourages environmental interaction",
      "description": "Trajectory-level decoding emphasises dynamic interaction, aligning objective closer to meaningful skills.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual argument; limited empirical support.",
      "edge_rationale": "Section 3.2 rationale."
    },
    {
      "type": "mitigated_by",
      "source_node": "Uniform context distribution yields sparse learning signal as K grows",
      "target_node": "Gradually increasing context count when decoder accuracy is high maintains training signal",
      "description": "Curriculum retains dense sampling per context until decoder is competent.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Strong empirical improvement in learning curves.",
      "edge_rationale": "Figures 2b, 4e."
    },
    {
      "type": "mitigated_by",
      "source_node": "Uniform context distribution yields sparse learning signal as K grows",
      "target_node": "Embedding vectors for contexts enhance representation capacity and learning stability",
      "description": "Embeddings help policy share structure across contexts, reducing gradient variance.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Systematic improvements across seeds.",
      "edge_rationale": "Exploring optimisation techniques section."
    },
    {
      "type": "mitigated_by",
      "source_node": "Decoder observability of actions allows direct context encoding",
      "target_node": "Using trajectory-level mutual information with state-only decoder encourages environmental interaction",
      "description": "Hiding actions forces context information to flow through state dynamics.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Design rationale with logical argument.",
      "edge_rationale": "Section 3.2 VALOR."
    },
    {
      "type": "mitigated_by",
      "source_node": "Learned behaviors may not span task-relevant action space",
      "target_node": "Fixed unsupervised option layer can provide expressive action abstractions equivalent to primitive actions",
      "description": "Evidence shows pre-trained option set is expressive enough for Ant-Maze.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Single environment empirical result.",
      "edge_rationale": "Figure 4h."
    },
    {
      "type": "enabled_by",
      "source_node": "Using trajectory-level mutual information with state-only decoder encourages environmental interaction",
      "target_node": "Design VALOR algorithm with trajectory-based decoder excluding actions",
      "description": "Insight directly motivates VALOR’s design choices.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Stated causally in Section 3.2.",
      "edge_rationale": "VALOR introduction."
    },
    {
      "type": "enabled_by",
      "source_node": "Gradually increasing context count when decoder accuracy is high maintains training signal",
      "target_node": "Introduce context count curriculum based on decoder performance threshold",
      "description": "Curriculum operationalises the theoretical insight.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Implementation exactly follows the insight.",
      "edge_rationale": "Equation 5 derivation."
    },
    {
      "type": "enabled_by",
      "source_node": "Embedding vectors for contexts enhance representation capacity and learning stability",
      "target_node": "Implement learnable context embeddings as policy inputs",
      "description": "Embeddings are the design realisation of the insight.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct mapping from idea to design.",
      "edge_rationale": "Exploring optimisation techniques."
    },
    {
      "type": "enabled_by",
      "source_node": "Fixed unsupervised option layer can provide expressive action abstractions equivalent to primitive actions",
      "target_node": "Use hierarchical RL with fixed pre-trained option policy for downstream tasks",
      "description": "Insight motivates freezing lower layer for downstream use.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical flow in Downstream Tasks section.",
      "edge_rationale": "Ant-Maze experiment setup."
    },
    {
      "type": "implemented_by",
      "source_node": "Design VALOR algorithm with trajectory-based decoder excluding actions",
      "target_node": "Bidirectional LSTM decoder over sparse state deltas",
      "description": "LSTM decoder realises VALOR’s design.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Detailed architecture figure.",
      "edge_rationale": "Figure 1."
    },
    {
      "type": "implemented_by",
      "source_node": "Introduce context count curriculum based on decoder performance threshold",
      "target_node": "Curriculum update rule K←min(int(1.5×K+1), Kmax)",
      "description": "Equation 5 specifies the curriculum implementation.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Exact formula given.",
      "edge_rationale": "Equation 5."
    },
    {
      "type": "implemented_by",
      "source_node": "Implement learnable context embeddings as policy inputs",
      "target_node": "Embedding lookup table for context IDs",
      "description": "Embedding matrix realises the design rationale.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Standard neural-network implementation described.",
      "edge_rationale": "Implementation details section."
    },
    {
      "type": "implemented_by",
      "source_node": "Use hierarchical RL with fixed pre-trained option policy for downstream tasks",
      "target_node": "Two-level policy with fixed VALOR lower layer",
      "description": "Architecture instantiates the design.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit description in experiment.",
      "edge_rationale": "Downstream Tasks."
    },
    {
      "type": "validated_by",
      "source_node": "Bidirectional LSTM decoder over sparse state deltas",
      "target_node": "VALOR produces naturalistic hand and locomotion behaviors",
      "description": "Naturalistic behaviors evidence the effectiveness of decoder implementation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Qualitative observations; limited quantitative metrics.",
      "edge_rationale": "Hand and locomotion results."
    },
    {
      "type": "validated_by",
      "source_node": "Curriculum update rule K←min(int(1.5×K+1), Kmax)",
      "target_node": "Curriculum yields faster convergence and hundreds of modes",
      "description": "Learning curves directly test curriculum.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Consistent empirical improvement.",
      "edge_rationale": "Figures 2b, 4e."
    },
    {
      "type": "validated_by",
      "source_node": "Embedding lookup table for context IDs",
      "target_node": "Context embeddings improve speed and stability for large K",
      "description": "Experiments compare embeddings vs one-hot.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Controlled ablation study.",
      "edge_rationale": "Figure 2a."
    },
    {
      "type": "validated_by",
      "source_node": "Two-level policy with fixed VALOR lower layer",
      "target_node": "Fixed VALOR lower-level matches scratch hierarchy in Ant-Maze",
      "description": "Return curves validate hierarchical implementation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Quantitative performance data.",
      "edge_rationale": "Figure 4h."
    },
    {
      "type": "motivates",
      "source_node": "VALOR produces naturalistic hand and locomotion behaviors",
      "target_node": "Train unsupervised skills with VALOR using trajectory-based state-only decoder",
      "description": "Positive evidence motivates adopting VALOR for pre-training.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Correlation between evidence and intervention adoption.",
      "edge_rationale": "Discussion of VALOR success."
    },
    {
      "type": "motivates",
      "source_node": "Curriculum yields faster convergence and hundreds of modes",
      "target_node": "Apply context-count curriculum during variational option discovery training",
      "description": "Empirical gains motivate curriculum use.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Clear performance improvement demonstrated.",
      "edge_rationale": "Figures 2b, 4e."
    },
    {
      "type": "motivates",
      "source_node": "Context embeddings improve speed and stability for large K",
      "target_node": "Provide learnable context embeddings in policy networks for option discovery",
      "description": "Demonstrated benefits motivate embedding adoption.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Ablation evidence.",
      "edge_rationale": "Figure 2a."
    },
    {
      "type": "motivates",
      "source_node": "Fixed VALOR lower-level matches scratch hierarchy in Ant-Maze",
      "target_node": "Use fixed pre-trained VALOR policy as lower-level in hierarchical RL for downstream tasks",
      "description": "Successful transfer motivates hierarchical deployment.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Single environment but positive result.",
      "edge_rationale": "Downstream Tasks section."
    }
  ],
  "meta": [
    {
      "key": "title",
      "value": "Variational Option Discovery Algorithms"
    },
    {
      "key": "date_published",
      "value": "2018-07-26T00:00:00Z"
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1807.10299"
    },
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "authors",
      "value": [
        "Joshua Achiam",
        "Harrison Edwards",
        "Dario Amodei",
        "Pieter Abbeel"
      ]
    }
  ]
}