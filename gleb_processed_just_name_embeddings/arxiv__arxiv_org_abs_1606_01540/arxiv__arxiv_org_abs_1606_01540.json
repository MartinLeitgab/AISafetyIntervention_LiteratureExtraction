{
  "nodes": [
    {
      "name": "Incomparable performance claims in reinforcement learning research",
      "aliases": [
        "lack of comparability of RL results",
        "unreliable RL performance reporting"
      ],
      "type": "concept",
      "description": "Research papers report different performance numbers that cannot be fairly compared, hindering cumulative progress and safety-relevant verification.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced implicitly in Introduction as a motivation for Gym."
    },
    {
      "name": "Absence of standardized, versioned benchmark environments in reinforcement learning",
      "aliases": [
        "no common RL benchmarks",
        "non-versioned tasks"
      ],
      "type": "concept",
      "description": "Different studies use different or changing tasks, making numerical results non-comparable and non-reproducible.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Stated in 1 Introduction & 3 Design Decisions (‘Environments, not agents’ and ‘Strict versioning’)."
    },
    {
      "name": "Standardized interfaces and version control enable reproducible comparisons in reinforcement learning",
      "aliases": [
        "interface plus versioning improves reproducibility",
        "shared environment API hypothesis"
      ],
      "type": "concept",
      "description": "If all algorithms interact through an identical, version-controlled API, results across papers become directly comparable.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in 3 Design Decisions under ‘Strict versioning for environments’."
    },
    {
      "name": "Provide common environment abstraction with strict versioning to support reproducibility",
      "aliases": [
        "design of shared, versioned environment interface"
      ],
      "type": "concept",
      "description": "Architectural choice to expose only a unified environment object whose name includes a version suffix, guaranteeing that changing functionality yields a new identifier.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in 3 Design Decisions."
    },
    {
      "name": "OpenAI Gym API offering diverse, versioned environments",
      "aliases": [
        "Gym environment API",
        "gym.make(...) interface"
      ],
      "type": "concept",
      "description": "Concrete software mechanism that exposes tasks like CartPole-v0, guarantees version increments on any change, and spans classic control, Atari, robotics, etc.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4 Environments and code snippets show the API."
    },
    {
      "name": "Early community adoption shows reproducible performance comparisons across algorithms",
      "aliases": [
        "initial Gym leaderboard reproducibility evidence"
      ],
      "type": "concept",
      "description": "Users post scores with code links on the Gym website, demonstrating that others can rerun and obtain the same results.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mentioned in 1 Introduction & Design Decisions regarding scoreboards."
    },
    {
      "name": "Adopt OpenAI Gym versioned benchmark suite in RL algorithm development",
      "aliases": [
        "use Gym benchmarks",
        "evaluate on Gym tasks"
      ],
      "type": "intervention",
      "description": "Researchers run their algorithms on Gym’s version-controlled environments and report scores tied to those exact versions.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Evaluation happens pre-deployment when testing algorithms (3 Design Decisions).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Gym is widely used in academic prototypes and systematic studies but not universal (community usage noted in Introduction).",
      "node_rationale": "Direct practical action implied throughout the paper to mitigate reproducibility risk."
    },
    {
      "name": "Overfitting of reinforcement learning algorithms to specific benchmark tasks",
      "aliases": [
        "task-specific over-tuning in RL",
        "benchmark overfitting"
      ],
      "type": "concept",
      "description": "Algorithms may exploit idiosyncrasies of the tasks they are tested on, leading to inflated reported performance that does not generalize.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Raised in 3 Design Decisions under ‘Encourage peer review, not competition’ and RL benchmarking challenges."
    },
    {
      "name": "Lack of hidden test sets and peer scrutiny in reinforcement learning evaluation",
      "aliases": [
        "no unseen environments",
        "insufficient peer review"
      ],
      "type": "concept",
      "description": "Without separate testing and external review, researchers can unknowingly or deliberately fit to known tasks.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in ‘Encourage peer review, not competition’."
    },
    {
      "name": "Public code sharing and peer review mitigate benchmark overfitting in reinforcement learning",
      "aliases": [
        "open code enables scrutiny",
        "peer verification hypothesis"
      ],
      "type": "concept",
      "description": "Access to implementation details lets others judge whether results rely on narrow tuning or general principles.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Justified in 3 Design Decisions (Writeups and source code)."
    },
    {
      "name": "Require writeups and source code submission to encourage peer verification",
      "aliases": [
        "scoreboard submission requirements"
      ],
      "type": "concept",
      "description": "Gym’s design mandates uploading a description and code link with each posted score, explicitly facilitating peer examination.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Stated in Design Decisions."
    },
    {
      "name": "OpenAI Gym website scoreboard with writeup and code submission functionality",
      "aliases": [
        "Gym leaderboard",
        "gym.openai.com scoreboard"
      ],
      "type": "concept",
      "description": "Web infrastructure that records scores, displays them publicly, and stores associated writeups and code links.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Described in Introduction and Design Decisions."
    },
    {
      "name": "Scoreboard entries with shared code enable replication and overfitting checks",
      "aliases": [
        "leaderboard replication evidence"
      ],
      "type": "concept",
      "description": "Multiple users have reproduced published results by running the provided code, showing the mechanism works.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implied by discussion of peer review successes."
    },
    {
      "name": "Publish RL code and writeups on OpenAI Gym scoreboard for peer review",
      "aliases": [
        "submit to Gym leaderboard",
        "share RL code publicly"
      ],
      "type": "intervention",
      "description": "Researchers post their implementation, parameters, and reproduction instructions along with performance numbers on the public Gym site.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Action occurs during evaluation before any deployment in real-world systems.",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Practice is common but not universal; still mostly research environments.",
      "node_rationale": "Direct response to overfitting risk articulated by authors."
    },
    {
      "name": "Misleading evaluation focusing solely on final performance in reinforcement learning",
      "aliases": [
        "ignoring sample complexity",
        "only end-performance measured"
      ],
      "type": "concept",
      "description": "Algorithms can trade vast compute for marginal score gains, hiding ineffectiveness at learning quickly—important for safe real-world training.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Emphasized in Design Decisions (‘Emphasize sample complexity, not just final performance’)."
    },
    {
      "name": "Neglect of sample complexity metrics in reinforcement learning benchmarks",
      "aliases": [
        "no learning curve tracking"
      ],
      "type": "concept",
      "description": "Many benchmarks record only post-training average reward, omitting data on how many episodes were needed to reach it.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined in ‘Emphasize sample complexity’ paragraph."
    },
    {
      "name": "Monitoring learning curves provides insight into sample efficiency of reinforcement learning algorithms",
      "aliases": [
        "learning curve insight hypothesis"
      ],
      "type": "concept",
      "description": "Recording reward vs. timestep enables researchers to compare how quickly different algorithms improve.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Design Decisions ‘Monitoring by default’ section."
    },
    {
      "name": "Instrument environments with default monitoring to capture learning progress",
      "aliases": [
        "default Gym Monitor design"
      ],
      "type": "concept",
      "description": "Design choice to have every environment automatically collect timestep data and optionally video without extra user code.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in ‘Monitoring by default’."
    },
    {
      "name": "Gym Monitor automatic logging of episodes, timesteps, and optional video recording",
      "aliases": [
        "Monitor API",
        "gym.wrappers.Monitor"
      ],
      "type": "concept",
      "description": "Software wrapper that records every reset and step, stores reward time-series, and periodically captures videos.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation details in Design Decisions."
    },
    {
      "name": "Published Gym learning curves reveal sample complexity differences between algorithms",
      "aliases": [
        "learning curve evidence"
      ],
      "type": "concept",
      "description": "Community-shared plots illustrate that some algorithms reach high reward in fewer episodes, validating the monitoring approach.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implied impact of Monitor in practice."
    },
    {
      "name": "Log and share learning curves using Gym Monitor to evaluate sample efficiency",
      "aliases": [
        "publish Gym Monitor logs",
        "provide learning curves"
      ],
      "type": "intervention",
      "description": "Researchers leave the default Monitor on and upload the resulting reward-versus-episode data when reporting results.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Evaluation stage practice before any deployment.",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Monitor is stable and widely used, but not yet enforced in all publications.",
      "node_rationale": "Suggested explicitly in Monitoring section."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Incomparable performance claims in reinforcement learning research",
      "target_node": "Absence of standardized, versioned benchmark environments in reinforcement learning",
      "description": "Without a common, version-controlled set of tasks, performance numbers are not directly comparable.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Stated connection in Introduction and Design Decisions.",
      "edge_rationale": "Paper motivates Gym primarily to fix this gap."
    },
    {
      "type": "caused_by",
      "source_node": "Absence of standardized, versioned benchmark environments in reinforcement learning",
      "target_node": "Standardized interfaces and version control enable reproducible comparisons in reinforcement learning",
      "description": "Recognition of the problem leads to the insight that standardization will resolve it.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical inference from text; not quantitatively proven.",
      "edge_rationale": "Design rationale is built directly on this insight."
    },
    {
      "type": "mitigated_by",
      "source_node": "Standardized interfaces and version control enable reproducible comparisons in reinforcement learning",
      "target_node": "Provide common environment abstraction with strict versioning to support reproducibility",
      "description": "The design choice operationalizes the theoretical insight.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit in Design Decisions.",
      "edge_rationale": "Authors describe abstraction to realize insight."
    },
    {
      "type": "implemented_by",
      "source_node": "Provide common environment abstraction with strict versioning to support reproducibility",
      "target_node": "OpenAI Gym API offering diverse, versioned environments",
      "description": "API is the concrete software realization of the design rationale.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "API shown in code and detailed descriptions.",
      "edge_rationale": "Clear mapping from concept to implementation."
    },
    {
      "type": "validated_by",
      "source_node": "OpenAI Gym API offering diverse, versioned environments",
      "target_node": "Early community adoption shows reproducible performance comparisons across algorithms",
      "description": "Community use demonstrates that the API indeed allows reproducible benchmarking.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Anecdotal but multiple examples referenced.",
      "edge_rationale": "Validation evidence stems from adoption."
    },
    {
      "type": "motivates",
      "source_node": "Early community adoption shows reproducible performance comparisons across algorithms",
      "target_node": "Adopt OpenAI Gym versioned benchmark suite in RL algorithm development",
      "description": "Evidence encourages broader adoption of the benchmark suite.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical step recommended implicitly by authors.",
      "edge_rationale": "Community success used to motivate action."
    },
    {
      "type": "caused_by",
      "source_node": "Overfitting of reinforcement learning algorithms to specific benchmark tasks",
      "target_node": "Lack of hidden test sets and peer scrutiny in reinforcement learning evaluation",
      "description": "Over-fitting arises because researchers can tune directly on the evaluated tasks without external checks.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explained in Design Decisions.",
      "edge_rationale": "Paper presents problem and underlying cause."
    },
    {
      "type": "caused_by",
      "source_node": "Lack of hidden test sets and peer scrutiny in reinforcement learning evaluation",
      "target_node": "Public code sharing and peer review mitigate benchmark overfitting in reinforcement learning",
      "description": "Insight that opening code to scrutiny helps detect overfitting.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual argument, no empirical data.",
      "edge_rationale": "Authors argue for peer review."
    },
    {
      "type": "mitigated_by",
      "source_node": "Public code sharing and peer review mitigate benchmark overfitting in reinforcement learning",
      "target_node": "Require writeups and source code submission to encourage peer verification",
      "description": "Design implements the insight by mandating write-ups and code links.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit design feature.",
      "edge_rationale": "Direct mapping."
    },
    {
      "type": "implemented_by",
      "source_node": "Require writeups and source code submission to encourage peer verification",
      "target_node": "OpenAI Gym website scoreboard with writeup and code submission functionality",
      "description": "Website provides practical means to upload and display code and write-ups.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Website described and operational.",
      "edge_rationale": "Implementation of design."
    },
    {
      "type": "validated_by",
      "source_node": "OpenAI Gym website scoreboard with writeup and code submission functionality",
      "target_node": "Scoreboard entries with shared code enable replication and overfitting checks",
      "description": "Existing entries successfully reproduced by others confirm the mechanism’s effectiveness.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Qualitative evidence in paper.",
      "edge_rationale": "Validation via community use."
    },
    {
      "type": "motivates",
      "source_node": "Scoreboard entries with shared code enable replication and overfitting checks",
      "target_node": "Publish RL code and writeups on OpenAI Gym scoreboard for peer review",
      "description": "Demonstrated benefits inspire continued practice.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical consequence encouraged by authors.",
      "edge_rationale": "Evidence leads to recommended action."
    },
    {
      "type": "caused_by",
      "source_node": "Misleading evaluation focusing solely on final performance in reinforcement learning",
      "target_node": "Neglect of sample complexity metrics in reinforcement learning benchmarks",
      "description": "Ignoring learning curves means only final scores are reported, causing the misleading evaluation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit reasoning in Design Decisions.",
      "edge_rationale": "Problem analysis underlying risk."
    },
    {
      "type": "caused_by",
      "source_node": "Neglect of sample complexity metrics in reinforcement learning benchmarks",
      "target_node": "Monitoring learning curves provides insight into sample efficiency of reinforcement learning algorithms",
      "description": "Insight recognizes that capturing curves resolves omission.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual; not empirically proven in text.",
      "edge_rationale": "Authors’ stated reasoning."
    },
    {
      "type": "mitigated_by",
      "source_node": "Monitoring learning curves provides insight into sample efficiency of reinforcement learning algorithms",
      "target_node": "Instrument environments with default monitoring to capture learning progress",
      "description": "Design decision enforces monitoring based on insight.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit description in paper.",
      "edge_rationale": "Mapping from insight to design."
    },
    {
      "type": "implemented_by",
      "source_node": "Instrument environments with default monitoring to capture learning progress",
      "target_node": "Gym Monitor automatic logging of episodes, timesteps, and optional video recording",
      "description": "Monitor is the software component that realizes default logging.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Detailed implementation description.",
      "edge_rationale": "Direct mapping."
    },
    {
      "type": "validated_by",
      "source_node": "Gym Monitor automatic logging of episodes, timesteps, and optional video recording",
      "target_node": "Published Gym learning curves reveal sample complexity differences between algorithms",
      "description": "Community-shared learning curves demonstrate the Monitor’s utility.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Qualitative evidence in discussion.",
      "edge_rationale": "Validation of mechanism."
    },
    {
      "type": "motivates",
      "source_node": "Published Gym learning curves reveal sample complexity differences between algorithms",
      "target_node": "Log and share learning curves using Gym Monitor to evaluate sample efficiency",
      "description": "Successful examples encourage others to log and publish curves.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical persuasion path in paper.",
      "edge_rationale": "Evidence leads to proposed action."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2016-06-05T00:00:00Z"
    },
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "title",
      "value": "OpenAI Gym"
    },
    {
      "key": "authors",
      "value": [
        "Greg Brockman",
        "Vicki Cheung",
        "Ludwig Pettersson",
        "Jonas Schneider",
        "John Schulman",
        "Jie Tang",
        "Wojciech Zaremba"
      ]
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1606.01540"
    }
  ]
}