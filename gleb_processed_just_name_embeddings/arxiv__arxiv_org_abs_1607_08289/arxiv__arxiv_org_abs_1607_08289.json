{
  "nodes": [
    {
      "name": "Catastrophic value misalignment by superintelligent AI systems",
      "aliases": [
        "AI goal misalignment catastrophe",
        "Superintelligence misaligned with human values"
      ],
      "type": "concept",
      "description": "Potential for highly capable AI agents to pursue goals that conflict with human values, leading to large-scale harm or loss of control.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in 1 Introduction and 3 Discussion as the primary risk motivating the paper."
    },
    {
      "name": "Incorrect or incomplete goal specification in autonomous agents",
      "aliases": [
        "Misspecified AI objectives",
        "Inadequate goal structures"
      ],
      "type": "concept",
      "description": "Designers fail to encode the full complexity of human values, producing goal structures that diverge from intended outcomes.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 2.1 and domestic-robot example identify misspecification as proximal cause of misalignment."
    },
    {
      "name": "Orthogonality between intelligence and goals in AI systems",
      "aliases": [
        "Orthogonality thesis",
        "Intelligence–motivation independence"
      ],
      "type": "concept",
      "description": "An AI’s level of intelligence is independent of the final goals it pursues, so highly intelligent agents can optimise arbitrary objectives.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in §2.1 as theoretical reason mis-specified goals persist and scale."
    },
    {
      "name": "Anthropomorphic bias in AI goal assumptions",
      "aliases": [
        "Default human-like goal assumption",
        "Anthropomorphism bias"
      ],
      "type": "concept",
      "description": "Humans tend to assume AI systems will share human-like motivations, which obscures the need for explicit value alignment.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in §2.3 as additional source of design error."
    },
    {
      "name": "Inferring human values from observed behavior reduces misspecification risk",
      "aliases": [
        "Value learning via observation",
        "Behavior-based value inference"
      ],
      "type": "concept",
      "description": "Letting the AI learn human preferences by watching actions avoids brittle hand-coded objectives.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined in §2.2 as core theoretical solution to misspecification."
    },
    {
      "name": "Human values decomposable into mammalian values, cognition, cultural evolution",
      "aliases": [
        "Three-component human values",
        "Decomposition of human values"
      ],
      "type": "concept",
      "description": "Proposes that basic mammalian emotional systems underpin human values, which are then shaped by higher cognition and culture.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central thesis in §2.4 justifying use of mammalian priors."
    },
    {
      "name": "Shared mammalian emotional systems as priors for value models",
      "aliases": [
        "Mammalian value priors",
        "Common affective circuitry"
      ],
      "type": "concept",
      "description": "Seven evolutionary conserved emotion/motivation systems (SEEKING, RAGE, FEAR, LUST, CARE, PANIC/GRIEF, PLAY) provide initial structure for value learning.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in §2.4.1 as concrete content of mammalian values."
    },
    {
      "name": "Anthropomorphic design using mammalian value priors for AI",
      "aliases": [
        "Value-primitive-based design",
        "Mammalian-prior goal architecture"
      ],
      "type": "concept",
      "description": "Purposefully building AI systems whose initial motivational structure mirrors mammalian emotions to facilitate alignment.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposed in §2.3–2.4 as solution leveraging theoretical priors."
    },
    {
      "name": "Uncertain initial goal structure refined through observation",
      "aliases": [
        "Goal uncertainty at deployment",
        "Self-refining value model"
      ],
      "type": "concept",
      "description": "AI starts unsure about exact human values and updates its model continuously from behavioural evidence.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Expressed in Russell’s principle 2 and §2.2."
    },
    {
      "name": "Russell's three principles for human compatible AI",
      "aliases": [
        "Three san-francisco principles",
        "Russell 2016 alignment principles"
      ],
      "type": "concept",
      "description": "1) Maximise human values only, 2) remain uncertain about those values, 3) learn them from observation.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explicitly listed in §2.2 as guiding development rules."
    },
    {
      "name": "Inverse Reinforcement Learning on human behavior datasets",
      "aliases": [
        "IRL for value alignment",
        "IRL value inference"
      ],
      "type": "concept",
      "description": "Machine-learning technique that infers reward functions underlying observed actions, providing a computational method to learn human preferences.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Cited in §2.2 with Ng & Russell 2000 and subsequent work."
    },
    {
      "name": "Bayesian Inverse Planning for goal inference",
      "aliases": [
        "BIP for preference learning",
        "Probabilistic goal inference"
      ],
      "type": "concept",
      "description": "Uses Bayesian reasoning to infer utility functions or plans that best explain observed agent actions.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mentioned in §2.2 as alternative implementation to IRL."
    },
    {
      "name": "Learning from large-scale video datasets of mammalian and human behavior",
      "aliases": [
        "Video-based behavioural IRL",
        "Cross-species video value learning"
      ],
      "type": "concept",
      "description": "Using extensive corpora (YouTube, documentaries) as input to IRL/BIP to learn value structures across species.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Suggested in §2.4.1 as future data source for value inference."
    },
    {
      "name": "Neuroscientific identification of seven mammalian emotional systems",
      "aliases": [
        "Panksepp affective circuitry evidence",
        "Seven primal emotion systems"
      ],
      "type": "concept",
      "description": "Comparative neuro-anatomy links specific sub-cortical circuits to SEEKING, RAGE, FEAR, LUST, CARE, PANIC/GRIEF and PLAY, supporting the mammalian-value hypothesis.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Summarised in §2.4.1 with references to Panksepp & Biven 2012."
    },
    {
      "name": "Proof-of-concept IRL/BIP demonstrations recovering human intent",
      "aliases": [
        "Experimental IRL value recovery",
        "BIP proof-of-concept"
      ],
      "type": "concept",
      "description": "Controlled experiments (Evans et al. 2015; Riedl 2016) show that IRL/BIP can learn human-like reward functions in restricted environments.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Referenced in §2.2 as empirical support for implementation mechanisms."
    },
    {
      "name": "Initialize AI agents with mammalian value priors and goal uncertainty",
      "aliases": [
        "Mammalian-prior agent initialization",
        "Uncertain mammalian-value seed"
      ],
      "type": "intervention",
      "description": "During model-design phase encode the seven mammalian motivational systems as a probabilistic prior over reward functions and leave final value weights uncertain.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Relates to initial architectural choices discussed in §2.4 Mammalian Value Systems.",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Purely theoretical in the paper; no implementation yet.",
      "node_rationale": "Direct actionable step derived from anthropomorphic design rationale."
    },
    {
      "name": "Fine-tune AI agents via inverse reinforcement learning to align with human behavior",
      "aliases": [
        "IRL fine-tuning for alignment",
        "Behavior-based alignment via IRL"
      ],
      "type": "intervention",
      "description": "During fine-tuning/RL stage, optimise the agent’s policy against reward models inferred from human behavioural datasets using IRL algorithms.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Applies during RL or fine-tuning stage, see §2.2 IRL references.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Proof-of-concept studies exist but no large-scale deployment.",
      "node_rationale": "Operationalises the theoretical value-learning insight."
    },
    {
      "name": "Pre-train AI models on diverse video datasets of mammalian and human social interactions",
      "aliases": [
        "Video-pretraining for value learning",
        "Cross-species video IRL pre-training"
      ],
      "type": "intervention",
      "description": "Before or during pre-training, expose models to large-scale annotated/unannotated videos to bootstrap learning of shared value structures.",
      "concept_category": null,
      "intervention_lifecycle": "2",
      "intervention_lifecycle_rationale": "Occurs during data collection/pre-training phase as proposed in §2.4.1.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Suggested future work; limited prototypes only.",
      "node_rationale": "Implements anthropomorphic design and observational learning at scale."
    },
    {
      "name": "Embed Russell's three principles into agent objective architecture",
      "aliases": [
        "Russell-principle objective design",
        "Three-principle architectural embedding"
      ],
      "type": "intervention",
      "description": "Hard-code meta-objectives: maximise human values, remain uncertain, and learn from observation; e.g., by including self-regularising uncertainty terms and prohibiting intrinsic self-preservation reward.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Principles guide the creation of the core objective at design time (see §2.2).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Conceptual recommendation; no systematic implementation reported.",
      "node_rationale": "Translates Russell’s design rationale into a concrete engineering step."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Catastrophic value misalignment by superintelligent AI systems",
      "target_node": "Incorrect or incomplete goal specification in autonomous agents",
      "description": "Mis-specified goals are the immediate cause leading capable systems to act against human interests.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Strong consensus argument in §2.1 & §3; widely accepted theoretical link.",
      "edge_rationale": "Section 2.1 domestic-robot example illustrates causal chain."
    },
    {
      "type": "caused_by",
      "source_node": "Incorrect or incomplete goal specification in autonomous agents",
      "target_node": "Orthogonality between intelligence and goals in AI systems",
      "description": "Because goals are independent of capability, higher capability magnifies any specification errors.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Theoretical reasoning in §2.1, no empirical data.",
      "edge_rationale": "Orthogonality thesis provides mechanism for misspecification."
    },
    {
      "type": "caused_by",
      "source_node": "Incorrect or incomplete goal specification in autonomous agents",
      "target_node": "Anthropomorphic bias in AI goal assumptions",
      "description": "Designers’ unchecked anthropomorphic assumptions lead them to leave goals underspecified.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Qualitative discussion in §2.3; limited systematic evidence.",
      "edge_rationale": "Paper cites Hollywood trope as illustration."
    },
    {
      "type": "mitigated_by",
      "source_node": "Incorrect or incomplete goal specification in autonomous agents",
      "target_node": "Inferring human values from observed behavior reduces misspecification risk",
      "description": "Learning values from behaviour offers a dynamic alternative to brittle hand-specification.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoned argument with some supporting demonstrations.",
      "edge_rationale": "Outlined in §2.2."
    },
    {
      "type": "refined_by",
      "source_node": "Inferring human values from observed behavior reduces misspecification risk",
      "target_node": "Human values decomposable into mammalian values, cognition, cultural evolution",
      "description": "Decomposition clarifies what structure the inference process should model first.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Conceptual refinement given in §2.4.",
      "edge_rationale": "Section 2.4 introduces decomposition as refinement."
    },
    {
      "type": "refined_by",
      "source_node": "Human values decomposable into mammalian values, cognition, cultural evolution",
      "target_node": "Shared mammalian emotional systems as priors for value models",
      "description": "Identifies the first component—mammalian emotions—as concrete prior structure.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Supported by comparative neuroscience evidence.",
      "edge_rationale": "Detailed mapping in §2.4.1."
    },
    {
      "type": "enabled_by",
      "source_node": "Anthropomorphic design using mammalian value priors for AI",
      "target_node": "Shared mammalian emotional systems as priors for value models",
      "description": "Design approach is made possible by recognising shared emotional circuitry.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical enablement articulated in §2.3–2.4.",
      "edge_rationale": "Design rationale leans on theoretical prior."
    },
    {
      "type": "implemented_by",
      "source_node": "Anthropomorphic design using mammalian value priors for AI",
      "target_node": "Learning from large-scale video datasets of mammalian and human behavior",
      "description": "Large video corpora supply the behavioural data necessary to operationalise mammalian-prior design.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Future-work suggestion in §2.4.1.",
      "edge_rationale": "Paper proposes video learning as practical route."
    },
    {
      "type": "specified_by",
      "source_node": "Uncertain initial goal structure refined through observation",
      "target_node": "Inferring human values from observed behavior reduces misspecification risk",
      "description": "Goal uncertainty is a direct specification derived from the value-learning insight.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit linkage in §2.2.",
      "edge_rationale": "Russell principle 2 embodies uncertainty."
    },
    {
      "type": "implemented_by",
      "source_node": "Uncertain initial goal structure refined through observation",
      "target_node": "Inverse Reinforcement Learning on human behavior datasets",
      "description": "IRL algorithms operationalise continuous updating of the goal model.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "IRL presented as primary method in §2.2.",
      "edge_rationale": "Cited work (Ng & Russell 2000) directly used for implementation."
    },
    {
      "type": "implemented_by",
      "source_node": "Uncertain initial goal structure refined through observation",
      "target_node": "Bayesian Inverse Planning for goal inference",
      "description": "BIP is an alternative probabilistic implementation of goal-learning.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Referenced in §2.2 with supporting citations.",
      "edge_rationale": "Provides complementary technique."
    },
    {
      "type": "specified_by",
      "source_node": "Russell's three principles for human compatible AI",
      "target_node": "Inferring human values from observed behavior reduces misspecification risk",
      "description": "The principles are a concrete specification derived from the theoretical need to learn values.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Principles explicitly rest on the value-learning argument.",
      "edge_rationale": "Listed immediately after theory in §2.2."
    },
    {
      "type": "implemented_by",
      "source_node": "Russell's three principles for human compatible AI",
      "target_node": "Inverse Reinforcement Learning on human behavior datasets",
      "description": "IRL addresses principles 2 & 3 by learning from behaviour while representing uncertainty.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Argument made in §2.2 and supporting literature.",
      "edge_rationale": "IRL matches the principles’ requirements."
    },
    {
      "type": "validated_by",
      "source_node": "Inverse Reinforcement Learning on human behavior datasets",
      "target_node": "Proof-of-concept IRL/BIP demonstrations recovering human intent",
      "description": "Empirical studies show IRL can recover underlying reward structures.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Controlled experiments cited (Evans et al. 2015).",
      "edge_rationale": "Section 2.2 references these demonstrations."
    },
    {
      "type": "validated_by",
      "source_node": "Bayesian Inverse Planning for goal inference",
      "target_node": "Proof-of-concept IRL/BIP demonstrations recovering human intent",
      "description": "Same experimental corpus validates BIP approaches.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Same studies include probabilistic models.",
      "edge_rationale": "Evidence covers both algorithms."
    },
    {
      "type": "validated_by",
      "source_node": "Learning from large-scale video datasets of mammalian and human behavior",
      "target_node": "Proof-of-concept IRL/BIP demonstrations recovering human intent",
      "description": "Early demonstrations of IRL on narrative/video data support feasibility of scaling to larger corpora.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Paper notes future potential; limited existing evidence.",
      "edge_rationale": "Cited Riedl & Harrison 2016 for story IRL."
    },
    {
      "type": "validated_by",
      "source_node": "Shared mammalian emotional systems as priors for value models",
      "target_node": "Neuroscientific identification of seven mammalian emotional systems",
      "description": "Comparative neuro-anatomy and affective neuroscience empirically support the existence of these systems.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Multiple converging neuroscientific studies referenced.",
      "edge_rationale": "§2.4.1 summarises extensive evidence."
    },
    {
      "type": "motivates",
      "source_node": "Neuroscientific identification of seven mammalian emotional systems",
      "target_node": "Initialize AI agents with mammalian value priors and goal uncertainty",
      "description": "Evidence of universal mammalian emotions motivates encoding them as initial priors.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical conclusion drawn in paper; no direct experiment.",
      "edge_rationale": "§2.4.1 to §3 transition argues for this step."
    },
    {
      "type": "motivates",
      "source_node": "Proof-of-concept IRL/BIP demonstrations recovering human intent",
      "target_node": "Fine-tune AI agents via inverse reinforcement learning to align with human behavior",
      "description": "Successful small-scale IRL experiments motivate applying IRL during fine-tuning for real systems.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Demonstrations directly show method works in principle.",
      "edge_rationale": "§2.2 applications discussion."
    },
    {
      "type": "motivates",
      "source_node": "Proof-of-concept IRL/BIP demonstrations recovering human intent",
      "target_node": "Pre-train AI models on diverse video datasets of mammalian and human social interactions",
      "description": "IRL success encourages scaling to larger, more diverse datasets such as video corpora.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Extrapolation from existing studies; proposed future work.",
      "edge_rationale": "§2.4.1 future-work paragraph."
    },
    {
      "type": "motivates",
      "source_node": "Proof-of-concept IRL/BIP demonstrations recovering human intent",
      "target_node": "Embed Russell's three principles into agent objective architecture",
      "description": "Validation of value-learning techniques strengthens the case for embedding the principles that rely on such learning.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Principles depend on feasibility of value learning; experiments provide supporting evidence.",
      "edge_rationale": "Link discussed in §2.2."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2016-07-28T00:00:00Z"
    },
    {
      "key": "title",
      "value": "Mammalian Value Systems"
    },
    {
      "key": "authors",
      "value": [
        "Gopal P. Sarma",
        "Nick J. Hay"
      ]
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1607.08289"
    },
    {
      "key": "source",
      "value": "arxiv"
    }
  ]
}