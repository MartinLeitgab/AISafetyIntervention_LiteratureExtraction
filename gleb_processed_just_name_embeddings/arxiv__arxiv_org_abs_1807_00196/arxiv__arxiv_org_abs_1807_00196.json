{
  "nodes": [
    {
      "name": "Exploitation of agent by adversarial environment in decision-making tasks",
      "aliases": [
        "agent exploitation by reactive environment",
        "adversarial environments exploiting agent strategies"
      ],
      "type": "concept",
      "description": "Risk that an environment or other agents that can observe or infer an agent’s policy will adapt to minimise the agent’s payoff, leading to safety-relevant failures in multi-agent and deployment settings.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in Section 1 Introduction as a core motivation for learning to ‘detect friendly and adversarial behaviour’."
    },
    {
      "name": "Inadequate modeling of environment attitude continuum in multi-agent systems",
      "aliases": [
        "lack of unified friend-foe framework",
        "missing continuous attitude model"
      ],
      "type": "concept",
      "description": "Problem that existing approaches treat adversarial, stochastic and friendly cases separately, offering no single model that smoothly spans the spectrum, hindering robust policy design.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Sections 2 & 3 as the motivation for introducing the inverse-temperature formulation."
    },
    {
      "name": "Information asymmetry enabling environment reaction to agent strategy",
      "aliases": [
        "environment privileged access to agent policy",
        "spy-on-policy asymmetry"
      ],
      "type": "concept",
      "description": "Problem where the environment can observe or infer private aspects of the agent’s strategy, allowing strategic manipulation of payoffs, as illustrated in the bandit examples.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted by the bandit examples in Section 2 (Observations 1–4)."
    },
    {
      "name": "Continuous inverse-temperature parameterization of environment attitude",
      "aliases": [
        "β attitude model",
        "inverse temperature friendliness-adversarialness continuum"
      ],
      "type": "concept",
      "description": "Theoretical insight that a single real parameter β in an information-constrained objective controls both the sign (friendly vs adversarial) and magnitude (strength) of environmental reactions.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Formally introduced in Section 3.2 Friendly and Adversarial Environments."
    },
    {
      "name": "Mutual information as indicator of environment reactivity",
      "aliases": [
        "information channel reactivity test",
        "I(π;z|x) reactivity measure"
      ],
      "type": "concept",
      "description": "Theoretical insight that non-zero conditional mutual information between agent policy parameters and environment actions signals a reactive (possibly adversarial) environment, enabling detection.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in Section 6.3 Learning the attitude of an environment."
    },
    {
      "name": "Risk-sensitive equilibrium strategy derived from information-constrained game",
      "aliases": [
        "risk-sensitive equilibrium policy",
        "information-constrained best response design"
      ],
      "type": "concept",
      "description": "Design principle that, by solving the coupled free-energy objective, an agent can obtain strategies that hedge against varying friendliness/adversarialness while exploiting friendliness when present.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Derived in Section 3.3 and motivates algorithmic solution."
    },
    {
      "name": "Detection of environment attitude via mutual information estimation",
      "aliases": [
        "attitude detection mechanism",
        "friend-foe estimation design"
      ],
      "type": "concept",
      "description": "Design principle that the agent should monitor dependency between its private policy parameters and observed environment actions to infer whether the environment is friendly, neutral or adversarial.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined in Section 6.3 as a practical application of the mutual-information insight."
    },
    {
      "name": "Gibbs best-response equilibrium computation with exponential update algorithm",
      "aliases": [
        "iterative Gibbs updates",
        "equilibrium algorithm Eqs 6–9"
      ],
      "type": "concept",
      "description": "Implementation mechanism using smoothed exponential-manifold updates (Eqs 6–9) to converge to the fixed-point equilibrium strategies defined by the Gibbs best responses.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in Section 4 Computing Equilibria."
    },
    {
      "name": "Thompson sampling estimation of inverse temperature parameter β",
      "aliases": [
        "β online inference",
        "Bayesian attitude estimation"
      ],
      "type": "concept",
      "description": "Implementation mechanism where the agent treats β as a latent parameter, maintains a posterior over it and samples to adjust its policy online, enabling adaptive friend-foe detection.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposed in Section 6.3 as a concrete learning procedure."
    },
    {
      "name": "Bernoulli bandit experiments showing attitude-dependent payoffs",
      "aliases": [
        "two-armed bandit friendly vs adversarial results"
      ],
      "type": "concept",
      "description": "Validation evidence from Section 5.1 demonstrating that the equilibrium solutions reproduce intuitive friendly, indifferent and adversarial behaviours across four bandits.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Empirical data underpinning the framework’s behavioural claims."
    },
    {
      "name": "Gaussian bandit variance dependence experiments demonstrating strategic adaptation",
      "aliases": [
        "variance sweep bandit results",
        "phase transition bandit study"
      ],
      "type": "concept",
      "description": "Validation evidence (Section 5.2) that agent strategies shift toward higher-variance arms as β becomes friendly, illustrating nuanced equilibrium predictions.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Extends validation to continuous payoffs and variance effects."
    },
    {
      "name": "Linear classifier experiment showing adversarial label flipping and mitigation",
      "aliases": [
        "classifier friend-foe simulation",
        "label flipping study"
      ],
      "type": "concept",
      "description": "Validation evidence (Section 5.3) where a reactive environment flips labels and an agent counters by randomising its classifier, confirming detection and mitigation concepts.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Demonstrates applicability beyond bandits to supervised learning."
    },
    {
      "name": "Embed inverse-temperature friend-foe modeling into agent decision policies during model design",
      "aliases": [
        "integrate β attitude model in policy",
        "friend-foe aware policy architecture"
      ],
      "type": "intervention",
      "description": "At design time, include β-parameterised objective terms so the resulting architecture can explicitly optimise across the friendliness-adversarialness spectrum.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Derived from core framework presentation in Sections 3 & 4 (model design stage).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Only evaluated in simulations; proof-of-concept level.",
      "node_rationale": "Direct safety-relevant design implication of the theoretical framework."
    },
    {
      "name": "Compute risk-sensitive equilibrium strategies via iterative Gibbs updates during fine-tuning",
      "aliases": [
        "apply Gibbs equilibrium algorithm in training",
        "risk-sensitive fine-tuning"
      ],
      "type": "intervention",
      "description": "During fine-tuning or RL phase, run the exponential update algorithm to converge to equilibrium strategies that hedge against adversarial environments.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Implemented after pre-training when policies are being optimised (Section 4 algorithm).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated only on toy domains; experimental.",
      "node_rationale": "Transforms the design rationale into a concrete training procedure."
    },
    {
      "name": "Estimate environment attitude online with Thompson sampling and adapt policies during deployment",
      "aliases": [
        "online β estimation and adaptation",
        "deployment-time friend-foe detection"
      ],
      "type": "intervention",
      "description": "At deployment, maintain a Bayesian posterior over β, sample to detect attitude shifts and dynamically adjust the policy to remain robust against newly adversarial or friendly behaviours.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Executed while the system is interacting with a live environment (Section 6.3 discussion).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Proposed but not yet empirically validated; purely theoretical suggestion.",
      "node_rationale": "Provides a deploy-time safety layer that reacts to changing environment attitudes."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Exploitation of agent by adversarial environment in decision-making tasks",
      "target_node": "Inadequate modeling of environment attitude continuum in multi-agent systems",
      "description": "Without a unified model of friendliness vs adversarialness, agents cannot anticipate hostile reactions, enabling exploitation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit motivation in Sections 1–3; qualitative but well-argued.",
      "edge_rationale": "Paper states that poorly understood friend/foe detection underlies risk."
    },
    {
      "type": "mitigated_by",
      "source_node": "Inadequate modeling of environment attitude continuum in multi-agent systems",
      "target_node": "Continuous inverse-temperature parameterization of environment attitude",
      "description": "Introducing β provides the missing continuous model to capture all attitudes.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct textual claim in Section 3.2.",
      "edge_rationale": "β is proposed specifically to solve this modelling gap."
    },
    {
      "type": "mitigated_by",
      "source_node": "Continuous inverse-temperature parameterization of environment attitude",
      "target_node": "Risk-sensitive equilibrium strategy derived from information-constrained game",
      "description": "Using β within the coupled free-energy objective yields policies that hedge against adversaries while leveraging friendly behaviour.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Derivations in Section 3.3 connect β to equilibrium policies.",
      "edge_rationale": "Strategy derivation follows directly from theoretical formulation."
    },
    {
      "type": "implemented_by",
      "source_node": "Risk-sensitive equilibrium strategy derived from information-constrained game",
      "target_node": "Gibbs best-response equilibrium computation with exponential update algorithm",
      "description": "The iterative Gibbs algorithm operationalises the equilibrium strategy in practice.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Algorithm formally derived and proven to converge (Section 4).",
      "edge_rationale": "Implementation is provided as the practical means to obtain strategies."
    },
    {
      "type": "validated_by",
      "source_node": "Gibbs best-response equilibrium computation with exponential update algorithm",
      "target_node": "Bernoulli bandit experiments showing attitude-dependent payoffs",
      "description": "Running the algorithm on two-armed bandits reproduces expected friendly and adversarial outcomes.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Empirical results in Section 5.1; single-domain but systematic.",
      "edge_rationale": "Simulation confirms algorithm behaviour."
    },
    {
      "type": "validated_by",
      "source_node": "Gibbs best-response equilibrium computation with exponential update algorithm",
      "target_node": "Gaussian bandit variance dependence experiments demonstrating strategic adaptation",
      "description": "Algorithm explains phase transitions where optimal arms switch with β and variance.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 5.2 experimental data.",
      "edge_rationale": "Further validation across richer payoff structures."
    },
    {
      "type": "validated_by",
      "source_node": "Gibbs best-response equilibrium computation with exponential update algorithm",
      "target_node": "Linear classifier experiment showing adversarial label flipping and mitigation",
      "description": "Algorithm scales to classification, capturing label-flipping attacks and counter-strategies.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 5.3 experimental evidence.",
      "edge_rationale": "Demonstrates applicability beyond bandits."
    },
    {
      "type": "motivates",
      "source_node": "Bernoulli bandit experiments showing attitude-dependent payoffs",
      "target_node": "Compute risk-sensitive equilibrium strategies via iterative Gibbs updates during fine-tuning",
      "description": "Positive empirical results motivate employing Gibbs updates during training for safety-critical agents.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference that toy-domain success transfers to training procedures.",
      "edge_rationale": "Reasonable extrapolation to fine-tuning stage."
    },
    {
      "type": "motivates",
      "source_node": "Gaussian bandit variance dependence experiments demonstrating strategic adaptation",
      "target_node": "Compute risk-sensitive equilibrium strategies via iterative Gibbs updates during fine-tuning",
      "description": "Variance-dependent successes suggest broad utility of Gibbs updates in training.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Moderate inference from experiment to intervention.",
      "edge_rationale": "Highlights robustness benefits in varied conditions."
    },
    {
      "type": "motivates",
      "source_node": "Bernoulli bandit experiments showing attitude-dependent payoffs",
      "target_node": "Embed inverse-temperature friend-foe modeling into agent decision policies during model design",
      "description": "Demonstrates that β modelling yields predictable improvements, encouraging integration at design time.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Evidence is indirect but supportive.",
      "edge_rationale": "Design implication from experiment."
    },
    {
      "type": "motivates",
      "source_node": "Gaussian bandit variance dependence experiments demonstrating strategic adaptation",
      "target_node": "Embed inverse-temperature friend-foe modeling into agent decision policies during model design",
      "description": "Phase-change behaviours further argue for embedding β in the architecture itself.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Same as above—moderate inference.",
      "edge_rationale": "Supports architectural inclusion."
    },
    {
      "type": "caused_by",
      "source_node": "Exploitation of agent by adversarial environment in decision-making tasks",
      "target_node": "Information asymmetry enabling environment reaction to agent strategy",
      "description": "Risk arises because environments often possess privileged information about the agent’s policy.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Observation 5 in Section 2 underlines symmetry and spying.",
      "edge_rationale": "Direct textual claim."
    },
    {
      "type": "mitigated_by",
      "source_node": "Information asymmetry enabling environment reaction to agent strategy",
      "target_node": "Mutual information as indicator of environment reactivity",
      "description": "Measuring mutual information offers a way to detect whether an environment is leveraging asymmetry.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Proposed conceptually in Section 6.3; no formal proof.",
      "edge_rationale": "Conceptual mitigation path."
    },
    {
      "type": "mitigated_by",
      "source_node": "Mutual information as indicator of environment reactivity",
      "target_node": "Detection of environment attitude via mutual information estimation",
      "description": "Design turns the theoretical MI test into a concrete detection mechanism.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Design suggestion in Section 6.3.",
      "edge_rationale": "Applies insight to mechanism design."
    },
    {
      "type": "implemented_by",
      "source_node": "Detection of environment attitude via mutual information estimation",
      "target_node": "Thompson sampling estimation of inverse temperature parameter β",
      "description": "Bayesian Thompson sampling operationalises MI-based detection by inferring β online.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 6.3 provides explicit Bayesian procedure.",
      "edge_rationale": "Implementation detail for detection design."
    },
    {
      "type": "validated_by",
      "source_node": "Thompson sampling estimation of inverse temperature parameter β",
      "target_node": "Linear classifier experiment showing adversarial label flipping and mitigation",
      "description": "Classifier experiment illustrates how adjusting policy in response to estimated β mitigates adversarial flips.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Empirical evidence in Section 5.3 aligns with adaptation concept.",
      "edge_rationale": "Supports feasibility of online estimation."
    },
    {
      "type": "motivates",
      "source_node": "Linear classifier experiment showing adversarial label flipping and mitigation",
      "target_node": "Estimate environment attitude online with Thompson sampling and adapt policies during deployment",
      "description": "Successful adaptation in classifier domain motivates applying online β estimation in real deployments.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Extension from toy domain to deployment is inferential.",
      "edge_rationale": "Intervention follows naturally from evidence."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2018-06-30T00:00:00Z"
    },
    {
      "key": "title",
      "value": "Modeling Friends and Foes"
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1807.00196"
    },
    {
      "key": "authors",
      "value": [
        "Pedro A. Ortega",
        "Shane Legg"
      ]
    },
    {
      "key": "source",
      "value": "arxiv"
    }
  ]
}