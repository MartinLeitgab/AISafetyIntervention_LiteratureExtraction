{
  "nodes": [
    {
      "name": "negative side effects from reward misspecification in reinforcement learning",
      "aliases": [
        "reward misspecification harms",
        "unsafe RL side effects"
      ],
      "type": "concept",
      "description": "Real-world deployment of RL agents can cause unintended environmental or self-damage when the reward function does not capture all designer preferences.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in Introduction as the primary safety risk motivating the work."
    },
    {
      "name": "incomplete reward specification in complex environments",
      "aliases": [
        "reward function misspecification mechanism",
        "unmodelled reward aspects"
      ],
      "type": "concept",
      "description": "Designers cannot feasibly enumerate and balance rewards for every relevant feature in rich environments, leading to gaps that agents exploit.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Introduction as the mechanism producing the above risk."
    },
    {
      "name": "maxmin formulation over consistent reward polytope in reinforcement learning",
      "aliases": [
        "adversarial reward formulation",
        "max-min safe RL"
      ],
      "type": "concept",
      "description": "Model the unknown reward as any weight vector consistent with expert knowledge; view policy selection as a zero-sum game where an adversary picks the worst consistent reward.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 2–3 present the theoretical framing enabling robust policy design."
    },
    {
      "name": "maxmin policy maximizes worst-case reward to ensure safety",
      "aliases": [
        "worst-case optimal policy rationale",
        "robust policy objective"
      ],
      "type": "concept",
      "description": "Choosing the policy with the highest minimum reward across the consistent weight set guarantees that performance never falls below a bound, mitigating misspecification.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivated in Section 4 as the goal guiding algorithm design."
    },
    {
      "name": "combining multiple expert demonstrations to define consistent reward polytope",
      "aliases": [
        "multi-expert constraint aggregation",
        "cross-environment expert fusion"
      ],
      "type": "concept",
      "description": "Intersect constraints derived from several expert policies operating in different environments to shrink the reward uncertainty set, improving robustness.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3 ‘Combining Multiple Experts’ explains this capability."
    },
    {
      "name": "ellipsoid method with separation oracles using MDP solver for exact maxmin policy",
      "aliases": [
        "ellipsoid-based robust RL algorithm",
        "exact maxmin solver"
      ],
      "type": "concept",
      "description": "Uses the equivalence of separation and optimization to iteratively query an exact MDP solver, solving the linear program for the max-min objective in polynomial time.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in Section 4.1 as Implementation Algorithm 1–2."
    },
    {
      "name": "FPL-based iterative maxmin policy learning with MDP solver",
      "aliases": [
        "follow-the-perturbed-leader robust RL",
        "online maxmin learning algorithm"
      ],
      "type": "concept",
      "description": "Applies Follow-the-Perturbed-Leader to alternate between best-response policies and adversarial rewards, requiring only polynomial-time MDP calls per iteration and converging at O(1/√T).",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4.2 introduces Algorithm 3 implementing this mechanism."
    },
    {
      "name": "weird separation oracle enabling approximate MDP solver usage",
      "aliases": [
        "WSO for approximate solvers",
        "approximate separation oracle"
      ],
      "type": "concept",
      "description": "A non-convex acceptance oracle compatible with additive-error MDP solvers, allowing the FPL framework to function when only approximate planning is feasible.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 8 extends the implementation to practical large-scale settings."
    },
    {
      "name": "polynomial-time solvability guarantee for ellipsoid maxmin algorithm",
      "aliases": [
        "Theorem 1 guarantee",
        "exact solver complexity bound"
      ],
      "type": "concept",
      "description": "Theorem 1 proves that, given an exact MDP solver and separation oracle, the ellipsoid method finds the optimal max-min policy in time polynomial in the MDP size.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Validation result in Section 4.1."
    },
    {
      "name": "O(1/√T) convergence guarantee for FPL maxmin algorithm",
      "aliases": [
        "Theorem 3 bound",
        "FPL regret analysis"
      ],
      "type": "concept",
      "description": "Theorem 3 shows that after T iterations the learned policy’s worst-case reward is within O(1/√T) of optimal with high probability.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Provided in Section 4.2."
    },
    {
      "name": "gridworld experiments showing avoidance of unseen harmful terrain",
      "aliases": [
        "gridworld safety empirical evidence",
        "terrain avoidance results"
      ],
      "type": "concept",
      "description": "In a 50×50 grid with a terrain type absent from demonstrations, the learned maxmin policy completely avoids the unknown terrain, unlike the baseline.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 5 ‘Gridworld’ empirical results."
    },
    {
      "name": "cartpole experiments demonstrating safe behavior with multiple expert policies",
      "aliases": [
        "cartpole dual-expert evidence",
        "continuous control safety proof-of-concept"
      ],
      "type": "concept",
      "description": "When trained with two expert policies, the maxmin learner safely exploits positive features and avoids negative ones, outperforming single-expert training.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 5 ‘CartPole’ empirical study."
    },
    {
      "name": "train RL agents with ellipsoid-based exact maxmin learning during policy optimisation",
      "aliases": [
        "apply exact maxmin solver",
        "ellipsoid training protocol"
      ],
      "type": "intervention",
      "description": "During fine-tuning/RL phase, invoke the ellipsoid method coupled with an exact MDP solver and separation oracles to compute the policy that maximises worst-case reward.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Implemented during policy optimisation (Section 4.1 Ellipsoid-Method-Based Solution).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated on toy domains only; proof-of-concept stage.",
      "node_rationale": "Actionable step proposed implicitly by the exact algorithm section."
    },
    {
      "name": "train RL agents with FPL-based iterative maxmin learning during policy optimisation",
      "aliases": [
        "apply FPL robust training",
        "online maxmin RL training"
      ],
      "type": "intervention",
      "description": "Iteratively alternate between best-response policy updates and adversarial reward selection using FPL, stopping once convergence bounds are acceptable.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Used during policy optimisation loop (Section 4.2 Finding the Maxmin Policy).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Validated by theoretical bounds and multiple prototype experiments.",
      "node_rationale": "Central practical intervention recommended by the paper."
    },
    {
      "name": "construct consistent reward polytope from multiple expert demonstrations before training",
      "aliases": [
        "build multi-expert reward constraints",
        "reward uncertainty set construction"
      ],
      "type": "intervention",
      "description": "Aggregate linear constraints derived from each expert’s near-optimality condition across different environments, yielding the convex set of consistent reward weights used for subsequent robust training.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Executed as a preprocessing step integral to the training pipeline (Section 3 Combining Multiple Experts).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated in experiments but not yet industrially deployed.",
      "node_rationale": "Enables the algorithms to leverage diverse expert data for greater safety."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "negative side effects from reward misspecification in reinforcement learning",
      "target_node": "incomplete reward specification in complex environments",
      "description": "Side effects arise because designers miss aspects of the environment when specifying rewards.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicitly stated in Introduction linking misspecification to harms.",
      "edge_rationale": "Introduction"
    },
    {
      "type": "caused_by",
      "source_node": "incomplete reward specification in complex environments",
      "target_node": "maxmin formulation over consistent reward polytope in reinforcement learning",
      "description": "The formulation is proposed to handle the adversarial uncertainty created by incomplete specification.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical derivation in Section 2–3; not experimentally proven.",
      "edge_rationale": "Sections 2,3"
    },
    {
      "type": "mitigated_by",
      "source_node": "maxmin formulation over consistent reward polytope in reinforcement learning",
      "target_node": "maxmin policy maximizes worst-case reward to ensure safety",
      "description": "Using the max-min objective directly mitigates the vulnerability to misspecification.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Design argument given in Section 4.",
      "edge_rationale": "Section 4"
    },
    {
      "type": "implemented_by",
      "source_node": "maxmin policy maximizes worst-case reward to ensure safety",
      "target_node": "ellipsoid method with separation oracles using MDP solver for exact maxmin policy",
      "description": "The ellipsoid method provides an exact computational realisation of the design rationale.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Direct constructive algorithm (Theorem 1).",
      "edge_rationale": "Section 4.1"
    },
    {
      "type": "implemented_by",
      "source_node": "maxmin policy maximizes worst-case reward to ensure safety",
      "target_node": "FPL-based iterative maxmin policy learning with MDP solver",
      "description": "FPL algorithm realises the same design goal with better practical efficiency.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Algorithm 3 explicitly mapped to design objective.",
      "edge_rationale": "Section 4.2"
    },
    {
      "type": "mitigated_by",
      "source_node": "maxmin formulation over consistent reward polytope in reinforcement learning",
      "target_node": "combining multiple expert demonstrations to define consistent reward polytope",
      "description": "Incorporating multiple experts further reduces uncertainty, strengthening mitigation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 3 discusses the benefit theoretically.",
      "edge_rationale": "Section 3"
    },
    {
      "type": "implemented_by",
      "source_node": "combining multiple expert demonstrations to define consistent reward polytope",
      "target_node": "weird separation oracle enabling approximate MDP solver usage",
      "description": "The oracle operationalises multi-expert constraints when only approximate solvers are available.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implementation relies on moderate inference from Section 8.",
      "edge_rationale": "Section 8"
    },
    {
      "type": "validated_by",
      "source_node": "ellipsoid method with separation oracles using MDP solver for exact maxmin policy",
      "target_node": "polynomial-time solvability guarantee for ellipsoid maxmin algorithm",
      "description": "Theorem 1 confirms polynomial-time correctness of the ellipsoid mechanism.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Formal proof in Section 4.1.",
      "edge_rationale": "Section 4.1"
    },
    {
      "type": "validated_by",
      "source_node": "FPL-based iterative maxmin policy learning with MDP solver",
      "target_node": "O(1/√T) convergence guarantee for FPL maxmin algorithm",
      "description": "Theorem 3 provides theoretical performance bounds.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Explicit theorem and proof in Section 4.2.",
      "edge_rationale": "Section 4.2"
    },
    {
      "type": "validated_by",
      "source_node": "FPL-based iterative maxmin policy learning with MDP solver",
      "target_node": "gridworld experiments showing avoidance of unseen harmful terrain",
      "description": "Empirically demonstrates safety in discrete environment.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Prototype experiments in Section 5.",
      "edge_rationale": "Section 5 Gridworld"
    },
    {
      "type": "validated_by",
      "source_node": "FPL-based iterative maxmin policy learning with MDP solver",
      "target_node": "cartpole experiments demonstrating safe behavior with multiple expert policies",
      "description": "Shows applicability to continuous control and multi-expert settings.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Prototype experiments in Section 5.",
      "edge_rationale": "Section 5 CartPole"
    },
    {
      "type": "validated_by",
      "source_node": "weird separation oracle enabling approximate MDP solver usage",
      "target_node": "polynomial-time solvability guarantee for ellipsoid maxmin algorithm",
      "description": "The theoretical framework ensures the oracle interacts correctly with optimisation routines.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implicit connection; oracle built on similar separation-optimisation equivalence.",
      "edge_rationale": "Section 8"
    },
    {
      "type": "motivates",
      "source_node": "polynomial-time solvability guarantee for ellipsoid maxmin algorithm",
      "target_node": "train RL agents with ellipsoid-based exact maxmin learning during policy optimisation",
      "description": "Proof of feasibility encourages adoption of the exact training protocol.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Strong theoretical evidence.",
      "edge_rationale": "Section 4.1"
    },
    {
      "type": "motivates",
      "source_node": "O(1/√T) convergence guarantee for FPL maxmin algorithm",
      "target_node": "train RL agents with FPL-based iterative maxmin learning during policy optimisation",
      "description": "Convergence bound motivates using FPL algorithm in practice.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit theorem supports intervention efficacy.",
      "edge_rationale": "Section 4.2"
    },
    {
      "type": "motivates",
      "source_node": "gridworld experiments showing avoidance of unseen harmful terrain",
      "target_node": "train RL agents with FPL-based iterative maxmin learning during policy optimisation",
      "description": "Empirical safety gains encourage the FPL-based training intervention.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Prototype experimental evidence.",
      "edge_rationale": "Section 5 Gridworld"
    },
    {
      "type": "motivates",
      "source_node": "cartpole experiments demonstrating safe behavior with multiple expert policies",
      "target_node": "train RL agents with FPL-based iterative maxmin learning during policy optimisation",
      "description": "Continuous-control evidence motivates broader adoption.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Prototype experimental evidence.",
      "edge_rationale": "Section 5 CartPole"
    },
    {
      "type": "motivates",
      "source_node": "polynomial-time solvability guarantee for ellipsoid maxmin algorithm",
      "target_node": "construct consistent reward polytope from multiple expert demonstrations before training",
      "description": "The guarantee relies on efficient polytope construction, motivating this preprocessing intervention.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Indirect relationship; moderate inference.",
      "edge_rationale": "Sections 3 & 4"
    }
  ],
  "meta": [
    {
      "key": "title",
      "value": "Learning Safe Policies with Expert Guidance"
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1805.08313"
    },
    {
      "key": "date_published",
      "value": "2018-05-21T00:00:00Z"
    },
    {
      "key": "authors",
      "value": [
        "Jessie Huang",
        "Fa Wu",
        "Doina Precup",
        "Yang Cai"
      ]
    },
    {
      "key": "source",
      "value": "arxiv"
    }
  ]
}