{
  "nodes": [
    {
      "name": "Competitive AI arms race from inability to share AI systems",
      "aliases": [
        "AI deployment race due to non-shareable agents",
        "Race dynamics among principals"
      ],
      "type": "concept",
      "description": "Risk that states, companies or individuals build separate, rapidly escalating AI systems instead of jointly owning one because shared systems cannot satisfy all principals, increasing accident and conflict likelihood.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined in Conclusion – ‘Implications for bargaining versus racing’ as a principal safety concern."
    },
    {
      "name": "Misallocation of utility among principals in multi-principal AI agents",
      "aliases": [
        "Unfair priority distribution",
        "Principal dissatisfaction"
      ],
      "type": "concept",
      "description": "Risk that a shared AI consistently favours some principals’ preferences over others, undermining cooperation and leading to conflict or abandonment of the system.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in Introduction and cake-betting example as the immediate failure mode of naïve utility aggregation."
    },
    {
      "name": "Differing priors between principals in shared decision-making contexts",
      "aliases": [
        "Belief disagreement among principals",
        "Different world-models"
      ],
      "type": "concept",
      "description": "Principals commonly hold distinct probabilistic beliefs about the environment; these disagreements are persistent and form the core difficulty in joint policy choice.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed under ‘Common knowledge assumptions’ and formalised throughout Section 3."
    },
    {
      "name": "Pareto suboptimality from naive fixed utility aggregation in multi-principal agents",
      "aliases": [
        "Failure of linear aggregation with fixed weights",
        "Static weighted-sum limitation"
      ],
      "type": "concept",
      "description": "Using a single fixed linear combination of principals’ utilities ignores belief differences and cannot implement some Pareto-optimal policies, as proven in Proposition 3.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central negative result in Example: cake betting and subsequent proposition."
    },
    {
      "name": "Bet-settling mechanism emerges from belief disagreements",
      "aliases": [
        "Implicit wagering between principals",
        "Observation-contingent preference shifts"
      ],
      "type": "concept",
      "description": "Theory that when principals disagree, optimal policies effectively settle bets: whichever principal’s prediction matches observations gains more influence.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained qualitatively in Introduction and formalised leading up to Theorem 2."
    },
    {
      "name": "Dynamic priority shifting proportional to observation likelihood enables Pareto optimality",
      "aliases": [
        "Likelihood-weighted priority update",
        "Observation-based weight adjustment"
      ],
      "type": "concept",
      "description": "Key insight: weights on principals’ utilities must be multiplied by the likelihood their priors assign to the agent’s observation history to remain Pareto-optimal over time.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Captured formally by the Pareto-optimal control theorem in Section 3."
    },
    {
      "name": "Use principals' individual models to evaluate expected utilities",
      "aliases": [
        "Model-specific expectation calculation",
        "Per-principal world-model evaluation"
      ],
      "type": "concept",
      "description": "Design principle that the agent must compute each principal’s expected utility using that principal’s own transition and observation model rather than a shared one.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explicit requirement (item 1) in theorem statement Section 3 ‘Pareto-optimal control theorem’."
    },
    {
      "name": "Represent agent objective as weighted mixture POMDP",
      "aliases": [
        "POMDP mixture formulation",
        "Stochastic principal selection model"
      ],
      "type": "concept",
      "description": "Construct a new single POMDP whose initial hidden variable selects a principal according to weights; this converts multi-principal optimisation to classical single-principal optimisation.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Defined in ‘Characterizing Pareto-optimality probabilistically’ with Figure 2 and Mixture Lemma."
    },
    {
      "name": "Likelihood-weighted backward recursion for action selection (Pareto-optimal control theorem)",
      "aliases": [
        "Dynamic weight Bellman recursion",
        "Pareto-optimal control algorithm"
      ],
      "type": "concept",
      "description": "Implementation mechanism: at each time-step, maximise a sum of each principal’s expected utility multiplied by likelihood of observation history under that principal, using backward recursion.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Equation block in theorem provides explicit recurrence for policy components."
    },
    {
      "name": "Full-memory policy framework storing observation history",
      "aliases": [
        "History-dependent policy",
        "Green nodes in Figure 1"
      ],
      "type": "concept",
      "description": "Assumes the agent’s policy retains the entire observation/action history enabling dynamic re-weighting; forms the base on which the recursion operates.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted in Section 3 ‘Serving multiple principals as a single POMDP’ and Figure 1."
    },
    {
      "name": "Cake betting scenario demonstrating Pareto improvement over static split",
      "aliases": [
        "Cake-color toy example",
        "Bet-settling toy model"
      ],
      "type": "concept",
      "description": "Numerical example where a policy that gives the whole cake to the colour-predicted winner yields expected utility 27 each, outperforming 20-20 static split, validating theory.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Example table and calculations in ‘Example: cake betting’ section."
    },
    {
      "name": "Formal proof of Pareto-optimal control theorem",
      "aliases": [
        "Mathematical validation",
        "Convexity and Bayes proof"
      ],
      "type": "concept",
      "description": "Rigorous derivation combining Polytope Lemma, Mixture Lemma and Bellman optimality to prove necessity and sufficiency of likelihood-weighted recursion.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proof chain across subsections ‘Characterizing Pareto-optimality geometrically/probabilistically’ and theorem proof."
    },
    {
      "name": "Design AI agents with dynamic likelihood-weighted utility aggregation for multi-principal scenarios",
      "aliases": [
        "Implement Pareto-optimal control theorem",
        "Dynamic re-weighting agent design"
      ],
      "type": "intervention",
      "description": "During model design, specify the agent’s objective as the likelihood-weighted sum of each principal’s utility as observations arrive, following the backward-recursion algorithm.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Applies at architecture specification stage – ‘Future work’ and theorem sections discuss designing such agents before training/deployment.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Currently theoretical with toy example only; no large-scale empirical validation (Conclusion, Future work).",
      "node_rationale": "Direct actionable step derived from theorem to avoid identified risks."
    },
    {
      "name": "Include bet-settling clauses in multi-principal AI contracts",
      "aliases": [
        "Contractual bet-settling mechanisms",
        "Explicit observation-contingent payoff terms"
      ],
      "type": "intervention",
      "description": "When drafting usage or ownership contracts, embed rules that automatically adjust payoffs or control rights according to which principals’ predictions are borne out, mirroring the agent’s internal re-weighting.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Occurs during deployment/operation agreements between principals (Section 4 ‘Implications for contract design’).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Conceptual suggestion only; no prototypes described (Conclusion).",
      "node_rationale": "Translates theoretical insight into organisational practice to reduce bargaining failure."
    },
    {
      "name": "Facilitate prior-sharing protocols between principals before agent creation",
      "aliases": [
        "Belief-sharing negotiation",
        "Common-knowledge establishment"
      ],
      "type": "intervention",
      "description": "Run structured information-sharing sessions or mechanism-design tools so that principals reach common knowledge (or persistent disagreement) of each other’s priors, satisfying theorem prerequisites.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Takes place pre-deployment while principals negotiate agent design (‘Common knowledge assumptions’ and ‘Future work’).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Suggested as future research; no implementations yet.",
      "node_rationale": "Addresses prerequisite for dynamic weighting to function and thereby mitigates risks."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Competitive AI arms race from inability to share AI systems",
      "target_node": "Pareto suboptimality from naive fixed utility aggregation in multi-principal agents",
      "description": "When shared agents are Pareto-suboptimal, principals prefer separate systems, triggering competitive dynamics.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical argument made in Conclusion ‘Implications for bargaining versus racing’, moderate empirical support.",
      "edge_rationale": "Risk flows from identified problem analysis in text."
    },
    {
      "type": "caused_by",
      "source_node": "Misallocation of utility among principals in multi-principal AI agents",
      "target_node": "Pareto suboptimality from naive fixed utility aggregation in multi-principal agents",
      "description": "Static aggregation fails to reflect observation evidence, leading to chronic utility misallocation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Supported by formal impossibility proof in Proposition 3.",
      "edge_rationale": "Problem analysis directly yields the risk."
    },
    {
      "type": "caused_by",
      "source_node": "Pareto suboptimality from naive fixed utility aggregation in multi-principal agents",
      "target_node": "Differing priors between principals in shared decision-making contexts",
      "description": "Suboptimality arises specifically when principals’ beliefs diverge.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Demonstrated analytically in theorem and example sections.",
      "edge_rationale": "Foundation laid in Introduction & Section 3."
    },
    {
      "type": "mitigated_by",
      "source_node": "Pareto suboptimality from naive fixed utility aggregation in multi-principal agents",
      "target_node": "Dynamic priority shifting proportional to observation likelihood enables Pareto optimality",
      "description": "Likelihood-based dynamic weighting resolves suboptimality.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Theorem shows necessity and sufficiency; strong theoretical evidence.",
      "edge_rationale": "Core claim of paper."
    },
    {
      "type": "enabled_by",
      "source_node": "Dynamic priority shifting proportional to observation likelihood enables Pareto optimality",
      "target_node": "Use principals' individual models to evaluate expected utilities",
      "description": "Observation-weighted scheme relies on computing expectations under each principal’s own model.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit item 1 in theorem statement.",
      "edge_rationale": "Design requirement stated in text."
    },
    {
      "type": "enabled_by",
      "source_node": "Dynamic priority shifting proportional to observation likelihood enables Pareto optimality",
      "target_node": "Represent agent objective as weighted mixture POMDP",
      "description": "POMDP mixture turns multi-principal weighting into single POMDP optimisation supporting dynamic shifts.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Mixture Lemma establishes equivalence; moderate inference for ‘enables’.",
      "edge_rationale": "Logical bridge in Section 3."
    },
    {
      "type": "implemented_by",
      "source_node": "Represent agent objective as weighted mixture POMDP",
      "target_node": "Likelihood-weighted backward recursion for action selection (Pareto-optimal control theorem)",
      "description": "Recursion is the concrete optimisation method derived from mixture formulation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct derivation in theorem proof.",
      "edge_rationale": "Implementation follows design."
    },
    {
      "type": "required_by",
      "source_node": "Likelihood-weighted backward recursion for action selection (Pareto-optimal control theorem)",
      "target_node": "Full-memory policy framework storing observation history",
      "description": "Algorithm presumes access to full history to compute likelihoods.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Stated assumption in ‘Full-memory assumption’.",
      "edge_rationale": "Mechanistic dependency."
    },
    {
      "type": "validated_by",
      "source_node": "Likelihood-weighted backward recursion for action selection (Pareto-optimal control theorem)",
      "target_node": "Cake betting scenario demonstrating Pareto improvement over static split",
      "description": "Toy scenario shows algorithm achieves higher expected utilities.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Numerical evidence in table supports claim.",
      "edge_rationale": "Example acts as empirical validation."
    },
    {
      "type": "validated_by",
      "source_node": "Likelihood-weighted backward recursion for action selection (Pareto-optimal control theorem)",
      "target_node": "Formal proof of Pareto-optimal control theorem",
      "description": "Mathematical proof confirms correctness and necessity.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Rigorous derivation with no unstated assumptions.",
      "edge_rationale": "Proof validates mechanism."
    },
    {
      "type": "motivates",
      "source_node": "Cake betting scenario demonstrating Pareto improvement over static split",
      "target_node": "Design AI agents with dynamic likelihood-weighted utility aggregation for multi-principal scenarios",
      "description": "Demonstrated utility gains encourage adopting dynamic aggregation in agent design.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Example provides persuasive but limited evidence.",
      "edge_rationale": "Practical takeaway from example."
    },
    {
      "type": "motivates",
      "source_node": "Formal proof of Pareto-optimal control theorem",
      "target_node": "Design AI agents with dynamic likelihood-weighted utility aggregation for multi-principal scenarios",
      "description": "Proof ensures designers that intervention is theoretically sound.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Deductive certainty from theorem.",
      "edge_rationale": "Drives confidence to implement."
    },
    {
      "type": "motivates",
      "source_node": "Dynamic priority shifting proportional to observation likelihood enables Pareto optimality",
      "target_node": "Include bet-settling clauses in multi-principal AI contracts",
      "description": "Insight that bet-settling is inherent suggests making it explicit contractually.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Suggested in implications section, limited empirical backing.",
      "edge_rationale": "Moderate inference turning theoretical point into contractual action."
    },
    {
      "type": "motivates",
      "source_node": "Differing priors between principals in shared decision-making contexts",
      "target_node": "Facilitate prior-sharing protocols between principals before agent creation",
      "description": "Because differing priors are root cause, sharing information mitigates risk.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Future work suggestion; conceptual link.",
      "edge_rationale": "Intervention tackles analysed problem."
    },
    {
      "type": "addressed_by",
      "source_node": "Competitive AI arms race from inability to share AI systems",
      "target_node": "Include bet-settling clauses in multi-principal AI contracts",
      "description": "Explicit bet-settling clauses make shared systems more attractive, reducing racing incentives.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Argumentative claim in Conclusion, speculative.",
      "edge_rationale": "Pathway from risk to organisational intervention."
    },
    {
      "type": "addressed_by",
      "source_node": "Misallocation of utility among principals in multi-principal AI agents",
      "target_node": "Design AI agents with dynamic likelihood-weighted utility aggregation for multi-principal scenarios",
      "description": "Dynamic aggregation ensures fairer allocation, mitigating misalignment.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct consequence of theorem & example.",
      "edge_rationale": "Intervention solves stated risk."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1711.00363"
    },
    {
      "key": "title",
      "value": "Servant of Many Masters: Shifting priorities in Pareto-optimal sequential decision-making"
    },
    {
      "key": "authors",
      "value": [
        "Andrew Critch",
        "Stuart Russell"
      ]
    },
    {
      "key": "date_published",
      "value": "2017-10-31T00:00:00Z"
    },
    {
      "key": "source",
      "value": "arxiv"
    }
  ]
}