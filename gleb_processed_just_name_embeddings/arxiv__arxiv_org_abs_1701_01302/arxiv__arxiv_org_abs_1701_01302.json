{
  "nodes": [
    {
      "name": "AI development arms race between nations due to non-cooperative deployment",
      "aliases": [
        "AI arms race",
        "competitive AI development risk"
      ],
      "type": "concept",
      "description": "Risk that states race to build increasingly powerful AI systems, reducing safety margins because they cannot agree on joint ownership or policy.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Introduction: potential arms race if bargaining over shared AI fails."
    },
    {
      "name": "Unfair value representation in multi-principal AI systems",
      "aliases": [
        "stakeholder misalignment",
        "compromise failure"
      ],
      "type": "concept",
      "description": "Risk that jointly owned AI policies disproportionately favour one stakeholder’s principles, leading to conflict or rejection of cooperation.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted in Introduction and Section 5 as undesirable outcome of naïve aggregation."
    },
    {
      "name": "Lack of attractive cooperative bargaining mechanism for shared AI control",
      "aliases": [
        "cooperative bargaining difficulty"
      ],
      "type": "concept",
      "description": "Without simple procedures to specify how an AI will trade off stakeholders’ preferences, parties default to non-cooperation.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Framed in Introduction as key obstacle to cooperation."
    },
    {
      "name": "Naive linear utility aggregation incompatible with Pareto optimality under differing beliefs",
      "aliases": [
        "fixed weight utility sum failure"
      ],
      "type": "concept",
      "description": "Simply maximising a static weighted sum of utilities fails to be Pareto-optimal when principals hold different priors, leading to sub-optimal or unacceptable policies.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposition 10 and Section 3.5 formally show inadequacy."
    },
    {
      "name": "Pareto optimal policy requires belief-weighted dynamic priority shifting",
      "aliases": [
        "dynamic re-weighting insight",
        "priority shifting theorem"
      ],
      "type": "concept",
      "description": "Theorem 8 proves that a Pareto-optimal sequential policy must re-weight each stakeholder’s expected utility over time in proportion to how well their beliefs predict observations.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Core theoretical result (Section 3.4)."
    },
    {
      "name": "Use of each stakeholder's own beliefs to evaluate expected utility",
      "aliases": [
        "belief-specific evaluation",
        "subjective expectations requirement"
      ],
      "type": "concept",
      "description": "For Pareto optimality the AI must compute each principal’s expected utility using that principal’s subjective model, not a shared model.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Component (1) of Theorem 8 listed in Abstract and Conclusion."
    },
    {
      "name": "Negotiable reinforcement learning via Pareto optimality recursion",
      "aliases": [
        "negotiable RL",
        "Pareto recursion based RL"
      ],
      "type": "concept",
      "description": "Design approach where the AI implements the recursion from Theorem 8 to decide actions, effectively settling bets between stakeholders across time.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined in Sections 3.4 and 3.5 as solution framework."
    },
    {
      "name": "POMDP mixture with coin-flip weight initialization",
      "aliases": [
        "weighted POMDP mixture",
        "latent B variable model"
      ],
      "type": "concept",
      "description": "Implementation treats environment as mixture of stakeholders’ POMDPs, chosen by latent variable B sampled with agreed weights, enabling Bayesian-style update of priorities.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Definition 4 constructs this mechanism."
    },
    {
      "name": "Policy mixing through random seed to create convex policy space",
      "aliases": [
        "randomised policy mixing",
        "convex combination of policies"
      ],
      "type": "concept",
      "description": "Machine can pre-sample a random seed to follow one of several policies with given probabilities, ensuring convexity needed for Pareto boundary characterisation.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Policy mixing assumption in Section 3.2 enables optimisation proof."
    },
    {
      "name": "Priority weight update proportional to predictive accuracy of beliefs",
      "aliases": [
        "predictive accuracy weighting",
        "belief score re-weighting"
      ],
      "type": "concept",
      "description": "During execution, the AI multiplies initial utility weights by likelihood of observed data under each stakeholder’s model, achieving dynamic reprioritisation.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explicit in Theorem 8 equations."
    },
    {
      "name": "Mathematical proof of Pareto recursion (Theorem 8)",
      "aliases": [
        "proof of priority shifting",
        "formal theorem 8"
      ],
      "type": "concept",
      "description": "Rigorous derivation showing necessity and sufficiency of the recursion for Pareto optimality in compatible POMDPs.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.4 presents full proof."
    },
    {
      "name": "Cake-splitting scenario demonstration of naive aggregation failure",
      "aliases": [
        "cake example",
        "bet-settling illustration"
      ],
      "type": "concept",
      "description": "Worked example in Section 3.5 shows that fixed linear utility aggregation cannot implement Pareto-optimal policy, validating need for new mechanism.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposition 10 illustrates empirical relevance."
    },
    {
      "name": "Develop AI bargaining module implementing negotiable RL with dynamic priority shifting",
      "aliases": [
        "implement Pareto recursion module",
        "negotiable RL system"
      ],
      "type": "intervention",
      "description": "Design and implement a module that executes the Pareto-optimal priority-shifting recursion during decision making, using stakeholders’ elicited priors and utilities.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Applies at model design stage where decision-making algorithm is chosen (see Section 3.4).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Currently theoretical with proof-of-concept examples only; no deployed systems (Sections 5 & Conclusion).",
      "node_rationale": "Direct practical instantiation of design rationale for AI safety."
    },
    {
      "name": "Use belief elicitation and policy mixing during AI fine-tuning to enable stakeholder bet-settling",
      "aliases": [
        "stakeholder belief elicitation for RL",
        "policy mixing fine-tuning"
      ],
      "type": "intervention",
      "description": "During fine-tuning/RL training, elicit each stakeholder’s belief model, sample initial weights, and mix candidate policies via random seeds to approximate convex Pareto solutions.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Executed during fine-tuning/RLHF-like phase when utilities and beliefs can still be injected (Section 3.2 Policy mixing assumption).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Not yet implemented; inferred practical step from theoretical mechanism (marked as foundational).",
      "node_rationale": "Operationalises implementation mechanisms 8 & 9 for training."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "AI development arms race between nations due to non-cooperative deployment",
      "target_node": "Lack of attractive cooperative bargaining mechanism for shared AI control",
      "description": "Without bargaining mechanisms, parties revert to competitive strategies, creating arms race.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit causal claim in Introduction.",
      "edge_rationale": "Introduction lines linking bargaining difficulty to arms race risk."
    },
    {
      "type": "caused_by",
      "source_node": "Unfair value representation in multi-principal AI systems",
      "target_node": "Naive linear utility aggregation incompatible with Pareto optimality under differing beliefs",
      "description": "Static aggregation mis-represents some stakeholders, leading to unfair outcomes.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Proposition 10 shows failure leads to stakeholder disadvantage.",
      "edge_rationale": "Section 3.5 discussion."
    },
    {
      "type": "caused_by",
      "source_node": "Lack of attractive cooperative bargaining mechanism for shared AI control",
      "target_node": "Naive linear utility aggregation incompatible with Pareto optimality under differing beliefs",
      "description": "Main obstacle stems from reliance on inadequate aggregation method.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Argument in Sections 3.1–3.3.",
      "edge_rationale": "Paper contrasts naïve aggregation with required mechanism."
    },
    {
      "type": "addressed_by",
      "source_node": "Naive linear utility aggregation incompatible with Pareto optimality under differing beliefs",
      "target_node": "Pareto optimal policy requires belief-weighted dynamic priority shifting",
      "description": "Theoretical insight provides alternative that satisfies Pareto optimality.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct statement in Theorem 8.",
      "edge_rationale": "Section 3.4 replaces naïve approach."
    },
    {
      "type": "implemented_by",
      "source_node": "Pareto optimal policy requires belief-weighted dynamic priority shifting",
      "target_node": "Negotiable reinforcement learning via Pareto optimality recursion",
      "description": "Design rationale operationalises theoretical requirement.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Design described immediately after theorem.",
      "edge_rationale": "Section 3.4 explanation."
    },
    {
      "type": "implemented_by",
      "source_node": "Negotiable reinforcement learning via Pareto optimality recursion",
      "target_node": "POMDP mixture with coin-flip weight initialization",
      "description": "Mixture model realises negotiable RL mathematically.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Definition 4 gives construction used in proofs.",
      "edge_rationale": "Section 3.3."
    },
    {
      "type": "implemented_by",
      "source_node": "Negotiable reinforcement learning via Pareto optimality recursion",
      "target_node": "Policy mixing through random seed to create convex policy space",
      "description": "Randomised policy mixing needed for convex Pareto optimisation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Policy mixing assumption Section 3.2.",
      "edge_rationale": "Section 3.2."
    },
    {
      "type": "implemented_by",
      "source_node": "Negotiable reinforcement learning via Pareto optimality recursion",
      "target_node": "Priority weight update proportional to predictive accuracy of beliefs",
      "description": "Dynamic re-weighting is core algorithmic step.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Equation set in Theorem 8.",
      "edge_rationale": "Section 3.4 equations."
    },
    {
      "type": "validated_by",
      "source_node": "POMDP mixture with coin-flip weight initialization",
      "target_node": "Mathematical proof of Pareto recursion (Theorem 8)",
      "description": "Proof relies on mixture construction to show optimality.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Formal derivation uses Definition 4.",
      "edge_rationale": "Proof of Theorem 8."
    },
    {
      "type": "validated_by",
      "source_node": "Priority weight update proportional to predictive accuracy of beliefs",
      "target_node": "Mathematical proof of Pareto recursion (Theorem 8)",
      "description": "Weight update rule derived and proven necessary.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Contained in theorem proof.",
      "edge_rationale": "Section 3.4 proof."
    },
    {
      "type": "validated_by",
      "source_node": "Naive linear utility aggregation incompatible with Pareto optimality under differing beliefs",
      "target_node": "Cake-splitting scenario demonstration of naive aggregation failure",
      "description": "Example empirically illustrates theoretical failure.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit worked example.",
      "edge_rationale": "Section 3.5."
    },
    {
      "type": "motivates",
      "source_node": "Mathematical proof of Pareto recursion (Theorem 8)",
      "target_node": "Develop AI bargaining module implementing negotiable RL with dynamic priority shifting",
      "description": "Proof provides theoretical foundation to implement module.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Motivation link rather than empirical test.",
      "edge_rationale": "Conclusion encourages implementation."
    },
    {
      "type": "motivates",
      "source_node": "Cake-splitting scenario demonstration of naive aggregation failure",
      "target_node": "Use belief elicitation and policy mixing during AI fine-tuning to enable stakeholder bet-settling",
      "description": "Scenario shows need for policy mixing and belief elicitation during training.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Example indicates practical training requirement.",
      "edge_rationale": "Section 3.5 discussion."
    },
    {
      "type": "enabled_by",
      "source_node": "Develop AI bargaining module implementing negotiable RL with dynamic priority shifting",
      "target_node": "Policy mixing through random seed to create convex policy space",
      "description": "Module depends on ability to mix policies randomly.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implementation inference (not detailed in paper).",
      "edge_rationale": "Based on Section 3.2 implementation suggestion."
    },
    {
      "type": "enabled_by",
      "source_node": "Develop AI bargaining module implementing negotiable RL with dynamic priority shifting",
      "target_node": "Priority weight update proportional to predictive accuracy of beliefs",
      "description": "Dynamic weighting algorithm is internal to bargaining module.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implementation inference from theory.",
      "edge_rationale": "Derived from Theorem 8."
    },
    {
      "type": "enabled_by",
      "source_node": "Use belief elicitation and policy mixing during AI fine-tuning to enable stakeholder bet-settling",
      "target_node": "POMDP mixture with coin-flip weight initialization",
      "description": "Fine-tuning step needs mixture representation of stakeholder models.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Practical inference; mechanism defined in paper but training use inferred.",
      "edge_rationale": "Linking Definition 4 to proposed fine-tuning."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2017-01-05T00:00:00Z"
    },
    {
      "key": "title",
      "value": "Toward negotiable reinforcement learning: shifting priorities in Pareto optimal sequential decision-making"
    },
    {
      "key": "authors",
      "value": [
        "Andrew Critch"
      ]
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1701.01302"
    },
    {
      "key": "source",
      "value": "arxiv"
    }
  ]
}