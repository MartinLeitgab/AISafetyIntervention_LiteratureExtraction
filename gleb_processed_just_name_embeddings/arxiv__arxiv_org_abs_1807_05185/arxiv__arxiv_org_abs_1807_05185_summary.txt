Summary  
Data source overview: The paper “Model Reconstruction from Model Explanations’’ proves—both theoretically (Theorem 1) and empirically (MNIST/CIFAR10 experiments)—that providing input-gradient based saliency maps allows an adversary to reconstruct a model orders of magnitude faster than with prediction-only access. This turns widely-used explanation APIs into vectors for proprietary-model and data leakage.  

Extraction confidence[82]  
Inference strategy justification: The source does not propose explicit defences, so moderate inference (confidence 2) was used to derive the most straightforward mitigation mechanisms (differential-privacy noise, rate-limiting, and privacy-oriented explanation redesign) that logically follow the authors’ discussion of leakage risk and of SmoothGrad’s ineffectiveness.  
Extraction completeness explanation: All reasoning steps appearing in the paper—from the high-level commercial risk, through gradient exposure, theoretical identifiability, practical reconstruction results, to actionable mitigation ideas—are captured. Every node is connected; no isolated components remain.  
Key limitations: 1) No empirical validation for differential-privacy defences is contained in the paper, so confidence for those edges is low. 2) The fabric reflects only gradient-based leakage; broader threats (e.g., data poisoning) are outside scope. 3) Naming consistency was optimised, but some future papers may introduce overlapping terminology that still needs manual reconciliation.



Improvements to instruction set:  
1. Clarify whether validation evidence must evaluate the *defensive* mechanism or can simply substantiate the threat; current wording forced indirect mapping.  
2. Provide explicit guidance for handling cases where the source proposes no mitigation (common in offence-focused papers).  
3. Consider allowing multiple acceptable path variants (e.g., risk→problem→theory→evidence→design rationale) when chronological order in original work differs from the template.