{
  "nodes": [
    {
      "name": "Suboptimal neural network architectures in deep learning systems",
      "aliases": [
        "inefficient model architectures",
        "hand-designed NN architecture limitations"
      ],
      "type": "concept",
      "description": "Deployment of manually crafted or poorly explored model designs can yield inaccurate, brittle or inefficient AI behaviour, undermining reliability and overall system safety.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in 1 Introduction as the motivating problem of expert-designed architectures."
    },
    {
      "name": "Manual architecture design requiring expert knowledge and limited exploration",
      "aliases": [
        "human-crafted architectures",
        "expert-centric architecture engineering"
      ],
      "type": "concept",
      "description": "Current practice depends on experts iterating by hand, which is time-consuming and often explores only a narrow subspace of possible networks.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in 1 Introduction and 2 Related Work as a key obstacle."
    },
    {
      "name": "Fixed-length hyperparameter optimisation limitations",
      "aliases": [
        "static HPO space constraint",
        "Bayesian opt fixed-length issue"
      ],
      "type": "concept",
      "description": "Traditional hyper-parameter search tools optimise over a pre-specified vector space and cannot easily generate variable-length structures.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in 2 Related Work comparing Bayesian optimisation methods."
    },
    {
      "name": "Slow heuristic-dependent neuro-evolution search methods",
      "aliases": [
        "scalability limits of NEAT-style evolution",
        "large-scale neuro-evolution bottleneck"
      ],
      "type": "concept",
      "description": "Evolutionary algorithms can explore variable-length models but scale poorly and depend on many ad-hoc heuristics.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted in 2 Related Work."
    },
    {
      "name": "Architecture encoding as variable-length string generated by controller RNN",
      "aliases": [
        "sequence representation of NN architectures",
        "string-based architecture specification"
      ],
      "type": "concept",
      "description": "Observes that layer types, filter sizes, connections etc. can be serialised as a token sequence that a recurrent network can emit.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.1 explains treating architectures as sequences."
    },
    {
      "name": "Validation accuracy of sampled architecture as reinforcement-learning reward",
      "aliases": [
        "accuracy-based reward signal",
        "non-differentiable performance metric as reward"
      ],
      "type": "concept",
      "description": "Uses the held-out validation accuracy of each child network as a scalar reward enabling policy-gradient optimisation despite non-differentiability.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.2 Training with REINFORCE."
    },
    {
      "name": "Use of policy-gradient reinforcement learning to train controller",
      "aliases": [
        "REINFORCE-based controller learning",
        "policy gradient for NAS"
      ],
      "type": "concept",
      "description": "Design decision to maximise expected reward via the REINFORCE algorithm with a moving-average baseline for variance reduction.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Justified throughout Section 3.2."
    },
    {
      "name": "Parallel distributed asynchronous training scheme for NAS",
      "aliases": [
        "parameter-server NAS scaling",
        "multi-replica controller training"
      ],
      "type": "concept",
      "description": "Introduces a parameter-server architecture with many controller replicas and parallel child model training to cut end-to-end search time.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.2 Accelerate Training with Parallelism."
    },
    {
      "name": "Controller RNN generates architecture tokens for child network",
      "aliases": [
        "sequence-to-architecture emission",
        "controller sampling process"
      ],
      "type": "concept",
      "description": "Concrete mechanism where each time-step predicts filter sizes, strides, layer types etc., stopping after a scheduled depth.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Visualised in Figure 2, Section 3.1."
    },
    {
      "name": "Child network training and reward collection",
      "aliases": [
        "train-to-convergence child model",
        "validation-based reward logging"
      ],
      "type": "concept",
      "description": "Each sampled architecture is instantiated, trained for a fixed number of epochs, and its validation accuracy recorded as reward.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in Section 3.1–3.2."
    },
    {
      "name": "REINFORCE gradient update with baseline",
      "aliases": [
        "variance-reduced policy gradient",
        "baseline-subtracted REINFORCE"
      ],
      "type": "concept",
      "description": "Computes an unbiased but lower-variance gradient by subtracting a moving-average reward baseline before parameter update.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Equation in Section 3.2."
    },
    {
      "name": "Set-selection attention for skip-connection prediction",
      "aliases": [
        "anchor-point skip connection mechanism",
        "probabilistic skip selection"
      ],
      "type": "concept",
      "description": "Adds content-based sigmoids at anchor points allowing the controller to propose residual or branching connections.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.3 Increase Architecture Complexity."
    },
    {
      "name": "Tree-based encoding of recurrent cell computation graph",
      "aliases": [
        "NAS recurrent cell search tree",
        "base-8 cell architecture encoding"
      ],
      "type": "concept",
      "description": "Represents recurrent cell operations as labelled tree nodes enabling the controller to choose combination and activation functions.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.4 Generate Recurrent Cell Architectures."
    },
    {
      "name": "NAS ConvNet achieves 3.65% error on CIFAR-10 with faster inference",
      "aliases": [
        "CIFAR-10 NAS validation evidence",
        "3.65 error NAS result"
      ],
      "type": "concept",
      "description": "Best automatically discovered 39-layer model attains 3.65% test error and runs 1.05× faster than DenseNet baseline.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Results table 1 in Section 4.1."
    },
    {
      "name": "NAS recurrent cell achieves 62.4 perplexity on Penn Treebank",
      "aliases": [
        "PTB NAS cell evidence",
        "62.4 perplexity result"
      ],
      "type": "concept",
      "description": "Automatically generated recurrent cell outperforms prior state-of-the-art LSTM models by 3.6 perplexity points.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Table 2 in Section 4.2."
    },
    {
      "name": "NAS cell improves GNMT BLEU by 0.5 without retuning",
      "aliases": [
        "GNMT transfer evidence",
        "0.5 BLEU gain NAS cell"
      ],
      "type": "concept",
      "description": "Direct substitution of NAS cell into an existing translation system yields measurable BLEU improvement demonstrating transferability.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Transfer learning subsection, Section 4.2."
    },
    {
      "name": "Conduct reinforcement-learning-based Neural Architecture Search during model design",
      "aliases": [
        "apply NAS controller",
        "automated architecture search"
      ],
      "type": "intervention",
      "description": "During model design, employ an RNN controller trained with policy-gradient reinforcement learning and distributed asynchronous execution to automatically generate, evaluate and select high-performing neural network architectures.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Applies at model-design time before any pre-training; referenced in 3 Methods.",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Demonstrated via prototype-level large-scale experiments on CIFAR-10 and PTB (Section 4 Experiments).",
      "node_rationale": "Represents the actionable procedure proposed by the paper to mitigate the identified risk."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Suboptimal neural network architectures in deep learning systems",
      "target_node": "Manual architecture design requiring expert knowledge and limited exploration",
      "description": "Reliance on limited human design directly results in sub-optimal or unsafe architectures.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit statement in 1 Introduction linking manual design burden to performance limits.",
      "edge_rationale": "Section 1 emphasises performance gaps due to manual architecture crafting."
    },
    {
      "type": "caused_by",
      "source_node": "Manual architecture design requiring expert knowledge and limited exploration",
      "target_node": "Fixed-length hyperparameter optimisation limitations",
      "description": "Manual design pressure is exacerbated because common HPO tools cannot propose variable-length structures, pushing work back to experts.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "2 Related Work explicitly contrasts fixed-length HPO with manual design.",
      "edge_rationale": "Section 2 paragraphs on Bayesian optimisation."
    },
    {
      "type": "caused_by",
      "source_node": "Manual architecture design requiring expert knowledge and limited exploration",
      "target_node": "Slow heuristic-dependent neuro-evolution search methods",
      "description": "Alternative automatic techniques (neuro-evolution) are impractically slow, leaving experts to step in.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 2 directly states evolutionary methods are less practical at scale.",
      "edge_rationale": "2 Related Work."
    },
    {
      "type": "mitigated_by",
      "source_node": "Manual architecture design requiring expert knowledge and limited exploration",
      "target_node": "Architecture encoding as variable-length string generated by controller RNN",
      "description": "Encoding architectures as sequences makes automated generation feasible, reducing reliance on experts.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Clear methodological proposal in Section 3.1 with worked example.",
      "edge_rationale": "3.1 Generate Model Descriptions."
    },
    {
      "type": "enabled_by",
      "source_node": "Architecture encoding as variable-length string generated by controller RNN",
      "target_node": "Validation accuracy of sampled architecture as reinforcement-learning reward",
      "description": "Sequence emission allows a reinforcement-learning loop where the emitted architecture can be trained and evaluated for reward.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Mechanistic description across 3.1–3.2.",
      "edge_rationale": "Section 3.1–3.2 flow."
    },
    {
      "type": "specified_by",
      "source_node": "Validation accuracy of sampled architecture as reinforcement-learning reward",
      "target_node": "Use of policy-gradient reinforcement learning to train controller",
      "description": "Non-differentiable reward requires policy-gradient methods to optimise controller parameters.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Mathematical derivation in Section 3.2.",
      "edge_rationale": "Equation under 3.2 Training with REINFORCE."
    },
    {
      "type": "implemented_by",
      "source_node": "Use of policy-gradient reinforcement learning to train controller",
      "target_node": "Controller RNN generates architecture tokens for child network",
      "description": "Policy defines categorical distributions over token outputs, realised by the controller RNN sampling mechanism.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Figure 2 concrete implementation.",
      "edge_rationale": "3.1 Generate Model Descriptions."
    },
    {
      "type": "required_by",
      "source_node": "Controller RNN generates architecture tokens for child network",
      "target_node": "Child network training and reward collection",
      "description": "Each sampled token sequence must be instantiated and trained to provide the reward signal.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Process order described in 3.1.",
      "edge_rationale": "3.1 narrative."
    },
    {
      "type": "refined_by",
      "source_node": "Controller RNN generates architecture tokens for child network",
      "target_node": "Set-selection attention for skip-connection prediction",
      "description": "Skip-connection module enhances controller’s token generation capabilities, increasing search expressiveness.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 3.3 presents as extension.",
      "edge_rationale": "3.3 Increase Architecture Complexity."
    },
    {
      "type": "refined_by",
      "source_node": "Controller RNN generates architecture tokens for child network",
      "target_node": "Tree-based encoding of recurrent cell computation graph",
      "description": "Tree encoding extends token generation to recurrent cell structures for sequence tasks.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 3.4 treats as modification of core mechanism.",
      "edge_rationale": "3.4 Generate Recurrent Cell Architectures."
    },
    {
      "type": "validated_by",
      "source_node": "Controller RNN generates architecture tokens for child network",
      "target_node": "NAS ConvNet achieves 3.65% error on CIFAR-10 with faster inference",
      "description": "Generated ConvNet’s performance empirically demonstrates effectiveness of the mechanism.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Experimental result with clear metric improvement (Section 4.1).",
      "edge_rationale": "Table 1."
    },
    {
      "type": "validated_by",
      "source_node": "Tree-based encoding of recurrent cell computation graph",
      "target_node": "NAS recurrent cell achieves 62.4 perplexity on Penn Treebank",
      "description": "Empirical gains in language modeling validate the recurrent-cell search mechanism.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 4.2 reports state-of-the-art result.",
      "edge_rationale": "Table 2."
    },
    {
      "type": "validated_by",
      "source_node": "Tree-based encoding of recurrent cell computation graph",
      "target_node": "NAS cell improves GNMT BLEU by 0.5 without retuning",
      "description": "Transfer experiment further confirms practical utility.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Sub-section Transfer Learning provides single experiment.",
      "edge_rationale": "Transfer learning subsection."
    },
    {
      "type": "motivates",
      "source_node": "NAS ConvNet achieves 3.65% error on CIFAR-10 with faster inference",
      "target_node": "Conduct reinforcement-learning-based Neural Architecture Search during model design",
      "description": "Positive vision results encourage adoption of NAS in design pipelines.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Quantitative superiority over human designs presented in Section 4.1.",
      "edge_rationale": "Discussion below Table 1."
    },
    {
      "type": "motivates",
      "source_node": "NAS recurrent cell achieves 62.4 perplexity on Penn Treebank",
      "target_node": "Conduct reinforcement-learning-based Neural Architecture Search during model design",
      "description": "Language modeling evidence further supports general applicability of NAS intervention.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 4.2 emphasises new state-of-the-art.",
      "edge_rationale": "Results paragraph after Table 2."
    },
    {
      "type": "motivates",
      "source_node": "NAS cell improves GNMT BLEU by 0.5 without retuning",
      "target_node": "Conduct reinforcement-learning-based Neural Architecture Search during model design",
      "description": "Transferability to machine translation suggests production readiness of intervention.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Single BLEU score improvement reported; limited runs.",
      "edge_rationale": "Transfer learning subsection."
    },
    {
      "type": "enabled_by",
      "source_node": "Use of policy-gradient reinforcement learning to train controller",
      "target_node": "Parallel distributed asynchronous training scheme for NAS",
      "description": "Scaling the optimisation relies on distributed asynchronous updates compatible with REINFORCE.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 3.2 explains need for parallelism due to slow child training.",
      "edge_rationale": "Accelerate Training subsection."
    },
    {
      "type": "implemented_by",
      "source_node": "Parallel distributed asynchronous training scheme for NAS",
      "target_node": "Conduct reinforcement-learning-based Neural Architecture Search during model design",
      "description": "Parallel scheme forms part of the practical recipe for carrying out the intervention.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Implementation details in Figure 3 referenced in intervention description.",
      "edge_rationale": "Section 3.2 description of parameter-server setup."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2016-11-05T00:00:00Z"
    },
    {
      "key": "title",
      "value": "Neural Architecture Search with Reinforcement Learning"
    },
    {
      "key": "authors",
      "value": [
        "Barret Zoph",
        "Quoc V. Le"
      ]
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1611.01578"
    },
    {
      "key": "source",
      "value": "arxiv"
    }
  ]
}