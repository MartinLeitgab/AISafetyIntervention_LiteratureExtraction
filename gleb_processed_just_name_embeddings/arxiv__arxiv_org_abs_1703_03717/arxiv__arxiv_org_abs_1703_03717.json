{
  "nodes": [
    {
      "name": "Confound-driven misgeneralization in differentiable models",
      "aliases": [
        "spurious correlation reliance",
        "dataset confound generalization gap"
      ],
      "type": "concept",
      "description": "When training data contain subtle confounds, neural networks can achieve high training accuracy yet fail catastrophically on shifted test data, posing safety risks in sensitive domains such as healthcare.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in Section 1 Introduction as the core safety concern motivating the paper."
    },
    {
      "name": "Opaque decision boundaries in neural networks",
      "aliases": [
        "uninterpretable model behavior",
        "black-box decision logic"
      ],
      "type": "concept",
      "description": "Because standard neural networks do not expose their internal rules, it is difficult for domain experts to see whether predictions rely on medically or ethically incorrect features.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed under 1 Introduction and 1.1 Related Work as the mechanism hiding confound use."
    },
    {
      "name": "Ineffectiveness of standard regularization under dataset confounds",
      "aliases": [
        "regularization gap with distribution shift",
        "failure of sparsity/L2 to remove confounds"
      ],
      "type": "concept",
      "description": "Traditional techniques such as weight decay or sparsity do not guarantee that a model will avoid confounding features when the train and test distributions differ qualitatively.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted in Introduction as a limitation of existing robustness methods."
    },
    {
      "name": "Input gradient explanations approximate local decision boundaries",
      "aliases": [
        "gradient-based local explanations",
        "first-order decision boundary description"
      ],
      "type": "concept",
      "description": "The gradient of the predicted log-probability with respect to inputs lies normal to the decision boundary, providing an efficient, differentiable and faithful explanation of how each input dimension influences the prediction.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Defined formally in Section 1.2 Background and motivates subsequent methods."
    },
    {
      "name": "Constraining input gradients on annotated irrelevant features",
      "aliases": [
        "gradient penalty for right reasons",
        "explanation-based regularization"
      ],
      "type": "concept",
      "description": "By shrinking the magnitude of input gradients at positions marked irrelevant by domain experts, the model is biased away from relying on those features, aligning its reasoning with prior knowledge.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in Section 2.1 as the main strategy to obtain ‘right for the right reasons’ models."
    },
    {
      "name": "Iterative penalization of salient gradients to uncover alternative rules",
      "aliases": [
        "find-another-explanation rationale",
        "successive gradient masking"
      ],
      "type": "concept",
      "description": "When annotations are unavailable, repeatedly identifying high-magnitude gradients and penalising them forces the optimiser to discover new, qualitatively different decision boundaries.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Presented in Section 2.2 as a way to explore model ambiguity without human labels."
    },
    {
      "name": "Loss function with input gradient penalty term",
      "aliases": [
        "gradient-regularized cross-entropy",
        "RRR loss"
      ],
      "type": "concept",
      "description": "The training objective adds an L2 penalty on the log-probability input gradients at annotated positions, alongside cross-entropy and weight decay, enabling end-to-end gradient-based optimisation.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Equation in Section 2.1 specifies the exact loss used in experiments."
    },
    {
      "name": "Annotation matrix specifying irrelevant input dimensions",
      "aliases": [
        "binary mask A",
        "human-provided irrelevance labels"
      ],
      "type": "concept",
      "description": "A binary matrix A ∈ {0,1}^{N×D} marks which input dimensions should have near-zero gradient for each training example, supplying domain knowledge to the loss.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Defined in Section 2.1 as part of the penalty implementation."
    },
    {
      "name": "Find-another-explanation iterative training algorithm",
      "aliases": [
        "FAE procedure",
        "successive gradient masking algorithm"
      ],
      "type": "concept",
      "description": "The algorithm trains a model, extracts gradients, builds a mask of the largest components, retrains with that mask penalised, and repeats, generating a spectrum of accurate yet diverse classifiers.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Algorithm detailed in Section 2.2 with formal definition."
    },
    {
      "name": "Improved generalization on Decoy MNIST after gradient penalties",
      "aliases": [
        "Decoy-MNIST RRR result",
        "55→98% test accuracy recovery"
      ],
      "type": "concept",
      "description": "Applying gradient penalties to corner swatches raises test accuracy from 55.2 % to the baseline 98 % on decoy MNIST, showing the method removes confound reliance.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Result graphed in Figure 9 under 3.2 Real-world Datasets."
    },
    {
      "name": "Recovered baseline accuracy on Iris-Cancer dataset using gradient constraints",
      "aliases": [
        "Iris-Cancer confound removal result",
        "maintained accuracy without Iris features"
      ],
      "type": "concept",
      "description": "Penalising gradients on Iris features yields identical accuracy whether those features are present or removed at test time, confirming reliance on cancer-specific signals instead of Iris confounds.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Shown in Figure 8 in 3.2 Real-world Datasets."
    },
    {
      "name": "Discovery of diverse classifiers on 20 Newsgroups without accuracy loss",
      "aliases": [
        "FAE 20NG result",
        "multiple rules with stable accuracy"
      ],
      "type": "concept",
      "description": "Successive FAE iterations yield qualitatively different word-based explanations while maintaining high accuracy, indicating multiple valid rules were found.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Demonstrated in Figure 10, supporting the iterative approach."
    },
    {
      "name": "Faster yet faithful explanations compared to LIME",
      "aliases": [
        "gradient vs LIME runtime and fidelity",
        "explanation efficiency result"
      ],
      "type": "concept",
      "description": "Across four datasets, gradient explanations are orders of magnitude faster (µs vs seconds) while matching or exceeding LIME in feature attribution fidelity.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Table 1 and Figures 5-7 provide the empirical comparison."
    },
    {
      "name": "Train models with input-gradient regularization to minimize confound reliance",
      "aliases": [
        "apply RRR during training",
        "gradient-penalised model training"
      ],
      "type": "intervention",
      "description": "During model training, add an L2 penalty on input gradients at annotated irrelevant positions, using the RRR loss to steer the optimiser away from spurious correlations.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Applies at Fine-Tuning/Training phase as described in Section 2.1 loss formulation.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated only in controlled experiments across several datasets (Section 3 Empirical Evaluation).",
      "node_rationale": "Direct actionable step inferred from implementation mechanism and validated by Decoy-MNIST and Iris-Cancer results."
    },
    {
      "name": "Apply find-another-explanation training to discover diverse decision boundaries",
      "aliases": [
        "use FAE for rule exploration",
        "iterative gradient masking training"
      ],
      "type": "intervention",
      "description": "Iteratively retrain the model while penalising gradients that were salient in previous runs, generating a pool of accurate models with different reasoning patterns for expert selection.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Performed during training iterations per Section 2.2.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Evaluated on toy and real datasets but not yet in production systems (Section 3.2).",
      "node_rationale": "Intervention directly implements the design rationale for cases without annotations."
    },
    {
      "name": "Use input gradient explanations in pre-deployment audits to detect dataset confounds",
      "aliases": [
        "gradient-based safety audit",
        "explanation-driven confound detection"
      ],
      "type": "intervention",
      "description": "Before deployment, generate input-gradient explanations across validation data to reveal whether the model relies on domain-inappropriate features, enabling corrective action.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Executed during pre-deployment testing as part of model auditing, implied in Introduction and Section 3 discussion.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Supported by experimental evidence of explanation faithfulness and speed but not yet industrial practice.",
      "node_rationale": "Motivated by evidence that gradient explanations are faithful and fast, making them feasible for audits."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Confound-driven misgeneralization in differentiable models",
      "target_node": "Opaque decision boundaries in neural networks",
      "description": "Hidden decision logic prevents detection of confound reliance, leading to mis-generalisation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Multiple examples (pneumonia/asthma case) in Introduction link opaque boundaries to safety failures.",
      "edge_rationale": "Section 1 explains that lack of interpretability causes unnoticed confounds."
    },
    {
      "type": "caused_by",
      "source_node": "Confound-driven misgeneralization in differentiable models",
      "target_node": "Ineffectiveness of standard regularization under dataset confounds",
      "description": "Even with weight decay or sparsity, models latch onto confounding signals, so risks persist.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Stated qualitatively in Introduction; no quantitative test but empirically supported later.",
      "edge_rationale": "Lines discussing why existing regularizers don’t fix the pneumonia example."
    },
    {
      "type": "addressed_by",
      "source_node": "Opaque decision boundaries in neural networks",
      "target_node": "Input gradient explanations approximate local decision boundaries",
      "description": "Gradient explanations make the decision surface locally transparent, countering opacity.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Formal derivation in Section 1.2 and visual evidence in Figure 1.",
      "edge_rationale": "Theoretical insight is explicitly proposed to solve transparency problem."
    },
    {
      "type": "addressed_by",
      "source_node": "Ineffectiveness of standard regularization under dataset confounds",
      "target_node": "Input gradient explanations approximate local decision boundaries",
      "description": "Using gradients as explanations enables novel regularization directly targeting confound use.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Link is argued conceptually; empirical sections later confirm.",
      "edge_rationale": "Section 2.1 motivates gradient penalties as an alternative to standard regularizers."
    },
    {
      "type": "enables",
      "source_node": "Input gradient explanations approximate local decision boundaries",
      "target_node": "Constraining input gradients on annotated irrelevant features",
      "description": "Because gradients are differentiable, they can be added to the loss to steer learning.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Equation in 2.1 relies on gradient property.",
      "edge_rationale": "Design rationale relies directly on the theoretical insight."
    },
    {
      "type": "enables",
      "source_node": "Input gradient explanations approximate local decision boundaries",
      "target_node": "Iterative penalization of salient gradients to uncover alternative rules",
      "description": "Cheap gradient computation allows repeatedly masking salient features to search decision space.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Algorithm definition in 2.2 depends on ability to compute gradients quickly.",
      "edge_rationale": "Section 2.2 explicitly builds on gradient explanations."
    },
    {
      "type": "implemented_by",
      "source_node": "Constraining input gradients on annotated irrelevant features",
      "target_node": "Loss function with input gradient penalty term",
      "description": "The loss adds an L2 penalty on gradients at annotated positions to realise the design.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Exact formula given in Section 2.1.",
      "edge_rationale": "Implementation is direct instantiation of design."
    },
    {
      "type": "implemented_by",
      "source_node": "Constraining input gradients on annotated irrelevant features",
      "target_node": "Annotation matrix specifying irrelevant input dimensions",
      "description": "Annotations indicate which gradients should be penalised, operationalising domain knowledge.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Matrix A defined alongside the loss in Section 2.1.",
      "edge_rationale": "Annotations are required component of the design."
    },
    {
      "type": "implemented_by",
      "source_node": "Iterative penalization of salient gradients to uncover alternative rules",
      "target_node": "Find-another-explanation iterative training algorithm",
      "description": "The FAE algorithm realises the design by cycling mask creation and retraining.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Pseudocode equations in Section 2.2.",
      "edge_rationale": "Algorithm directly operationalises the design rationale."
    },
    {
      "type": "validated_by",
      "source_node": "Loss function with input gradient penalty term",
      "target_node": "Improved generalization on Decoy MNIST after gradient penalties",
      "description": "Applying the loss recovers baseline accuracy despite confounding swatches.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Figure 9 shows quantitative improvement.",
      "edge_rationale": "Empirical result confirms effectiveness of penalty loss."
    },
    {
      "type": "validated_by",
      "source_node": "Annotation matrix specifying irrelevant input dimensions",
      "target_node": "Recovered baseline accuracy on Iris-Cancer dataset using gradient constraints",
      "description": "Using the annotation mask prevents reliance on Iris features, maintaining accuracy.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Figure 8 demonstrates restoration of performance.",
      "edge_rationale": "Evidence ties annotation-based constraints to outcome."
    },
    {
      "type": "validated_by",
      "source_node": "Find-another-explanation iterative training algorithm",
      "target_node": "Discovery of diverse classifiers on 20 Newsgroups without accuracy loss",
      "description": "FAE produces multiple high-accuracy models with different salient words.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Figure 10 presents qualitative and quantitative results but limited statistical analysis.",
      "edge_rationale": "Shows algorithm achieves its intended effect."
    },
    {
      "type": "validated_by",
      "source_node": "Input gradient explanations approximate local decision boundaries",
      "target_node": "Faster yet faithful explanations compared to LIME",
      "description": "Runtime table and similarity figures confirm gradients are an efficient, faithful explanation method.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Table 1 plus Figures 5-7 provide consistent empirical evidence.",
      "edge_rationale": "Evidence supports the theoretical insight’s practical utility."
    },
    {
      "type": "motivates",
      "source_node": "Improved generalization on Decoy MNIST after gradient penalties",
      "target_node": "Train models with input-gradient regularization to minimize confound reliance",
      "description": "Demonstrated gains motivate adopting gradient-regularised training.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Large accuracy jump provides strong empirical support.",
      "edge_rationale": "Experimental success justifies the intervention."
    },
    {
      "type": "motivates",
      "source_node": "Recovered baseline accuracy on Iris-Cancer dataset using gradient constraints",
      "target_node": "Train models with input-gradient regularization to minimize confound reliance",
      "description": "Reproducing baseline accuracy despite removed features reinforces the training approach.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Consistent across 50 random splits (Figure 8).",
      "edge_rationale": "Evidence generalises method beyond images."
    },
    {
      "type": "motivates",
      "source_node": "Discovery of diverse classifiers on 20 Newsgroups without accuracy loss",
      "target_node": "Apply find-another-explanation training to discover diverse decision boundaries",
      "description": "Successful exploration of multiple rules supports using FAE in ambiguous domains.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Result qualitative but consistent with expectation.",
      "edge_rationale": "Directly demonstrates value of the intervention."
    },
    {
      "type": "motivates",
      "source_node": "Faster yet faithful explanations compared to LIME",
      "target_node": "Use input gradient explanations in pre-deployment audits to detect dataset confounds",
      "description": "Efficiency and fidelity make gradient explanations practical for large-scale auditing.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Table 1 shows >1000× speedup enabling audit feasibility.",
      "edge_rationale": "Affords real-time inspection needed in deployment pipelines."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2017-03-10T00:00:00Z"
    },
    {
      "key": "title",
      "value": "Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations"
    },
    {
      "key": "authors",
      "value": [
        "Andrew Slavin Ross",
        "Michael C. Hughes",
        "Finale Doshi-Velez"
      ]
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1703.03717"
    },
    {
      "key": "source",
      "value": "arxiv"
    }
  ]
}