{
  "nodes": [
    {
      "name": "Ineffective reinforcement learning from high-dimensional sensory inputs in Atari environments",
      "aliases": [
        "poor RL performance on raw pixels",
        "difficulty learning from vision in Atari RL"
      ],
      "type": "concept",
      "description": "Standard RL agents fail to learn competent policies when observations are 210×160 RGB frames without engineered features.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in 1 Introduction as the overarching challenge motivating the work."
    },
    {
      "name": "Instability and divergence in Q-learning with nonlinear function approximators in Atari environments",
      "aliases": [
        "Q-learning divergence with neural nets",
        "instable value estimation in deep RL"
      ],
      "type": "concept",
      "description": "Prior research showed Q-learning combined with nonlinear networks can oscillate or diverge, preventing reliable policy learning.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in 3 Related Work as a central barrier to deep RL."
    },
    {
      "name": "Correlated sequential data in on-policy RL training in Atari games",
      "aliases": [
        "high sample correlation",
        "non-i.i.d state sequences"
      ],
      "type": "concept",
      "description": "Consecutive frames generated by a single policy are strongly correlated, violating the i.i.d assumption of stochastic gradient descent.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in 1 Introduction and 4 Deep Reinforcement Learning as a source of learning instability."
    },
    {
      "name": "Non-stationary data distribution due to evolving policies in Atari RL",
      "aliases": [
        "changing state distribution",
        "policy-induced non-stationarity"
      ],
      "type": "concept",
      "description": "As the agent improves, the states it visits change, causing the training distribution to drift over time.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined in 1 Introduction as problematic for deep learners."
    },
    {
      "name": "Sparse and delayed reward signals in Atari tasks",
      "aliases": [
        "delayed sparse rewards",
        "credit assignment difficulty"
      ],
      "type": "concept",
      "description": "In many games, rewards occur infrequently and only after long action chains, complicating credit assignment.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted in 1 Introduction when contrasting RL with supervised learning."
    },
    {
      "name": "High computational cost of per-frame action selection in Atari emulator",
      "aliases": [
        "slow decision frequency",
        "compute burden of every frame"
      ],
      "type": "concept",
      "description": "Selecting actions for all 60 Hz frames is expensive, limiting total training experience under fixed compute budgets.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mentioned in 5 Experiments where frame skipping is motivated."
    },
    {
      "name": "High-dimensional visual input without hand-crafted features in Atari games",
      "aliases": [
        "raw pixel challenges",
        "lack of engineered vision features"
      ],
      "type": "concept",
      "description": "210×160×3 inputs contain 100× more dimensions than prior tabular or feature-based RL settings, stressing traditional methods.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Core obstacle stated in 1 Introduction."
    },
    {
      "name": "Experience replay improving sample independence in Atari RL",
      "aliases": [
        "replay buffer decorrelation",
        "memory-based random sampling"
      ],
      "type": "concept",
      "description": "Storing past transitions and sampling them uniformly breaks temporal correlations and increases data reuse efficiency.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Justified in 4 Deep Reinforcement Learning."
    },
    {
      "name": "Off-policy ε-greedy exploration enabling data reuse in Atari RL",
      "aliases": [
        "ε-greedy for exploration",
        "off-policy sampling"
      ],
      "type": "concept",
      "description": "Maintaining action randomness via ε-greedy lets the learner gather diverse data while evaluating the greedy target policy.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in 2 Background and Algorithm 1."
    },
    {
      "name": "End-to-end deep feature learning from raw pixels in Atari RL",
      "aliases": [
        "automatic visual representation learning",
        "deep vision features for RL"
      ],
      "type": "concept",
      "description": "Convolutional networks can directly transform stacked frames into high-level features useful for Q-value estimation.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivated by successes in vision, Section 4."
    },
    {
      "name": "Temporal redundancy in Atari frame sequences permitting frame skipping",
      "aliases": [
        "redundant consecutive frames",
        "action persistence opportunity"
      ],
      "type": "concept",
      "description": "Because game dynamics change slowly relative to 60 Hz rendering, identical actions across short spans rarely harm control.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implicit in 5 Experiments where k-frame skipping was chosen (light inference)."
    },
    {
      "name": "Reward magnitude normalization stabilizing gradients across Atari games",
      "aliases": [
        "reward clipping theory",
        "scale-invariant error signals"
      ],
      "type": "concept",
      "description": "Mapping diverse game scores to a fixed small range limits Q-value targets and prevents exploding gradients.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in 5 Experiments when clipping rewards."
    },
    {
      "name": "Integrate experience replay with deep Q-learning for data efficiency in Atari RL",
      "aliases": [
        "deep Q-learning with replay",
        "combining replay and SGD"
      ],
      "type": "concept",
      "description": "Couples replay sampling with stochastic gradient Q-updates to achieve stable, efficient value learning.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined in Algorithm 1, Section 4."
    },
    {
      "name": "CNN with shared visual representation and action-specific outputs for efficient Q-value computation in Atari RL",
      "aliases": [
        "shared conv torso, multi-head Q outputs",
        "single forward pass Q-network"
      ],
      "type": "concept",
      "description": "Computes Q-values for all actions simultaneously, avoiding linear scaling with action count.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Design choice described in 4.1 Preprocessing and Model Architecture."
    },
    {
      "name": "Frame skipping to reduce computational burden in Atari RL",
      "aliases": [
        "action repeat strategy",
        "k-frame skip"
      ],
      "type": "concept",
      "description": "Agent chooses actions every k frames, replaying the last action on skipped frames to gather k× more experience per compute.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Stated in 5 Experiments as key efficiency measure."
    },
    {
      "name": "Replay memory of one million frames with uniform random minibatch sampling in DQN",
      "aliases": [
        "uniform replay buffer",
        "1M transition memory"
      ],
      "type": "concept",
      "description": "Concrete implementation storing latest 1 M tuples and sampling 32-sized minibatches for each SGD step.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation details in Algorithm 1 and 5 Experiments."
    },
    {
      "name": "Deep Q-network architecture with conv(16x8 stride4,32x4 stride2)+256FC action outputs",
      "aliases": [
        "DQN conv architecture",
        "Atari CNN torso"
      ],
      "type": "concept",
      "description": "Stacked 84×84×4 frames fed through two convolutional layers and a 256-unit fully-connected layer before per-action outputs.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Exact network described in 4.1 Preprocessing and Model Architecture."
    },
    {
      "name": "Reward clipping implementation mapping rewards to {-1,0,1} during DQN training",
      "aliases": [
        "unit reward clipping",
        "score normalization implementation"
      ],
      "type": "concept",
      "description": "Positive game scores set to +1, negatives to −1, zeros unchanged, applied only during training.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation detail in 5 Experiments."
    },
    {
      "name": "Frame skipping implementation repeating last action for k=4 (k=3 in Space Invaders)",
      "aliases": [
        "repeat action k frames",
        "k=4 skip scheme"
      ],
      "type": "concept",
      "description": "Action chosen every k frames; emulator steps forward internally on skipped frames to cut inference cost.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in 5 Experiments."
    },
    {
      "name": "DQN outperforming prior RL methods on six of seven Atari games",
      "aliases": [
        "state-of-the-art Atari scores",
        "empirical performance gains"
      ],
      "type": "concept",
      "description": "Average and best-episode scores exceed Sarsa, Contingency, and HNeat baselines on multiple games, some above human.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Main evaluation in 5.3 Main Evaluation."
    },
    {
      "name": "Smooth predicted Q-value increase without divergence during DQN training",
      "aliases": [
        "stable training curves",
        "no catastrophic divergence"
      ],
      "type": "concept",
      "description": "Held-out state set shows monotonic Q-value growth; average episode returns trend upward, indicating stable learning.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "5.1 Training and Stability, Figure 2."
    },
    {
      "name": "ε-greedy exploration strategy for off-policy learning in Atari RL",
      "aliases": [
        "epsilon exploration design",
        "stochastic action choice"
      ],
      "type": "concept",
      "description": "Chooses random action with probability ε to ensure exploration while still favoring high-Q moves.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Algorithm 1 description."
    },
    {
      "name": "Linear annealing of ε from 1 to 0.1 over first million frames in DQN training",
      "aliases": [
        "epsilon schedule implementation",
        "annealed ε policy"
      ],
      "type": "concept",
      "description": "Gradually reduces exploration rate to balance discovery and exploitation during learning.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Parameter schedule given in 5 Experiments."
    },
    {
      "name": "Clip rewards to unit magnitude for consistent gradient scales in Atari DQN training",
      "aliases": [
        "reward clipping design",
        "score normalization strategy"
      ],
      "type": "concept",
      "description": "Applies theoretical insight about gradient stability to propose specific clipping scheme.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Described directly before the clipping implementation in 5 Experiments."
    },
    {
      "name": "Implement experience replay with uniform random sampling during DQN training",
      "aliases": [
        "apply replay buffer",
        "use uniform experience replay"
      ],
      "type": "intervention",
      "description": "During fine-tuning/RL, maintain a fixed-size memory of transitions and sample minibatches uniformly for every SGD update.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Experience replay is applied throughout the RL fine-tuning phase (Algorithm 1).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Validated across seven Atari games but not yet widely operational (Section 5).",
      "node_rationale": "Actionable method directly derived from mechanism and evidence."
    },
    {
      "name": "Train deep convolutional Q-networks end-to-end from raw pixel inputs in Atari RL",
      "aliases": [
        "end-to-end pixel DQN training",
        "deep vision-RL coupling"
      ],
      "type": "intervention",
      "description": "Use the specified CNN architecture and SGD to jointly learn visual features and Q-values during fine-tuning.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Implemented during RL fine-tuning of policy parameters (Section 4.1).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Prototype validated on seven games; broader deployment pending.",
      "node_rationale": "Core training approach producing the reported gains."
    },
    {
      "name": "Clip reward magnitudes to [-1,1] during training of Atari DQN agents",
      "aliases": [
        "apply reward clipping",
        "unit reward scaling"
      ],
      "type": "intervention",
      "description": "Before computing TD targets, map positive rewards to +1 and negative to −1 to stabilize gradients across heterogeneous games.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Applied throughout learning updates (Section 5).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Experimental heuristic with empirical but limited evaluation evidence.",
      "node_rationale": "Directly actionable tweak inferred to aid stability."
    },
    {
      "name": "Apply frame skipping (k=3–4) during Atari RL agent training",
      "aliases": [
        "use action repeat",
        "k-frame skip intervention"
      ],
      "type": "intervention",
      "description": "Have the agent choose an action every k frames (k=4 generally, 3 for Space Invaders) and repeat it on skipped frames.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Implemented during policy fine-tuning (Section 5).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Prototype level; validated on seven games.",
      "node_rationale": "Improves data throughput and was key to reported performance."
    },
    {
      "name": "Use ε-greedy exploration with linear annealing of ε during DQN training",
      "aliases": [
        "annealed epsilon exploration",
        "stochastic exploration intervention"
      ],
      "type": "intervention",
      "description": "Start with ε=1 and linearly reduce to 0.1 over the first million frames, then keep ε=0.1.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Exploration schedule is part of fine-tuning/RL updates (Algorithm 1).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Empirically validated on seven Atari games.",
      "node_rationale": "Defines a concrete actionable exploration policy."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Ineffective reinforcement learning from high-dimensional sensory inputs in Atari environments",
      "target_node": "High-dimensional visual input without hand-crafted features in Atari games",
      "description": "Raw pixels overwhelm traditional feature-based RL, leading to poor learning.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicitly stated in 1 Introduction.",
      "edge_rationale": "Section connects challenge to lack of features."
    },
    {
      "type": "caused_by",
      "source_node": "Ineffective reinforcement learning from high-dimensional sensory inputs in Atari environments",
      "target_node": "High computational cost of per-frame action selection in Atari emulator",
      "description": "Need to decide at 60 Hz inflates compute, limiting practical training.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Implied in 5 Experiments; not quantified.",
      "edge_rationale": "Paper motivates frame skipping due to compute cost."
    },
    {
      "type": "caused_by",
      "source_node": "Instability and divergence in Q-learning with nonlinear function approximators in Atari environments",
      "target_node": "Correlated sequential data in on-policy RL training in Atari games",
      "description": "Highly correlated updates bias gradients and can lead to divergence.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Discussed in 4 Deep Reinforcement Learning.",
      "edge_rationale": "Correlation identified as core instability source."
    },
    {
      "type": "caused_by",
      "source_node": "Instability and divergence in Q-learning with nonlinear function approximators in Atari environments",
      "target_node": "Non-stationary data distribution due to evolving policies in Atari RL",
      "description": "Changing policy shifts the data distribution, upsetting learning stability.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit in 4 Deep Reinforcement Learning.",
      "edge_rationale": "Non-stationarity named as divergence driver."
    },
    {
      "type": "caused_by",
      "source_node": "Instability and divergence in Q-learning with nonlinear function approximators in Atari environments",
      "target_node": "Sparse and delayed reward signals in Atari tasks",
      "description": "Sparse delayed rewards create large, infrequent TD errors destabilising updates.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Mentioned qualitatively in 1 Introduction.",
      "edge_rationale": "Paper lists sparse rewards among challenges."
    },
    {
      "type": "mitigated_by",
      "source_node": "Correlated sequential data in on-policy RL training in Atari games",
      "target_node": "Experience replay improving sample independence in Atari RL",
      "description": "Random sampling from a replay buffer breaks temporal correlations.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct claim in Section 4.",
      "edge_rationale": "Replay mechanism justification."
    },
    {
      "type": "mitigated_by",
      "source_node": "Non-stationary data distribution due to evolving policies in Atari RL",
      "target_node": "Experience replay improving sample independence in Atari RL",
      "description": "Averaging over many past policies smooths distribution changes.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explained in Section 4.",
      "edge_rationale": "Replay averages behaviour distribution."
    },
    {
      "type": "enabled_by",
      "source_node": "Experience replay improving sample independence in Atari RL",
      "target_node": "Integrate experience replay with deep Q-learning for data efficiency in Atari RL",
      "description": "Insight motivates designing a Q-learning variant that incorporates replay.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical flow from Section 4 narrative.",
      "edge_rationale": "Design follows from insight."
    },
    {
      "type": "implemented_by",
      "source_node": "Integrate experience replay with deep Q-learning for data efficiency in Atari RL",
      "target_node": "Replay memory of one million frames with uniform random minibatch sampling in DQN",
      "description": "Replay memory is the concrete mechanism realising the design.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Algorithm 1 gives exact buffer spec.",
      "edge_rationale": "Mechanism instantiates design."
    },
    {
      "type": "validated_by",
      "source_node": "Replay memory of one million frames with uniform random minibatch sampling in DQN",
      "target_node": "Smooth predicted Q-value increase without divergence during DQN training",
      "description": "Stable learning curves indicate effectiveness of replay implementation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Empirical correlation in Figure 2.",
      "edge_rationale": "Evidence linked to mechanism."
    },
    {
      "type": "motivates",
      "source_node": "Smooth predicted Q-value increase without divergence during DQN training",
      "target_node": "Implement experience replay with uniform random sampling during DQN training",
      "description": "Observed stability encourages adopting replay as an intervention.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Paper argues replay prevents divergence.",
      "edge_rationale": "Evidence drives actionable recommendation."
    },
    {
      "type": "mitigated_by",
      "source_node": "High-dimensional visual input without hand-crafted features in Atari games",
      "target_node": "End-to-end deep feature learning from raw pixels in Atari RL",
      "description": "Learning features with CNNs removes need for manual engineering.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit in 1 Introduction & 4.",
      "edge_rationale": "CNNs solve absence of features."
    },
    {
      "type": "enabled_by",
      "source_node": "End-to-end deep feature learning from raw pixels in Atari RL",
      "target_node": "CNN with shared visual representation and action-specific outputs for efficient Q-value computation in Atari RL",
      "description": "Feature learning insight motivates this specific CNN design.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Design choice justified by need to learn features.",
      "edge_rationale": "Insight shapes architecture."
    },
    {
      "type": "implemented_by",
      "source_node": "CNN with shared visual representation and action-specific outputs for efficient Q-value computation in Atari RL",
      "target_node": "Deep Q-network architecture with conv(16x8 stride4,32x4 stride2)+256FC action outputs",
      "description": "The architecture realises the design rationale.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Exact layer spec provided in 4.1.",
      "edge_rationale": "Concrete instantiation."
    },
    {
      "type": "validated_by",
      "source_node": "Deep Q-network architecture with conv(16x8 stride4,32x4 stride2)+256FC action outputs",
      "target_node": "DQN outperforming prior RL methods on six of seven Atari games",
      "description": "Superior scores demonstrate effectiveness of architecture.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Table 1 results.",
      "edge_rationale": "Empirical validation."
    },
    {
      "type": "motivates",
      "source_node": "DQN outperforming prior RL methods on six of seven Atari games",
      "target_node": "Train deep convolutional Q-networks end-to-end from raw pixel inputs in Atari RL",
      "description": "Performance gains argue for adopting end-to-end CNN training.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct performance evidence.",
      "edge_rationale": "Evidence drives intervention."
    },
    {
      "type": "mitigated_by",
      "source_node": "Sparse and delayed reward signals in Atari tasks",
      "target_node": "Reward magnitude normalization stabilizing gradients across Atari games",
      "description": "Normalizing magnitudes makes infrequent rewards easier to learn from.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explained qualitatively in 5 Experiments.",
      "edge_rationale": "Theory addresses sparse rewards."
    },
    {
      "type": "enabled_by",
      "source_node": "Reward magnitude normalization stabilizing gradients across Atari games",
      "target_node": "Clip rewards to unit magnitude for consistent gradient scales in Atari DQN training",
      "description": "Insight leads to specific clipping design.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical design step.",
      "edge_rationale": "Design follows theory."
    },
    {
      "type": "implemented_by",
      "source_node": "Clip rewards to unit magnitude for consistent gradient scales in Atari DQN training",
      "target_node": "Reward clipping implementation mapping rewards to {-1,0,1} during DQN training",
      "description": "Implementation realises the design.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Specified in 5 Experiments.",
      "edge_rationale": "Concrete mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Reward clipping implementation mapping rewards to {-1,0,1} during DQN training",
      "target_node": "Smooth predicted Q-value increase without divergence during DQN training",
      "description": "Training stability suggests reward clipping is effective.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Indirect evidence; no ablation study.",
      "edge_rationale": "Light inference linking clipping to stability."
    },
    {
      "type": "motivates",
      "source_node": "Smooth predicted Q-value increase without divergence during DQN training",
      "target_node": "Clip reward magnitudes to [-1,1] during training of Atari DQN agents",
      "description": "Stability gain motivates adopting reward clipping.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference; paper recommends but does not isolate effect.",
      "edge_rationale": "Observation informs intervention."
    },
    {
      "type": "mitigated_by",
      "source_node": "High computational cost of per-frame action selection in Atari emulator",
      "target_node": "Temporal redundancy in Atari frame sequences permitting frame skipping",
      "description": "Recognising redundancy suggests skipping frames alleviates cost.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Light inference from 5 Experiments.",
      "edge_rationale": "Temporal redundancy enables cost fix."
    },
    {
      "type": "enabled_by",
      "source_node": "Temporal redundancy in Atari frame sequences permitting frame skipping",
      "target_node": "Frame skipping to reduce computational burden in Atari RL",
      "description": "Insight directly motivates design rationale of frame skipping.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Derived reasoning; not deeply elaborated.",
      "edge_rationale": "Design emerges from insight."
    },
    {
      "type": "implemented_by",
      "source_node": "Frame skipping to reduce computational burden in Atari RL",
      "target_node": "Frame skipping implementation repeating last action for k=4 (k=3 in Space Invaders)",
      "description": "Specific k values constitute the implementation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit in 5 Experiments.",
      "edge_rationale": "Implementation instantiates design."
    },
    {
      "type": "validated_by",
      "source_node": "Frame skipping implementation repeating last action for k=4 (k=3 in Space Invaders)",
      "target_node": "DQN outperforming prior RL methods on six of seven Atari games",
      "description": "Overall performance includes benefit from frame skipping.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "No isolation; part of full system.",
      "edge_rationale": "Evidence indirectly validates frame skipping."
    },
    {
      "type": "motivates",
      "source_node": "DQN outperforming prior RL methods on six of seven Atari games",
      "target_node": "Apply frame skipping (k=3–4) during Atari RL agent training",
      "description": "Performance success encourages adopting frame skipping.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Inference linking system-level success to component.",
      "edge_rationale": "Evidence informs intervention."
    },
    {
      "type": "mitigated_by",
      "source_node": "Non-stationary data distribution due to evolving policies in Atari RL",
      "target_node": "Off-policy ε-greedy exploration enabling data reuse in Atari RL",
      "description": "Off-policy exploration allows learning from outdated behavior data.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explained conceptually in 2 Background.",
      "edge_rationale": "Theoretical mitigation."
    },
    {
      "type": "enabled_by",
      "source_node": "Off-policy ε-greedy exploration enabling data reuse in Atari RL",
      "target_node": "ε-greedy exploration strategy for off-policy learning in Atari RL",
      "description": "Insight leads to choosing ε-greedy as the concrete exploration design.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Algorithm 1 specifies ε-greedy.",
      "edge_rationale": "Design arises from insight."
    },
    {
      "type": "implemented_by",
      "source_node": "ε-greedy exploration strategy for off-policy learning in Atari RL",
      "target_node": "Linear annealing of ε from 1 to 0.1 over first million frames in DQN training",
      "description": "Linear schedule realises the exploration strategy.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Exact schedule given in 5 Experiments.",
      "edge_rationale": "Implementation detail."
    },
    {
      "type": "validated_by",
      "source_node": "Linear annealing of ε from 1 to 0.1 over first million frames in DQN training",
      "target_node": "DQN outperforming prior RL methods on six of seven Atari games",
      "description": "Overall performance reflects effective exploration.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Indirect evidence; no ablation.",
      "edge_rationale": "Performance suggests schedule efficacy."
    },
    {
      "type": "motivates",
      "source_node": "DQN outperforming prior RL methods on six of seven Atari games",
      "target_node": "Use ε-greedy exploration with linear annealing of ε during DQN training",
      "description": "Success motivates adopting the ε schedule as an intervention.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Empirical success implies usefulness.",
      "edge_rationale": "Evidence informs deployment."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2013-12-19T00:00:00Z"
    },
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "title",
      "value": "Playing Atari with Deep Reinforcement Learning"
    },
    {
      "key": "authors",
      "value": [
        "Volodymyr Mnih",
        "Koray Kavukcuoglu",
        "David Silver",
        "Alex Graves",
        "Ioannis Antonoglou",
        "Daan Wierstra",
        "Martin Riedmiller"
      ]
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1312.5602"
    }
  ]
}