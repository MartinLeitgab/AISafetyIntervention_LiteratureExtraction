{
  "nodes": [
    {
      "name": "Misestimation of agent reward functions in online partially observable environments",
      "aliases": [
        "incorrect preference inference under occlusion",
        "reward estimation error in streaming settings"
      ],
      "type": "concept",
      "description": "Failure to correctly infer an expert’s reward when observations arrive as a stream and parts of the trajectory are hidden can cause unsafe or ineffective downstream decisions by the learning agent.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in Section 1 Introduction as the core motivation for online IRL."
    },
    {
      "name": "Computational timeouts of batch IRL under streaming data constraints",
      "aliases": [
        "batch IRL scalability bottlenecks",
        "time-limited failures of one-shot IRL"
      ],
      "type": "concept",
      "description": "Traditional one-shot IRL recomputes over the whole demonstration and becomes too slow for time-limited, continuously arriving data, leading to frequent timeouts.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Sections 1 and 4 where batch LME experiences >50 % timeouts."
    },
    {
      "name": "Occluded trajectory segments causing missing feature data",
      "aliases": [
        "hidden state-action observations",
        "trajectory occlusion"
      ],
      "type": "concept",
      "description": "Sensors with limited field-of-view yield trajectories with unknown portions; naïve IRL methods cannot match feature expectations when data are missing.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 2.2 ‘IRL under Occlusion’ formalises this issue."
    },
    {
      "name": "Incremental session-based reward update improves scalability in streaming IRL",
      "aliases": [
        "session-wise reward refinement",
        "incremental learning assumption"
      ],
      "type": "concept",
      "description": "Updating the reward estimate after each small batch of new observations rather than re-solving from scratch can control computational cost and accommodate endless streams.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.1 introduces I2RL sessions and stopping criteria."
    },
    {
      "name": "Latent variable expectation over hidden trajectory data recovers missing information",
      "aliases": [
        "expectation over unobserved data",
        "latent trajectory completion theory"
      ],
      "type": "concept",
      "description": "Treating unobserved parts of a trajectory as latent variables and taking their expectation conditioned on visible data allows constraints on full feature expectations to be restored.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Formalised in Eq. (2) and accompanying text in Section 2.2."
    },
    {
      "name": "Online IRL framework I2RL enabling continuous sessions",
      "aliases": [
        "I2RL framework",
        "incremental IRL formulation"
      ],
      "type": "concept",
      "description": "I2RL defines IRL as an infinite or terminating sequence of learning sessions, each taking the previous reward estimate and new data, with formal stopping criteria.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Defined in Definitions 3-6, Section 3.1."
    },
    {
      "name": "Latent maximum entropy formulation with EM optimizes reward under occlusion",
      "aliases": [
        "LME IRL design",
        "EM-based latent max entropy"
      ],
      "type": "concept",
      "description": "Extends maximum-entropy IRL by introducing an EM procedure that maximises trajectory likelihood while satisfying expected feature constraints in the presence of hidden data.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 2.2 and 3.2.1 describe the latent MaxEnt EM procedure."
    },
    {
      "name": "Incremental LME algorithm with per-session E-step and M-step updating feature expectations",
      "aliases": [
        "incremental LME procedure",
        "session-wise latent max entropy algorithm"
      ],
      "type": "concept",
      "description": "Implements the I2RL idea by keeping running counts of feature expectations (Eq. 4) and performing EM within each session, yielding a new reward weight vector θᵢ.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Algorithm summarised in Section 3.2.1 ‘Incremental Latent MaxEnt’."
    },
    {
      "name": "Log-likelihood convergence stopping criterion for I2RL session termination",
      "aliases": [
        "likelihood threshold halt",
        "epsilon-based stopping rule"
      ],
      "type": "concept",
      "description": "Sessions stop when successive log-likelihoods differ by ≤ ε, preventing unnecessary computation during online learning.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Stopping Criterion #1 defined in Definition 4, Section 3.1."
    },
    {
      "name": "Monotonic likelihood improvement and probabilistic convergence guarantees for incremental LME",
      "aliases": [
        "monotonicity lemma",
        "confidence bounds for incremental LME"
      ],
      "type": "concept",
      "description": "Lemma 1 and Theorems 1-2 prove that each session non-decreasingly improves log-likelihood and bounds estimation error with high confidence.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.2.1 presents these formal results."
    },
    {
      "name": "Perimeter patrol experiments showing 4x speedup and higher success rate compared to batch LME",
      "aliases": [
        "experimental evaluation on patrol domain",
        "empirical performance study"
      ],
      "type": "concept",
      "description": "In a ROS-simulated hallway-patrol task, incremental LME matches batch accuracy but reduces run-time by up to 4 × and increases mission success from <40 % to ≈65 %.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Results plotted in Figure 2, Section 4 Experiments."
    },
    {
      "name": "Deploy incremental latent maximum entropy IRL for online preference inference in partially observable tasks",
      "aliases": [
        "apply incremental LME",
        "use I2RL with latent max entropy"
      ],
      "type": "intervention",
      "description": "Integrate the incremental-LME algorithm into deployed autonomous systems to continuously update inferred reward models while operating in environments with occlusion.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Applies during system operation when new observations arrive (Section 3.2.1 Incremental LME).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Validated in a realistic robotics simulation (Section 4), but not yet field-deployed at scale.",
      "node_rationale": "Represents the central actionable outcome proposed by the paper."
    },
    {
      "name": "Terminate I2RL sessions using small log-likelihood threshold to reduce computation",
      "aliases": [
        "use likelihood-based stopping",
        "apply epsilon loglikelihood stop"
      ],
      "type": "intervention",
      "description": "Set ε so that a session ends once |LLᵢ − LLᵢ₋₁| ≤ ε, avoiding wasteful EM iterations and curbing latency.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Executed online when deciding whether to begin a new session (Definition 4, Section 3.1).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Criterion is theoretically motivated but only indirectly evaluated in experiments.",
      "node_rationale": "Reduces the computational-timeout problem emphasised in Section 4."
    },
    {
      "name": "Sample hidden trajectory completions during E-step to estimate feature expectations under occlusion",
      "aliases": [
        "sampling latent trajectories",
        "Monte Carlo hidden data completion"
      ],
      "type": "intervention",
      "description": "Use Monte-Carlo or analytic sampling of Z ∼ P(Z|Y,θ) within each EM E-step to approximate expectations for unobserved states/actions.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Performed during model fine-tuning/RL phases of IRL (Section 2.2 and 3.2.1).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Technique is standard in EM but only evaluated here within a proof-of-concept domain.",
      "node_rationale": "Essential mechanism that allows incremental LME to function when observations are occluded."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Misestimation of agent reward functions in online partially observable environments",
      "target_node": "Computational timeouts of batch IRL under streaming data constraints",
      "description": "Timeouts prevent successful convergence of reward estimation, leading directly to misestimation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Empirical evidence of high timeout rates linked to worse success in Section 4.",
      "edge_rationale": "Section 4 shows batch LME timeouts correlate with lower learning accuracy."
    },
    {
      "type": "caused_by",
      "source_node": "Misestimation of agent reward functions in online partially observable environments",
      "target_node": "Occluded trajectory segments causing missing feature data",
      "description": "Missing data violates feature-matching constraints, degrading reward inference.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Problem formally analysed in Section 2.2 with examples.",
      "edge_rationale": "Section 2.2 explains how occlusion leads to incorrect feature expectations."
    },
    {
      "type": "mitigated_by",
      "source_node": "Computational timeouts of batch IRL under streaming data constraints",
      "target_node": "Incremental session-based reward update improves scalability in streaming IRL",
      "description": "Session-wise updates reduce per-update complexity, addressing timeout risk.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Algorithmic argument plus runtime measurements in Figure 2.",
      "edge_rationale": "Sections 3.1 and 4 connect incremental updates to lower runtime."
    },
    {
      "type": "mitigated_by",
      "source_node": "Occluded trajectory segments causing missing feature data",
      "target_node": "Latent variable expectation over hidden trajectory data recovers missing information",
      "description": "Expectation over latent variables reconstructs the missing portions needed for learning.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Equation (2) derives this remedy.",
      "edge_rationale": "Section 2.2 establishes this theoretical fix."
    },
    {
      "type": "enabled_by",
      "source_node": "Online IRL framework I2RL enabling continuous sessions",
      "target_node": "Incremental session-based reward update improves scalability in streaming IRL",
      "description": "I2RL formalises how the session-based insight can be applied in practice.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual linking; formal definitions provided but relationship implicit.",
      "edge_rationale": "Definitions 3-6 outline the framework that instantiates the insight."
    },
    {
      "type": "enabled_by",
      "source_node": "Latent maximum entropy formulation with EM optimizes reward under occlusion",
      "target_node": "Latent variable expectation over hidden trajectory data recovers missing information",
      "description": "The MaxEnt-EM design operationalises the theoretical latent-variable expectation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct construction from theory to design in Sections 2.2-3.2.1.",
      "edge_rationale": "Section 3.2.1 shows EM implementing the latent expectation."
    },
    {
      "type": "implemented_by",
      "source_node": "Online IRL framework I2RL enabling continuous sessions",
      "target_node": "Incremental LME algorithm with per-session E-step and M-step updating feature expectations",
      "description": "Algorithm realises the I2RL framework under occlusion.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Algorithm description immediately follows framework.",
      "edge_rationale": "Section 3.2.1 presents the algorithm as an I2RL instance."
    },
    {
      "type": "implemented_by",
      "source_node": "Latent maximum entropy formulation with EM optimizes reward under occlusion",
      "target_node": "Incremental LME algorithm with per-session E-step and M-step updating feature expectations",
      "description": "Incremental LME applies the latent MaxEnt EM in an online setting.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct mapping shown in algorithm derivation.",
      "edge_rationale": "Section 3.2.1 derives Eq. (4) for incremental EM."
    },
    {
      "type": "required_by",
      "source_node": "Log-likelihood convergence stopping criterion for I2RL session termination",
      "target_node": "Incremental LME algorithm with per-session E-step and M-step updating feature expectations",
      "description": "Algorithm needs a termination rule; the ε log-likelihood threshold supplies it.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Explicit in algorithm pseudocode but not deeply analysed.",
      "edge_rationale": "Definition 4 referenced in algorithm description."
    },
    {
      "type": "validated_by",
      "source_node": "Incremental LME algorithm with per-session E-step and M-step updating feature expectations",
      "target_node": "Monotonic likelihood improvement and probabilistic convergence guarantees for incremental LME",
      "description": "Formal proofs confirm correctness and convergence of the algorithm.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Mathematical lemmas and theorems in Section 3.2.1.",
      "edge_rationale": "Lemma 1, Theorems 1-2 validate algorithm properties."
    },
    {
      "type": "validated_by",
      "source_node": "Incremental LME algorithm with per-session E-step and M-step updating feature expectations",
      "target_node": "Perimeter patrol experiments showing 4x speedup and higher success rate compared to batch LME",
      "description": "Empirical study demonstrates practical effectiveness.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "100-trial averages and quantitative metrics in Figure 2.",
      "edge_rationale": "Section 4 shows performance gains."
    },
    {
      "type": "motivates",
      "source_node": "Monotonic likelihood improvement and probabilistic convergence guarantees for incremental LME",
      "target_node": "Deploy incremental latent maximum entropy IRL for online preference inference in partially observable tasks",
      "description": "Theoretical guarantees justify deploying incremental LME in safety-critical systems.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Guarantees provide strong support but not yet field trials.",
      "edge_rationale": "Section 5 argues for practical adoption based on guarantees."
    },
    {
      "type": "motivates",
      "source_node": "Perimeter patrol experiments showing 4x speedup and higher success rate compared to batch LME",
      "target_node": "Deploy incremental latent maximum entropy IRL for online preference inference in partially observable tasks",
      "description": "Empirical evidence of higher success rates encourages deployment.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Consistent performance improvements over baseline.",
      "edge_rationale": "Figure 2c success-rate comparison."
    },
    {
      "type": "validated_by",
      "source_node": "Log-likelihood convergence stopping criterion for I2RL session termination",
      "target_node": "Monotonic likelihood improvement and probabilistic convergence guarantees for incremental LME",
      "description": "Proofs show that using the likelihood criterion maintains monotonic improvement.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Lemma 1 incorporates the stopping rule condition.",
      "edge_rationale": "Section 3.2.1 links stopping rule to monotonicity."
    },
    {
      "type": "motivates",
      "source_node": "Monotonic likelihood improvement and probabilistic convergence guarantees for incremental LME",
      "target_node": "Terminate I2RL sessions using small log-likelihood threshold to reduce computation",
      "description": "Confidence bounds guide selection of ε for practical stopping.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical inference from proofs; parameter choice not empirically tuned in paper.",
      "edge_rationale": "Definition 4 and Theorem 1 inform ε selection."
    },
    {
      "type": "motivates",
      "source_node": "Latent variable expectation over hidden trajectory data recovers missing information",
      "target_node": "Sample hidden trajectory completions during E-step to estimate feature expectations under occlusion",
      "description": "Expectation requires practical sampling of hidden data inside EM.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Sampling step explicitly stated in Section 2.2.",
      "edge_rationale": "E-step description specifies sampling Z ~ P(Z|Y,θ)."
    }
  ],
  "meta": [
    {
      "key": "title",
      "value": "A Framework and Method for Online Inverse Reinforcement Learning"
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1805.07871"
    },
    {
      "key": "date_published",
      "value": "2018-05-21T00:00:00Z"
    },
    {
      "key": "authors",
      "value": [
        "Saurabh Arora",
        "Prashant Doshi",
        "Bikramjit Banerjee"
      ]
    },
    {
      "key": "source",
      "value": "arxiv"
    }
  ]
}