{
  "nodes": [
    {
      "name": "mutual defection in Prisoner's Dilemma among AI-controlled firms",
      "aliases": [
        "cooperation failure between strategic AI agents",
        "defection equilibrium in AI multi-agent PD"
      ],
      "type": "concept",
      "description": "AI systems representing large firms face incentives to sue/defect, yielding negative-sum outcomes.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in Introduction as motivating example of welfare loss."
    },
    {
      "name": "inefficient equilibria from classical rationalizable strategies in AI agents",
      "aliases": [
        "suboptimal outcomes under standard rationalizability",
        "welfare loss due to Nash rationality assumptions"
      ],
      "type": "concept",
      "description": "When agents assume unilateral deviations go unnoticed, minimax-dominated strategies may persist, yielding low social welfare.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Characterised in sections 3.1–3.3 as downside of traditional modelling."
    },
    {
      "name": "lack of strategy translucency causing defection in PD settings",
      "aliases": [
        "opaque intentions between agents",
        "no-leak assumption in PD"
      ],
      "type": "concept",
      "description": "If each firm believes its deviation will not be noticed, defection remains dominant.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in Introduction and motivates translucency model."
    },
    {
      "name": "assumption of unilateral deviations leading to minimax dominated strategies persisting",
      "aliases": [
        "unchanged-opponent assumption",
        "no counterfactual response assumption"
      ],
      "type": "concept",
      "description": "Standard models fix opponents’ strategies when a player deviates; this allows dominated strategies to survive.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in Counterfactual Structures section."
    },
    {
      "name": "translucency shifts equilibrium to cooperation when penalty exceeds reward and leak probability epsilon",
      "aliases": [
        "epsilon-leak cooperation condition",
        "probabilistic detection induces cooperation"
      ],
      "type": "concept",
      "description": "If each agent believes a deviation is leaked with probability ε and p·ε > r, cooperation maximises expected utility.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proved in Introduction example."
    },
    {
      "name": "common counterfactual belief of rationality linked to iterated minimax deletion",
      "aliases": [
        "CCBR–minimax equivalence",
        "CCBR characterisation theorem"
      ],
      "type": "concept",
      "description": "CCBR characterises strategies that survive iterated removal of minimax-dominated strategies; avoids inefficient equilibria.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Main result in Section 3."
    },
    {
      "name": "introduce controlled intention leakage to enforce cooperative equilibria",
      "aliases": [
        "design partial transparency",
        "designing translucency for cooperation"
      ],
      "type": "concept",
      "description": "Making agents slightly transparent aligns incentives to cooperate, improving social welfare.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Derived from theoretical insight 5 in Introduction."
    },
    {
      "name": "adopt CCBR-based reasoning in AI policy formation",
      "aliases": [
        "counterfactual-rational policy design",
        "CCBR-aligned agent design"
      ],
      "type": "concept",
      "description": "Training agents to reason about opponents’ counterfactual switches removes minimax-dominated actions.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Stated in Characterizing CCBR section."
    },
    {
      "name": "randomised low-probability strategy disclosure channels between agents",
      "aliases": [
        "epsilon intention signalling",
        "controlled leak channel"
      ],
      "type": "concept",
      "description": "Implementation where each agent’s planned action is broadcast with small probability ε, realising translucency.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implied implementation of design rationale 7."
    },
    {
      "name": "iterated minimax-dominated strategy deletion algorithm during policy search",
      "aliases": [
        "minimax pruning",
        "CCBR pruning"
      ],
      "type": "concept",
      "description": "At training time recursively remove policies where a safer policy beats their best-case payoff.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation mechanism suggested by Section 3.2."
    },
    {
      "name": "theoretical proof of cooperation under translucency",
      "aliases": [
        "PD cooperation theorem",
        "translucent PD proof"
      ],
      "type": "concept",
      "description": "Mathematical demonstration that when p·ε > r, cooperation is rational in translucent PD.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Found in Introduction example."
    },
    {
      "name": "proof that CCBR strategies equal those surviving minimax deletion",
      "aliases": [
        "CCBR–NSD equivalence proof",
        "iterated minimax theorem"
      ],
      "type": "concept",
      "description": "Section 3.3 shows equivalence between CCBR and strategies after iterated minimax dominance removal.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Main theorem 3.13."
    },
    {
      "name": "deploy intention-signalling protocols in multi-agent AI systems",
      "aliases": [
        "epsilon transparency deployment",
        "strategic translucency deployment"
      ],
      "type": "intervention",
      "description": "Implement at deployment a secure channel that reveals each agent’s intended action with configurable small probability ε to other agents.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Affects live interaction behaviour (Analogues of Nash Equilibrium section).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "No empirical deployments yet; grounded in theoretical model (Introduction).",
      "node_rationale": "Direct actionable step derived from implementation mechanism 9."
    },
    {
      "name": "apply iterated minimax-dominated strategy pruning during multi-agent RL training",
      "aliases": [
        "CCBR pruning in RL",
        "minimax deletion training"
      ],
      "type": "intervention",
      "description": "Integrate an algorithm that repeatedly removes minimax-dominated policies from the candidate set during fine-tuning/reinforcement learning.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Alters the fine-tuning/RL phase of policy optimisation (Characterizing CCBR section).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Algorithmic proposal without large-scale empirical studies.",
      "node_rationale": "Based on implementation mechanism 10."
    },
    {
      "name": "embed counterfactual individual-rationality checks in agent architecture",
      "aliases": [
        "IR counterfactual checks",
        "self-consistent rationality layer"
      ],
      "type": "intervention",
      "description": "At model-design time include a module that rejects actions yielding payoffs below the agent’s maximin value under opponents’ counterfactual responses.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Architectural feature added at design (Counterfactual Structures section).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Conceptual; requires further research.",
      "node_rationale": "Translates IR condition into design-time enforcement."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "lack of strategy translucency causing defection in PD settings",
      "target_node": "mutual defection in Prisoner's Dilemma among AI-controlled firms",
      "description": "Opacity leads each AI agent to view defection as dominant.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit example in Introduction.",
      "edge_rationale": "Introduction"
    },
    {
      "type": "mitigated_by",
      "source_node": "mutual defection in Prisoner's Dilemma among AI-controlled firms",
      "target_node": "introduce controlled intention leakage to enforce cooperative equilibria",
      "description": "Partial transparency removes incentive to defect.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Derived from theoretical condition p·ε>r.",
      "edge_rationale": "Introduction"
    },
    {
      "type": "enabled_by",
      "source_node": "translucency shifts equilibrium to cooperation when penalty exceeds reward and leak probability epsilon",
      "target_node": "introduce controlled intention leakage to enforce cooperative equilibria",
      "description": "Insight provides rationale for design.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Directly proved in paper.",
      "edge_rationale": "Introduction"
    },
    {
      "type": "implemented_by",
      "source_node": "introduce controlled intention leakage to enforce cooperative equilibria",
      "target_node": "randomised low-probability strategy disclosure channels between agents",
      "description": "Design is realised via probabilistic disclosure mechanism.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inferred practical mechanism.",
      "edge_rationale": "Implementation inference"
    },
    {
      "type": "validated_by",
      "source_node": "randomised low-probability strategy disclosure channels between agents",
      "target_node": "theoretical proof of cooperation under translucency",
      "description": "Mathematical proof supports mechanism’s efficacy.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Formal theorem.",
      "edge_rationale": "Introduction proof"
    },
    {
      "type": "motivates",
      "source_node": "theoretical proof of cooperation under translucency",
      "target_node": "deploy intention-signalling protocols in multi-agent AI systems",
      "description": "Proof provides motivation for deployment intervention.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Theoretical but clear implication.",
      "edge_rationale": "Conclusion/Discussion"
    },
    {
      "type": "caused_by",
      "source_node": "assumption of unilateral deviations leading to minimax dominated strategies persisting",
      "target_node": "inefficient equilibria from classical rationalizable strategies in AI agents",
      "description": "Fixed-opponent assumption keeps dominated strategies viable.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Formal discussion in sections 3.1–3.2.",
      "edge_rationale": "Section 3.1"
    },
    {
      "type": "mitigated_by",
      "source_node": "inefficient equilibria from classical rationalizable strategies in AI agents",
      "target_node": "adopt CCBR-based reasoning in AI policy formation",
      "description": "CCBR removes minimax-dominated strategies.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Proved equivalence in Theorem 3.13.",
      "edge_rationale": "Section 3.3"
    },
    {
      "type": "implemented_by",
      "source_node": "adopt CCBR-based reasoning in AI policy formation",
      "target_node": "iterated minimax-dominated strategy deletion algorithm during policy search",
      "description": "Algorithmic pruning realises CCBR policies.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Implementation inference from theory.",
      "edge_rationale": "Section 3.2"
    },
    {
      "type": "validated_by",
      "source_node": "iterated minimax-dominated strategy deletion algorithm during policy search",
      "target_node": "proof that CCBR strategies equal those surviving minimax deletion",
      "description": "Equivalence theorem confirms algorithm’s correctness.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Theorem 3.13.",
      "edge_rationale": "Section 3.3"
    },
    {
      "type": "motivates",
      "source_node": "proof that CCBR strategies equal those surviving minimax deletion",
      "target_node": "apply iterated minimax-dominated strategy pruning during multi-agent RL training",
      "description": "Proof encourages integrating pruning in RL.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical implication, no experiments.",
      "edge_rationale": "Section 3.3"
    },
    {
      "type": "motivates",
      "source_node": "adopt CCBR-based reasoning in AI policy formation",
      "target_node": "embed counterfactual individual-rationality checks in agent architecture",
      "description": "Ensuring IR via counterfactual reasoning enforces safer actions.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Design inference based on IR definitions.",
      "edge_rationale": "Section 4"
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2013-08-17T00:00:00Z"
    },
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "title",
      "value": "Game Theory with Translucent Players"
    },
    {
      "key": "authors",
      "value": [
        "Joseph Y. Halpern",
        "Rafael Pass"
      ]
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1308.3778"
    }
  ]
}