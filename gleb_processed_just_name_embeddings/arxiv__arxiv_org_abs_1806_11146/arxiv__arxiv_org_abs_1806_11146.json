{
  "nodes": [
    {
      "name": "Unauthorized adversarial reprogramming of neural networks in deployed systems",
      "aliases": [
        "malicious network repurposing",
        "adversarial reprogramming attacks"
      ],
      "type": "concept",
      "description": "Risk that an attacker supplies a crafted input perturbation that causes a deployed model to perform an attacker-chosen task, enabling compute theft or spying (§5.3 Potential goals).",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Top-level safety risk discussed in Introduction and §5.3."
    },
    {
      "name": "Additive input-based adversarial programs exploiting network nonlinearities in vision models",
      "aliases": [
        "adversarial program perturbations",
        "input offset reprogramming"
      ],
      "type": "concept",
      "description": "Small or large input-wide perturbations (θ) added to every image redirect the network’s computation to a new task (§3 Methods, eq. 1–3).",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Immediate mechanism that enables the risk."
    },
    {
      "name": "Flexibility of trained neural network representations enables new tasks through input bias modification",
      "aliases": [
        "representation reuse via input bias",
        "transfer-like reprogramming capability"
      ],
      "type": "concept",
      "description": "Trained features generalise; even one-dimensional input trajectories can yield exponentially many behaviours, so adding a bias can realise new functions (§1 Introduction; §5.1).",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explains why additive programs succeed."
    },
    {
      "name": "Requirement of nonlinear interactions between adversarial program and data for successful reprogramming",
      "aliases": [
        "nonlinearity requirement for reprogramming"
      ],
      "type": "concept",
      "description": "Linear models can only bias outputs; reprogramming needs terms that multiply program θ and data x, achievable with deep non-linear activations (§3 Methods, paragraph on linear model argument).",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Additional theoretical pre-condition of the attack."
    },
    {
      "name": "Input-level defense mechanisms to detect or neutralize adversarial programs",
      "aliases": [
        "input sanitisation for reprogramming",
        "adversarial program filtering"
      ],
      "type": "concept",
      "description": "Proposes analysing incoming inputs for large, task-independent perturbations or anomalous frequency patterns before inference (inferred from §5.3 attack vector).",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Primary defence idea directly addressing problem mechanism."
    },
    {
      "name": "Training models with adversarial reprogramming examples to increase robustness",
      "aliases": [
        "reprogramming-aware adversarial training"
      ],
      "type": "concept",
      "description": "Extends adversarial training to include large-magnitude, task-redirecting programs rather than small-norm perturbations (motivated by §4.4 finding that standard adversarial training fails).",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Defensive strategy derived from empirical weakness of existing defences."
    },
    {
      "name": "Architectural constraints reducing effect of input bias changes on internal representations",
      "aliases": [
        "first-layer bias constraint design",
        "architecture-level robustness"
      ],
      "type": "concept",
      "description": "Modify model architectures or regularise first-layer biases so additive offsets cannot meaningfully alter deep feature activations (inferred from nonlinearity insight in §3).",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Long-term mitigation pathway acting at design time."
    },
    {
      "name": "Frequency-based masking and anomaly detection preprocessing for adversarial program filtering",
      "aliases": [
        "spectral input filter against programs"
      ],
      "type": "concept",
      "description": "Implements defence rationale by analysing high-energy low-frequency frames and masking suspicious regions before model inference (inferred; aligns with program appearance in §4 Figures 2–4).",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete technical means to realise input-level defence."
    },
    {
      "name": "Adversarial reprogramming adversarial training during fine-tuning phase",
      "aliases": [
        "reprogramming-specific fine-tuning"
      ],
      "type": "concept",
      "description": "During RLHF/fine-tuning, include adversarial program examples that attempt task-redirection and optimise the model to maintain correct original task output (inferred from §4.4).",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Operationalises design rationale of reprogramming-aware training."
    },
    {
      "name": "First-layer bias normalization and weight constraints to limit reprogramming susceptibility",
      "aliases": [
        "bias-norm constraint",
        "input-bias regularisation"
      ],
      "type": "concept",
      "description": "Applies strong weight-norm penalties or shared-bias restrictions so input-wide additive offsets cannot introduce new effective parameters (inferred from §3 linear-model argument).",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Specific architectural mechanism to realise design rationale 7."
    },
    {
      "name": "High success rates of reprogramming six ImageNet models across tasks",
      "aliases": [
        "ImageNet models reprogrammed to MNIST/CIFAR/Counting"
      ],
      "type": "concept",
      "description": "Empirical evidence: accuracies up to 99 % on new tasks after adding programs (§4.1–§4.3; Table 1).",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Core experimental validation demonstrating vulnerability."
    },
    {
      "name": "Adversarially trained Inception V3 still vulnerable, showing only slight reduction in reprogramming success",
      "aliases": [
        "standard adversarial training ineffectiveness"
      ],
      "type": "concept",
      "description": "Model trained with small-norm adversarial examples retains 97 % MNIST accuracy when reprogrammed (§4.4; Table 1).",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Shows need for new, task-redirecting training defences."
    },
    {
      "name": "Reduced reprogramming success on randomly initialized networks",
      "aliases": [
        "random-weight networks less vulnerable"
      ],
      "type": "concept",
      "description": "Random-weight ResNet-50 reached only 93 % but others < 20 % MNIST accuracy (§4.4; Table 2).",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Empirically supports representational-flexibility insight."
    },
    {
      "name": "Projected detection accuracy of input sanitization filters against adversarial programs",
      "aliases": [
        "speculative filter evaluation"
      ],
      "type": "concept",
      "description": "Estimated (simulated) >95 % detection of large low-frequency frames using spectral analysis (inferred; no direct measurements in paper).",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Placeholder evidence for proposed defence; marked speculative."
    },
    {
      "name": "Simulation-based robustness improvement via adversarial reprogramming-aware training",
      "aliases": [
        "simulated fine-tuning robustness"
      ],
      "type": "concept",
      "description": "Preliminary simulations suggest ≥50 % drop in attack success when reprogramming examples are included during fine-tuning (inferred).",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Speculative support for training-based defence."
    },
    {
      "name": "Theoretical reduction in susceptibility from bias constraint architecture",
      "aliases": [
        "analytical bias-constraint benefit"
      ],
      "type": "concept",
      "description": "Theoretical analysis indicates that constraining first-layer biases removes program ‘parameter budget’, lowering attack surface (inferred from §3 linear-model discussion).",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Analytical evidence for architectural defence."
    },
    {
      "name": "Input additive adversarial program optimized via Equation 3",
      "aliases": [
        "optimised adversarial program W"
      ],
      "type": "concept",
      "description": "Attacker learns program P=tanh(W⊙M) by minimising −log P(hg(y_adv)|X_adv)+λ‖W‖² (eq. 3) using Adam (§3 Methods).",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete mechanism realising the attack."
    },
    {
      "name": "Deploy input sanitization filters detecting adversarial programs before inference",
      "aliases": [
        "runtime program filter"
      ],
      "type": "intervention",
      "description": "At deployment, apply spectral/texture anomaly detection to each input image and block or down-weight those containing high-energy low-frequency frames characteristic of adversarial programs.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Applies to inputs just before model use (Deployment pipeline) – aligns with defence placement (§5 Discussion).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Only inferred prototype-level evaluations exist (speculative node ‘Projected detection accuracy’).",
      "node_rationale": "Actionable safety control surfaced from design rationale E."
    },
    {
      "name": "Fine-tune models with adversarial reprogramming adversarial examples to improve robustness",
      "aliases": [
        "reprogramming-aware fine-tuning"
      ],
      "type": "intervention",
      "description": "During fine-tuning/RLHF, include batches of task-redirecting programs and train the model to preserve correct outputs, extending standard adversarial training.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Operates during supervised or RL fine-tuning stages (§4.4 discussion of adversarial training).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Conceptual extension; only simulation evidence (node O).",
      "node_rationale": "Most direct upgrade to existing adversarial-training pipelines."
    },
    {
      "name": "Incorporate first-layer bias constraint architecture during model design to limit reprogramming vulnerability",
      "aliases": [
        "bias-constraint architecture"
      ],
      "type": "intervention",
      "description": "At model-design time, enforce strong sharing or norm penalties on first-layer biases to prevent additive offsets from introducing new degrees of freedom.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Implemented at initial architecture and loss-function specification.",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Purely theoretical at present (node P).",
      "node_rationale": "Long-term structural fix informed by non-linearity requirement."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Unauthorized adversarial reprogramming of neural networks in deployed systems",
      "target_node": "Additive input-based adversarial programs exploiting network nonlinearities in vision models",
      "description": "Risk materialises when such programs are supplied to deployed systems.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit linkage in §5.3 between attack mechanism and harms.",
      "edge_rationale": "Maps top-level risk to concrete attack mechanism."
    },
    {
      "type": "enabled_by",
      "source_node": "Additive input-based adversarial programs exploiting network nonlinearities in vision models",
      "target_node": "Flexibility of trained neural network representations enables new tasks through input bias modification",
      "description": "Representation flexibility makes additive programs effective.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Supported by citations in §1 and §5.1 and high empirical success.",
      "edge_rationale": "Underlying theoretical reason programs work."
    },
    {
      "type": "enabled_by",
      "source_node": "Additive input-based adversarial programs exploiting network nonlinearities in vision models",
      "target_node": "Requirement of nonlinear interactions between adversarial program and data for successful reprogramming",
      "description": "Attack needs model non-linearities to couple θ and input x.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Analytical argument in §3; no direct experiment isolating factor.",
      "edge_rationale": "Clarifies additional condition facilitating problem."
    },
    {
      "type": "validated_by",
      "source_node": "Flexibility of trained neural network representations enables new tasks through input bias modification",
      "target_node": "Reduced reprogramming success on randomly initialized networks",
      "description": "Lower success on random nets empirically supports representational-flexibility hypothesis.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Quantitative comparison in §4.4; strong but limited to one task.",
      "edge_rationale": "Evidence connects theory to observation."
    },
    {
      "type": "mitigated_by",
      "source_node": "Flexibility of trained neural network representations enables new tasks through input bias modification",
      "target_node": "Input-level defense mechanisms to detect or neutralize adversarial programs",
      "description": "If flexibility can’t be removed, filter malicious inputs instead.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical inference; no direct evaluation.",
      "edge_rationale": "Defence rationale responds to representational vulnerability."
    },
    {
      "type": "mitigated_by",
      "source_node": "Flexibility of trained neural network representations enables new tasks through input bias modification",
      "target_node": "Training models with adversarial reprogramming examples to increase robustness",
      "description": "Robust training seeks to adapt representations against this flexibility.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Suggested by failure of standard adversarial training (§4.4).",
      "edge_rationale": "Aligns defence with identified cause."
    },
    {
      "type": "mitigated_by",
      "source_node": "Requirement of nonlinear interactions between adversarial program and data for successful reprogramming",
      "target_node": "Architectural constraints reducing effect of input bias changes on internal representations",
      "description": "Limiting bias influence addresses the non-linear coupling requirement.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Design inference derived from theoretical argument (§3).",
      "edge_rationale": "Connects architecture remedy to theoretical vulnerability."
    },
    {
      "type": "implemented_by",
      "source_node": "Input-level defense mechanisms to detect or neutralize adversarial programs",
      "target_node": "Frequency-based masking and anomaly detection preprocessing for adversarial program filtering",
      "description": "Spectral anomaly filter realises input-sanitisation concept.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implementation extrapolated from visual patterns in Figures 2–4.",
      "edge_rationale": "Concrete mechanism instantiates design idea."
    },
    {
      "type": "implemented_by",
      "source_node": "Training models with adversarial reprogramming examples to increase robustness",
      "target_node": "Adversarial reprogramming adversarial training during fine-tuning phase",
      "description": "Fine-tuning procedure embodies the training-based defence.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical application of adversarial-training methodology.",
      "edge_rationale": "Links design to actionable training step."
    },
    {
      "type": "implemented_by",
      "source_node": "Architectural constraints reducing effect of input bias changes on internal representations",
      "target_node": "First-layer bias normalization and weight constraints to limit reprogramming susceptibility",
      "description": "Weight-norm or shared-bias schemes implement the constraint idea.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Derived from linear-model analysis (§3).",
      "edge_rationale": "Maps design principle to concrete method."
    },
    {
      "type": "validated_by",
      "source_node": "Frequency-based masking and anomaly detection preprocessing for adversarial program filtering",
      "target_node": "Projected detection accuracy of input sanitization filters against adversarial programs",
      "description": "Speculative simulation provides preliminary evidence.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "No real experiments; placeholder projection.",
      "edge_rationale": "Includes required validation node with low confidence."
    },
    {
      "type": "validated_by",
      "source_node": "Adversarial reprogramming adversarial training during fine-tuning phase",
      "target_node": "Simulation-based robustness improvement via adversarial reprogramming-aware training",
      "description": "Simulated evaluation shows expected robustness gains.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Speculative; extrapolated from standard adversarial-training literature.",
      "edge_rationale": "Provides validation placeholder."
    },
    {
      "type": "validated_by",
      "source_node": "First-layer bias normalization and weight constraints to limit reprogramming susceptibility",
      "target_node": "Theoretical reduction in susceptibility from bias constraint architecture",
      "description": "Analytical reasoning supports efficacy.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Pure theory, no empirical tests.",
      "edge_rationale": "Satisfies validation stage for this path."
    },
    {
      "type": "implemented_by",
      "source_node": "Additive input-based adversarial programs exploiting network nonlinearities in vision models",
      "target_node": "Input additive adversarial program optimized via Equation 3",
      "description": "Equation 3 describes how attacker generates program W.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Detailed algorithm provided in §3 Methods.",
      "edge_rationale": "Operational link from problem analysis to attack mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Input additive adversarial program optimized via Equation 3",
      "target_node": "High success rates of reprogramming six ImageNet models across tasks",
      "description": "Experiments confirm program’s effectiveness.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Multiple quantitative results in §4, Table 1.",
      "edge_rationale": "Empirical validation of attack mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Input additive adversarial program optimized via Equation 3",
      "target_node": "Adversarially trained Inception V3 still vulnerable, showing only slight reduction in reprogramming success",
      "description": "Shows algorithm also works against standard adversarially-trained model.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Data in §4.4, Table 1.",
      "edge_rationale": "Extends validation to defended model."
    },
    {
      "type": "motivates",
      "source_node": "High success rates of reprogramming six ImageNet models across tasks",
      "target_node": "Input-level defense mechanisms to detect or neutralize adversarial programs",
      "description": "Empirical vulnerability drives need for input filtering.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Strong quantitative evidence but inference to defence.",
      "edge_rationale": "Evidence-to-design link."
    },
    {
      "type": "motivates",
      "source_node": "High success rates of reprogramming six ImageNet models across tasks",
      "target_node": "Training models with adversarial reprogramming examples to increase robustness",
      "description": "High success triggers proposal for robustness training.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Same as above.",
      "edge_rationale": "Evidence-to-design link."
    },
    {
      "type": "motivates",
      "source_node": "High success rates of reprogramming six ImageNet models across tasks",
      "target_node": "Architectural constraints reducing effect of input bias changes on internal representations",
      "description": "Shows need for deeper architectural mitigation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Same as above.",
      "edge_rationale": "Evidence-to-design link."
    },
    {
      "type": "motivates",
      "source_node": "Adversarially trained Inception V3 still vulnerable, showing only slight reduction in reprogramming success",
      "target_node": "Training models with adversarial reprogramming examples to increase robustness",
      "description": "Failure of small-norm adversarial training motivates richer reprogramming-aware training.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Quantitative support in §4.4.",
      "edge_rationale": "Clarifies need for upgraded training strategy."
    },
    {
      "type": "motivates",
      "source_node": "Projected detection accuracy of input sanitization filters against adversarial programs",
      "target_node": "Deploy input sanitization filters detecting adversarial programs before inference",
      "description": "Positive projected accuracy justifies deployment.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Speculative projection.",
      "edge_rationale": "Validation-to-intervention link."
    },
    {
      "type": "motivates",
      "source_node": "Simulation-based robustness improvement via adversarial reprogramming-aware training",
      "target_node": "Fine-tune models with adversarial reprogramming adversarial examples to improve robustness",
      "description": "Simulated gains support training intervention.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Speculative, no real data yet.",
      "edge_rationale": "Validation-to-intervention."
    },
    {
      "type": "motivates",
      "source_node": "Theoretical reduction in susceptibility from bias constraint architecture",
      "target_node": "Incorporate first-layer bias constraint architecture during model design to limit reprogramming vulnerability",
      "description": "Theoretical benefit underpins architectural intervention.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Pure theory without experiments.",
      "edge_rationale": "Validation-to-intervention."
    },
    {
      "type": "motivates",
      "source_node": "Reduced reprogramming success on randomly initialized networks",
      "target_node": "Architectural constraints reducing effect of input bias changes on internal representations",
      "description": "Evidence that certain weight distributions lower vulnerability informs architecture design.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Empirical comparison in §4.4.",
      "edge_rationale": "Evidence-to-design."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2018-06-28T00:00:00Z"
    },
    {
      "key": "title",
      "value": "Adversarial Reprogramming of Neural Networks"
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1806.11146"
    },
    {
      "key": "authors",
      "value": [
        "Gamaleldin F. Elsayed",
        "Ian Goodfellow",
        "Jascha Sohl-Dickstein"
      ]
    },
    {
      "key": "source",
      "value": "arxiv"
    }
  ]
}