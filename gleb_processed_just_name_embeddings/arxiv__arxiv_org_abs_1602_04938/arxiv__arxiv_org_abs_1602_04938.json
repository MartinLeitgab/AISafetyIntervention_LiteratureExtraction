{
  "nodes": [
    {
      "name": "Erroneous AI-assisted decision making in high-stakes contexts",
      "aliases": [
        "model-driven catastrophic decisions",
        "wrong ML predictions in critical applications"
      ],
      "type": "concept",
      "description": "Situations such as medical diagnosis or terrorism detection where acting on an incorrect or misunderstood ML prediction can lead to severe harm.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explicitly referenced as catastrophic consequences if users cannot verify predictions – see 1 Introduction."
    },
    {
      "name": "Lack of user trust due to opaque model predictions",
      "aliases": [
        "black-box model opacity",
        "uninterpretable predictions undermine trust"
      ],
      "type": "concept",
      "description": "Users cannot see why a prediction was made, making them either over-trust or under-trust the output.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivating problem described throughout 1 Introduction and 2 The case for explanations."
    },
    {
      "name": "Validation accuracy overestimation due to dataset shift and data leakage in ML models",
      "aliases": [
        "misleading held-out accuracy",
        "hidden data leakage & shift"
      ],
      "type": "concept",
      "description": "When training and validation data contain spurious correlations or shift, reported accuracy is inflated and misleads deployment decisions.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed with patient-ID leak and 20 newsgroups shift – section 2."
    },
    {
      "name": "Faithful local explanations align model reasoning with human prior knowledge",
      "aliases": [
        "locally faithful interpretable explanations",
        "explanation fidelity to model behavior"
      ],
      "type": "concept",
      "description": "Providing explanations that accurately reflect the model’s behavior in the vicinity of each instance lets humans judge predictions using their domain expertise.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Formalized as requirement of local fidelity in 2 Desired Characteristics and 3.2."
    },
    {
      "name": "Representative explanation subset enables global understanding of model behavior",
      "aliases": [
        "coverage-based explanation selection",
        "global trust through chosen instances"
      ],
      "type": "concept",
      "description": "Selecting a small, diverse set of explanation-instance pairs helps users form an overall mental model of when to trust the classifier.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined in 4 Submodular Pick for Explaining Models."
    },
    {
      "name": "Model-agnostic explanation framework to handle any classifier",
      "aliases": [
        "classifier-agnostic explanation approach",
        "black-box explainability strategy"
      ],
      "type": "concept",
      "description": "Design choice to approximate any underlying black-box model using an interpretable surrogate without needing model internals.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Described as core rationale of LIME in 3 Local Interpretable Model-Agnostic Explanations."
    },
    {
      "name": "Selection of diverse non-redundant explanations via submodular optimization",
      "aliases": [
        "submodular pick rationale",
        "coverage-oriented explanation selection"
      ],
      "type": "concept",
      "description": "Design choice to maximize feature coverage while respecting user attention budget, formalized as a weighted coverage problem.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivates SP-LIME algorithm – section 4."
    },
    {
      "name": "Perturbation-based sparse linear surrogate modeling (LIME)",
      "aliases": [
        "local surrogate linear model",
        "K-LASSO explanation mechanism"
      ],
      "type": "concept",
      "description": "Generate synthetic neighborhood around an instance, weight samples by proximity, fit a sparse linear model to approximate the black-box locally.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Algorithm 1 detailed in 3.3–3.4."
    },
    {
      "name": "Explanation matrix coverage maximization algorithm (SP-LIME)",
      "aliases": [
        "submodular pick algorithm",
        "coverage-greedy explanation selector"
      ],
      "type": "concept",
      "description": "Construct explanation matrix, compute feature importance, greedily pick B instances that maximize covered importance with 1-1/e guarantee.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Algorithm 2 in section 4."
    },
    {
      "name": "Simulated user experiments demonstrate >90% recall of important features and improved trust decisions",
      "aliases": [
        "simulation evidence for LIME",
        "quantitative evaluation with synthetic users"
      ],
      "type": "concept",
      "description": "Experiments on sentiment datasets show LIME explanations capture gold features and let simulated users correctly trust/mistrust predictions.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Results in 5.2–5.4."
    },
    {
      "name": "Human subject studies show better classifier selection, feature engineering improvements, detection of spurious correlations",
      "aliases": [
        "user study evidence for LIME",
        "empirical human evaluation"
      ],
      "type": "concept",
      "description": "Mechanical-Turk and graduate-student studies confirm that explanations help non-experts pick better models, clean features, and uncover snow-background bias.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Sections 6.2–6.4."
    },
    {
      "name": "Deploy LIME explanations to accompany individual predictions in deployment interfaces",
      "aliases": [
        "provide on-demand local explanations at runtime",
        "integrate LIME in user UI"
      ],
      "type": "intervention",
      "description": "Present LIME-generated highlighted features or super-pixels alongside each prediction so end-users can judge trustworthiness.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Addresses runtime decision support – referenced in Figure 1 and 6 user studies.",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Prototype implementations tested with real users but not yet industry-wide – sections 5 & 6.",
      "node_rationale": "Direct actionable step implied by paper’s deployment scenarios."
    },
    {
      "name": "Use SP-LIME to present B representative explanations for global model assessment before deployment",
      "aliases": [
        "pre-deployment SP-LIME audit",
        "coverage-based explanation audit"
      ],
      "type": "intervention",
      "description": "Run SP-LIME on validation data to select a small set of explanations that auditors inspect to judge model readiness.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Applied after training but before deployment to audit model – section 4 experiments.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Evaluated experimentally with users but not systematically validated at scale.",
      "node_rationale": "Enables global trust decision paths demonstrated in 6.2."
    },
    {
      "name": "Iteratively remove untrustworthy features via human-in-the-loop feature engineering guided by LIME explanations",
      "aliases": [
        "LIME-assisted feature pruning",
        "explanation-guided dataset cleaning"
      ],
      "type": "intervention",
      "description": "Users inspect LIME explanations, mark non-generalizing features, retrain the model, and repeat, improving real-world accuracy.",
      "concept_category": null,
      "intervention_lifecycle": "2",
      "intervention_lifecycle_rationale": "Operates on training data/features before final training – section 6.3.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated with crowd workers but limited scale and domain.",
      "node_rationale": "Suggested in paper as concrete way to convert untrustworthy models into trustworthy ones."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Erroneous AI-assisted decision making in high-stakes contexts",
      "target_node": "Lack of user trust due to opaque model predictions",
      "description": "When predictions are opaque, users may either ignore correct advice or follow incorrect advice, both leading to erroneous decisions.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoned from examples in 1 Introduction; explicitly stated but not quantified.",
      "edge_rationale": "1 Introduction links opacity to inability to act safely on predictions."
    },
    {
      "type": "caused_by",
      "source_node": "Erroneous AI-assisted decision making in high-stakes contexts",
      "target_node": "Validation accuracy overestimation due to dataset shift and data leakage in ML models",
      "description": "Inflated validation accuracy masks real-world failure modes, leading to wrong deployment decisions.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Supported by patient-ID leakage and 20 newsgroups examples – section 2.",
      "edge_rationale": "Section 2 shows how leakage/shift causes misuse."
    },
    {
      "type": "mitigated_by",
      "source_node": "Lack of user trust due to opaque model predictions",
      "target_node": "Faithful local explanations align model reasoning with human prior knowledge",
      "description": "Faithful explanations let users verify or contest each prediction, calibrating trust.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Backed by user studies showing improved trust calibration – sections 5.3 and 6.4.",
      "edge_rationale": "Paper argues explanations solve opacity."
    },
    {
      "type": "mitigated_by",
      "source_node": "Validation accuracy overestimation due to dataset shift and data leakage in ML models",
      "target_node": "Representative explanation subset enables global understanding of model behavior",
      "description": "Reviewing diverse explanations reveals spurious features and dataset issues that accuracy alone hides.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Demonstrated in 6.2 & 6.3 where users detect leakage and shift via explanations.",
      "edge_rationale": "Global explanation set surfaces leakage indicators."
    },
    {
      "type": "implemented_by",
      "source_node": "Faithful local explanations align model reasoning with human prior knowledge",
      "target_node": "Model-agnostic explanation framework to handle any classifier",
      "description": "To achieve faithful explanations, framework must work regardless of underlying model internals.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Design choice explicitly justified in 3 Introduction to LIME.",
      "edge_rationale": "Model-agnosticism is presented as how to realize local fidelity."
    },
    {
      "type": "implemented_by",
      "source_node": "Representative explanation subset enables global understanding of model behavior",
      "target_node": "Selection of diverse non-redundant explanations via submodular optimization",
      "description": "Coverage-based selection operationalizes the idea of representative explanations.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Formal optimization in section 4 directly derived from theoretical goal.",
      "edge_rationale": "Submodular pick embodies the theory."
    },
    {
      "type": "implemented_by",
      "source_node": "Model-agnostic explanation framework to handle any classifier",
      "target_node": "Perturbation-based sparse linear surrogate modeling (LIME)",
      "description": "LIME’s perturb-and-fit surrogate provides the concrete mechanism for model-agnostic local explanations.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Algorithm fully specified and evaluated – 3.3–3.4.",
      "edge_rationale": "LIME instantiates the framework."
    },
    {
      "type": "implemented_by",
      "source_node": "Selection of diverse non-redundant explanations via submodular optimization",
      "target_node": "Explanation matrix coverage maximization algorithm (SP-LIME)",
      "description": "SP-LIME applies greedy submodular maximization to choose explanations under user budget.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Algorithm 2 and theoretical guarantee – section 4.",
      "edge_rationale": "SP-LIME operationalizes selection design."
    },
    {
      "type": "validated_by",
      "source_node": "Perturbation-based sparse linear surrogate modeling (LIME)",
      "target_node": "Simulated user experiments demonstrate >90% recall of important features and improved trust decisions",
      "description": "Experiments show surrogate explanations recover ground-truth features and guide simulated trust.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Quantitative metrics in 5.2–5.4.",
      "edge_rationale": "Validation focuses on surrogate fidelity."
    },
    {
      "type": "validated_by",
      "source_node": "Explanation matrix coverage maximization algorithm (SP-LIME)",
      "target_node": "Human subject studies show better classifier selection, feature engineering improvements, detection of spurious correlations",
      "description": "Human studies confirm SP-LIME helps users audit models and engineer features effectively.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Empirical results in 6.2–6.4 with statistical significance.",
      "edge_rationale": "SP-LIME selection validated by user outcomes."
    },
    {
      "type": "motivates",
      "source_node": "Simulated user experiments demonstrate >90% recall of important features and improved trust decisions",
      "target_node": "Deploy LIME explanations to accompany individual predictions in deployment interfaces",
      "description": "High fidelity and trust calibration motivate embedding LIME into prediction UIs.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Paper suggests deployment and reports prototype latency – section 3.4.",
      "edge_rationale": "Experimental success leads to deployment recommendation."
    },
    {
      "type": "motivates",
      "source_node": "Human subject studies show better classifier selection, feature engineering improvements, detection of spurious correlations",
      "target_node": "Use SP-LIME to present B representative explanations for global model assessment before deployment",
      "description": "Demonstrated ability of SP-LIME to help users choose better models motivates its use in formal audits.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 6.2 shows users selecting higher-generalizing models.",
      "edge_rationale": "Validation evidence supports audit intervention."
    },
    {
      "type": "motivates",
      "source_node": "Human subject studies show better classifier selection, feature engineering improvements, detection of spurious correlations",
      "target_node": "Iteratively remove untrustworthy features via human-in-the-loop feature engineering guided by LIME explanations",
      "description": "Feature-engineering study shows users can iteratively improve models when guided by explanations.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 6.3 quantitative gains on religion dataset.",
      "edge_rationale": "Evidence directly demonstrates efficacy of intervention."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2016-02-16T00:00:00Z"
    },
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "title",
      "value": "\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier"
    },
    {
      "key": "authors",
      "value": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ]
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1602.04938"
    }
  ]
}