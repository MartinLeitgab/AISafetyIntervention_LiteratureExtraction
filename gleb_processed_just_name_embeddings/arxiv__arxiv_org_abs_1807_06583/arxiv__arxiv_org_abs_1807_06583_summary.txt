Summary  
This paper proposes a weakly–supervised β-VAE framework that aligns each latent dimension with a single user-defined concept group (e.g. colour, shape) so that robots can (1) ground linguistic symbols in vision, (2) recognise unfamiliar objects (“unknown unknowns”), and (3) offer human-interpretable internal representations.  Experiments on modified dSprites and real table-top object data show markedly better axis-alignment and F1 scores—especially for unknown-object detection—than unsupervised β-VAE and a classifier-only baseline.

EXTRACTION CONFIDENCE[84]  
The fabric captures every causal pathway the paper presents—from safety risks through representational problems, theoretical insights, design choices, concrete mechanisms, empirical validation, to two actionable interventions.  I am reasonably confident (84 %) that nodes and edges are correctly typed, named and connected; minor wording differences in section titles could be improved with stricter citation rules.

Inference strategy justification  
Where the text implied but did not explicitly label interventions (e.g. “deploy classifier at test-time”), I used light inference (edge confidence 2) and marked it.  Otherwise paths follow explicit statements.

Extraction completeness explanation  
All key reasoning steps (risks, entanglement problems, axis-alignment insight, auxiliary classifier design, β-VAE implementation, validation, interventions) are represented.  No isolated nodes remain; the two interventions are both downstream of validation evidence and connect back to the original risks.

Key limitations  
• The paper does not quantify real-world deployment, so intervention maturity is capped at 2 (experimental).  
• Section cross-references inside arXiv vanity renderings are approximate; finer-grained paragraph IDs would raise traceability.