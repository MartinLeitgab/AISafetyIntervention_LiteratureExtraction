Summary:
Data source overview: The survey summarises the risks created by opaque-box machine-learning models, analyses why a lack of interpretability and completeness leads to bias and mistrust, introduces theoretical insights such as the interpretability-completeness trade-off, proposes a taxonomy-guided design rationale for explainable AI, reviews concrete implementation mechanisms (proxy models, decision-tree extraction, salience maps, attention networks, disentangled representations, explanation generators) and reports the kinds of validation evidence used in the literature.  
Extraction confidence[83]  
Inference strategy justification: The paper is a survey; explicit interventions are rarely stated. Interventions were therefore lightly inferred from the implementation mechanisms and evaluation recommendations that the authors describe (edge confidence 2 for such inferences).  
Extraction completeness: All causal-interventional pathways appearing in the survey have been captured, starting from top-level risks and ending in concrete, actionable interventions. Shared nodes (risk, problem analysis, theoretical insights, design rationales) inter-connect all branches so that no path is isolated.  
Key limitations: The survey does not supply quantitative results, so validation-evidence nodes rely on qualitative consensus; edge strengths are therefore 3â€“4. Some intervention names required moderate inference and are marked with lower maturity.

JSON fabric:



Key limitations:
1. The survey aggregates rather than contributes new quantitative studies; therefore validation evidence nodes reflect secondary reporting.
2. Some implementation branches (e.g., automatic rule extraction) are not followed through to separate interventions to keep the graph concise; downstream merging can add them.
3. Intervention maturity and lifecycle rely on light inference that readers of the survey would reasonably make.

Please let us know if further granularity or additional branches are desired.