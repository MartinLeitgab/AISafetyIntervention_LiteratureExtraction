{
  "nodes": [
    {
      "name": "Unreliable and biased decisions from opaque machine learning models in safety-critical contexts",
      "aliases": [
        "unsafe black-box AI outcomes",
        "biased algorithmic decisions"
      ],
      "type": "concept",
      "description": "Real-world failures such as racial bias in COMPAS and adversarial mis-classifications show that opaque ML models can act unreliably in domains like justice, finance and safety-critical control.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduction – examples of unsafe behaviour motivate the need for explanations."
    },
    {
      "name": "Erosion of user trust and regulatory non-compliance from unexplained AI decisions",
      "aliases": [
        "mistrust of AI",
        "Right-to-Explanation violations"
      ],
      "type": "concept",
      "description": "Opaque models hinder compliance with the EU Right-to-Explanation and reduce public confidence in AI outputs.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduction – discussion of regulation and public mistrust."
    },
    {
      "name": "Opaqueness of deep neural network inner workings",
      "aliases": [
        "black-box nature of DNNs",
        "hidden internal mechanisms"
      ],
      "type": "concept",
      "description": "DNNs employ millions of parameters and billions of operations, making their decision logic inaccessible to humans.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section III – scale and complexity of modern networks."
    },
    {
      "name": "Lack of interpretability and completeness in model explanations",
      "aliases": [
        "missing faithful explanations",
        "inadequate transparency"
      ],
      "type": "concept",
      "description": "Current explanation techniques often sacrifice faithfulness for simplicity, or vice-versa, leaving stakeholders without dependable understanding.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section II-B – interpretability vs. completeness."
    },
    {
      "name": "Tradeoff between interpretability and completeness in AI explanations",
      "aliases": [
        "interpretability-completeness dilemma",
        "simplicity-accuracy trade-off"
      ],
      "type": "concept",
      "description": "The paper argues that no single explanation can be both maximally human-understandable and fully faithful; systems must expose multiple levels of detail.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section II-B – central theoretical claim."
    },
    {
      "name": "Ethical imperative to avoid persuasive but unfaithful explanations",
      "aliases": [
        "danger of misleading explanations",
        "persuasion vs. transparency issue"
      ],
      "type": "concept",
      "description": "Providing simplified, trust-inducing narratives without revealing limitations is deemed unethical and may cause harmful reliance.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section II-B – ethical dilemmas in explanation design."
    },
    {
      "name": "Taxonomy-guided approach to develop explainable AI methods",
      "aliases": [
        "explanation taxonomy",
        "categorisation of explanation methods"
      ],
      "type": "concept",
      "description": "Section V proposes categorising approaches into processing, representation, and explanation-producing to guide researchers toward appropriate techniques.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section V – motivation for a taxonomy."
    },
    {
      "name": "Multi-level explanation capability accommodating diverse user needs",
      "aliases": [
        "progressive disclosure of explanations",
        "layered explanation design"
      ],
      "type": "concept",
      "description": "Systems should allow users to select explanation granularity along the interpretability-completeness curve, enabling audits and detailed analysis when needed.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section II-B and VI – recommendation for adjustable detail."
    },
    {
      "name": "Local proxy models for processing explanations in deep networks",
      "aliases": [
        "LIME local linear surrogates",
        "proxy explanation models"
      ],
      "type": "concept",
      "description": "Methods such as LIME fit simple, interpretable models to local perturbations of an input, approximating the complex model’s behaviour nearby.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section III-A1 – description of LIME."
    },
    {
      "name": "Decision tree extraction from neural networks",
      "aliases": [
        "DeepRED decision-tree proxies",
        "ANN-DT extraction"
      ],
      "type": "concept",
      "description": "Algorithms such as DeepRED decompose a trained network into a decision tree that approximates its logic, enabling symbolic inspection.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section III-A2 – decision-tree methods."
    },
    {
      "name": "Salience mapping via input occlusion and gradient analysis",
      "aliases": [
        "saliency maps",
        "feature importance heat-maps"
      ],
      "type": "concept",
      "description": "Techniques like occlusion, Integrated Gradients and Grad-CAM highlight regions of the input that most influence the prediction.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section III-A4 – salience methods."
    },
    {
      "name": "Automatic rule extraction techniques for neural network decisions",
      "aliases": [
        "KT rule extraction",
        "RxREN"
      ],
      "type": "concept",
      "description": "Decompositional and pedagogical methods generate IF-THEN rules explaining neuron or input–output relations, increasing transparency of model logic.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section III-A3 – rule extraction survey."
    },
    {
      "name": "Attention-based network architectures exposing internal focus",
      "aliases": [
        "neural attention mechanisms",
        "self-explanatory attention"
      ],
      "type": "concept",
      "description": "Models employing learned attention weights reveal which parts of the input the network relies on, and can be steered to align with human focus.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section III-C1 – attention networks."
    },
    {
      "name": "Disentangled representation learning for interpretable factors",
      "aliases": [
        "beta-VAE disentanglement",
        "InfoGAN latent separation"
      ],
      "type": "concept",
      "description": "Training objectives such as β-VAE encourage latent variables to align with independent, human-interpretable factors, supporting explanation.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section III-C2 – disentangled representations."
    },
    {
      "name": "Explanation-generating models producing natural language and visual rationales",
      "aliases": [
        "VQA explanation generators",
        "\"because\" sentence models"
      ],
      "type": "concept",
      "description": "Architectures jointly train task performance with modules that output human-readable textual or multimodal explanations of each decision.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section III-C3 – generated explanations."
    },
    {
      "name": "Local fidelity metrics assessing proxy model faithfulness",
      "aliases": [
        "proxy completeness measures",
        "surrogate accuracy metrics"
      ],
      "type": "concept",
      "description": "The survey recommends quantifying how closely a local proxy model matches the original model around the instance being explained.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section VI-A – completeness to model for proxy methods."
    },
    {
      "name": "Occlusion-grounded sensitivity benchmarks for salience methods",
      "aliases": [
        "brute-force occlusion testing",
        "ground-truth salience evaluation"
      ],
      "type": "concept",
      "description": "Saliency maps are compared with exhaustive occlusion tests that measure how masking each region affects predictions.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section VI-A – salience evaluation example."
    },
    {
      "name": "Transfer learning performance measuring layer representation utility",
      "aliases": [
        "representation transfer benchmarks",
        "feature reuse tests"
      ],
      "type": "concept",
      "description": "Internal layer outputs reused on new tasks can gauge how informative and interpretable those representations are.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section III-B1 and VI-B – role-of-layers evaluation."
    },
    {
      "name": "Human attention alignment scores for attention mechanisms",
      "aliases": [
        "attention-human overlap metrics",
        "attention faithfulness tests"
      ],
      "type": "concept",
      "description": "Datasets of human eye-tracking or click attention enable comparison between model attention maps and human focus to validate explanation quality.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section VI-C – attention evaluation discussion."
    },
    {
      "name": "User studies evaluating clarity and helpfulness of generated explanations",
      "aliases": [
        "human evaluation of explanations",
        "trust and usability studies"
      ],
      "type": "concept",
      "description": "Participants judge generated textual and visual explanations for helpfulness, trust improvement and correctness understanding.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section VI-C – human evaluation for explanation-producing systems."
    },
    {
      "name": "Deploy local proxy explanation tools (e.g., LIME) to audit model predictions before deployment",
      "aliases": [
        "pre-deployment LIME auditing"
      ],
      "type": "intervention",
      "description": "Integrate local surrogate models such as LIME into the validation pipeline so auditors can inspect decision logic on representative cases.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Applies to Pre-Deployment Testing phase where predictions are audited (Section VI-A).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "LIME is widely prototyped and used in industry pilots but lacks large-scale regulated deployment evidence (Section III-A1).",
      "node_rationale": "Provides actionable step derived from proxy model mechanism."
    },
    {
      "name": "Apply decision tree extraction to neural networks for logic verification during pre-deployment review",
      "aliases": [
        "DeepRED compliance checking"
      ],
      "type": "intervention",
      "description": "Run algorithms such as DeepRED or ANN-DT on trained models to obtain decision trees for compliance and bias analysis.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Executed after training but prior to release to reveal internal logic (Section III-A2).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Methods are experimental and limited in scalability (Section III-A2).",
      "node_rationale": "Derived from decision-tree extraction mechanism for safety analysis."
    },
    {
      "name": "Conduct salience map sensitivity analyses to identify spurious correlations during model validation",
      "aliases": [
        "saliency auditing workflow"
      ],
      "type": "intervention",
      "description": "Systematically generate and benchmark saliency maps against occlusion tests to discover features the model relies on inappropriately.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Performed in validation phase to catch problems early (Section VI-A).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Salience techniques are broadly available and studied but not yet standardised (Section III-A4).",
      "node_rationale": "Actionable use of salience mechanism and evaluation evidence."
    },
    {
      "name": "Regularize attention weights during fine-tuning to align with human attention patterns",
      "aliases": [
        "attention alignment regularization"
      ],
      "type": "intervention",
      "description": "During fine-tuning, add loss terms that penalise divergence between model attention maps and human attention datasets to ensure faithful focus.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Implemented in Fine-Tuning/RL stage where additional losses are injected (Section III-C1).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Technique proposed in literature but validated only on limited benchmarks (Section III-C1).",
      "node_rationale": "Utilises attention mechanism and human alignment evidence."
    },
    {
      "name": "Incorporate disentanglement objectives in model design to enhance transparency",
      "aliases": [
        "β-VAE-based transparent design"
      ],
      "type": "intervention",
      "description": "Include β-VAE style KL-divergence scaling or InfoGAN mutual-information objectives at model-design time to yield factorised latent codes.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Modifies objective during initial architecture and loss design (Section III-C2).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Disentanglement successes shown on benchmarks but limited industrial usage (Section III-C2).",
      "node_rationale": "Directly addresses transparency via disentangled factors."
    },
    {
      "name": "Co-train natural language explanation generators with task models to provide end-user rationales at deployment",
      "aliases": [
        "joint explanation generation training"
      ],
      "type": "intervention",
      "description": "Train an auxiliary decoder that outputs textual or multimodal rationales conditioned on model decisions for user-facing systems.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Implemented during fine-tuning where auxiliary generative heads are trained (Section III-C3).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated in research prototypes such as VQA explainers (Section III-C3).",
      "node_rationale": "Transforms explanation-producing mechanism into deployable practice."
    },
    {
      "name": "Adopt taxonomy-based multi-metric evaluation framework for explainability compliance testing",
      "aliases": [
        "taxonomy-driven XAI evaluation"
      ],
      "type": "intervention",
      "description": "Use the paper’s taxonomy to choose appropriate explanation class and apply completeness, bias detection and human evaluation metrics before release.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Evaluation framework is applied during pre-deployment assessments (Section VI).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Conceptual guideline without systematic tooling yet (Sections V & VI).",
      "node_rationale": "Ensures systematic selection and testing of explanation approaches."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Unreliable and biased decisions from opaque machine learning models in safety-critical contexts",
      "target_node": "Opaqueness of deep neural network inner workings",
      "description": "Hidden internal mechanisms prevent detection of biases and errors, leading directly to unreliable outcomes.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Multiple documented failures in Introduction link opacity to bad outcomes.",
      "edge_rationale": "Introduction examples (COMPAS, adversarial errors)."
    },
    {
      "type": "caused_by",
      "source_node": "Erosion of user trust and regulatory non-compliance from unexplained AI decisions",
      "target_node": "Opaqueness of deep neural network inner workings",
      "description": "When workings are opaque, users cannot obtain legally required explanations, eroding trust.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section on EU Right-to-Explanation provides qualitative evidence.",
      "edge_rationale": "Introduction regulatory discussion."
    },
    {
      "type": "caused_by",
      "source_node": "Unreliable and biased decisions from opaque machine learning models in safety-critical contexts",
      "target_node": "Lack of interpretability and completeness in model explanations",
      "description": "Absence of faithful explanations makes it impossible to detect or correct bias, leading to unsafe outputs.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Link discussed in Sections I & II.",
      "edge_rationale": "Early motivation for explainability."
    },
    {
      "type": "mitigated_by",
      "source_node": "Opaqueness of deep neural network inner workings",
      "target_node": "Tradeoff between interpretability and completeness in AI explanations",
      "description": "Recognising and working along the interpretability-completeness trade-off offers a strategy to expose internal reasoning.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Conceptual mitigation laid out in Section II-B.",
      "edge_rationale": "Theoretical framing addresses opacity."
    },
    {
      "type": "enabled_by",
      "source_node": "Tradeoff between interpretability and completeness in AI explanations",
      "target_node": "Taxonomy-guided approach to develop explainable AI methods",
      "description": "The taxonomy operationalises the trade-off by grouping methods according to what aspect they can explain.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section V derives taxonomy from earlier theory.",
      "edge_rationale": "Taxonomy design justification."
    },
    {
      "type": "enabled_by",
      "source_node": "Tradeoff between interpretability and completeness in AI explanations",
      "target_node": "Multi-level explanation capability accommodating diverse user needs",
      "description": "The trade-off motivates layered explanations adjustable to user expertise.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section II-B proposes progressive detail.",
      "edge_rationale": "Design implication from theory."
    },
    {
      "type": "implemented_by",
      "source_node": "Taxonomy-guided approach to develop explainable AI methods",
      "target_node": "Local proxy models for processing explanations in deep networks",
      "description": "Proxy models correspond to the processing-explanation category in the taxonomy.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section III-A1 explicitly maps LIME into taxonomy Table I.",
      "edge_rationale": "Implementation example."
    },
    {
      "type": "implemented_by",
      "source_node": "Taxonomy-guided approach to develop explainable AI methods",
      "target_node": "Decision tree extraction from neural networks",
      "description": "Decision tree proxies are another processing-explanation mechanism classified in the taxonomy.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section III-A2 and Table I.",
      "edge_rationale": "Implementation example."
    },
    {
      "type": "implemented_by",
      "source_node": "Taxonomy-guided approach to develop explainable AI methods",
      "target_node": "Salience mapping via input occlusion and gradient analysis",
      "description": "Salience mapping is listed under processing explanations in the taxonomy.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section III-A4, Table I.",
      "edge_rationale": "Implementation example."
    },
    {
      "type": "implemented_by",
      "source_node": "Taxonomy-guided approach to develop explainable AI methods",
      "target_node": "Automatic rule extraction techniques for neural network decisions",
      "description": "Rule extraction is another processing mechanism in the taxonomy.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section III-A3 inclusion.",
      "edge_rationale": "Implementation example."
    },
    {
      "type": "implemented_by",
      "source_node": "Taxonomy-guided approach to develop explainable AI methods",
      "target_node": "Attention-based network architectures exposing internal focus",
      "description": "Attention mechanisms fall under explanation-producing systems in the taxonomy.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section III-C1, Table I.",
      "edge_rationale": "Implementation example."
    },
    {
      "type": "implemented_by",
      "source_node": "Taxonomy-guided approach to develop explainable AI methods",
      "target_node": "Disentangled representation learning for interpretable factors",
      "description": "Disentanglement techniques map to representation explanations in the taxonomy.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section III-C2, Table I.",
      "edge_rationale": "Implementation example."
    },
    {
      "type": "implemented_by",
      "source_node": "Multi-level explanation capability accommodating diverse user needs",
      "target_node": "Explanation-generating models producing natural language and visual rationales",
      "description": "Generated explanations provide user-tailored narrative layers.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section III-C3 discusses user-readable rationales.",
      "edge_rationale": "Mechanism realises layered explanations."
    },
    {
      "type": "validated_by",
      "source_node": "Local proxy models for processing explanations in deep networks",
      "target_node": "Local fidelity metrics assessing proxy model faithfulness",
      "description": "Fidelity metrics measure how well a local proxy tracks the original model.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section VI-A describes this as standard evaluation.",
      "edge_rationale": "Direct evaluation method."
    },
    {
      "type": "validated_by",
      "source_node": "Decision tree extraction from neural networks",
      "target_node": "Local fidelity metrics assessing proxy model faithfulness",
      "description": "Extracted trees are benchmarked for faithfulness using the same proxy metrics.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section VI-A extends metrics to tree proxies.",
      "edge_rationale": "Shared evaluation."
    },
    {
      "type": "validated_by",
      "source_node": "Salience mapping via input occlusion and gradient analysis",
      "target_node": "Occlusion-grounded sensitivity benchmarks for salience methods",
      "description": "Occlusion tests provide ground-truth sensitivity for salience map validation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section VI-A cites Ancona comparison.",
      "edge_rationale": "Standard benchmark."
    },
    {
      "type": "validated_by",
      "source_node": "Disentangled representation learning for interpretable factors",
      "target_node": "Transfer learning performance measuring layer representation utility",
      "description": "Transfer-task performance indicates how meaningful disentangled factors are.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference based on Section III-B1 usage; limited specific metrics.",
      "edge_rationale": "Utility of representations."
    },
    {
      "type": "validated_by",
      "source_node": "Attention-based network architectures exposing internal focus",
      "target_node": "Human attention alignment scores for attention mechanisms",
      "description": "Alignment with human attention datasets validates attention explanations.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section VI-C cites human alignment benchmarks.",
      "edge_rationale": "Evaluation practice."
    },
    {
      "type": "validated_by",
      "source_node": "Explanation-generating models producing natural language and visual rationales",
      "target_node": "User studies evaluating clarity and helpfulness of generated explanations",
      "description": "User studies test whether generated rationales are understandable and trustworthy.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section VI-C mentions human evaluations.",
      "edge_rationale": "Primary validation route."
    },
    {
      "type": "motivates",
      "source_node": "Local fidelity metrics assessing proxy model faithfulness",
      "target_node": "Deploy local proxy explanation tools (e.g., LIME) to audit model predictions before deployment",
      "description": "Demonstrated faithfulness encourages integrating LIME into audit workflows.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Metrics show sufficient accuracy to justify deployment (Section VI-A).",
      "edge_rationale": "Metric-driven adoption."
    },
    {
      "type": "motivates",
      "source_node": "Local fidelity metrics assessing proxy model faithfulness",
      "target_node": "Apply decision tree extraction to neural networks for logic verification during pre-deployment review",
      "description": "Faithfulness of extracted trees supports their use in compliance checks.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Limited scalability noted (Section III-A2).",
      "edge_rationale": "Validation enables intervention."
    },
    {
      "type": "motivates",
      "source_node": "Occlusion-grounded sensitivity benchmarks for salience methods",
      "target_node": "Conduct salience map sensitivity analyses to identify spurious correlations during model validation",
      "description": "Reliable salience evaluation encourages systematic sensitivity audits.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Benchmarks demonstrate method reliability (Section VI-A).",
      "edge_rationale": "Evidence -> practice."
    },
    {
      "type": "motivates",
      "source_node": "Human attention alignment scores for attention mechanisms",
      "target_node": "Regularize attention weights during fine-tuning to align with human attention patterns",
      "description": "Demonstrated alignment metrics motivate adding regularisation losses.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Alignment studies are promising but small-scale (Section III-C1).",
      "edge_rationale": "Validation -> intervention."
    },
    {
      "type": "motivates",
      "source_node": "Transfer learning performance measuring layer representation utility",
      "target_node": "Incorporate disentanglement objectives in model design to enhance transparency",
      "description": "Evidence that interpretable factors aid transfer motivates their inclusion during design.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Transfer success is suggestive but indirect (Section III-B1).",
      "edge_rationale": "Empirical utility drives design choice."
    },
    {
      "type": "motivates",
      "source_node": "User studies evaluating clarity and helpfulness of generated explanations",
      "target_node": "Co-train natural language explanation generators with task models to provide end-user rationales at deployment",
      "description": "Positive user feedback justifies co-training explanation generators for deployment systems.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "User studies report increased trust (Section III-C3).",
      "edge_rationale": "Evidence encourages intervention."
    },
    {
      "type": "motivates",
      "source_node": "Taxonomy-guided approach to develop explainable AI methods",
      "target_node": "Adopt taxonomy-based multi-metric evaluation framework for explainability compliance testing",
      "description": "The taxonomy itself suggests building an evaluation workflow that selects suitable metrics per explanation category.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Sections V & VI argue for standardised metrics.",
      "edge_rationale": "Design rationale -> practice."
    }
  ],
  "meta": [
    {
      "key": "title",
      "value": "Explaining Explanations: An Overview of Interpretability of Machine Learning"
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1806.00069"
    },
    {
      "key": "date_published",
      "value": "2018-05-31T00:00:00Z"
    },
    {
      "key": "authors",
      "value": [
        "Leilani H. Gilpin",
        "David Bau",
        "Ben Z. Yuan",
        "Ayesha Bajwa",
        "Michael Specter",
        "Lalana Kagal"
      ]
    },
    {
      "key": "source",
      "value": "arxiv"
    }
  ]
}