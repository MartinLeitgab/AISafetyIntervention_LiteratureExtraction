{
  "id": "resp_067e05041dbc39a80068de4c84a6f48194bc2a789ba1b92425",
  "object": "response",
  "created_at": 1759399044,
  "status": "completed",
  "background": false,
  "billing": {
    "payer": "developer"
  },
  "error": null,
  "incomplete_details": null,
  "instructions": null,
  "max_output_tokens": null,
  "max_tool_calls": null,
  "model": "o3-2025-04-16",
  "output": [
    {
      "id": "rs_067e05041dbc39a80068de4c858350819486fbc0ea907d9c5a",
      "type": "reasoning",
      "summary": []
    },
    {
      "id": "msg_067e05041dbc39a80068de4cb7409081949f8d7d8606680fc5",
      "type": "message",
      "status": "completed",
      "content": [
        {
          "type": "output_text",
          "annotations": [],
          "logprobs": [],
          "text": "Summary:\nData source overview: The survey summarises the risks created by opaque-box machine-learning models, analyses why a lack of interpretability and completeness leads to bias and mistrust, introduces theoretical insights such as the interpretability-completeness trade-off, proposes a taxonomy-guided design rationale for explainable AI, reviews concrete implementation mechanisms (proxy models, decision-tree extraction, salience maps, attention networks, disentangled representations, explanation generators) and reports the kinds of validation evidence used in the literature.  \nExtraction confidence[83]  \nInference strategy justification: The paper is a survey; explicit interventions are rarely stated. Interventions were therefore lightly inferred from the implementation mechanisms and evaluation recommendations that the authors describe (edge confidence 2 for such inferences).  \nExtraction completeness: All causal-interventional pathways appearing in the survey have been captured, starting from top-level risks and ending in concrete, actionable interventions. Shared nodes (risk, problem analysis, theoretical insights, design rationales) inter-connect all branches so that no path is isolated.  \nKey limitations: The survey does not supply quantitative results, so validation-evidence nodes rely on qualitative consensus; edge strengths are therefore 3–4. Some intervention names required moderate inference and are marked with lower maturity.\n\nJSON fabric:\n\n```json\n{\n  \"nodes\": [\n    {\n      \"name\": \"Unreliable and biased decisions from opaque machine learning models in safety-critical contexts\",\n      \"aliases\": [\"unsafe black-box AI outcomes\", \"biased algorithmic decisions\"],\n      \"type\": \"concept\",\n      \"description\": \"Real-world failures such as racial bias in COMPAS and adversarial mis-classifications show that opaque ML models can act unreliably in domains like justice, finance and safety-critical control.\",\n      \"concept_category\": \"risk\",\n      \"intervention_lifecycle\": null,\n      \"intervention_lifecycle_rationale\": null,\n      \"intervention_maturity\": null,\n      \"intervention_maturity_rationale\": null,\n      \"node_rationale\": \"Introduction – examples of unsafe behaviour motivate the need for explanations.\"\n    },\n    {\n      \"name\": \"Erosion of user trust and regulatory non-compliance from unexplained AI decisions\",\n      \"aliases\": [\"mistrust of AI\", \"Right-to-Explanation violations\"],\n      \"type\": \"concept\",\n      \"description\": \"Opaque models hinder compliance with the EU Right-to-Explanation and reduce public confidence in AI outputs.\",\n      \"concept_category\": \"risk\",\n      \"intervention_lifecycle\": null,\n      \"intervention_lifecycle_rationale\": null,\n      \"intervention_maturity\": null,\n      \"intervention_maturity_rationale\": null,\n      \"node_rationale\": \"Introduction – discussion of regulation and public mistrust.\"\n    },\n    {\n      \"name\": \"Opaqueness of deep neural network inner workings\",\n      \"aliases\": [\"black-box nature of DNNs\", \"hidden internal mechanisms\"],\n      \"type\": \"concept\",\n      \"description\": \"DNNs employ millions of parameters and billions of operations, making their decision logic inaccessible to humans.\",\n      \"concept_category\": \"problem analysis\",\n      \"intervention_lifecycle\": null,\n      \"intervention_lifecycle_rationale\": null,\n      \"intervention_maturity\": null,\n      \"intervention_maturity_rationale\": null,\n      \"node_rationale\": \"Section III – scale and complexity of modern networks.\"\n    },\n    {\n      \"name\": \"Lack of interpretability and completeness in model explanations\",\n      \"aliases\": [\"missing faithful explanations\", \"inadequate transparency\"],\n      \"type\": \"concept\",\n      \"description\": \"Current explanation techniques often sacrifice faithfulness for simplicity, or vice-versa, leaving stakeholders without dependable understanding.\",\n      \"concept_category\": \"problem analysis\",\n      \"intervention_lifecycle\": null,\n      \"intervention_lifecycle_rationale\": null,\n      \"intervention_maturity\": null,\n      \"intervention_maturity_rationale\": null,\n      \"node_rationale\": \"Section II-B – interpretability vs. completeness.\"\n    },\n    {\n      \"name\": \"Tradeoff between interpretability and completeness in AI explanations\",\n      \"aliases\": [\"interpretability-completeness dilemma\", \"simplicity-accuracy trade-off\"],\n      \"type\": \"concept\",\n      \"description\": \"The paper argues that no single explanation can be both maximally human-understandable and fully faithful; systems must expose multiple levels of detail.\",\n      \"concept_category\": \"theoretical insight\",\n      \"intervention_lifecycle\": null,\n      \"intervention_lifecycle_rationale\": null,\n      \"intervention_maturity\": null,\n      \"intervention_maturity_rationale\": null,\n      \"node_rationale\": \"Section II-B – central theoretical claim.\"\n    },\n    {\n      \"name\": \"Ethical imperative to avoid persuasive but unfaithful explanations\",\n      \"aliases\": [\"danger of misleading explanations\", \"persuasion vs. transparency issue\"],\n      \"type\": \"concept\",\n      \"description\": \"Providing simplified, trust-inducing narratives without revealing limitations is deemed unethical and may cause harmful reliance.\",\n      \"concept_category\": \"theoretical insight\",\n      \"intervention_lifecycle\": null,\n      \"intervention_lifecycle_rationale\": null,\n      \"intervention_maturity\": null,\n      \"intervention_maturity_rationale\": null,\n      \"node_rationale\": \"Section II-B – ethical dilemmas in explanation design.\"\n    },\n    {\n      \"name\": \"Taxonomy-guided approach to develop explainable AI methods\",\n      \"aliases\": [\"explanation taxonomy\", \"categorisation of explanation methods\"],\n      \"type\": \"concept\",\n      \"description\": \"Section V proposes categorising approaches into processing, representation, and explanation-producing to guide researchers toward appropriate techniques.\",\n      \"concept_category\": \"design rationale\",\n      \"intervention_lifecycle\": null,\n      \"intervention_lifecycle_rationale\": null,\n      \"intervention_maturity\": null,\n      \"intervention_maturity_rationale\": null,\n      \"node_rationale\": \"Section V – motivation for a taxonomy.\"\n    },\n    {\n      \"name\": \"Multi-level explanation capability accommodating diverse user needs\",\n      \"aliases\": [\"progressive disclosure of explanations\", \"layered explanation design\"],\n      \"type\": \"concept\",\n      \"description\": \"Systems should allow users to select explanation granularity along the interpretability-completeness curve, enabling audits and detailed analysis when needed.\",\n      \"concept_category\": \"design rationale\",\n      \"intervention_lifecycle\": null,\n      \"intervention_lifecycle_rationale\": null,\n      \"intervention_maturity\": null,\n      \"intervention_maturity_rationale\": null,\n      \"node_rationale\": \"Section II-B and VI – recommendation for adjustable detail.\"\n    },\n    {\n      \"name\": \"Local proxy models for processing explanations in deep networks\",\n      \"aliases\": [\"LIME local linear surrogates\", \"proxy explanation models\"],\n      \"type\": \"concept\",\n      \"description\": \"Methods such as LIME fit simple, interpretable models to local perturbations of an input, approximating the complex model’s behaviour nearby.\",\n      \"concept_category\": \"implementation mechanism\",\n      \"intervention_lifecycle\": null,\n      \"intervention_lifecycle_rationale\": null,\n      \"intervention_maturity\": null,\n      \"intervention_maturity_rationale\": null,\n      \"node_rationale\": \"Section III-A1 – description of LIME.\"\n    },\n    {\n      \"name\": \"Decision tree extraction from neural networks\",\n      \"aliases\": [\"DeepRED decision-tree proxies\", \"ANN-DT extraction\"],\n      \"type\": \"concept\",\n      \"description\": \"Algorithms such as DeepRED decompose a trained network into a decision tree that approximates its logic, enabling symbolic inspection.\",\n      \"concept_category\": \"implementation mechanism\",\n      \"intervention_lifecycle\": null,\n      \"intervention_lifecycle_rationale\": null,\n      \"intervention_maturity\": null,\n      \"intervention_maturity_rationale\": null,\n      \"node_rationale\": \"Section III-A2 – decision-tree methods.\"\n    },\n    {\n      \"name\": \"Salience mapping via input occlusion and gradient analysis\",\n      \"aliases\": [\"saliency maps\", \"feature importance heat-maps\"],\n      \"type\": \"concept\",\n      \"description\": \"Techniques like occlusion, Integrated Gradients and Grad-CAM highlight regions of the input that most influence the prediction.\",\n      \"concept_category\": \"implementation mechanism\",\n      \"intervention_lifecycle\": null,\n      \"intervention_lifecycle_rationale\": null,\n      \"intervention_maturity\": null,\n      \"intervention_maturity_rationale\": null,\n      \"node_rationale\": \"Section III-A4 – salience methods.\"\n    },\n    {\n      \"name\": \"Automatic rule extraction techniques for neural network decisions\",\n      \"aliases\": [\"KT rule extraction\", \"RxREN\"],\n      \"type\": \"concept\",\n      \"description\": \"Decompositional and pedagogical methods generate IF-THEN rules explaining neuron or input–output relations, increasing transparency of model logic.\",\n      \"concept_category\": \"implementation mechanism\",\n      \"intervention_lifecycle\": null,\n      \"intervention_lifecycle_rationale\": null,\n      \"intervention_maturity\": null,\n      \"intervention_maturity_rationale\": null,\n      \"node_rationale\": \"Section III-A3 – rule extraction survey.\"\n    },\n    {\n      \"name\": \"Attention-based network architectures exposing internal focus\",\n      \"aliases\": [\"neural attention mechanisms\", \"self-explanatory attention\"],\n      \"type\": \"concept\",\n      \"description\": \"Models employing learned attention weights reveal which parts of the input the network relies on, and can be steered to align with human focus.\",\n      \"concept_category\": \"implementation mechanism\",\n      \"intervention_lifecycle\": null,\n      \"intervention_lifecycle_rationale\": null,\n      \"intervention_maturity\": null,\n      \"intervention_maturity_rationale\": null,\n      \"node_rationale\": \"Section III-C1 – attention networks.\"\n    },\n    {\n      \"name\": \"Disentangled representation learning for interpretable factors\",\n      \"aliases\": [\"beta-VAE disentanglement\", \"InfoGAN latent separation\"],\n      \"type\": \"concept\",\n      \"description\": \"Training objectives such as β-VAE encourage latent variables to align with independent, human-interpretable factors, supporting explanation.\",\n      \"concept_category\": \"implementation mechanism\",\n      \"intervention_lifecycle\": null,\n      \"intervention_lifecycle_rationale\": null,\n      \"intervention_maturity\": null,\n      \"intervention_maturity_rationale\": null,\n      \"node_rationale\": \"Section III-C2 – disentangled representations.\"\n    },\n    {\n      \"name\": \"Explanation-generating models producing natural language and visual rationales\",\n      \"aliases\": [\"VQA explanation generators\", \"\\\"because\\\" sentence models\"],\n      \"type\": \"concept\",\n      \"description\": \"Architectures jointly train task performance with modules that output human-readable textual or multimodal explanations of each decision.\",\n      \"concept_category\": \"implementation mechanism\",\n      \"intervention_lifecycle\": null,\n      \"intervention_lifecycle_rationale\": null,\n      \"intervention_maturity\": null,\n      \"intervention_maturity_rationale\": null,\n      \"node_rationale\": \"Section III-C3 – generated explanations.\"\n    },\n    {\n      \"name\": \"Local fidelity metrics assessing proxy model faithfulness\",\n      \"aliases\": [\"proxy completeness measures\", \"surrogate accuracy metrics\"],\n      \"type\": \"concept\",\n      \"description\": \"The survey recommends quantifying how closely a local proxy model matches the original model around the instance being explained.\",\n      \"concept_category\": \"validation evidence\",\n      \"intervention_lifecycle\": null,\n      \"intervention_lifecycle_rationale\": null,\n      \"intervention_maturity\": null,\n      \"intervention_maturity_rationale\": null,\n      \"node_rationale\": \"Section VI-A – completeness to model for proxy methods.\"\n    },\n    {\n      \"name\": \"Occlusion-grounded sensitivity benchmarks for salience methods\",\n      \"aliases\": [\"brute-force occlusion testing\", \"ground-truth salience evaluation\"],\n      \"type\": \"concept\",\n      \"description\": \"Saliency maps are compared with exhaustive occlusion tests that measure how masking each region affects predictions.\",\n      \"concept_category\": \"validation evidence\",\n      \"intervention_lifecycle\": null,\n      \"intervention_lifecycle_rationale\": null,\n      \"intervention_maturity\": null,\n      \"intervention_maturity_rationale\": null,\n      \"node_rationale\": \"Section VI-A – salience evaluation example.\"\n    },\n    {\n      \"name\": \"Transfer learning performance measuring layer representation utility\",\n      \"aliases\": [\"representation transfer benchmarks\", \"feature reuse tests\"],\n      \"type\": \"concept\",\n      \"description\": \"Internal layer outputs reused on new tasks can gauge how informative and interpretable those representations are.\",\n      \"concept_category\": \"validation evidence\",\n      \"intervention_lifecycle\": null,\n      \"intervention_lifecycle_rationale\": null,\n      \"intervention_maturity\": null,\n      \"intervention_maturity_rationale\": null,\n      \"node_rationale\": \"Section III-B1 and VI-B – role-of-layers evaluation.\"\n    },\n    {\n      \"name\": \"Human attention alignment scores for attention mechanisms\",\n      \"aliases\": [\"attention-human overlap metrics\", \"attention faithfulness tests\"],\n      \"type\": \"concept\",\n      \"description\": \"Datasets of human eye-tracking or click attention enable comparison between model attention maps and human focus to validate explanation quality.\",\n      \"concept_category\": \"validation evidence\",\n      \"intervention_lifecycle\": null,\n      \"intervention_lifecycle_rationale\": null,\n      \"intervention_maturity\": null,\n      \"intervention_maturity_rationale\": null,\n      \"node_rationale\": \"Section VI-C – attention evaluation discussion.\"\n    },\n    {\n      \"name\": \"User studies evaluating clarity and helpfulness of generated explanations\",\n      \"aliases\": [\"human evaluation of explanations\", \"trust and usability studies\"],\n      \"type\": \"concept\",\n      \"description\": \"Participants judge generated textual and visual explanations for helpfulness, trust improvement and correctness understanding.\",\n      \"concept_category\": \"validation evidence\",\n      \"intervention_lifecycle\": null,\n      \"intervention_lifecycle_rationale\": null,\n      \"intervention_maturity\": null,\n      \"intervention_maturity_rationale\": null,\n      \"node_rationale\": \"Section VI-C – human evaluation for explanation-producing systems.\"\n    },\n    {\n      \"name\": \"Deploy local proxy explanation tools (e.g., LIME) to audit model predictions before deployment\",\n      \"aliases\": [\"pre-deployment LIME auditing\"],\n      \"type\": \"intervention\",\n      \"description\": \"Integrate local surrogate models such as LIME into the validation pipeline so auditors can inspect decision logic on representative cases.\",\n      \"concept_category\": null,\n      \"intervention_lifecycle\": \"4\",\n      \"intervention_lifecycle_rationale\": \"Applies to Pre-Deployment Testing phase where predictions are audited (Section VI-A).\",\n      \"intervention_maturity\": \"3\",\n      \"intervention_maturity_rationale\": \"LIME is widely prototyped and used in industry pilots but lacks large-scale regulated deployment evidence (Section III-A1).\",\n      \"node_rationale\": \"Provides actionable step derived from proxy model mechanism.\"\n    },\n    {\n      \"name\": \"Apply decision tree extraction to neural networks for logic verification during pre-deployment review\",\n      \"aliases\": [\"DeepRED compliance checking\"],\n      \"type\": \"intervention\",\n      \"description\": \"Run algorithms such as DeepRED or ANN-DT on trained models to obtain decision trees for compliance and bias analysis.\",\n      \"concept_category\": null,\n      \"intervention_lifecycle\": \"4\",\n      \"intervention_lifecycle_rationale\": \"Executed after training but prior to release to reveal internal logic (Section III-A2).\",\n      \"intervention_maturity\": \"2\",\n      \"intervention_maturity_rationale\": \"Methods are experimental and limited in scalability (Section III-A2).\",\n      \"node_rationale\": \"Derived from decision-tree extraction mechanism for safety analysis.\"\n    },\n    {\n      \"name\": \"Conduct salience map sensitivity analyses to identify spurious correlations during model validation\",\n      \"aliases\": [\"saliency auditing workflow\"],\n      \"type\": \"intervention\",\n      \"description\": \"Systematically generate and benchmark saliency maps against occlusion tests to discover features the model relies on inappropriately.\",\n      \"concept_category\": null,\n      \"intervention_lifecycle\": \"4\",\n      \"intervention_lifecycle_rationale\": \"Performed in validation phase to catch problems early (Section VI-A).\",\n      \"intervention_maturity\": \"3\",\n      \"intervention_maturity_rationale\": \"Salience techniques are broadly available and studied but not yet standardised (Section III-A4).\",\n      \"node_rationale\": \"Actionable use of salience mechanism and evaluation evidence.\"\n    },\n    {\n      \"name\": \"Regularize attention weights during fine-tuning to align with human attention patterns\",\n      \"aliases\": [\"attention alignment regularization\"],\n      \"type\": \"intervention\",\n      \"description\": \"During fine-tuning, add loss terms that penalise divergence between model attention maps and human attention datasets to ensure faithful focus.\",\n      \"concept_category\": null,\n      \"intervention_lifecycle\": \"3\",\n      \"intervention_lifecycle_rationale\": \"Implemented in Fine-Tuning/RL stage where additional losses are injected (Section III-C1).\",\n      \"intervention_maturity\": \"2\",\n      \"intervention_maturity_rationale\": \"Technique proposed in literature but validated only on limited benchmarks (Section III-C1).\",\n      \"node_rationale\": \"Utilises attention mechanism and human alignment evidence.\"\n    },\n    {\n      \"name\": \"Incorporate disentanglement objectives in model design to enhance transparency\",\n      \"aliases\": [\"β-VAE-based transparent design\"],\n      \"type\": \"intervention\",\n      \"description\": \"Include β-VAE style KL-divergence scaling or InfoGAN mutual-information objectives at model-design time to yield factorised latent codes.\",\n      \"concept_category\": null,\n      \"intervention_lifecycle\": \"1\",\n      \"intervention_lifecycle_rationale\": \"Modifies objective during initial architecture and loss design (Section III-C2).\",\n      \"intervention_maturity\": \"2\",\n      \"intervention_maturity_rationale\": \"Disentanglement successes shown on benchmarks but limited industrial usage (Section III-C2).\",\n      \"node_rationale\": \"Directly addresses transparency via disentangled factors.\"\n    },\n    {\n      \"name\": \"Co-train natural language explanation generators with task models to provide end-user rationales at deployment\",\n      \"aliases\": [\"joint explanation generation training\"],\n      \"type\": \"intervention\",\n      \"description\": \"Train an auxiliary decoder that outputs textual or multimodal rationales conditioned on model decisions for user-facing systems.\",\n      \"concept_category\": null,\n      \"intervention_lifecycle\": \"3\",\n      \"intervention_lifecycle_rationale\": \"Implemented during fine-tuning where auxiliary generative heads are trained (Section III-C3).\",\n      \"intervention_maturity\": \"2\",\n      \"intervention_maturity_rationale\": \"Demonstrated in research prototypes such as VQA explainers (Section III-C3).\",\n      \"node_rationale\": \"Transforms explanation-producing mechanism into deployable practice.\"\n    },\n    {\n      \"name\": \"Adopt taxonomy-based multi-metric evaluation framework for explainability compliance testing\",\n      \"aliases\": [\"taxonomy-driven XAI evaluation\"],\n      \"type\": \"intervention\",\n      \"description\": \"Use the paper’s taxonomy to choose appropriate explanation class and apply completeness, bias detection and human evaluation metrics before release.\",\n      \"concept_category\": null,\n      \"intervention_lifecycle\": \"4\",\n      \"intervention_lifecycle_rationale\": \"Evaluation framework is applied during pre-deployment assessments (Section VI).\",\n      \"intervention_maturity\": \"1\",\n      \"intervention_maturity_rationale\": \"Conceptual guideline without systematic tooling yet (Sections V & VI).\",\n      \"node_rationale\": \"Ensures systematic selection and testing of explanation approaches.\"\n    }\n  ],\n  \"edges\": [\n    {\n      \"type\": \"caused_by\",\n      \"source_node\": \"Unreliable and biased decisions from opaque machine learning models in safety-critical contexts\",\n      \"target_node\": \"Opaqueness of deep neural network inner workings\",\n      \"description\": \"Hidden internal mechanisms prevent detection of biases and errors, leading directly to unreliable outcomes.\",\n      \"edge_confidence\": \"4\",\n      \"edge_confidence_rationale\": \"Multiple documented failures in Introduction link opacity to bad outcomes.\",\n      \"edge_rationale\": \"Introduction examples (COMPAS, adversarial errors).\"\n    },\n    {\n      \"type\": \"caused_by\",\n      \"source_node\": \"Erosion of user trust and regulatory non-compliance from unexplained AI decisions\",\n      \"target_node\": \"Opaqueness of deep neural network inner workings\",\n      \"description\": \"When workings are opaque, users cannot obtain legally required explanations, eroding trust.\",\n      \"edge_confidence\": \"3\",\n      \"edge_confidence_rationale\": \"Section on EU Right-to-Explanation provides qualitative evidence.\",\n      \"edge_rationale\": \"Introduction regulatory discussion.\"\n    },\n    {\n      \"type\": \"caused_by\",\n      \"source_node\": \"Unreliable and biased decisions from opaque machine learning models in safety-critical contexts\",\n      \"target_node\": \"Lack of interpretability and completeness in model explanations\",\n      \"description\": \"Absence of faithful explanations makes it impossible to detect or correct bias, leading to unsafe outputs.\",\n      \"edge_confidence\": \"3\",\n      \"edge_confidence_rationale\": \"Link discussed in Sections I & II.\",\n      \"edge_rationale\": \"Early motivation for explainability.\"\n    },\n    {\n      \"type\": \"mitigated_by\",\n      \"source_node\": \"Opaqueness of deep neural network inner workings\",\n      \"target_node\": \"Tradeoff between interpretability and completeness in AI explanations\",\n      \"description\": \"Recognising and working along the interpretability-completeness trade-off offers a strategy to expose internal reasoning.\",\n      \"edge_confidence\": \"3\",\n      \"edge_confidence_rationale\": \"Conceptual mitigation laid out in Section II-B.\",\n      \"edge_rationale\": \"Theoretical framing addresses opacity.\"\n    },\n    {\n      \"type\": \"enabled_by\",\n      \"source_node\": \"Tradeoff between interpretability and completeness in AI explanations\",\n      \"target_node\": \"Taxonomy-guided approach to develop explainable AI methods\",\n      \"description\": \"The taxonomy operationalises the trade-off by grouping methods according to what aspect they can explain.\",\n      \"edge_confidence\": \"3\",\n      \"edge_confidence_rationale\": \"Section V derives taxonomy from earlier theory.\",\n      \"edge_rationale\": \"Taxonomy design justification.\"\n    },\n    {\n      \"type\": \"enabled_by\",\n      \"source_node\": \"Tradeoff between interpretability and completeness in AI explanations\",\n      \"target_node\": \"Multi-level explanation capability accommodating diverse user needs\",\n      \"description\": \"The trade-off motivates layered explanations adjustable to user expertise.\",\n      \"edge_confidence\": \"3\",\n      \"edge_confidence_rationale\": \"Section II-B proposes progressive detail.\",\n      \"edge_rationale\": \"Design implication from theory.\"\n    },\n    {\n      \"type\": \"implemented_by\",\n      \"source_node\": \"Taxonomy-guided approach to develop explainable AI methods\",\n      \"target_node\": \"Local proxy models for processing explanations in deep networks\",\n      \"description\": \"Proxy models correspond to the processing-explanation category in the taxonomy.\",\n      \"edge_confidence\": \"4\",\n      \"edge_confidence_rationale\": \"Section III-A1 explicitly maps LIME into taxonomy Table I.\",\n      \"edge_rationale\": \"Implementation example.\"\n    },\n    {\n      \"type\": \"implemented_by\",\n      \"source_node\": \"Taxonomy-guided approach to develop explainable AI methods\",\n      \"target_node\": \"Decision tree extraction from neural networks\",\n      \"description\": \"Decision tree proxies are another processing-explanation mechanism classified in the taxonomy.\",\n      \"edge_confidence\": \"4\",\n      \"edge_confidence_rationale\": \"Section III-A2 and Table I.\",\n      \"edge_rationale\": \"Implementation example.\"\n    },\n    {\n      \"type\": \"implemented_by\",\n      \"source_node\": \"Taxonomy-guided approach to develop explainable AI methods\",\n      \"target_node\": \"Salience mapping via input occlusion and gradient analysis\",\n      \"description\": \"Salience mapping is listed under processing explanations in the taxonomy.\",\n      \"edge_confidence\": \"4\",\n      \"edge_confidence_rationale\": \"Section III-A4, Table I.\",\n      \"edge_rationale\": \"Implementation example.\"\n    },\n    {\n      \"type\": \"implemented_by\",\n      \"source_node\": \"Taxonomy-guided approach to develop explainable AI methods\",\n      \"target_node\": \"Automatic rule extraction techniques for neural network decisions\",\n      \"description\": \"Rule extraction is another processing mechanism in the taxonomy.\",\n      \"edge_confidence\": \"3\",\n      \"edge_confidence_rationale\": \"Section III-A3 inclusion.\",\n      \"edge_rationale\": \"Implementation example.\"\n    },\n    {\n      \"type\": \"implemented_by\",\n      \"source_node\": \"Taxonomy-guided approach to develop explainable AI methods\",\n      \"target_node\": \"Attention-based network architectures exposing internal focus\",\n      \"description\": \"Attention mechanisms fall under explanation-producing systems in the taxonomy.\",\n      \"edge_confidence\": \"3\",\n      \"edge_confidence_rationale\": \"Section III-C1, Table I.\",\n      \"edge_rationale\": \"Implementation example.\"\n    },\n    {\n      \"type\": \"implemented_by\",\n      \"source_node\": \"Taxonomy-guided approach to develop explainable AI methods\",\n      \"target_node\": \"Disentangled representation learning for interpretable factors\",\n      \"description\": \"Disentanglement techniques map to representation explanations in the taxonomy.\",\n      \"edge_confidence\": \"3\",\n      \"edge_confidence_rationale\": \"Section III-C2, Table I.\",\n      \"edge_rationale\": \"Implementation example.\"\n    },\n    {\n      \"type\": \"implemented_by\",\n      \"source_node\": \"Multi-level explanation capability accommodating diverse user needs\",\n      \"target_node\": \"Explanation-generating models producing natural language and visual rationales\",\n      \"description\": \"Generated explanations provide user-tailored narrative layers.\",\n      \"edge_confidence\": \"3\",\n      \"edge_confidence_rationale\": \"Section III-C3 discusses user-readable rationales.\",\n      \"edge_rationale\": \"Mechanism realises layered explanations.\"\n    },\n    {\n      \"type\": \"validated_by\",\n      \"source_node\": \"Local proxy models for processing explanations in deep networks\",\n      \"target_node\": \"Local fidelity metrics assessing proxy model faithfulness\",\n      \"description\": \"Fidelity metrics measure how well a local proxy tracks the original model.\",\n      \"edge_confidence\": \"4\",\n      \"edge_confidence_rationale\": \"Section VI-A describes this as standard evaluation.\",\n      \"edge_rationale\": \"Direct evaluation method.\"\n    },\n    {\n      \"type\": \"validated_by\",\n      \"source_node\": \"Decision tree extraction from neural networks\",\n      \"target_node\": \"Local fidelity metrics assessing proxy model faithfulness\",\n      \"description\": \"Extracted trees are benchmarked for faithfulness using the same proxy metrics.\",\n      \"edge_confidence\": \"3\",\n      \"edge_confidence_rationale\": \"Section VI-A extends metrics to tree proxies.\",\n      \"edge_rationale\": \"Shared evaluation.\"\n    },\n    {\n      \"type\": \"validated_by\",\n      \"source_node\": \"Salience mapping via input occlusion and gradient analysis\",\n      \"target_node\": \"Occlusion-grounded sensitivity benchmarks for salience methods\",\n      \"description\": \"Occlusion tests provide ground-truth sensitivity for salience map validation.\",\n      \"edge_confidence\": \"4\",\n      \"edge_confidence_rationale\": \"Section VI-A cites Ancona comparison.\",\n      \"edge_rationale\": \"Standard benchmark.\"\n    },\n    {\n      \"type\": \"validated_by\",\n      \"source_node\": \"Disentangled representation learning for interpretable factors\",\n      \"target_node\": \"Transfer learning performance measuring layer representation utility\",\n      \"description\": \"Transfer-task performance indicates how meaningful disentangled factors are.\",\n      \"edge_confidence\": \"2\",\n      \"edge_confidence_rationale\": \"Inference based on Section III-B1 usage; limited specific metrics.\",\n      \"edge_rationale\": \"Utility of representations.\"\n    },\n    {\n      \"type\": \"validated_by\",\n      \"source_node\": \"Attention-based network architectures exposing internal focus\",\n      \"target_node\": \"Human attention alignment scores for attention mechanisms\",\n      \"description\": \"Alignment with human attention datasets validates attention explanations.\",\n      \"edge_confidence\": \"3\",\n      \"edge_confidence_rationale\": \"Section VI-C cites human alignment benchmarks.\",\n      \"edge_rationale\": \"Evaluation practice.\"\n    },\n    {\n      \"type\": \"validated_by\",\n      \"source_node\": \"Explanation-generating models producing natural language and visual rationales\",\n      \"target_node\": \"User studies evaluating clarity and helpfulness of generated explanations\",\n      \"description\": \"User studies test whether generated rationales are understandable and trustworthy.\",\n      \"edge_confidence\": \"3\",\n      \"edge_confidence_rationale\": \"Section VI-C mentions human evaluations.\",\n      \"edge_rationale\": \"Primary validation route.\"\n    },\n    {\n      \"type\": \"motivates\",\n      \"source_node\": \"Local fidelity metrics assessing proxy model faithfulness\",\n      \"target_node\": \"Deploy local proxy explanation tools (e.g., LIME) to audit model predictions before deployment\",\n      \"description\": \"Demonstrated faithfulness encourages integrating LIME into audit workflows.\",\n      \"edge_confidence\": \"3\",\n      \"edge_confidence_rationale\": \"Metrics show sufficient accuracy to justify deployment (Section VI-A).\",\n      \"edge_rationale\": \"Metric-driven adoption.\"\n    },\n    {\n      \"type\": \"motivates\",\n      \"source_node\": \"Local fidelity metrics assessing proxy model faithfulness\",\n      \"target_node\": \"Apply decision tree extraction to neural networks for logic verification during pre-deployment review\",\n      \"description\": \"Faithfulness of extracted trees supports their use in compliance checks.\",\n      \"edge_confidence\": \"2\",\n      \"edge_confidence_rationale\": \"Limited scalability noted (Section III-A2).\",\n      \"edge_rationale\": \"Validation enables intervention.\"\n    },\n    {\n      \"type\": \"motivates\",\n      \"source_node\": \"Occlusion-grounded sensitivity benchmarks for salience methods\",\n      \"target_node\": \"Conduct salience map sensitivity analyses to identify spurious correlations during model validation\",\n      \"description\": \"Reliable salience evaluation encourages systematic sensitivity audits.\",\n      \"edge_confidence\": \"3\",\n      \"edge_confidence_rationale\": \"Benchmarks demonstrate method reliability (Section VI-A).\",\n      \"edge_rationale\": \"Evidence -> practice.\"\n    },\n    {\n      \"type\": \"motivates\",\n      \"source_node\": \"Human attention alignment scores for attention mechanisms\",\n      \"target_node\": \"Regularize attention weights during fine-tuning to align with human attention patterns\",\n      \"description\": \"Demonstrated alignment metrics motivate adding regularisation losses.\",\n      \"edge_confidence\": \"2\",\n      \"edge_confidence_rationale\": \"Alignment studies are promising but small-scale (Section III-C1).\",\n      \"edge_rationale\": \"Validation -> intervention.\"\n    },\n    {\n      \"type\": \"motivates\",\n      \"source_node\": \"Transfer learning performance measuring layer representation utility\",\n      \"target_node\": \"Incorporate disentanglement objectives in model design to enhance transparency\",\n      \"description\": \"Evidence that interpretable factors aid transfer motivates their inclusion during design.\",\n      \"edge_confidence\": \"2\",\n      \"edge_confidence_rationale\": \"Transfer success is suggestive but indirect (Section III-B1).\",\n      \"edge_rationale\": \"Empirical utility drives design choice.\"\n    },\n    {\n      \"type\": \"motivates\",\n      \"source_node\": \"User studies evaluating clarity and helpfulness of generated explanations\",\n      \"target_node\": \"Co-train natural language explanation generators with task models to provide end-user rationales at deployment\",\n      \"description\": \"Positive user feedback justifies co-training explanation generators for deployment systems.\",\n      \"edge_confidence\": \"3\",\n      \"edge_confidence_rationale\": \"User studies report increased trust (Section III-C3).\",\n      \"edge_rationale\": \"Evidence encourages intervention.\"\n    },\n    {\n      \"type\": \"motivates\",\n      \"source_node\": \"Taxonomy-guided approach to develop explainable AI methods\",\n      \"target_node\": \"Adopt taxonomy-based multi-metric evaluation framework for explainability compliance testing\",\n      \"description\": \"The taxonomy itself suggests building an evaluation workflow that selects suitable metrics per explanation category.\",\n      \"edge_confidence\": \"3\",\n      \"edge_confidence_rationale\": \"Sections V & VI argue for standardised metrics.\",\n      \"edge_rationale\": \"Design rationale -> practice.\"\n    }\n  ]\n}\n```\n\nKey limitations:\n1. The survey aggregates rather than contributes new quantitative studies; therefore validation evidence nodes reflect secondary reporting.\n2. Some implementation branches (e.g., automatic rule extraction) are not followed through to separate interventions to keep the graph concise; downstream merging can add them.\n3. Intervention maturity and lifecycle rely on light inference that readers of the survey would reasonably make.\n\nPlease let us know if further granularity or additional branches are desired."
        }
      ],
      "role": "assistant"
    }
  ],
  "parallel_tool_calls": true,
  "previous_response_id": null,
  "prompt_cache_key": null,
  "reasoning": {
    "effort": "medium",
    "summary": null
  },
  "safety_identifier": null,
  "service_tier": "default",
  "store": true,
  "temperature": 1.0,
  "text": {
    "format": {
      "type": "text"
    },
    "verbosity": "medium"
  },
  "tool_choice": "auto",
  "tools": [],
  "top_logprobs": 0,
  "top_p": 1.0,
  "truncation": "disabled",
  "usage": {
    "input_tokens": 12451,
    "input_tokens_details": {
      "cached_tokens": 0
    },
    "output_tokens": 10288,
    "output_tokens_details": {
      "reasoning_tokens": 3136
    },
    "total_tokens": 22739
  },
  "user": null,
  "metadata": {}
}