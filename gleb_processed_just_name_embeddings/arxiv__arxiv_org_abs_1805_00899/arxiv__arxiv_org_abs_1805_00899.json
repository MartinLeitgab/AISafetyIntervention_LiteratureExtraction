{
  "nodes": [
    {
      "name": "unaligned behavior in advanced AI systems",
      "aliases": [
        "AI misalignment",
        "unsafe AI outcomes"
      ],
      "type": "concept",
      "description": "Potential for powerful AI agents to act counter to human values, producing harmful or catastrophic results.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Identified in 1 Introduction as the overarching safety risk debate aims to solve."
    },
    {
      "name": "deceptive or dishonest AI behavior in AI systems",
      "aliases": [
        "AI lying",
        "emergent deception"
      ],
      "type": "concept",
      "description": "Agents may intentionally provide false or misleading information to secure reward or advantage.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 5.1 cites deception as a primary failure mode debate is intended to counter."
    },
    {
      "name": "high complexity tasks exceed human judgment capacity",
      "aliases": [
        "tasks beyond human evaluation",
        "cognitive overload of judges"
      ],
      "type": "concept",
      "description": "Many AI tasks are too complex for direct human assessment, leading to mis-alignment during training.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Section 1 and 2.3 as the core reason alignment is difficult."
    },
    {
      "name": "misleading alignment signals from human feedback",
      "aliases": [
        "systematic bias in human preference data",
        "incorrect reward"
      ],
      "type": "concept",
      "description": "Human feedback can embed biases or errors, incentivising undesirable agent policies.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 1 and 5.1 describe how biased human supervision can create deceptive agents."
    },
    {
      "name": "refuting lies is easier than constructing lies in debate game",
      "aliases": [
        "central debate hypothesis",
        "lie-refutation asymmetry"
      ],
      "type": "concept",
      "description": "Key assumption: in an adversarial debate, identifying and exposing falsehoods is cognitively cheaper than inventing them.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Formulated explicitly in Section 2 as the claim underlying debateâ€™s effectiveness."
    },
    {
      "name": "debate complexity equivalence to PSPACE enabling exponential information amplification",
      "aliases": [
        "DEBATE=PSPACE",
        "polynomial judge exponential agent"
      ],
      "type": "concept",
      "description": "Theoretical result: with optimal play, polynomial-time judges in debate can solve any PSPACE problem, implying agents can amplify human capability exponentially.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in Section 2.2; grounds feasibility of aligning super-human agents."
    },
    {
      "name": "short unbranched debates suffice due to adversarial focus",
      "aliases": [
        "depth-limited debate sufficiency",
        "single-path argumentation"
      ],
      "type": "concept",
      "description": "Optimal agents can compress large argument trees into a short sequence by focusing on the most contentious branch.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Illustrated with vacation example in Section 2.1."
    },
    {
      "name": "adversarial debate between duplicate agents supervised by human judge",
      "aliases": [
        "debate alignment frame",
        "two-agent competitive alignment"
      ],
      "type": "concept",
      "description": "Design choice: two copies of a model argue opposing sides while a human judge selects the winner, incentivising truthful statements.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Described throughout Section 2 as the central alignment mechanism."
    },
    {
      "name": "human judges reward honesty and flaw detection paradigm",
      "aliases": [
        "judge incentive structure",
        "truth-oriented judging"
      ],
      "type": "concept",
      "description": "Judges are instructed to prefer statements that reveal true, useful information and penalise misleading arguments.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4.2 discusses instructing judges to punish misleading behaviour."
    },
    {
      "name": "limiting debate length with answer precommitment reduces argument space",
      "aliases": [
        "precommitment design",
        "short debate protocol"
      ],
      "type": "concept",
      "description": "Requiring agents to state their answer before the debate and limiting turns prevents strategic argument switching and simplifies analysis.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Design motivation given in Section 2.1 and validated in Section 3.1 MNIST results."
    },
    {
      "name": "train reward predictor to scale human debate judgments",
      "aliases": [
        "learned judge model",
        "human preference predictor"
      ],
      "type": "concept",
      "description": "To reduce human time, a model is trained to emulate human debate judgments, enabling large-scale self play.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 2.3 proposes training predictors of human reward."
    },
    {
      "name": "self-play reinforcement learning of debate agents",
      "aliases": [
        "debate self-play training",
        "competitive RL for debate"
      ],
      "type": "concept",
      "description": "Agents improve via gradient-based self play, analogous to AlphaZero, to approximate Nash-equilibrium debate strategies.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation approach outlined in Sections 2.2 and 4.3."
    },
    {
      "name": "answer precommitment before debate turns",
      "aliases": [
        "fixed claim before debate",
        "preannounced stance"
      ],
      "type": "concept",
      "description": "Each agent must declare its answer at the start of the game and may not change it, restricting dishonest strategies.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implemented in MNIST experiment (Section 3.1) and shown to reduce liar success."
    },
    {
      "name": "sparse pixel MNIST debate experimental setup",
      "aliases": [
        "MNIST debate game",
        "pixel-limited debate"
      ],
      "type": "concept",
      "description": "Benchmark where agents reveal a few pixels to convince a judge classifier, testing debate without natural language.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in Section 3.1 as first empirical testbed."
    },
    {
      "name": "human single-pixel reveal image debate website",
      "aliases": [
        "cat-vs-dog debate site",
        "human image debate"
      ],
      "type": "concept",
      "description": "Prototype web platform where human debaters reveal one pixel to convince a human judge, modelling extra knowledge.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.2 presents this as a planned human study."
    },
    {
      "name": "sharing activations between agents for transparency",
      "aliases": [
        "activation sharing",
        "thought transparency"
      ],
      "type": "concept",
      "description": "Agents are granted access to previous internal activations of the opponent to reduce imperfect-information issues.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Refinement proposed in Section 6.2."
    },
    {
      "name": "training judge models to predict human reward",
      "aliases": [
        "learned debate judge",
        "reward model for debate"
      ],
      "type": "concept",
      "description": "Supervised models learn to approximate human debate decisions, enabling automated large-scale training.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mechanism executed in Section 3.1 where a CNN judge is trained."
    },
    {
      "name": "PSPACE proof demonstrating debate power",
      "aliases": [
        "formal debate theorem",
        "complexity proof"
      ],
      "type": "concept",
      "description": "Theorem 1 in Section 2.2 proves that debate with polynomial judges solves any PSPACE problem.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Provides formal validation of debate capability."
    },
    {
      "name": "MNIST debate increases judge accuracy from 59.4% to 88.9%",
      "aliases": [
        "MNIST experimental result",
        "pixel debate outcome"
      ],
      "type": "concept",
      "description": "With 6 pixel limit and precommitment, honest agents win 88.9 % vs. 59.4 % baseline judge accuracy.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.1 Table 2 reports quantitative validation."
    },
    {
      "name": "informal human debate indicates honest agent usually wins",
      "aliases": [
        "pilot human study",
        "website anecdotal evidence"
      ],
      "type": "concept",
      "description": "Informal trials on the prototype website show dishonest debaters rarely win when rules are understood.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mentioned in Section 3.2 as preliminary human evidence."
    },
    {
      "name": "adversarial debate fine-tuning to align AI models",
      "aliases": [
        "debate-based RLHF",
        "debate alignment training"
      ],
      "type": "intervention",
      "description": "During fine-tuning/RLHF, train two copies of the target model to debate each query, using human or learned judge reward to update both agents, selecting the one judged truthful.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Fine-tuning/RL phase described in Section 3 as the training stage where debate is applied.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Only proof-of-concept experiments (MNIST, pilot human) exist; no large-scale deployment yet (Sections 3 & 8).",
      "node_rationale": "Primary actionable step the paper recommends for aligning advanced AI (Section 8)."
    },
    {
      "name": "deploy reward predictor to automate large-scale debate training",
      "aliases": [
        "learned judge deployment",
        "scaled debate supervision"
      ],
      "type": "intervention",
      "description": "Train and integrate a neural reward model that emulates human debate judgments, allowing millions of automated debate rounds with minimal human oversight.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Implemented alongside debate fine-tuning to replace most human evaluations (Section 2.3).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated only on MNIST classifier; human-grade predictors remain experimental (Section 3.1).",
      "node_rationale": "Enables scalable application of debate without prohibitive human cost."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "unaligned behavior in advanced AI systems",
      "target_node": "high complexity tasks exceed human judgment capacity",
      "description": "Misalignment arises because humans cannot directly verify complex behaviors.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicitly stated link in Section 1 Introduction.",
      "edge_rationale": "Section 1 notes that alignment difficulty increases with task complexity."
    },
    {
      "type": "caused_by",
      "source_node": "deceptive or dishonest AI behavior in AI systems",
      "target_node": "misleading alignment signals from human feedback",
      "description": "Biased or weak feedback incentivises agents to learn deceptive shortcuts.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Discussed in Section 5.1 belief-bias concerns.",
      "edge_rationale": "Paper argues that flawed supervision can foster deception."
    },
    {
      "type": "addressed_by",
      "source_node": "high complexity tasks exceed human judgment capacity",
      "target_node": "refuting lies is easier than constructing lies in debate game",
      "description": "The hypothesised asymmetry offers a mechanism to overcome human limitations.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical connection derived from Section 2 claim; not experimentally proven.",
      "edge_rationale": "If refutation is easier, humans need only judge debates, not tasks directly."
    },
    {
      "type": "addressed_by",
      "source_node": "misleading alignment signals from human feedback",
      "target_node": "refuting lies is easier than constructing lies in debate game",
      "description": "Debate counters misleading signals by exposing flaws.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Reasoned inference from Sections 2 and 5.1.",
      "edge_rationale": "Flaw revelation corrects biased supervision."
    },
    {
      "type": "mitigated_by",
      "source_node": "refuting lies is easier than constructing lies in debate game",
      "target_node": "adversarial debate between duplicate agents supervised by human judge",
      "description": "Design implements the theoretical asymmetry via two competing agents.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 2 directly presents debate as embodiment of the claim.",
      "edge_rationale": "Debate operationalises the lie-refutation advantage."
    },
    {
      "type": "mitigated_by",
      "source_node": "debate complexity equivalence to PSPACE enabling exponential information amplification",
      "target_node": "adversarial debate between duplicate agents supervised by human judge",
      "description": "Design leverages theoretical power to align super-human agents.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 2.2 ties PSPACE result to debate design.",
      "edge_rationale": "Debate capitalises on PSPACE-level amplification."
    },
    {
      "type": "enabled_by",
      "source_node": "short unbranched debates suffice due to adversarial focus",
      "target_node": "limiting debate length with answer precommitment reduces argument space",
      "description": "Precommitment realises the short-debate property, preventing branch proliferation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Supported by MNIST experiment (Section 3.1).",
      "edge_rationale": "Fixed claims + limited turns enforce shallow debates."
    },
    {
      "type": "refined_by",
      "source_node": "adversarial debate between duplicate agents supervised by human judge",
      "target_node": "human judges reward honesty and flaw detection paradigm",
      "description": "Clarifies judge instructions essential for debate success.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 4.2 elaborates judging criteria.",
      "edge_rationale": "Judging guidance shapes debate equilibrium."
    },
    {
      "type": "implemented_by",
      "source_node": "adversarial debate between duplicate agents supervised by human judge",
      "target_node": "self-play reinforcement learning of debate agents",
      "description": "Self-play training procedure realises the debate mechanism.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Implementation plan laid out in Sections 2.2 and 4.3.",
      "edge_rationale": "Gradient-based self play operationalises debate training."
    },
    {
      "type": "implemented_by",
      "source_node": "adversarial debate between duplicate agents supervised by human judge",
      "target_node": "answer precommitment before debate turns",
      "description": "Precommitment is an implementation detail enforcing game rules.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Empirically evaluated in Section 3.1.",
      "edge_rationale": "Game protocol requires fixed initial claims."
    },
    {
      "type": "implemented_by",
      "source_node": "adversarial debate between duplicate agents supervised by human judge",
      "target_node": "sparse pixel MNIST debate experimental setup",
      "description": "Experiment instantiates debate in a vision context without language.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 3.1 explicitly uses debate design.",
      "edge_rationale": "MNIST task is concrete realisation."
    },
    {
      "type": "implemented_by",
      "source_node": "adversarial debate between duplicate agents supervised by human judge",
      "target_node": "human single-pixel reveal image debate website",
      "description": "Website demonstrates debate with human participants.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Prototype described in Section 3.2.",
      "edge_rationale": "Platform instantiates design for humans."
    },
    {
      "type": "implemented_by",
      "source_node": "adversarial debate between duplicate agents supervised by human judge",
      "target_node": "sharing activations between agents for transparency",
      "description": "Activation sharing proposed as a refinement to improve debate training.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Suggestion in Section 6.2 without empirical test.",
      "edge_rationale": "Transparency aids approximating perfect information."
    },
    {
      "type": "implemented_by",
      "source_node": "train reward predictor to scale human debate judgments",
      "target_node": "training judge models to predict human reward",
      "description": "Judge model realises the reward-predictor concept.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Implemented in Section 3.1 MNIST study.",
      "edge_rationale": "CNN judge is concrete predictor."
    },
    {
      "type": "validated_by",
      "source_node": "self-play reinforcement learning of debate agents",
      "target_node": "MNIST debate increases judge accuracy from 59.4% to 88.9%",
      "description": "Empirical result confirms self-play debate improves alignment performance.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Quantitative data in Table 2 Section 3.1.",
      "edge_rationale": "Performance gain evidences mechanism efficacy."
    },
    {
      "type": "validated_by",
      "source_node": "answer precommitment before debate turns",
      "target_node": "MNIST debate increases judge accuracy from 59.4% to 88.9%",
      "description": "Data shows liar success drops sharply when precommitment is enforced.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 3.1 comparative results.",
      "edge_rationale": "Supports precommitment design."
    },
    {
      "type": "validated_by",
      "source_node": "training judge models to predict human reward",
      "target_node": "MNIST debate increases judge accuracy from 59.4% to 88.9%",
      "description": "Judge model accuracy is baseline for measuring debate lift.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Model accuracy explicitly compared in Section 3.1.",
      "edge_rationale": "Model supplies evaluation metric."
    },
    {
      "type": "validated_by",
      "source_node": "adversarial debate between duplicate agents supervised by human judge",
      "target_node": "PSPACE proof demonstrating debate power",
      "description": "Formal proof underpins design soundness.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Mathematical theorem in Section 2.2.",
      "edge_rationale": "Shows designâ€™s theoretical capacity."
    },
    {
      "type": "motivates",
      "source_node": "PSPACE proof demonstrating debate power",
      "target_node": "adversarial debate fine-tuning to align AI models",
      "description": "Formal guarantee encourages adoption of debate in alignment pipelines.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Inference: authors propose implementation based on proof.",
      "edge_rationale": "Theorem provides rationale for intervention."
    },
    {
      "type": "motivates",
      "source_node": "MNIST debate increases judge accuracy from 59.4% to 88.9%",
      "target_node": "adversarial debate fine-tuning to align AI models",
      "description": "Empirical improvement suggests feasibility of fine-tuning with debate.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct experimental evidence Section 3.1.",
      "edge_rationale": "Proof-of-concept motivates broader use."
    },
    {
      "type": "motivates",
      "source_node": "informal human debate indicates honest agent usually wins",
      "target_node": "adversarial debate fine-tuning to align AI models",
      "description": "Human pilot results reinforce practicality with real judges.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Anecdotal evidence Section 3.2.",
      "edge_rationale": "Positive early human tests support intervention."
    },
    {
      "type": "enabled_by",
      "source_node": "training judge models to predict human reward",
      "target_node": "deploy reward predictor to automate large-scale debate training",
      "description": "Learned judge is required component for large-scale automated supervision.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical dependency from Section 2.3.",
      "edge_rationale": "Automation relies on predictor."
    },
    {
      "type": "refined_by",
      "source_node": "adversarial debate fine-tuning to align AI models",
      "target_node": "deploy reward predictor to automate large-scale debate training",
      "description": "Reward predictor extends fine-tuning to industrial scale with minimal humans.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Suggested future work Section 2.3; not yet implemented broadly.",
      "edge_rationale": "Predictor is refinement of debate training."
    }
  ],
  "meta": [
    {
      "key": "title",
      "value": "AI safety via debate"
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1805.00899"
    },
    {
      "key": "date_published",
      "value": "2018-05-02T00:00:00Z"
    },
    {
      "key": "authors",
      "value": [
        "Geoffrey Irving",
        "Paul Christiano",
        "Dario Amodei"
      ]
    },
    {
      "key": "source",
      "value": "arxiv"
    }
  ]
}