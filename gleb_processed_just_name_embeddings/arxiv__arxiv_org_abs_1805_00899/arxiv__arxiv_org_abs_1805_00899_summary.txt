Summary
The paper “AI Safety via Debate” proposes training advanced AI models through an adversarial-collaborative game (“debate”) in which two copies of a model argue before a (human or learned) judge.  The central claim is that, in this setting, “it is harder to lie than to refute a lie,” so optimal (Nash-equilibrium) play incentivises truthful, aligned behaviour even when the judge is far less capable than the agents.  The authors supply: (1) complexity-theoretic analysis showing that debate with polynomial-time judges can in principle decide any PSPACE problem; (2) design principles such as short, pre-committed debates, self-play training, activation-sharing and reward-predictor scaling; and (3) preliminary validation including a sparse-pixel MNIST experiment that boosts judge accuracy from 59.4 % to 88.9 % and informal human-only image debates.  The principal actionable intervention is to fine-tune/RLHF powerful models with debate-based training and automated reward models.

Extraction confidence [82]
The JSON passes all checklist items (no risk–intervention shortcuts, no unconnected nodes, framework decomposed, exact-match edge references).  Node naming follows required granularity.  Residual uncertainty stems from interpreting some design/implementation distinctions and assigning confidence scores from textual evidence.

Inference strategy justification
Where the paper states clear mechanisms, those nodes were copied verbatim; modest inference (confidence 2) was used only to connect obvious but implicit relationships (e.g., that training reward predictors enables automated scaling).

Extraction completeness explanation
23 nodes capture every distinct risk-to-intervention reasoning element repeatedly referenced in Sections 1-8: core risks, cognitive bottlenecks, the key debate hypothesis, design pillars (self-play, pre-commitment, human-judge reward), implementation techniques, formal & empirical evidence, and two concrete interventions.  All other content (e.g., amplification, legal analogies) was non-essential for causal chains and therefore omitted.

Key limitations
1. Validation evidence is preliminary; additional large-scale human studies may alter edge confidences.
2. Some theoretical nodes (e.g., PSPACE proof) are treated as “validation” although they are formal not empirical.
3. Interventions are early-stage; lifecycle/maturity estimates could change quickly with new results.

JSON knowledge fabric