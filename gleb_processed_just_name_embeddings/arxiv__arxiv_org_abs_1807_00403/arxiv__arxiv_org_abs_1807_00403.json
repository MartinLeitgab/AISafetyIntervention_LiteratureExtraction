{
  "nodes": [
    {
      "name": "Uninterpretable reinforcement learning policies in safety-critical domains",
      "aliases": [
        "lack of interpretability in RL agents",
        "black-box RL policy opacity"
      ],
      "type": "concept",
      "description": "Deep-RL policies implemented as neural networks are opaque, hindering debugging, trust and certification when used in safety-critical settings.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "1 Introduction – authors cite difficulty to interpret learned policies."
    },
    {
      "name": "Absence of verifiable safety guarantees in black-box RL policies",
      "aliases": [
        "no worst-case guarantees in neural RL",
        "lack of safety constraint validation"
      ],
      "type": "concept",
      "description": "Because the policy internals are opaque, it is hard to impose and check worst-case or safety constraints on standard deep-RL policies.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "1 Introduction – paper stresses difficulty to validate safety constraints."
    },
    {
      "name": "Sample inefficiency of gradient-based RL under poor initialization",
      "aliases": [
        "slow convergence in RL",
        "sample-inefficient RL learning"
      ],
      "type": "concept",
      "description": "Pure gradient-based reinforcement learning can require large numbers of trajectories, especially when starting from badly initialised policies.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "2 Mixed Optimization & 3 Experiments – authors note TRPO takes orders of magnitude longer from poor initial policies."
    },
    {
      "name": "Opaque decision logic in deep neural network policies",
      "aliases": [
        "hidden reasoning in DNN policies",
        "black-box policy decisions"
      ],
      "type": "concept",
      "description": "Neural network representations hide the conditional structure that determines action selection.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "1 Introduction – identified as mechanism behind uninterpretability."
    },
    {
      "name": "Inability to impose and validate safety constraints on black-box policies",
      "aliases": [
        "constraint enforcement difficulty",
        "verification gap in RL"
      ],
      "type": "concept",
      "description": "Without a symbolic form, formal verification or constraint satisfaction techniques cannot be readily applied.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "1 Introduction – motivates need for program repair/verification."
    },
    {
      "name": "Gradient descent local minima causing slow policy convergence",
      "aliases": [
        "local minima in policy optimisation",
        "poor landscape for RL"
      ],
      "type": "concept",
      "description": "Policy-gradient methods can become trapped in sub-optimal regions, making learning inefficient.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "2 Mixed Optimization – rationale for alternation with program space."
    },
    {
      "name": "Symbolic policy representations enable formal verification and repair",
      "aliases": [
        "symbolic programs for policy analysis",
        "programmatic policy representations"
      ],
      "type": "concept",
      "description": "Expressing policies as decision-tree or DSL programs allows program-repair and verification tools (SAT/SMT) or human inspection to enforce properties.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "2 Mixed Optimization – key premise for using synthesis and repair."
    },
    {
      "name": "Alternating optimization between symbolic and neural representations improves learning efficiency",
      "aliases": [
        "mixed optimisation benefits",
        "dual representation learning"
      ],
      "type": "concept",
      "description": "Switching between program space (global edits) and differentiable space (fine tuning) can escape local minima and speed convergence.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "2 Mixed Optimization – authors posit better escape from local minima."
    },
    {
      "name": "Iterative synthesis–repair–imitation–finetuning framework for policy improvement",
      "aliases": [
        "MORL pipeline",
        "mixed optimisation scheme"
      ],
      "type": "concept",
      "description": "The design decomposes learning into four repeating steps: extract a symbolic program, repair it, clone back to a neural policy, and fine-tune with gradient descent.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Figure 1 & 2.2 Model – core design proposed."
    },
    {
      "name": "Human-in-the-loop program repair for adding desired behaviors",
      "aliases": [
        "manual debugging of symbolic policy",
        "expert edits to program"
      ],
      "type": "concept",
      "description": "Experts can directly edit the symbolic policy to remove undesirable actions or encode domain knowledge before re-distillation.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "2 Mixed Optimization – manual modification step highlighted."
    },
    {
      "name": "Policy synthesis to decision tree via VIPER",
      "aliases": [
        "decision-tree extraction",
        "VIPER policy synthesis"
      ],
      "type": "concept",
      "description": "VIPER imitation-learning approach converts a neural policy into an interpretable decision tree within a DSL.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "2.2 Model – synthesis step description."
    },
    {
      "name": "Program repair using constraint solving or manual edits to symbolic program",
      "aliases": [
        "policy program repair",
        "safety-oriented program debugging"
      ],
      "type": "concept",
      "description": "Once a symbolic program is extracted, SAT/SMT-based repair or expert modifications adjust it to satisfy constraints.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "2.2 Model – repair step; 3 Experiments – manual edits demonstrated."
    },
    {
      "name": "Behavioral cloning of repaired program into neural policy",
      "aliases": [
        "distillation from program",
        "imitation learning from symbolic oracle"
      ],
      "type": "concept",
      "description": "Supervised imitation learning trains a neural network to match the outputs of the repaired program, producing a reactive policy.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "2.2 Model – imitation step; Figure 3 shows convergence."
    },
    {
      "name": "TRPO finetuning of cloned policy",
      "aliases": [
        "gradient descent after cloning",
        "policy optimisation with TRPO"
      ],
      "type": "concept",
      "description": "Trust-region policy optimisation further improves the cloned neural policy’s reward while retaining repaired behaviours.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "2.2 Model & 3 Experiments – final finetuning step."
    },
    {
      "name": "CartPole experiments showing reward improvements after program repair",
      "aliases": [
        "reward lift from repaired policies",
        "CartPole repair results"
      ],
      "type": "concept",
      "description": "Empirical data: average reward jumps from ~10 (worst) to 200 (near-optimal) after program repair and imitation.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "3 Experiments – performance table and Fig 3."
    },
    {
      "name": "Faster convergence of TRPO from repaired initialization versus worst policy",
      "aliases": [
        "speed-up in convergence",
        "sample efficiency gain"
      ],
      "type": "concept",
      "description": "TRPO starting from repaired policies reaches high reward in far fewer episodes than starting from poorly initialised networks.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "3 Experiments – Figure 2 demonstrates faster learning curves."
    },
    {
      "name": "Deploy MORL pipeline during RL training to iteratively synthesize, repair, imitate, and finetune policies",
      "aliases": [
        "apply mixed optimisation",
        "use MORL in training loop"
      ],
      "type": "intervention",
      "description": "During model development, alternate between policy synthesis, program repair, behavioral cloning, and TRPO fine-tuning until performance and safety goals are met.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "2.2 Model – procedure carried out during policy training phase.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "3 Experiments – demonstrated only on CartPole (proof-of-concept).",
      "node_rationale": "Summarises full actionable pipeline proposed."
    },
    {
      "name": "Perform program repair on symbolic policies to enforce safety constraints before imitation",
      "aliases": [
        "repair symbolic policy",
        "apply constraint-based debugging"
      ],
      "type": "intervention",
      "description": "After extracting a decision tree, use SAT/SMT or expert edits to force the program to satisfy specified safety or performance constraints prior to distillation.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Repair step sits inside the iterative training loop (2.2 Model).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Manual edits evaluated on CartPole only (3 Experiments).",
      "node_rationale": "Directly addresses lack of verifiable guarantees."
    },
    {
      "name": "Extract decision tree representations from neural policies for interpretability and verification",
      "aliases": [
        "policy extraction with VIPER",
        "create symbolic policy"
      ],
      "type": "intervention",
      "description": "Run VIPER to convert the current neural policy into a decision-tree program that can be inspected and analysed.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Synthesis occurs during development before deployment (2.2 Model).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Used experimentally on CartPole only (3 Experiments).",
      "node_rationale": "Enables interpretability and sets up subsequent repair."
    },
    {
      "name": "Finetune behavioral cloned policies using TRPO to escape local minima",
      "aliases": [
        "post-cloning TRPO",
        "fine-tune repaired policy"
      ],
      "type": "intervention",
      "description": "Apply a restricted-step policy-gradient algorithm after cloning to further raise reward while preserving repaired behaviours.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Final phase of training loop before evaluation (2.2 Model).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Evaluated on CartPole only (3 Experiments).",
      "node_rationale": "Targets sample inefficiency by leveraging good initialisation."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Uninterpretable reinforcement learning policies in safety-critical domains",
      "target_node": "Opaque decision logic in deep neural network policies",
      "description": "Opaque decision logic makes it impossible to understand the learned policy, leading directly to uninterpretability.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit link in 1 Introduction.",
      "edge_rationale": "Mechanistic explanation provided by authors."
    },
    {
      "type": "caused_by",
      "source_node": "Absence of verifiable safety guarantees in black-box RL policies",
      "target_node": "Inability to impose and validate safety constraints on black-box policies",
      "description": "If constraints cannot be imposed or checked, no guarantees can be provided.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct statement in 1 Introduction.",
      "edge_rationale": "Authors cite verification difficulty as root cause."
    },
    {
      "type": "caused_by",
      "source_node": "Sample inefficiency of gradient-based RL under poor initialization",
      "target_node": "Gradient descent local minima causing slow policy convergence",
      "description": "Local minima trap gradient methods, wasting samples and slowing learning.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Discussed qualitatively in 2 Mixed Optimization.",
      "edge_rationale": "Authors argue alternation helps escape these minima."
    },
    {
      "type": "mitigated_by",
      "source_node": "Opaque decision logic in deep neural network policies",
      "target_node": "Symbolic policy representations enable formal verification and repair",
      "description": "Replacing the opaque network by a symbolic program exposes logic for inspection.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoned argument in 2 Mixed Optimization.",
      "edge_rationale": "Symbolic forms are interpretable."
    },
    {
      "type": "mitigated_by",
      "source_node": "Inability to impose and validate safety constraints on black-box policies",
      "target_node": "Symbolic policy representations enable formal verification and repair",
      "description": "Symbolic programs can be subjected to SAT/SMT checking, allowing constraint enforcement.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "2 Mixed Optimization describes verification benefits.",
      "edge_rationale": "Formal tools need symbolic input."
    },
    {
      "type": "mitigated_by",
      "source_node": "Gradient descent local minima causing slow policy convergence",
      "target_node": "Alternating optimization between symbolic and neural representations improves learning efficiency",
      "description": "Switching to program space permits larger jumps that escape local minima.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Authors hypothesise, limited empirical support.",
      "edge_rationale": "Claimed advantage in 2 Mixed Optimization."
    },
    {
      "type": "mitigated_by",
      "source_node": "Symbolic policy representations enable formal verification and repair",
      "target_node": "Iterative synthesis–repair–imitation–finetuning framework for policy improvement",
      "description": "The design operationalises the insight by embedding synthesis and repair steps.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Framework description in 2.2 Model.",
      "edge_rationale": "Design embodies theoretical claim."
    },
    {
      "type": "mitigated_by",
      "source_node": "Symbolic policy representations enable formal verification and repair",
      "target_node": "Human-in-the-loop program repair for adding desired behaviors",
      "description": "Human editing of symbolic programs leverages interpretability to fix issues.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Mentioned in 2.2 Model and 3 Experiments.",
      "edge_rationale": "Manual repair relies on interpretability."
    },
    {
      "type": "mitigated_by",
      "source_node": "Alternating optimization between symbolic and neural representations improves learning efficiency",
      "target_node": "Iterative synthesis–repair–imitation–finetuning framework for policy improvement",
      "description": "The framework alternates between the two representations to realise the efficiency gain.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "2.2 Model explains alternation benefits.",
      "edge_rationale": "Design maps directly onto insight."
    },
    {
      "type": "implemented_by",
      "source_node": "Iterative synthesis–repair–imitation–finetuning framework for policy improvement",
      "target_node": "Policy synthesis to decision tree via VIPER",
      "description": "VIPER extraction realises the synthesis phase of the framework.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit step in Figure 1.",
      "edge_rationale": "Named as first step."
    },
    {
      "type": "implemented_by",
      "source_node": "Iterative synthesis–repair–imitation–finetuning framework for policy improvement",
      "target_node": "Program repair using constraint solving or manual edits to symbolic program",
      "description": "Repair constitutes the second phase of the iterative loop.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Figure 1 and text describe sequence.",
      "edge_rationale": "Framework includes repair."
    },
    {
      "type": "implemented_by",
      "source_node": "Iterative synthesis–repair–imitation–finetuning framework for policy improvement",
      "target_node": "Behavioral cloning of repaired program into neural policy",
      "description": "Imitation learning is the third phase of the loop.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Figure 1 shows distillation step.",
      "edge_rationale": "Direct mapping."
    },
    {
      "type": "implemented_by",
      "source_node": "Iterative synthesis–repair–imitation–finetuning framework for policy improvement",
      "target_node": "TRPO finetuning of cloned policy",
      "description": "Gradient-based fine-tuning is the fourth phase.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Figure 1 and 2.2 Model.",
      "edge_rationale": "Final step in cyclic design."
    },
    {
      "type": "implemented_by",
      "source_node": "Human-in-the-loop program repair for adding desired behaviors",
      "target_node": "Program repair using constraint solving or manual edits to symbolic program",
      "description": "Manual edits are a concrete mode of program repair.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "3 Experiments – manual debugging example.",
      "edge_rationale": "Edge shows embodiment of design rationale."
    },
    {
      "type": "preceded_by",
      "source_node": "Program repair using constraint solving or manual edits to symbolic program",
      "target_node": "Policy synthesis to decision tree via VIPER",
      "description": "Repair can only occur after a symbolic program has been synthesised.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical ordering inferred from framework diagram.",
      "edge_rationale": "Sequence requirement."
    },
    {
      "type": "preceded_by",
      "source_node": "Behavioral cloning of repaired program into neural policy",
      "target_node": "Program repair using constraint solving or manual edits to symbolic program",
      "description": "Imitation learning is performed after repair produces the final symbolic oracle.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Framework sequencing in Figure 1.",
      "edge_rationale": "Temporal order."
    },
    {
      "type": "preceded_by",
      "source_node": "TRPO finetuning of cloned policy",
      "target_node": "Behavioral cloning of repaired program into neural policy",
      "description": "Fine-tuning happens after the cloned policy exists.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Framework diagram.",
      "edge_rationale": "Temporal ordering."
    },
    {
      "type": "validated_by",
      "source_node": "Program repair using constraint solving or manual edits to symbolic program",
      "target_node": "CartPole experiments showing reward improvements after program repair",
      "description": "Experiments show that repaired programs achieve significantly higher reward.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Quantitative results in 3 Experiments.",
      "edge_rationale": "Empirical validation."
    },
    {
      "type": "validated_by",
      "source_node": "Behavioral cloning of repaired program into neural policy",
      "target_node": "CartPole experiments showing reward improvements after program repair",
      "description": "Cloned policies retain much of the repaired program’s performance.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Figure 3 cloning performance.",
      "edge_rationale": "Evidence shows cloning effectiveness."
    },
    {
      "type": "validated_by",
      "source_node": "TRPO finetuning of cloned policy",
      "target_node": "Faster convergence of TRPO from repaired initialization versus worst policy",
      "description": "TRPO fine-tuning after repair converges faster than from random initialisation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Figure 2 learning curves.",
      "edge_rationale": "Quantitative speed-up."
    },
    {
      "type": "motivates",
      "source_node": "CartPole experiments showing reward improvements after program repair",
      "target_node": "Perform program repair on symbolic policies to enforce safety constraints before imitation",
      "description": "Performance gains encourage use of program repair as an intervention.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Empirical support in 3 Experiments.",
      "edge_rationale": "Positive evidence drives recommendation."
    },
    {
      "type": "motivates",
      "source_node": "CartPole experiments showing reward improvements after program repair",
      "target_node": "Extract decision tree representations from neural policies for interpretability and verification",
      "description": "Observed benefits depend on initial extraction, motivating its adoption.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Results depend on successful synthesis.",
      "edge_rationale": "Evidence underpins extraction usefulness."
    },
    {
      "type": "motivates",
      "source_node": "CartPole experiments showing reward improvements after program repair",
      "target_node": "Deploy MORL pipeline during RL training to iteratively synthesize, repair, imitate, and finetune policies",
      "description": "Overall gains motivate adopting the full MORL process.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Demonstrated effectiveness on benchmark.",
      "edge_rationale": "Evidence supports pipeline deployment."
    },
    {
      "type": "motivates",
      "source_node": "Faster convergence of TRPO from repaired initialization versus worst policy",
      "target_node": "Finetune behavioral cloned policies using TRPO to escape local minima",
      "description": "Data showing faster learning encourages TRPO fine-tuning step.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Episode counts in Figure 2.",
      "edge_rationale": "Supports intervention’s efficacy."
    },
    {
      "type": "motivates",
      "source_node": "Faster convergence of TRPO from repaired initialization versus worst policy",
      "target_node": "Deploy MORL pipeline during RL training to iteratively synthesize, repair, imitate, and finetune policies",
      "description": "Sample-efficiency gains motivate using MORL end-to-end.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Consistent improvement over baseline.",
      "edge_rationale": "Evidence strengthens overall pipeline case."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2018-07-01T00:00:00Z"
    },
    {
      "key": "title",
      "value": "Towards Mixed Optimization for Reinforcement Learning with Program Synthesis"
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1807.00403"
    },
    {
      "key": "authors",
      "value": [
        "Surya Bhupatiraju",
        "Kumar Krishna Agrawal",
        "Rishabh Singh"
      ]
    },
    {
      "key": "source",
      "value": "arxiv"
    }
  ]
}