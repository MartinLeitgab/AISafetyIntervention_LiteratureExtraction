{
  "nodes": [
    {
      "name": "Ineffective mapping of variable-length sequences in neural networks",
      "aliases": [
        "sequence-to-sequence performance limitations",
        "variable-length sequence handling risk"
      ],
      "type": "concept",
      "description": "Standard DNNs expect fixed-size inputs/outputs, which leads to poor accuracy when tasks require mapping input sequences of arbitrary length to output sequences, e.g. machine translation.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in Introduction: highlights the top-level problem the paper addresses."
    },
    {
      "name": "Fixed-dimensional input and output requirement in DNNs",
      "aliases": [
        "fixed-size vector constraint",
        "static dimensionality limitation"
      ],
      "type": "concept",
      "description": "Conventional DNN architectures can only process data that can be encoded as vectors of known, fixed length, impeding sequence modelling tasks.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in §1: root mechanism causing the above risk."
    },
    {
      "name": "Encoder-decoder LSTM learning variable-length sequence mapping",
      "aliases": [
        "LSTM conditional sequence modelling",
        "sequence-to-sequence LSTM insight"
      ],
      "type": "concept",
      "description": "Long Short-Term Memory networks can transform an input sequence into a fixed-size vector and then decode a target sequence, thus bypassing fixed-dimensional constraints.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined in §2: central hypothesis enabling solution."
    },
    {
      "name": "Separate encoder and decoder LSTMs for sequence mapping",
      "aliases": [
        "two-network encoder/decoder design",
        "dual-LSTM architecture rationale"
      ],
      "type": "concept",
      "description": "Using distinct LSTMs for reading the source sequence and for generating the target increases parameter capacity at negligible computation and facilitates multi-language training.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Design choice detailed in §2 (\"two different LSTMs\")."
    },
    {
      "name": "Deep 4-layer LSTM encoder-decoder with 8000-dimensional sentence vectors",
      "aliases": [
        "384M-parameter deep LSTM implementation",
        "4-layer encoder/decoder network"
      ],
      "type": "concept",
      "description": "Implementation uses four 1000-unit layers in both encoder and decoder, yielding 8 000-dimensional sentence embeddings and 384 M parameters, trained with SGD.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation specifics described in §2 & §3.4."
    },
    {
      "name": "BLEU 34.81 on WMT’14 with reversed LSTM ensemble",
      "aliases": [
        "state-of-the-art BLEU evidence",
        "ensemble BLEU validation"
      ],
      "type": "concept",
      "description": "An ensemble of five reversed input deep LSTMs achieves BLEU 34.81, outperforming the phrase-based SMT baseline (BLEU 33.30).",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Primary quantitative result in Table 1 (§3.6)."
    },
    {
      "name": "Design and train deep encoder-decoder LSTM models with reversed input sequence",
      "aliases": [
        "full sequence-to-sequence LSTM system",
        "end-to-end deep LSTM translation"
      ],
      "type": "intervention",
      "description": "During model design and training, employ a 4-layer encoder/decoder LSTM architecture that reads the source sequence in reverse order and decodes left-to-right, decoded with small-beam search.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Fits the Model Design phase (Architecture & objectives decided in §2).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Validated on a large public dataset with strong empirical results (§3.6).",
      "node_rationale": "Actionable prescription implied by combined design & evidence sections."
    },
    {
      "name": "Translation degradation on long sentences in RNN models",
      "aliases": [
        "long-sentence performance drop",
        "length-dependent translation risk"
      ],
      "type": "concept",
      "description": "Prior RNN architectures exhibit sharply reduced translation quality on lengthy source sentences, jeopardising usability for real-world documents.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivating problem referenced in §1 and §3.7."
    },
    {
      "name": "Large minimal time lag between corresponding source and target words",
      "aliases": [
        "long alignment lag",
        "temporal distance problem"
      ],
      "type": "concept",
      "description": "In normal sentence order, early source words are distant in time from their target counterparts, making back-propagation across many steps difficult.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in §3.3 as cause of long-sentence failures."
    },
    {
      "name": "Reversing source sentences reduces minimal time lag",
      "aliases": [
        "reverse-order time-lag insight",
        "short-dependency hypothesis"
      ],
      "type": "concept",
      "description": "Ordering the source words backwards brings early source tokens near early target tokens, shortening dependencies and easing optimisation.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Primary insight of §3.3."
    },
    {
      "name": "Apply input sentence reversal before training",
      "aliases": [
        "data reversal rationale",
        "reverse-source design choice"
      ],
      "type": "concept",
      "description": "The model is intentionally trained on datasets where every source sentence is reversed, keeping target sentences intact.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Design step motivated in §3.3."
    },
    {
      "name": "Reverse source sentence order during training",
      "aliases": [
        "reversed-input implementation",
        "data preprocessing reversal"
      ],
      "type": "concept",
      "description": "A preprocessing pipeline that inverts token order for every source sentence before it is fed into the encoder LSTM.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation detail in §3.3 & Figure 1."
    },
    {
      "name": "Perplexity drop to 4.7 and BLEU increase to 30.6 with reversed input",
      "aliases": [
        "reversal validation metric",
        "perplexity and BLEU evidence"
      ],
      "type": "concept",
      "description": "Reversing inputs lowers test perplexity from 5.8→4.7 and raises single-model BLEU from 25.9→30.6, demonstrating efficacy on long sentences.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Quantitative proof presented in §3.3."
    },
    {
      "name": "Reverse source sentences during data preprocessing for sequence models",
      "aliases": [
        "apply sentence reversal intervention",
        "source-order inversion procedure"
      ],
      "type": "intervention",
      "description": "Integrate a preprocessing step that automatically reverses word order of all source sentences before training any sequence-to-sequence model.",
      "concept_category": null,
      "intervention_lifecycle": "2",
      "intervention_lifecycle_rationale": "Data manipulation occurs prior to model training (§3.3).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Validated in one large-scale study; further replication needed.",
      "node_rationale": "Direct actionable step inferred from design & evidence."
    },
    {
      "name": "Training instability due to exploding gradients in deep LSTM training",
      "aliases": [
        "optimizer divergence risk",
        "unstable gradient dynamics"
      ],
      "type": "concept",
      "description": "Large recurrent nets can experience gradient norms that grow without bound, halting learning or corrupting parameter updates.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Problem acknowledged in §3.4 (gradient clipping discussion)."
    },
    {
      "name": "Exploding gradients in deep LSTM optimization",
      "aliases": [
        "large gradient norms",
        "RNN gradient explosion mechanism"
      ],
      "type": "concept",
      "description": "During back-propagation, the L2 norm of gradients can exceed safe thresholds, destabilising SGD.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mechanism detailed in §3.4."
    },
    {
      "name": "Gradient norm constraint to control exploding gradients",
      "aliases": [
        "gradient clipping insight",
        "norm-bounding hypothesis"
      ],
      "type": "concept",
      "description": "Bounding gradient norms prevents instability while preserving useful learning signal.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Insight motivating clipping (citing Graves 2013) in §3.4."
    },
    {
      "name": "Apply hard gradient norm clipping",
      "aliases": [
        "gradient clipping rationale",
        "norm threshold design choice"
      ],
      "type": "concept",
      "description": "Adopt a policy that rescales gradients when their norm exceeds a chosen threshold.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Design step described in bullet list of §3.4."
    },
    {
      "name": "Scale gradient when norm exceeds threshold 5",
      "aliases": [
        "threshold-5 gradient clipping",
        "implementation of clipping"
      ],
      "type": "concept",
      "description": "For each minibatch, compute s=||g||₂; if s>5, set g = 5·g/s before the parameter update.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Precise implementation detail in §3.4."
    },
    {
      "name": "Successful training of 384M parameter LSTM over 7.5 epochs",
      "aliases": [
        "training stability evidence",
        "gradient clipping validation"
      ],
      "type": "concept",
      "description": "Model converged without divergence, processing 6  300 words/sec on 8 GPUs, demonstrating clipping’s effectiveness.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Training outcome reported in §3.4-§3.5."
    },
    {
      "name": "Apply gradient norm clipping at threshold 5 during LSTM training",
      "aliases": [
        "use gradient clipping intervention",
        "norm-bounded training procedure"
      ],
      "type": "intervention",
      "description": "During any fine-tuning or training step, compute gradient norm and rescale when it exceeds 5 to prevent optimisation instability.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Operational during parameter update phase (§3.4).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Supported by single-study empirical success; broader validation outstanding.",
      "node_rationale": "Directly actionable method derived from design & evidence."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Ineffective mapping of variable-length sequences in neural networks",
      "target_node": "Fixed-dimensional input and output requirement in DNNs",
      "description": "Fixed vector constraints directly hinder sequence-to-sequence generalisation, producing the stated risk.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit causal discussion in §1.",
      "edge_rationale": "Introduction links fixed-size assumption to inability to handle sequences."
    },
    {
      "type": "caused_by",
      "source_node": "Fixed-dimensional input and output requirement in DNNs",
      "target_node": "Encoder-decoder LSTM learning variable-length sequence mapping",
      "description": "The theoretical insight explains the mechanism giving rise to the problem.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implied relation – moderate inference from §2.",
      "edge_rationale": "Paper motivates LSTM encoder/decoder as response to fixed-size issue."
    },
    {
      "type": "mitigated_by",
      "source_node": "Encoder-decoder LSTM learning variable-length sequence mapping",
      "target_node": "Separate encoder and decoder LSTMs for sequence mapping",
      "description": "Design separates roles to implement the theoretical concept effectively.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit design claim in §2.",
      "edge_rationale": "Authors state dual LSTMs ‘increases parameters’ and eases training."
    },
    {
      "type": "implemented_by",
      "source_node": "Separate encoder and decoder LSTMs for sequence mapping",
      "target_node": "Deep 4-layer LSTM encoder-decoder with 8000-dimensional sentence vectors",
      "description": "The concrete network instantiation realises the design rationale.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Detailed implementation specs in §2 & §3.4.",
      "edge_rationale": "Four-layer architecture embodies dual-LSTM design."
    },
    {
      "type": "validated_by",
      "source_node": "Deep 4-layer LSTM encoder-decoder with 8000-dimensional sentence vectors",
      "target_node": "BLEU 34.81 on WMT’14 with reversed LSTM ensemble",
      "description": "Empirical benchmark shows that the implemented architecture achieves superior accuracy.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Controlled benchmark result in Table 1.",
      "edge_rationale": "Performance metric validates architecture."
    },
    {
      "type": "motivates",
      "source_node": "BLEU 34.81 on WMT’14 with reversed LSTM ensemble",
      "target_node": "Design and train deep encoder-decoder LSTM models with reversed input sequence",
      "description": "Strong empirical evidence encourages adoption of the intervention.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical inference based on result magnitude.",
      "edge_rationale": "Higher BLEU motivates using the approach in practice."
    },
    {
      "type": "caused_by",
      "source_node": "Translation degradation on long sentences in RNN models",
      "target_node": "Large minimal time lag between corresponding source and target words",
      "description": "Long alignment gaps are identified as mechanism for degradation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Argued in §3.3 referencing minimal time lag literature.",
      "edge_rationale": "Paper cites time-lag concept as root cause."
    },
    {
      "type": "caused_by",
      "source_node": "Large minimal time lag between corresponding source and target words",
      "target_node": "Reversing source sentences reduces minimal time lag",
      "description": "Insight explains how reversing order addresses large lags.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Stated reasoning in §3.3.",
      "edge_rationale": "Authors attribute improvements to reduced time lag."
    },
    {
      "type": "mitigated_by",
      "source_node": "Reversing source sentences reduces minimal time lag",
      "target_node": "Apply input sentence reversal before training",
      "description": "Design step operationalises the insight.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct design decision in §3.3.",
      "edge_rationale": "Paper explicitly decides to reverse inputs."
    },
    {
      "type": "implemented_by",
      "source_node": "Apply input sentence reversal before training",
      "target_node": "Reverse source sentence order during training",
      "description": "Preprocessing routine embodies the design choice.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Implementation detail provided.",
      "edge_rationale": "Turning design into code."
    },
    {
      "type": "validated_by",
      "source_node": "Reverse source sentence order during training",
      "target_node": "Perplexity drop to 4.7 and BLEU increase to 30.6 with reversed input",
      "description": "Quantitative metrics verify the benefit of reversal.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Results in §3.3 & Figure 3.",
      "edge_rationale": "Perplexity/BLEU improvements validate mechanism."
    },
    {
      "type": "motivates",
      "source_node": "Perplexity drop to 4.7 and BLEU increase to 30.6 with reversed input",
      "target_node": "Reverse source sentences during data preprocessing for sequence models",
      "description": "Evidence supports adopting reversal as a standard preprocessing intervention.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical policy suggestion from empirical gain.",
      "edge_rationale": "Practitioners motivated by improved metrics."
    },
    {
      "type": "caused_by",
      "source_node": "Training instability due to exploding gradients in deep LSTM training",
      "target_node": "Exploding gradients in deep LSTM optimization",
      "description": "Gradient explosion is the mechanism producing instability.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explained in §3.4.",
      "edge_rationale": "Direct causal link."
    },
    {
      "type": "caused_by",
      "source_node": "Exploding gradients in deep LSTM optimization",
      "target_node": "Gradient norm constraint to control exploding gradients",
      "description": "Insight proposes norm constraint as solution to the mechanism.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Based on referenced prior work (§3.4).",
      "edge_rationale": "Theory suggests mitigation."
    },
    {
      "type": "mitigated_by",
      "source_node": "Gradient norm constraint to control exploding gradients",
      "target_node": "Apply hard gradient norm clipping",
      "description": "Design formalises using a hard clip to enforce the constraint.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Design described explicitly.",
      "edge_rationale": "Transforms insight into design choice."
    },
    {
      "type": "implemented_by",
      "source_node": "Apply hard gradient norm clipping",
      "target_node": "Scale gradient when norm exceeds threshold 5",
      "description": "Specific threshold-based rescaling realises the clipping design.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Exact algorithm given.",
      "edge_rationale": "Implementation step."
    },
    {
      "type": "validated_by",
      "source_node": "Scale gradient when norm exceeds threshold 5",
      "target_node": "Successful training of 384M parameter LSTM over 7.5 epochs",
      "description": "Stable convergence demonstrates the effectiveness of clipping.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Evidence section provides successful training details.",
      "edge_rationale": "Empirical validation."
    },
    {
      "type": "motivates",
      "source_node": "Successful training of 384M parameter LSTM over 7.5 epochs",
      "target_node": "Apply gradient norm clipping at threshold 5 during LSTM training",
      "description": "Demonstrated stability motivates adopting clipping in other projects.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical extrapolation from observed success.",
      "edge_rationale": "Evidence influences intervention choice."
    },
    {
      "type": "required_by",
      "source_node": "Deep 4-layer LSTM encoder-decoder with 8000-dimensional sentence vectors",
      "target_node": "Scale gradient when norm exceeds threshold 5",
      "description": "Large deep model necessitates gradient clipping to avoid instability.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Mentioned implicitly in §3.4; moderate inference.",
      "edge_rationale": "Clipping is prerequisite for stable training of deep model."
    },
    {
      "type": "required_by",
      "source_node": "Deep 4-layer LSTM encoder-decoder with 8000-dimensional sentence vectors",
      "target_node": "Reverse source sentence order during training",
      "description": "Architecture performance depends on reversed input preprocessing as per authors’ findings.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Authors emphasise reversal as key technical contribution; moderate inference to mark requirement.",
      "edge_rationale": "Reversal integral to achieving reported BLEU."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2014-09-10T00:00:00Z"
    },
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "title",
      "value": "Sequence to Sequence Learning with Neural Networks"
    },
    {
      "key": "authors",
      "value": [
        "Ilya Sutskever",
        "Oriol Vinyals",
        "Quoc V. Le"
      ]
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1409.3215"
    }
  ]
}