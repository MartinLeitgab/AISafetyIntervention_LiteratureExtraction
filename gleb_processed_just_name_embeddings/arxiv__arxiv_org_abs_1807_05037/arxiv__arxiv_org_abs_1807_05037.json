{
  "nodes": [
    {
      "name": "Pathologically wrong value inference from hierarchical human planning in IRL",
      "aliases": [
        "preference misalignment in hierarchical settings",
        "mis-specified reward inference with hierarchical behaviour"
      ],
      "type": "concept",
      "description": "Standard IRL systems that ignore human hierarchical planning can learn reward functions that diverge sharply from true human preferences, risking unsafe goal pursuit.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in Section 1 Introduction as the motivating danger of ‘accidentally inferring pathologically wrong values’."
    },
    {
      "name": "Assumption of flat optimal human behaviour in standard IRL",
      "aliases": [
        "revealed-preference optimality assumption",
        "flat-planning assumption in IRL"
      ],
      "type": "concept",
      "description": "Classical IRL frameworks model humans as perfectly rational agents selecting primitive actions, ignoring biases, bounded rationality and hierarchical skills.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Sections 1 and 3 as the core limitation causing mis-inference."
    },
    {
      "name": "Human decision-making uses hierarchical options",
      "aliases": [
        "hierarchical planning with options",
        "option-based human skills"
      ],
      "type": "concept",
      "description": "Humans simplify planning by selecting extended ‘options’—temporally extended action sequences such as ‘go to taxi rank’—rather than reasoning over all primitive actions.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 2 Our Model and reference to Botvinick et al. 2009 highlight hierarchy as key cognitive feature."
    },
    {
      "name": "Boltzmann-rational bounded planning models human action selection",
      "aliases": [
        "self-consistent Boltzmann policy for humans",
        "bounded-rationality temperature model"
      ],
      "type": "concept",
      "description": "Humans trade reward for cognitive effort; this can be captured by a Boltzmann policy with parameter β that yields stochastic, energetically-bounded choices.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in Sections 2 and 3.1 as theoretical grounding."
    },
    {
      "name": "Model human planning with options in inverse reinforcement learning",
      "aliases": [
        "hierarchy-aware reward modelling",
        "option-aware goal inference design"
      ],
      "type": "concept",
      "description": "Design principle: extend IRL to treat options as choice units, enabling correct preference inference when observing hierarchical behaviour.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined in Section 2 as first design response to hierarchical insight."
    },
    {
      "name": "Self-consistent hierarchical Boltzmann policy captures bounded rationality",
      "aliases": [
        "hierarchical Boltzmann design",
        "β-consistent option policy"
      ],
      "type": "concept",
      "description": "Design principle: ensure the agent’s value and choice computations are mutually consistent under Boltzmann rationality across both primitive actions and options.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 2 ‘key features of our model’ justifies this design."
    },
    {
      "name": "Generative model of hierarchical planners with extended options",
      "aliases": [
        "hierarchical option generative process",
        "state-option trajectory model"
      ],
      "type": "concept",
      "description": "Implementation creates a probabilistic process mapping latent state-option trajectories to observed state-action trajectories, parameterised by reward θ and option set ω.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Formalised in Section 5 Bayesian Description."
    },
    {
      "name": "BIHRL Bayesian inference algorithm with Policy-Walk MCMC sampling",
      "aliases": [
        "Policy-Walk BIHRL sampler",
        "hierarchical MCMC reward inference"
      ],
      "type": "concept",
      "description": "Uses enumeration of consistent option-trajectories and an MCMC ‘Policy-Walk’ approach to sample reward parameters that best explain observed behaviour.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Described in Sections 5 and 6 with Appendix A."
    },
    {
      "name": "Option-set marginalization to jointly infer options and rewards",
      "aliases": [
        "latent option inference",
        "option-reward joint posterior"
      ],
      "type": "concept",
      "description": "Extends BIHRL by placing a prior over option sets and marginalising them during inference, enabling preference learning without prior knowledge of human skills.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 8 Inferring Option-Sets introduces this refinement."
    },
    {
      "name": "Improved goal prediction accuracy in Taxi-driver environment",
      "aliases": [
        "Taxi-driver BIHRL accuracy boost",
        "toy example validation"
      ],
      "type": "concept",
      "description": "BIHRL retains high posterior on true reward θ and outperforms baseline BIRL across multiple trajectories in the modified Taxi environment.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Empirical results shown in Section 6 and Figure 2."
    },
    {
      "name": "Reduced negative log marginal likelihood and 66% accuracy in Wikispeedia benchmark",
      "aliases": [
        "Wikispeedia BIHRL validation",
        "hierarchical model NLML reduction"
      ],
      "type": "concept",
      "description": "Adding ~150 hierarchical options halves NLML and raises target-page prediction accuracy from 62 % (BIRL) to 66 %, matching feature-engineered baselines.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Presented in Section 7 and Figures 3–4."
    },
    {
      "name": "Higher posterior probability on ground truth reward under option-set inference",
      "aliases": [
        "option-marginalisation validation",
        "BIHRL posterior improvement"
      ],
      "type": "concept",
      "description": "When marginalising over unknown option sets, BIHRL assigns ~0.55 probability to true θ versus <0.03 under BIRL, demonstrating robustness to option uncertainty.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Reported in Section 8 and Figure 5."
    },
    {
      "name": "Incorporate hierarchical option-aware BIHRL in value learning pipelines during model fine-tuning to improve preference inference",
      "aliases": [
        "apply BIHRL during RLHF",
        "hierarchical value learning integration"
      ],
      "type": "intervention",
      "description": "During reinforcement-learning-from-human-feedback or similar fine-tuning, replace flat IRL components with BIHRL inference to capture hierarchical human preferences and reduce reward-mis-specification.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Section 6 describes applying BIHRL while analysing trajectories, corresponding to the fine-tuning/reward-model training stage.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Evidence limited to experimental Taxi and Wikispeedia domains (Section 6–7).",
      "node_rationale": "Directly follows validated mechanisms to mitigate mis-inference risk."
    },
    {
      "name": "Provide explicit option libraries reflecting common human hierarchical skills during reward model design",
      "aliases": [
        "design option libraries for BIHRL",
        "inject human skill options"
      ],
      "type": "intervention",
      "description": "At model-design time, construct and supply option sets (e.g., ‘go-to-landmark’, ‘navigate-to-topic’) that mirror typical human sub-tasks, enabling BIHRL to reason with realistic skills and improve inference.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Creating option libraries is a design-time activity before any training (Sections 4 and 7).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated experimentally in Taxi and Wikispeedia benchmarks but not yet in production settings.",
      "node_rationale": "Empirically shown to boost prediction accuracy when such libraries are provided."
    },
    {
      "name": "Apply scalable variational or Hamiltonian MC inference to marginalize over latent options in BIHRL for large-scale environments",
      "aliases": [
        "scalable BIHRL inference",
        "variational option-marginalisation"
      ],
      "type": "intervention",
      "description": "Research and implement advanced inference techniques (e.g., HMC, variational Bayes) to approximate the high-dimensional posterior over option sets and rewards, enabling BIHRL deployment in realistic, complex domains.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "Represents forward-looking methodological research beyond standard development stages (Section 9 Conclusion).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Proposed future work with no empirical validation yet.",
      "node_rationale": "Identified in Section 9 as essential to overcome current scalability limits."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Pathologically wrong value inference from hierarchical human planning in IRL",
      "target_node": "Assumption of flat optimal human behaviour in standard IRL",
      "description": "Ignoring hierarchy leads IRL to misinterpret observed trajectories, producing incorrect rewards.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit claim in Section 1; moderate empirical backing.",
      "edge_rationale": "Section 1 states that applying existing algorithms to hierarchical planners ‘will fail to infer correct preferences’."
    },
    {
      "type": "mitigated_by",
      "source_node": "Assumption of flat optimal human behaviour in standard IRL",
      "target_node": "Human decision-making uses hierarchical options",
      "description": "Recognising hierarchical option use directly addresses the flawed flat assumption.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Paper argues hierarchy is ‘key feature’ for accurate modelling (Section 2).",
      "edge_rationale": "Section 2 introduces hierarchy specifically to fix this problem."
    },
    {
      "type": "mitigated_by",
      "source_node": "Assumption of flat optimal human behaviour in standard IRL",
      "target_node": "Boltzmann-rational bounded planning models human action selection",
      "description": "Stochastic bounded-rational model corrects the unrealistic optimality assumption.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 3.1 links Boltzmann rationality to bounded agents.",
      "edge_rationale": "Text explains β parameter captures deviations from perfect rationality."
    },
    {
      "type": "enabled_by",
      "source_node": "Human decision-making uses hierarchical options",
      "target_node": "Model human planning with options in inverse reinforcement learning",
      "description": "Design adopts option representation because humans plan hierarchically.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct design step in Section 2.",
      "edge_rationale": "Hierarchy insight motivates option-aware modelling."
    },
    {
      "type": "enabled_by",
      "source_node": "Boltzmann-rational bounded planning models human action selection",
      "target_node": "Self-consistent hierarchical Boltzmann policy captures bounded rationality",
      "description": "Design makes Boltzmann policy self-consistent across hierarchy.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 2 emphasises self-consistency requirement.",
      "edge_rationale": "Design rationale directly implements theoretical insight."
    },
    {
      "type": "implemented_by",
      "source_node": "Model human planning with options in inverse reinforcement learning",
      "target_node": "Generative model of hierarchical planners with extended options",
      "description": "The generative model realises the option-aware design within IRL.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Formal model equations in Section 5.",
      "edge_rationale": "Implementation mechanism is concrete instantiation of design."
    },
    {
      "type": "implemented_by",
      "source_node": "Self-consistent hierarchical Boltzmann policy captures bounded rationality",
      "target_node": "BIHRL Bayesian inference algorithm with Policy-Walk MCMC sampling",
      "description": "Inference algorithm operationalises the self-consistent hierarchical Boltzmann policy.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Algorithm details follow directly after design (Sections 5–6).",
      "edge_rationale": "Policy-Walk MCMC relies on Boltzmann-consistent Q-values."
    },
    {
      "type": "required_by",
      "source_node": "Generative model of hierarchical planners with extended options",
      "target_node": "BIHRL Bayesian inference algorithm with Policy-Walk MCMC sampling",
      "description": "Inference needs the generative model to compute likelihoods over option trajectories.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Algorithm repeatedly evaluates model likelihoods (Section 5).",
      "edge_rationale": "Without generative model, MCMC cannot sample."
    },
    {
      "type": "refined_by",
      "source_node": "BIHRL Bayesian inference algorithm with Policy-Walk MCMC sampling",
      "target_node": "Option-set marginalization to jointly infer options and rewards",
      "description": "Joint option-reward inference extends and refines baseline BIHRL algorithm.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 8 positions this as an extension.",
      "edge_rationale": "Adds additional latent variables to existing sampler."
    },
    {
      "type": "validated_by",
      "source_node": "Generative model of hierarchical planners with extended options",
      "target_node": "Improved goal prediction accuracy in Taxi-driver environment",
      "description": "Taxi-driver results confirm the generative model leads to better reward inference.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Clear empirical comparison in Figure 2.",
      "edge_rationale": "Same model with/without hierarchy tested."
    },
    {
      "type": "validated_by",
      "source_node": "BIHRL Bayesian inference algorithm with Policy-Walk MCMC sampling",
      "target_node": "Reduced negative log marginal likelihood and 66% accuracy in Wikispeedia benchmark",
      "description": "Wikispeedia evaluation supports effectiveness of BIHRL inference algorithm.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 7 quantitative results.",
      "edge_rationale": "Algorithm directly applied to dataset."
    },
    {
      "type": "validated_by",
      "source_node": "Option-set marginalization to jointly infer options and rewards",
      "target_node": "Higher posterior probability on ground truth reward under option-set inference",
      "description": "Taxi-driver experiment shows option-set marginalisation boosts posterior accuracy.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 8 empirical figure.",
      "edge_rationale": "Validation uses method that marginalises option sets."
    },
    {
      "type": "motivates",
      "source_node": "Improved goal prediction accuracy in Taxi-driver environment",
      "target_node": "Incorporate hierarchical option-aware BIHRL in value learning pipelines during model fine-tuning to improve preference inference",
      "description": "Demonstrated accuracy gains motivate applying BIHRL in practical fine-tuning.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Accuracy improvement suggests utility; inference applied.",
      "edge_rationale": "Section 6 discusses broader applicability."
    },
    {
      "type": "motivates",
      "source_node": "Reduced negative log marginal likelihood and 66% accuracy in Wikispeedia benchmark",
      "target_node": "Incorporate hierarchical option-aware BIHRL in value learning pipelines during model fine-tuning to improve preference inference",
      "description": "Large-scale benchmark success encourages deployment in real pipelines.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 7 shows near state-of-art results.",
      "edge_rationale": "Empirical win supports intervention adoption."
    },
    {
      "type": "motivates",
      "source_node": "Improved goal prediction accuracy in Taxi-driver environment",
      "target_node": "Provide explicit option libraries reflecting common human hierarchical skills during reward model design",
      "description": "Taxi example uses hand-specified ‘go-to-landmark’ skills, motivating option library design.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 4 notes usefulness of such options.",
      "edge_rationale": "Providing these options enabled observed gains."
    },
    {
      "type": "motivates",
      "source_node": "Higher posterior probability on ground truth reward under option-set inference",
      "target_node": "Apply scalable variational or Hamiltonian MC inference to marginalize over latent options in BIHRL for large-scale environments",
      "description": "Successful but computationally heavy option-marginalisation motivates research into scalable inference techniques.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Future-work suggestion (Section 9) – light inference.",
      "edge_rationale": "Paper explicitly raises scalability as open problem."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2018-07-13T00:00:00Z"
    },
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1807.05037"
    },
    {
      "key": "title",
      "value": "Exploring Hierarchy-Aware Inverse Reinforcement Learning"
    },
    {
      "key": "authors",
      "value": [
        "Chris Cundy",
        "Daniel Filan"
      ]
    }
  ]
}