{
  "nodes": [
    {
      "name": "policy misgeneralization from sparse expert trajectories in apprenticeship learning",
      "aliases": [
        "generalisation failure from limited demonstrations",
        "unsafe extrapolation of imitation policies"
      ],
      "type": "concept",
      "description": "If the expert rarely visits parts of the state-space, policies cloned by supervised imitation may behave unpredictably there, jeopardising safety.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted in INTRODUCTION: direct imitation struggle in rarely visited states – core risk motivating the method."
    },
    {
      "name": "sparse state coverage causing unreliable supervised imitation policies in apprenticeship learning",
      "aliases": [
        "demonstration sparsity in state space",
        "lack of samples in avoided regions"
      ],
      "type": "concept",
      "description": "Supervised cloning relies on state-action pairs that the expert visits; regions the expert avoids have little or no data, leading to poor learnt behaviour.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explicitly mentioned in INTRODUCTION as limitation of direct approaches."
    },
    {
      "name": "combining supervised loss with reward parameter tuning to leverage model generalisation in apprenticeship learning",
      "aliases": [
        "hybrid direct-indirect optimisation insight",
        "loss on policy via reward search"
      ],
      "type": "concept",
      "description": "Treat mismatch to the expert’s policy as a supervised loss but search in reward-function space, exploiting the environmental model to generalise beyond demonstrations.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposed in APPRENTICESHIP LEARNING Section (Eq. 6)."
    },
    {
      "name": "gradient-based inverse reinforcement learning minimising expert policy deviation",
      "aliases": [
        "gradient IRL with policy loss",
        "reward tuning via gradient descent"
      ],
      "type": "concept",
      "description": "Designs an optimisation objective that directly penalises deviation from expert policy while adjusting reward parameters, embodying the hybrid approach.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section APPRENTICESHIP LEARNING defines the optimisation formulation."
    },
    {
      "name": "computation of natural gradients using induced metric from policy space in apprenticeship learning",
      "aliases": [
        "natural gradient in IRL parameter space",
        "riemannian metric induced by policy mapping"
      ],
      "type": "concept",
      "description": "Uses the Moore-Penrose inverse of the induced metric (h′(µ)ᵀh′(µ)) to obtain natural gradients that are invariant to redundant parameterisations.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in NATURAL GRADIENTS Section and Theorem 1."
    },
    {
      "name": "value iteration with Boltzmann stochastic policy enabling differentiable optimisation in apprenticeship learning",
      "aliases": [
        "Boltzmann action selection proxy",
        "softmax policy for differentiability"
      ],
      "type": "concept",
      "description": "Introduces a temperature-controlled Boltzmann policy as a smooth approximation of greedy action-selection, making policy gradients well-defined.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Equation 7 in NATURAL GRADIENTS explains its use."
    },
    {
      "name": "Q-value parameter gradients via subdifferential fixed-point equation in apprenticeship learning",
      "aliases": [
        "fixed-point derivative of optimal Q",
        "Bellman subgradient method"
      ],
      "type": "concept",
      "description": "Derives a Lipschitz continuous mapping (Eq. 9) whose fixed point yields ∂Q*/∂µ, enabling exact gradient computation almost everywhere.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "CALCULATING THE GRADIENT Section Proposition 4."
    },
    {
      "name": "grid-world experiments demonstrating robustness of natural gradient IRL",
      "aliases": [
        "gridworld evaluation of gradient IRL",
        "feature-scaling test in grids"
      ],
      "type": "concept",
      "description": "Natural-gradient IRL achieved lower policy error than max-margin IRL across sample sizes and under extreme feature-scaling perturbations.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Results summarised in COMPUTER EXPERIMENTS Figure 1, Figure 2, Table 1."
    },
    {
      "name": "sailing domain experiments confirming sample efficiency and robustness of gradient IRL",
      "aliases": [
        "sailing SSP evaluation",
        "lake navigation test"
      ],
      "type": "concept",
      "description": "In a stochastic shortest-path sailing task, gradient IRL cut policy error relative to max-margin across 10¹–10⁴ episodes.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Figure 3 in COMPUTER EXPERIMENTS."
    },
    {
      "name": "feature scaling sensitivity analysis demonstrating robustness of natural gradient IRL",
      "aliases": [
        "linear transform stress-test",
        "scaling mismatch study"
      ],
      "type": "concept",
      "description": "When feature vectors were linearly transformed, max-margin IRL performance collapsed while natural-gradient IRL remained stable.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "COMPUTER EXPERIMENTS — transformed-feature study."
    },
    {
      "name": "ill-posed reward recovery causing unsafe behavior in inverse reinforcement learning",
      "aliases": [
        "reward ambiguity in IRL",
        "non-unique reward safety risk"
      ],
      "type": "concept",
      "description": "Multiple reward functions can rationalise the expert, potentially selecting ones with unsafe optima.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in INTRODUCTION as IRL non-uniqueness issue."
    },
    {
      "name": "non-uniqueness of reward functions leading multiple optimal policies in inverse RL",
      "aliases": [
        "reward redundancy",
        "degenerate reward solutions"
      ],
      "type": "concept",
      "description": "Scaling and additive constants yield infinite reward functions that match expert behaviour but induce divergent optimal policies.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section APPRENTICESHIP LEARNING highlights ill-posedness."
    },
    {
      "name": "Boltzmann stochastic policies enforce uniqueness of solution in inverse RL optimisation",
      "aliases": [
        "soft uniqueness via temperature",
        "stochastic tie-breaking insight"
      ],
      "type": "concept",
      "description": "Strictly stochastic policies break degeneracy, ensuring a single optimal policy for any parameter setting, stabilising learning.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Remark in NATURAL GRADIENTS below Eq. 7."
    },
    {
      "name": "using stochastic policies with temperature to smooth optimisation landscape in inverse RL",
      "aliases": [
        "temperature-controlled smoothing design",
        "softmax design rationale"
      ],
      "type": "concept",
      "description": "By treating Boltzmann temperature as part of the optimisation design, gradients become smooth and parameter redundancy is reduced.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Design choice justified in NATURAL GRADIENTS."
    },
    {
      "name": "feature scaling sensitivity leading performance degradation in IRL algorithms",
      "aliases": [
        "scaling mismatch risk",
        "feature re-weighting hazard"
      ],
      "type": "concept",
      "description": "Algorithms that assume known feature scales (e.g., max-margin IRL) can select unsafe policies when scales are wrong.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "RELATED WORK section critiques scaling assumption."
    },
    {
      "name": "linear feature scaling mismatch in max-margin IRL",
      "aliases": [
        "unmatched scaling problem",
        "feature weight error"
      ],
      "type": "concept",
      "description": "If the learning algorithm and the true reward use different feature scalings, the bound on policy loss no longer holds.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in RELATED WORK example with µ=(√2/2,√2/2)."
    },
    {
      "name": "natural gradient optimisation mitigates parameter redundancy and scaling issues in IRL",
      "aliases": [
        "scale-invariant gradient insight",
        "natural gradient robustness"
      ],
      "type": "concept",
      "description": "Natural gradients use the policy-induced metric, rendering updates invariant to feature rescaling and redundant parameters.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "NATURAL GRADIENTS Section and experiment evidence."
    },
    {
      "name": "apply natural gradient inverse reinforcement learning during apprenticeship learning",
      "aliases": [
        "use natural-gradient IRL",
        "natural gradient reward learning"
      ],
      "type": "intervention",
      "description": "During fine-tuning, optimise reward parameters with natural gradients computed from the policy-space metric to learn robust policies from limited demonstrations.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Apprenticeship learning occurs post pre-training but before deployment – APPRENTICESHIP LEARNING section setup.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Evaluated in two controlled domains only – COMPUTER EXPERIMENTS.",
      "node_rationale": "Central actionable step emerging from methodology."
    },
    {
      "name": "incorporate Boltzmann stochastic policy as differentiable proxy for greedy selection in reward optimisation",
      "aliases": [
        "use softmax action selection during training",
        "temperature-controlled policy sampling"
      ],
      "type": "intervention",
      "description": "Replace hard arg-max with softmax at optimisation time, choosing temperature β to balance exploration and gradient smoothness.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Used only during IRL optimisation before deployment – NATURAL GRADIENTS Eq. 7.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated experimentally but not yet deployed in large-scale systems.",
      "node_rationale": "Directly implements uniqueness/smoothness rationale."
    },
    {
      "name": "compute optimal-value gradients via subdifferential fixed-point solver to guide reward updates",
      "aliases": [
        "solve Eq. 9 for ∂Q*/∂µ",
        "Bellman gradient solver"
      ],
      "type": "intervention",
      "description": "Iteratively solve the fixed-point equation for each reward-parameter component to obtain accurate gradients for parameter updates.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Part of the fine-tuning optimisation loop – CALCULATING THE GRADIENT.",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Theoretical derivation with proof-of-concept implementation only.",
      "node_rationale": "Needed to implement natural-gradient updates."
    },
    {
      "name": "combine supervised loss with reward parameter tuning for sample-efficient policy imitation",
      "aliases": [
        "hybrid loss optimisation in IRL",
        "policy-loss-driven reward search"
      ],
      "type": "intervention",
      "description": "Minimise a supervised squared-error loss between expert and learner policies while adjusting reward parameters, exploiting the model for generalisation.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Executed during fine-tuning of apprentice model – APPRENTICESHIP LEARNING Eq. 6.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Validated only on small domains in experiments.",
      "node_rationale": "Addresses sparse-demonstration risk directly."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "policy misgeneralization from sparse expert trajectories in apprenticeship learning",
      "target_node": "sparse state coverage causing unreliable supervised imitation policies in apprenticeship learning",
      "description": "Misgeneralisation arises because supervised learners have no data in unvisited states.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Stated explicitly in INTRODUCTION; widely accepted phenomenon.",
      "edge_rationale": "Direct causal statement in paper."
    },
    {
      "type": "caused_by",
      "source_node": "sparse state coverage causing unreliable supervised imitation policies in apprenticeship learning",
      "target_node": "combining supervised loss with reward parameter tuning to leverage model generalisation in apprenticeship learning",
      "description": "Theoretical insight is proposed to address the sparsity-induced unreliability.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoned argument in APPRENTICESHIP LEARNING; limited empirical proof.",
      "edge_rationale": "Insight offered as cure for the identified problem."
    },
    {
      "type": "mitigated_by",
      "source_node": "combining supervised loss with reward parameter tuning to leverage model generalisation in apprenticeship learning",
      "target_node": "gradient-based inverse reinforcement learning minimising expert policy deviation",
      "description": "Design operationalises the insight by formulating a gradient-based optimisation objective.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Equation 6 provides concrete formulation.",
      "edge_rationale": "Design directly realises the insight."
    },
    {
      "type": "implemented_by",
      "source_node": "gradient-based inverse reinforcement learning minimising expert policy deviation",
      "target_node": "computation of natural gradients using induced metric from policy space in apprenticeship learning",
      "description": "Natural gradients are the practical optimisation algorithm to carry out the design.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "NATURAL GRADIENTS derives algorithm for the proposed design.",
      "edge_rationale": "Algorithmic implementation."
    },
    {
      "type": "implemented_by",
      "source_node": "gradient-based inverse reinforcement learning minimising expert policy deviation",
      "target_node": "value iteration with Boltzmann stochastic policy enabling differentiable optimisation in apprenticeship learning",
      "description": "Uses Boltzmann policies to realise differentiable policy mapping in the optimisation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Presented as necessary practical step in NATURAL GRADIENTS.",
      "edge_rationale": "Part of same implementation cluster."
    },
    {
      "type": "implemented_by",
      "source_node": "gradient-based inverse reinforcement learning minimising expert policy deviation",
      "target_node": "Q-value parameter gradients via subdifferential fixed-point equation in apprenticeship learning",
      "description": "Computes exact gradients needed for the optimisation objective.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "CALCULATING THE GRADIENT proves correctness.",
      "edge_rationale": "Critical computational mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "computation of natural gradients using induced metric from policy space in apprenticeship learning",
      "target_node": "grid-world experiments demonstrating robustness of natural gradient IRL",
      "description": "Gridworld results confirm efficacy of natural gradients.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Controlled experiments with repeated trials.",
      "edge_rationale": "Empirical validation."
    },
    {
      "type": "validated_by",
      "source_node": "computation of natural gradients using induced metric from policy space in apprenticeship learning",
      "target_node": "sailing domain experiments confirming sample efficiency and robustness of gradient IRL",
      "description": "Sailing SSP results further validate natural-gradient approach.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Second domain provides converging evidence.",
      "edge_rationale": "Additional validation."
    },
    {
      "type": "validated_by",
      "source_node": "natural gradient optimisation mitigates parameter redundancy and scaling issues in IRL",
      "target_node": "feature scaling sensitivity analysis demonstrating robustness of natural gradient IRL",
      "description": "Scaling experiment shows natural-gradient algorithm maintains performance.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Quantitative comparison in Table 1.",
      "edge_rationale": "Empirical support for robustness claim."
    },
    {
      "type": "motivates",
      "source_node": "grid-world experiments demonstrating robustness of natural gradient IRL",
      "target_node": "apply natural gradient inverse reinforcement learning during apprenticeship learning",
      "description": "Positive results motivate adopting natural-gradient IRL in practice.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Evidence from small-scale experiments, not yet industrial scale.",
      "edge_rationale": "Empirical motivation."
    },
    {
      "type": "motivates",
      "source_node": "sailing domain experiments confirming sample efficiency and robustness of gradient IRL",
      "target_node": "apply natural gradient inverse reinforcement learning during apprenticeship learning",
      "description": "Cross-domain success further encourages the intervention.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Replicated in second domain only.",
      "edge_rationale": "Strengthens motivation."
    },
    {
      "type": "motivates",
      "source_node": "feature scaling sensitivity analysis demonstrating robustness of natural gradient IRL",
      "target_node": "incorporate Boltzmann stochastic policy as differentiable proxy for greedy selection in reward optimisation",
      "description": "Robustness owes partly to softmax policy; experiment encourages its use.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Indirect but plausible linkage; authors discuss in text.",
      "edge_rationale": "Feature scaling test underscores importance of stochastic policies."
    },
    {
      "type": "caused_by",
      "source_node": "ill-posed reward recovery causing unsafe behavior in inverse reinforcement learning",
      "target_node": "non-uniqueness of reward functions leading multiple optimal policies in inverse RL",
      "description": "Ill-posedness stems from reward non-uniqueness (scaling, offsets).",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit discussion in INTRODUCTION.",
      "edge_rationale": "Direct logical connection."
    },
    {
      "type": "caused_by",
      "source_node": "non-uniqueness of reward functions leading multiple optimal policies in inverse RL",
      "target_node": "Boltzmann stochastic policies enforce uniqueness of solution in inverse RL optimisation",
      "description": "Theoretical insight shows how stochasticity breaks degeneracy.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoned argument, not mathematically proved unique but claimed.",
      "edge_rationale": "Insight addresses problem."
    },
    {
      "type": "mitigated_by",
      "source_node": "Boltzmann stochastic policies enforce uniqueness of solution in inverse RL optimisation",
      "target_node": "using stochastic policies with temperature to smooth optimisation landscape in inverse RL",
      "description": "Design rationale adopts the insight by fixing a temperature parameter.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Design immediately follows insight in NATURAL GRADIENTS.",
      "edge_rationale": "Design realises theoretical insight."
    },
    {
      "type": "implemented_by",
      "source_node": "using stochastic policies with temperature to smooth optimisation landscape in inverse RL",
      "target_node": "value iteration with Boltzmann stochastic policy enabling differentiable optimisation in apprenticeship learning",
      "description": "Implementation mechanism realises temperature-controlled policy during value iteration.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Equation 7 implementation details.",
      "edge_rationale": "Mechanism concretely enacts design."
    },
    {
      "type": "caused_by",
      "source_node": "feature scaling sensitivity leading performance degradation in IRL algorithms",
      "target_node": "linear feature scaling mismatch in max-margin IRL",
      "description": "Scaling risk materialises when learning algorithm’s assumed scales differ from true scales.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "RELATED WORK example analysis.",
      "edge_rationale": "Problem analysis explains risk."
    },
    {
      "type": "caused_by",
      "source_node": "linear feature scaling mismatch in max-margin IRL",
      "target_node": "natural gradient optimisation mitigates parameter redundancy and scaling issues in IRL",
      "description": "Natural gradients’ scale-invariance directly counters the mismatch problem.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Theory and experiments jointly support claim.",
      "edge_rationale": "Insight provides mitigation."
    },
    {
      "type": "mitigated_by",
      "source_node": "natural gradient optimisation mitigates parameter redundancy and scaling issues in IRL",
      "target_node": "apply natural gradient inverse reinforcement learning during apprenticeship learning",
      "description": "Adopting natural-gradient IRL in practice removes scaling sensitivity.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical step; empirical but limited domain scope.",
      "edge_rationale": "Intervention embodies insight."
    },
    {
      "type": "motivates",
      "source_node": "grid-world experiments demonstrating robustness of natural gradient IRL",
      "target_node": "combine supervised loss with reward parameter tuning for sample-efficient policy imitation",
      "description": "Performance gains motivate using the combined loss strategy.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Empirical evidence supports design’s effectiveness.",
      "edge_rationale": "Reinforces intervention adoption."
    },
    {
      "type": "motivates",
      "source_node": "sailing domain experiments confirming sample efficiency and robustness of gradient IRL",
      "target_node": "compute optimal-value gradients via subdifferential fixed-point solver to guide reward updates",
      "description": "Successful optimisation relies on accurate Q* gradients; experiment underscores importance.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implicit in discussion; not separately ablated.",
      "edge_rationale": "Evidence suggests mechanism’s value."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2012-06-20T00:00:00Z"
    },
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "title",
      "value": "Apprenticeship Learning using Inverse Reinforcement Learning and Gradient Methods"
    },
    {
      "key": "authors",
      "value": [
        "Gergely Neu",
        "Csaba Szepesvari"
      ]
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1206.5264"
    }
  ]
}