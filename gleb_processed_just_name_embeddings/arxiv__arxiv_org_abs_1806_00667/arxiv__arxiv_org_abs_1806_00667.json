{
  "nodes": [
    {
      "name": "misclassification from adversarial examples in discriminative ML models",
      "aliases": [
        "adversarial example misclassification",
        "incorrect predictions due to adversarial inputs"
      ],
      "type": "concept",
      "description": "The primary risk that inputs crafted by an adversary can cause a classifier to output wrong labels with high confidence, leading to failures in deployed ML systems.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in Introduction as the motivating concern of the study."
    },
    {
      "name": "existence of adversarial examples in low or high input density regions",
      "aliases": [
        "adversarial examples across density regions",
        "density-based adversarial presence"
      ],
      "type": "concept",
      "description": "Observation from prior work that adversarial inputs can occur both in low-density (off-manifold) and, under some conditions, in high-density (on-manifold) areas of the input space.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Introduction & Related Literature sections as a core puzzle the paper resolves."
    },
    {
      "name": "inadequate uncertainty estimation in deep neural networks",
      "aliases": [
        "lack of epistemic uncertainty estimation",
        "poor uncertainty quantification in DNNs"
      ],
      "type": "concept",
      "description": "Modern deterministic neural networks do not express epistemic uncertainty, enabling confidently wrong predictions on inputs far from the training distribution.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted in Background and Preliminaries as a key mechanism enabling adversarial examples."
    },
    {
      "name": "idealised models with invariance capture and fast-growing epistemic uncertainty cannot have adversarial examples",
      "aliases": [
        "idealised robust model condition",
        "idealised BNN robustness theory"
      ],
      "type": "concept",
      "description": "The theorem proving that if a model embeds data invariances and raises epistemic uncertainty beyond a threshold outside neighbourhoods of training points, no adversarial example can exist.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central theorem in Sections 4 & 5."
    },
    {
      "name": "bayesian neural networks leveraging epistemic uncertainty to detect out-of-distribution inputs",
      "aliases": [
        "BNNs for OOD detection",
        "epistemic-aware Bayesian classifiers"
      ],
      "type": "concept",
      "description": "Design principle that Bayesian neural networks, by averaging over parameter distributions, provide predictive distributions whose uncertainty rises away from seen data, supporting detection of anomalous or adversarial inputs.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivated throughout Sections 3–5 as the practical path to realise the theoretical conditions."
    },
    {
      "name": "incorporating data invariances into model architecture to ensure coverage",
      "aliases": [
        "architecture-level invariance incorporation",
        "model-invariant design for coverage"
      ],
      "type": "concept",
      "description": "Design decision to hard-code known transformations (e.g. rotations, translations) into the architecture so that few samples suffice to generalise over the entire equivalence class, avoiding trivial rejection of valid inputs.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in Preliminaries with the spheres dataset example."
    },
    {
      "name": "using mutual information between parameters and outputs to quantify epistemic uncertainty",
      "aliases": [
        "mutual information uncertainty metric",
        "MI-based epistemic measure"
      ],
      "type": "concept",
      "description": "Rationale to employ the mutual information I(ω,y|x,D) as a principled metric that distinguishes epistemic from aleatoric uncertainty, enabling robust detection thresholds.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Defined in Background (eq. 1) and used throughout empirical sections."
    },
    {
      "name": "hamiltonian monte carlo inference for near-ideal Bayesian neural networks",
      "aliases": [
        "HMC inference for BNNs",
        "gold-standard BNN sampling"
      ],
      "type": "concept",
      "description": "Implementation technique that draws near-exact posterior samples of BNN weights via HMC, yielding almost perfect epistemic uncertainty estimates.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Used in Section 6.1 to approximate the idealised model."
    },
    {
      "name": "monte carlo dropout approximate inference for scalable uncertainty estimation",
      "aliases": [
        "MC dropout uncertainty",
        "dropout variational inference"
      ],
      "type": "concept",
      "description": "Practical variational approximation where dropout is kept at test time, enabling cheap posterior sampling and uncertainty estimates at scale.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Evaluated in Sections 6.2–6.4 as a scalable but imperfect implementation."
    },
    {
      "name": "invariant architecture components capturing data transformations",
      "aliases": [
        "rotation equivariant layers",
        "architectural invariance mechanisms"
      ],
      "type": "concept",
      "description": "Concrete architectural choices (e.g. equivariant convolutions) that enforce the known symmetry transformations of the data distribution inside the model.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed conceptually in Sections 4–5 using the concentric spheres example."
    },
    {
      "name": "mutual information thresholding for adversarial example detection",
      "aliases": [
        "MI threshold rejection",
        "uncertainty threshold defence"
      ],
      "type": "concept",
      "description": "Operational mechanism that rejects or flags inputs whose estimated mutual information exceeds (or falls below) a predefined safety threshold.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Used in experiments (Sections 6.1–6.4) to declare images adversarial."
    },
    {
      "name": "dropout ensemble technique to smooth uncertainty estimates",
      "aliases": [
        "ensemble of dropout models",
        "dropout model aggregation"
      ],
      "type": "concept",
      "description": "Combining predictions and uncertainty estimates of several independently initialised dropout networks to reduce 'uncertainty holes' inherent to single dropout approximations.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposed and evaluated in Sections 6.2–6.4 as a mitigation for dropout weaknesses."
    },
    {
      "name": "HMC robustness experiments on Manifold MNIST showing low adversarial success rate",
      "aliases": [
        "Manifold MNIST HMC robustness study",
        "HMC adversarial resistance evidence"
      ],
      "type": "concept",
      "description": "Empirical finding that non-targeted attacks achieve <0.32 success at ε=0.2 on HMC-BNNs, comparable to random noise, demonstrating strong robustness.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Table 1 in Section 6.1."
    },
    {
      "name": "correlation of mutual information with data density on synthetic dataset",
      "aliases": [
        "MI-density correlation evidence",
        "uncertainty-density empirical relationship"
      ],
      "type": "concept",
      "description": "Experimental result (Fig. 6) that MI rises as log-density decreases on Manifold-MNIST grid points, validating MI as a proxy for detecting low-density (potentially adversarial) regions.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 6.1."
    },
    {
      "name": "dropout uncertainty holes exploited by garbage image attack",
      "aliases": [
        "garbage image attack evidence",
        "uncertainty hole exploitation proof"
      ],
      "type": "concept",
      "description": "Observation that adversarial 'garbage' inputs can be found in regions where single-dropout MI is spuriously low, leading to high-confidence misclassification.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 6.2 and Fig. 9."
    },
    {
      "name": "dropout ensemble improves adversarial detection AUROC on cats vs dogs",
      "aliases": [
        "ensemble AUROC improvement evidence",
        "dropout ensemble robustness result"
      ],
      "type": "concept",
      "description": "Empirical result (Table 2) that a 5-member dropout ensemble raises adversarial-detection AUC from 0.63 to 0.77 on a real-world VGG13 cats-vs-dogs task.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 6.4."
    },
    {
      "name": "train Bayesian neural networks with HMC inference to maximize epistemic uncertainty and reject adversarial inputs",
      "aliases": [
        "HMC-trained BNN robustness",
        "HMC-based BNN defence"
      ],
      "type": "intervention",
      "description": "During model development, perform Hamiltonian Monte-Carlo posterior sampling for the chosen Bayesian neural network architecture so that epistemic uncertainty rises sharply away from the data manifold; reject predictions with MI below/above a threshold.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Defined at model design stage, referencing Section 6.1 'Idealised case'.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated experimentally on MNIST-scale tasks but not yet scalable to large-scale production (Section 6.1).",
      "node_rationale": "Directly follows from validation evidence of HMC robustness."
    },
    {
      "name": "deploy MC dropout inference with mutual information thresholding to flag and reject adversarial inputs at runtime",
      "aliases": [
        "MC dropout MI defence",
        "runtime uncertainty rejection"
      ],
      "type": "intervention",
      "description": "At inference time, perform multiple stochastic forward passes with dropout, compute MI, and block or further inspect samples whose MI exceeds a calibrated threshold.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Applied during deployment for real-time input filtering (Sections 6.2–6.4).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Evaluated on MNIST and cats-vs-dogs datasets with measurable detection AUC, but not yet widespread production use.",
      "node_rationale": "Supported by MI-density correlation evidence."
    },
    {
      "name": "ensemble multiple MC dropout models during deployment to mitigate uncertainty holes and improve adversarial robustness",
      "aliases": [
        "dropout ensemble defence",
        "aggregated dropout uncertainty"
      ],
      "type": "intervention",
      "description": "Maintain several independently initialised MC-dropout networks and average their predictive distributions and MI estimates to reduce variance and patch low-uncertainty regions exploited by attackers.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Implemented as a deployment-time defence (Section 6.4).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Shown experimentally to improve AUROC but still limited-scale studies.",
      "node_rationale": "Motivated by dropout-hole evidence and validated by ensemble experiments."
    },
    {
      "name": "design model architectures with built-in data invariances to reduce adversarial vulnerability",
      "aliases": [
        "invariance-centric architecture design",
        "equivariant model design"
      ],
      "type": "intervention",
      "description": "During model design, embed known symmetries (e.g. rotation, translation) via equivariant layers or data-augmentation-induced weight tying so that the model generalises over entire transformation orbits using few samples.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Architectural decision taken before any training (Section 4 Preliminaries).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Primarily theoretical and demonstrated on synthetic spheres example; limited empirical validation.",
      "node_rationale": "Implements the theoretical requirement for high coverage through invariance."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "misclassification from adversarial examples in discriminative ML models",
      "target_node": "inadequate uncertainty estimation in deep neural networks",
      "description": "Lack of calibrated epistemic uncertainty allows the model to output high-confidence wrong labels.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Multiple references in Introduction and Related Work link uncertainty failure to adversarial vulnerability.",
      "edge_rationale": "Sections 1 & 3 discuss deterministic nets' overconfidence."
    },
    {
      "type": "caused_by",
      "source_node": "misclassification from adversarial examples in discriminative ML models",
      "target_node": "existence of adversarial examples in low or high input density regions",
      "description": "Adversarial instances in either density regime can trigger wrong high-confidence predictions.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Cited works (Gilmer 2018, Li 2018) demonstrate both densities.",
      "edge_rationale": "Introduced in Introduction."
    },
    {
      "type": "mitigated_by",
      "source_node": "inadequate uncertainty estimation in deep neural networks",
      "target_node": "idealised models with invariance capture and fast-growing epistemic uncertainty cannot have adversarial examples",
      "description": "The theoretical result shows that proper uncertainty growth eliminates attacks based on overconfidence.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical derivation, not empirical proof.",
      "edge_rationale": "Sections 4-5 theorem."
    },
    {
      "type": "mitigated_by",
      "source_node": "existence of adversarial examples in low or high input density regions",
      "target_node": "idealised models with invariance capture and fast-growing epistemic uncertainty cannot have adversarial examples",
      "description": "Shows that if the model’s uncertainty behaves ideally, adversarial examples cannot persist in any density region.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Based on theoretical proof.",
      "edge_rationale": "Sections 5.2-5.3."
    },
    {
      "type": "mitigated_by",
      "source_node": "idealised models with invariance capture and fast-growing epistemic uncertainty cannot have adversarial examples",
      "target_node": "bayesian neural networks leveraging epistemic uncertainty to detect out-of-distribution inputs",
      "description": "BNNs are proposed as a concrete way to fulfil the idealised model assumptions.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoned connection with supporting citations.",
      "edge_rationale": "Sections 3-5."
    },
    {
      "type": "mitigated_by",
      "source_node": "idealised models with invariance capture and fast-growing epistemic uncertainty cannot have adversarial examples",
      "target_node": "incorporating data invariances into model architecture to ensure coverage",
      "description": "Embedding invariances ensures the high-coverage condition required by the theorem.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explained with spheres dataset thought experiment.",
      "edge_rationale": "Section 4."
    },
    {
      "type": "refined_by",
      "source_node": "bayesian neural networks leveraging epistemic uncertainty to detect out-of-distribution inputs",
      "target_node": "using mutual information between parameters and outputs to quantify epistemic uncertainty",
      "description": "MI is chosen as the specific metric to realise the BNN-based uncertainty detection.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit equations in Section 3 support this refinement.",
      "edge_rationale": "Equation 1 in Background."
    },
    {
      "type": "implemented_by",
      "source_node": "bayesian neural networks leveraging epistemic uncertainty to detect out-of-distribution inputs",
      "target_node": "hamiltonian monte carlo inference for near-ideal Bayesian neural networks",
      "description": "HMC provides near-exact posterior sampling for BNNs.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Widely accepted gold-standard; paper uses it.",
      "edge_rationale": "Section 6.1."
    },
    {
      "type": "implemented_by",
      "source_node": "bayesian neural networks leveraging epistemic uncertainty to detect out-of-distribution inputs",
      "target_node": "monte carlo dropout approximate inference for scalable uncertainty estimation",
      "description": "MC-dropout offers a practical scalable implementation of Bayesian inference.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Established in Gal 2016 and used here.",
      "edge_rationale": "Sections 3 & 6.2."
    },
    {
      "type": "implemented_by",
      "source_node": "incorporating data invariances into model architecture to ensure coverage",
      "target_node": "invariant architecture components capturing data transformations",
      "description": "Equivariant layers and similar mechanisms realise invariance in practice.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Conceptual link with example; limited experiments.",
      "edge_rationale": "Section 4."
    },
    {
      "type": "implemented_by",
      "source_node": "using mutual information between parameters and outputs to quantify epistemic uncertainty",
      "target_node": "mutual information thresholding for adversarial example detection",
      "description": "Operationalises the MI metric by applying a threshold for rejection.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Used throughout experiments.",
      "edge_rationale": "Sections 6.1–6.4."
    },
    {
      "type": "refined_by",
      "source_node": "monte carlo dropout approximate inference for scalable uncertainty estimation",
      "target_node": "dropout ensemble technique to smooth uncertainty estimates",
      "description": "Aggregating several dropout models reduces variance and addresses uncertainty holes.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Proposed and justified in Section 6.2.",
      "edge_rationale": "Section 6.2."
    },
    {
      "type": "validated_by",
      "source_node": "hamiltonian monte carlo inference for near-ideal Bayesian neural networks",
      "target_node": "HMC robustness experiments on Manifold MNIST showing low adversarial success rate",
      "description": "Empirical study confirms HMC-BNN robustness predicted by theory.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Controlled experiment with quantitative metrics.",
      "edge_rationale": "Table 1, Section 6.1."
    },
    {
      "type": "validated_by",
      "source_node": "monte carlo dropout approximate inference for scalable uncertainty estimation",
      "target_node": "correlation of mutual information with data density on synthetic dataset",
      "description": "Shows that even approximate dropout inference produces MI correlated to density.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Systematic evaluation on grid points.",
      "edge_rationale": "Fig. 6."
    },
    {
      "type": "validated_by",
      "source_node": "mutual information thresholding for adversarial example detection",
      "target_node": "correlation of mutual information with data density on synthetic dataset",
      "description": "Correlation underpins the effectiveness of MI-based thresholds.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Same empirical result supports mechanism.",
      "edge_rationale": "Section 6.1."
    },
    {
      "type": "validated_by",
      "source_node": "dropout ensemble technique to smooth uncertainty estimates",
      "target_node": "dropout ensemble improves adversarial detection AUROC on cats vs dogs",
      "description": "Shows concrete detection performance gains from ensembling.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Real-world dataset experiment.",
      "edge_rationale": "Table 2, Section 6.4."
    },
    {
      "type": "validated_by",
      "source_node": "monte carlo dropout approximate inference for scalable uncertainty estimation",
      "target_node": "dropout uncertainty holes exploited by garbage image attack",
      "description": "Evidence that single-dropout approximation leaves vulnerability regions.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Demonstrative attack on selected inputs.",
      "edge_rationale": "Fig. 9, Section 6.2."
    },
    {
      "type": "motivates",
      "source_node": "HMC robustness experiments on Manifold MNIST showing low adversarial success rate",
      "target_node": "train Bayesian neural networks with HMC inference to maximize epistemic uncertainty and reject adversarial inputs",
      "description": "Successful robustness motivates adopting HMC-trained BNNs as a defence.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct empirical support.",
      "edge_rationale": "Section 6.1."
    },
    {
      "type": "motivates",
      "source_node": "correlation of mutual information with data density on synthetic dataset",
      "target_node": "deploy MC dropout inference with mutual information thresholding to flag and reject adversarial inputs at runtime",
      "description": "Strong MI-density link justifies threshold-based rejection in practice.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Correlation provides actionable signal.",
      "edge_rationale": "Section 6.1."
    },
    {
      "type": "motivates",
      "source_node": "dropout ensemble improves adversarial detection AUROC on cats vs dogs",
      "target_node": "ensemble multiple MC dropout models during deployment to mitigate uncertainty holes and improve adversarial robustness",
      "description": "Observed AUROC gains support deploying ensembles.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Quantitative improvement on real dataset.",
      "edge_rationale": "Section 6.4."
    },
    {
      "type": "motivates",
      "source_node": "dropout uncertainty holes exploited by garbage image attack",
      "target_node": "ensemble multiple MC dropout models during deployment to mitigate uncertainty holes and improve adversarial robustness",
      "description": "Attack success exposes need for ensemble-based mitigation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Shows specific weakness ensembles address.",
      "edge_rationale": "Section 6.2."
    },
    {
      "type": "motivates",
      "source_node": "incorporating data invariances into model architecture to ensure coverage",
      "target_node": "design model architectures with built-in data invariances to reduce adversarial vulnerability",
      "description": "Design principle naturally leads to intervention of embedding invariances.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Mainly theoretical reasoning.",
      "edge_rationale": "Section 4."
    },
    {
      "type": "validated_by",
      "source_node": "invariant architecture components capturing data transformations",
      "target_node": "HMC robustness experiments on Manifold MNIST showing low adversarial success rate",
      "description": "Implicit rotational invariance in the synthetic experiment supports the mechanism’s effectiveness.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Indirect evidence; invariance not isolated experimentally.",
      "edge_rationale": "Discussion around spheres example and Section 6.1."
    }
  ],
  "meta": [
    {
      "key": "title",
      "value": "Sufficient Conditions for Idealised Models to Have No Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks"
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1806.00667"
    },
    {
      "key": "date_published",
      "value": "2018-06-02T00:00:00Z"
    },
    {
      "key": "authors",
      "value": [
        "Yarin Gal",
        "Lewis Smith"
      ]
    },
    {
      "key": "source",
      "value": "arxiv"
    }
  ]
}