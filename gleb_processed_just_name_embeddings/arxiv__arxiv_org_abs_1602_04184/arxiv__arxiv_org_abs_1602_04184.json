{
  "nodes": [
    {
      "name": "persistent mutual defection in source-transparent one-shot prisoner's dilemma",
      "aliases": [
        "mutual defection among code-reading agents",
        "non-cooperation in transparent PD"
      ],
      "type": "concept",
      "description": "When two AI agents that can read each other’s source code play a single-shot Prisoner’s Dilemma, classical game theory predicts both will defect, leading to a (D,D) outcome that wastes utility for both sides.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in Section 1.1 as the baseline undesirable equilibrium the paper seeks to overcome."
    },
    {
      "name": "cooperative agent exploitation in source-transparent prisoner’s dilemma",
      "aliases": [
        "unilateral defection against cooperators",
        "C,D exploitation risk"
      ],
      "type": "concept",
      "description": "Agents that cooperate based on brittle criteria (e.g., exact code equality) can be exploited by opponents that defect while the cooperator continues to cooperate, giving payoffs (C,D).",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Sections 1.1 and 1.3 via IsMeBot examples showing vulnerability to tiny code changes."
    },
    {
      "name": "decision theory inadequacy for source-transparent algorithmic agents",
      "aliases": [
        "causal decision theory failure",
        "classical decision theory mismatch"
      ],
      "type": "concept",
      "description": "Causal decision theory and classical Nash reasoning ignore logical dependencies created by agents inspecting each other’s code, leading to incorrect predictions of optimal actions.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in Sections 1.6 and 6.1 as a high-level risk for future multi-agent AI systems."
    },
    {
      "name": "nash equilibrium prediction of defection in prisoner’s dilemma",
      "aliases": [
        "classical PD equilibrium defection",
        "D,D Nash outcome"
      ],
      "type": "concept",
      "description": "Standard game-theoretic analysis declares mutual defection the only Nash and correlated equilibrium in a fully specified single-shot Prisoner’s Dilemma.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 1.1 frames this as the mechanism producing the primary risk."
    },
    {
      "name": "fragility of program-equality-based cooperation in algorithmic game play",
      "aliases": [
        "IsMeBot fragility",
        "equality check vulnerability"
      ],
      "type": "concept",
      "description": "Earlier program equilibria (e.g., IsMeBot) rely on exact code equality; tiny irrelevant code changes break cooperation, leading to defection or exploitation.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Sections 1.1 and 1.5 as the mechanism behind exploitation risk."
    },
    {
      "name": "causal decision theory ignoring logical dependence between source codes",
      "aliases": [
        "CDT ignores logical correlations",
        "logical dependence blind spot"
      ],
      "type": "concept",
      "description": "CDT treats an agent’s action as causally independent of the opponent’s simultaneous action, even when both actions are logically linked by mutual source-code reasoning.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 6.1 critiques CDT under source transparency, identifying the mechanism behind decision-theory risk."
    },
    {
      "name": "logical dependence via source code transparency enables alternative equilibria",
      "aliases": [
        "source-code logical coupling",
        "algorithmic mutual prediction"
      ],
      "type": "concept",
      "description": "When agents can see each other’s code, their actions can be logically (not causally) correlated: each decision can depend on a proof of the other’s decision, allowing new cooperative equilibria.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 1.2 introduces this insight while analysing FairBot k vs FairBot k."
    },
    {
      "name": "bounded Löb theorem allows proofs of mutual cooperation under bounded resources",
      "aliases": [
        "parametric bounded Löb property",
        "resource-bounded Löb reasoning"
      ],
      "type": "concept",
      "description": "The new parametric, bounded version of Löb’s theorem (Section 5) shows that for sufficiently large proof-length parameters k, agents can prove statements about each other’s cooperation within bounded resources.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Core theoretical contribution enabling robust cooperation under realistic computation limits."
    },
    {
      "name": "fairness principle based on proof search enables robust cooperation",
      "aliases": [
        "G-fairness principle",
        "proof-triggered cooperation rule"
      ],
      "type": "concept",
      "description": "An agent is G-fair if it cooperates whenever it can find, within a proof-length bound, a proof that its opponent cooperates; this principle underpins robust mutual cooperation without exploitation.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Defined formally in Section 1.3 and generalised in Section 6."
    },
    {
      "name": "proof-based cooperation strategy requiring internal proof of opponent cooperation",
      "aliases": [
        "cooperate-if-provable strategy",
        "internal proof trigger"
      ],
      "type": "concept",
      "description": "Design strategy: an agent decides to cooperate only if its internal theorem prover finds a proof (within a length bound) that the opponent cooperates with it.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Articulated in Section 1.3 as the operational logic behind FairBot."
    },
    {
      "name": "G-fair bounded agents avoiding exploitation while promoting cooperation",
      "aliases": [
        "robust cooperative agent class",
        "bounded proof-fair agents"
      ],
      "type": "concept",
      "description": "Agents that follow the G-fair rule are unexploitable (they never get C,D) yet achieve C,C with many opponents who are similarly fair, independent of code equality.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Summarised in Section 1.3 and proved in Theorem 4 (Section 6)."
    },
    {
      "name": "logical decision-theoretic agent design replacing causal-only reasoning",
      "aliases": [
        "logical dependence aware decision theory",
        "updating CDT with Löbian reasoning"
      ],
      "type": "concept",
      "description": "Agents should base decisions on logical correlations (provability links) in addition to causal consequences, correcting the shortcomings of classical CDT in transparent settings.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Argued in Section 6.1 as a necessary redesign of AI decision procedures."
    },
    {
      "name": "FairBot_k algorithm with bounded proof search for cooperation proofs",
      "aliases": [
        "FairBot k implementation",
        "bounded FairBot"
      ],
      "type": "concept",
      "description": "Algorithm: search up to k+G(len(opponent)) symbols for a proof that the opponent cooperates with FairBot_k; if found, output C else D.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Code sketch given in Section 1.1; analysed throughout Sections 1.2–6."
    },
    {
      "name": "G-fair agent architecture parameterized by proof length and code size",
      "aliases": [
        "proof-length bounded cooperative architecture",
        "G-fair framework"
      ],
      "type": "concept",
      "description": "General implementation template: parameter k and function G determine how large a proof the agent searches for, enabling many variants beyond FairBot_k.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Formally defined in Section 1.3 and used to state Theorem 4."
    },
    {
      "name": "parametric bounded Löb theorem used for agent soundness guarantees",
      "aliases": [
        "bounded Löb as implementation component",
        "resource-bounded Löb module"
      ],
      "type": "concept",
      "description": "Agents employ the parametric bounded Löb theorem internally to reason about proofs of their own and others’ behaviour, ensuring the logical validity of cooperation decisions.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Theorem 3 in Section 5 is explicitly applied in Section 6 to justify agent behaviour."
    },
    {
      "name": "theoretical proof of FairBot_k vs FairBot_k cooperation",
      "aliases": [
        "FairBot mutual cooperation proof",
        "k-bounded mutual C proof"
      ],
      "type": "concept",
      "description": "Section 1.2 and Section 6 prove that for sufficiently large k, two identical FairBot_k agents both output C, contradicting naïve expectations of defection.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Serves as first concrete validation of the design rationale."
    },
    {
      "name": "robust cooperation theorem for any G-fair agents",
      "aliases": [
        "Theorem 4 robust cooperation",
        "general G-fair cooperation proof"
      ],
      "type": "concept",
      "description": "Section 6 proves that any two sufficiently parameterised G-fair agents mutually cooperate and are unexploitable, demonstrating generality beyond FairBot_k.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Provides broad validation supporting proposed interventions."
    },
    {
      "name": "logical analysis showing defection impossible in FairBot interaction",
      "aliases": [
        "defection impossibility proof",
        "C,D unattainability"
      ],
      "type": "concept",
      "description": "Section 6.1 argues that outcomes like (D,C) are logically inconsistent when agents are FairBots, negating CDT’s suggestion to defect for higher payoff.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Validates design rationale against decision-theoretic criticism."
    },
    {
      "name": "Design AI agents with bounded proof-based cooperation mechanisms",
      "aliases": [
        "implement proof-based cooperation",
        "build FairBot-like agents"
      ],
      "type": "intervention",
      "description": "During model design, embed bounded proof searches so the agent cooperates only when it can prove the opponent will cooperate, realising FairBot_k-style behaviour.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Applies at algorithm/architecture selection stage described in Sections 1.3 and 6.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Currently supported by theoretical proofs and toy code sketches but lacks empirical deployment (Sections 6–7).",
      "node_rationale": "Direct actionable prescription derived from the paper’s main results."
    },
    {
      "name": "Integrate G-fair fairness principle into AI negotiation protocols",
      "aliases": [
        "apply G-fairness in multi-agent systems",
        "fairness rule in negotiations"
      ],
      "type": "intervention",
      "description": "In multi-agent protocol design, require agents to be G-fair – i.e., to cooperate when proofs of counterpart cooperation exist within set length bounds – to guarantee robustness against exploitation.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Forms part of high-level protocol/architecture decisions (Section 6).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Backed by theoretical theorems; practical protocol engineering yet to be done.",
      "node_rationale": "Leverages validation evidence from Theorem 4 to create system-level safeguards."
    },
    {
      "name": "Adopt logical-dependence-aware decision theory in AI agent design",
      "aliases": [
        "logical CDT replacement",
        "upgraded decision theory"
      ],
      "type": "intervention",
      "description": "Replace purely causal decision rules with ones that account for logical correlations (e.g., bounded Löb reasoning) so that agents act optimally in source-transparent environments.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Impacts the decision-making module chosen at model design (Section 6.1).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Currently conceptual and supported by formal argumentation; requires experimental confirmation.",
      "node_rationale": "Addresses the identified inadequacy of existing decision theories."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "persistent mutual defection in source-transparent one-shot prisoner's dilemma",
      "target_node": "nash equilibrium prediction of defection in prisoner’s dilemma",
      "description": "The classical Nash analysis directly leads to the expectation of mutual defection among rational agents.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Well-established game-theoretic result reiterated in Section 1.1.",
      "edge_rationale": "Shows how the risk arises from accepted theoretical analysis."
    },
    {
      "type": "mitigated_by",
      "source_node": "nash equilibrium prediction of defection in prisoner’s dilemma",
      "target_node": "logical dependence via source code transparency enables alternative equilibria",
      "description": "Introducing logical dependence breaks the Nash ­defection prediction by allowing correlated decisions.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Paper argues this qualitatively in Sections 1.2–1.3.",
      "edge_rationale": "Connects problem analysis to new theoretical insight."
    },
    {
      "type": "enabled_by",
      "source_node": "logical dependence via source code transparency enables alternative equilibria",
      "target_node": "proof-based cooperation strategy requiring internal proof of opponent cooperation",
      "description": "Logical dependence motivates designing strategies that rely on internal proofs of opponent cooperation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicitly stated in Section 1.3 when introducing FairBot.",
      "edge_rationale": "Transitions from theory to design rationale."
    },
    {
      "type": "implemented_by",
      "source_node": "proof-based cooperation strategy requiring internal proof of opponent cooperation",
      "target_node": "FairBot_k algorithm with bounded proof search for cooperation proofs",
      "description": "FairBot_k realises the proof-based strategy by searching for proofs within a resource bound.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Detailed algorithm listing in Section 1.1.",
      "edge_rationale": "Maps design rationale to concrete implementation."
    },
    {
      "type": "validated_by",
      "source_node": "FairBot_k algorithm with bounded proof search for cooperation proofs",
      "target_node": "theoretical proof of FairBot_k vs FairBot_k cooperation",
      "description": "Formal analysis proves that two FairBot_k instances mutually cooperate for large k.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Formal proof provided in Sections 1.2 and 6.",
      "edge_rationale": "Supplies evidence for implementation effectiveness."
    },
    {
      "type": "motivates",
      "source_node": "theoretical proof of FairBot_k vs FairBot_k cooperation",
      "target_node": "Design AI agents with bounded proof-based cooperation mechanisms",
      "description": "The cooperation proof motivates adopting proof-based mechanisms in AI agent design.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical extrapolation explicitly advocated in Section 7.",
      "edge_rationale": "Connects evidence to actionable intervention."
    },
    {
      "type": "implemented_by",
      "source_node": "proof-based cooperation strategy requiring internal proof of opponent cooperation",
      "target_node": "G-fair agent architecture parameterized by proof length and code size",
      "description": "The G-fair architecture generalises the proof-based strategy beyond a single algorithm.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Design generalised in Section 1.3.",
      "edge_rationale": "Shows broader implementation path."
    },
    {
      "type": "validated_by",
      "source_node": "G-fair agent architecture parameterized by proof length and code size",
      "target_node": "robust cooperation theorem for any G-fair agents",
      "description": "Theorem 4 proves mutual cooperation and unexploitable behaviour for any G-fair agents.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Strong formal theorem in Section 6.",
      "edge_rationale": "Evidence for general architecture."
    },
    {
      "type": "motivates",
      "source_node": "robust cooperation theorem for any G-fair agents",
      "target_node": "Integrate G-fair fairness principle into AI negotiation protocols",
      "description": "General theorem encourages embedding G-fairness into negotiation protocols.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Suggested as future application in Sections 6–7.",
      "edge_rationale": "Derives intervention from validation evidence."
    },
    {
      "type": "caused_by",
      "source_node": "cooperative agent exploitation in source-transparent prisoner’s dilemma",
      "target_node": "fragility of program-equality-based cooperation in algorithmic game play",
      "description": "Exploitation arises because equality-based agents defect when tiny code differences exist.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit examples in Section 1.1.",
      "edge_rationale": "Explains exploitation mechanism."
    },
    {
      "type": "mitigated_by",
      "source_node": "fragility of program-equality-based cooperation in algorithmic game play",
      "target_node": "fairness principle based on proof search enables robust cooperation",
      "description": "Proof-search fairness circumvents equality checks, removing fragility.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 1.3 positions G-fairness as a solution.",
      "edge_rationale": "Transitions to new theoretical insight."
    },
    {
      "type": "enables",
      "source_node": "fairness principle based on proof search enables robust cooperation",
      "target_node": "G-fair bounded agents avoiding exploitation while promoting cooperation",
      "description": "Applying the fairness principle creates agents that are both cooperative and unexploitable.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Demonstrated formally in Section 6.",
      "edge_rationale": "Moves from theory to design rationale."
    },
    {
      "type": "implemented_by",
      "source_node": "G-fair bounded agents avoiding exploitation while promoting cooperation",
      "target_node": "G-fair agent architecture parameterized by proof length and code size",
      "description": "Architecture realises the G-fair design in practical agent code.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 1.3 gives construction details.",
      "edge_rationale": "Design to implementation link."
    },
    {
      "type": "caused_by",
      "source_node": "decision theory inadequacy for source-transparent algorithmic agents",
      "target_node": "causal decision theory ignoring logical dependence between source codes",
      "description": "Inadequacy stems from CDT’s omission of logical correlations created by code transparency.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Analysis in Section 6.1.",
      "edge_rationale": "Clarifies mechanism of decision-theory failure."
    },
    {
      "type": "mitigated_by",
      "source_node": "causal decision theory ignoring logical dependence between source codes",
      "target_node": "logical decision-theoretic agent design replacing causal-only reasoning",
      "description": "Adopting logical dependence-aware decision rules addresses CDT’s blind spot.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 6.1 proposes this remedy.",
      "edge_rationale": "Problem analysis to new design rationale."
    },
    {
      "type": "implemented_by",
      "source_node": "logical decision-theoretic agent design replacing causal-only reasoning",
      "target_node": "parametric bounded Löb theorem used for agent soundness guarantees",
      "description": "Bounded Löb reasoning is the specific mechanism enabling logical-dependence-aware decisions.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Implementation details in Sections 5–6.",
      "edge_rationale": "Design rationale to mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "parametric bounded Löb theorem used for agent soundness guarantees",
      "target_node": "logical analysis showing defection impossible in FairBot interaction",
      "description": "Logical analysis confirms the mechanism prevents unilateral profitable defection.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Formal reasoning in Section 6.1.",
      "edge_rationale": "Evidence for mechanism efficacy."
    },
    {
      "type": "motivates",
      "source_node": "logical analysis showing defection impossible in FairBot interaction",
      "target_node": "Adopt logical-dependence-aware decision theory in AI agent design",
      "description": "Proof demonstrates benefits, motivating adoption of upgraded decision theories.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 6.1 extrapolates practical implications.",
      "edge_rationale": "From validation to intervention."
    },
    {
      "type": "refined_by",
      "source_node": "logical dependence via source code transparency enables alternative equilibria",
      "target_node": "bounded Löb theorem allows proofs of mutual cooperation under bounded resources",
      "description": "Bounded Löb theorem formalises and strengthens the general logical-dependence insight under resource constraints.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 5 derives the bounded theorem from the general insight.",
      "edge_rationale": "Shows theoretical refinement."
    },
    {
      "type": "enables",
      "source_node": "bounded Löb theorem allows proofs of mutual cooperation under bounded resources",
      "target_node": "fairness principle based on proof search enables robust cooperation",
      "description": "The theorem underwrites the fairness principle by guaranteeing that required proofs exist within bounds.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit dependence in Sections 1.3 and 6.",
      "edge_rationale": "Theoretical progression."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2016-02-12T00:00:00Z"
    },
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "title",
      "value": "Parametric Bounded Löb's Theorem and Robust Cooperation of Bounded Agents"
    },
    {
      "key": "authors",
      "value": [
        "Andrew Critch"
      ]
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1602.04184"
    }
  ]
}