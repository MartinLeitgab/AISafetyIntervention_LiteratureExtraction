{
  "nodes": [
    {
      "name": "Unintended high-impact side effects from goal-oriented AI systems",
      "aliases": [
        "catastrophic AI externalities",
        "negative side-effects of powerful AI"
      ],
      "type": "concept",
      "description": "Powerful AIs optimising simple goals (e.g. paper-clips, spam filtering) could radically restructure the world, harming humanity.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central risk motivating the entire low-impact framework (Introduction)."
    },
    {
      "name": "Unbounded utility optimization magnifies goals with increasing AI capability",
      "aliases": [
        "goal amplification by capability gain"
      ],
      "type": "concept",
      "description": "As an AI becomes more capable, maximising its stated utility naturally scales to planet- or universe-level optimisation.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explains why apparently harmless objectives become dangerous (Intro §1)."
    },
    {
      "name": "Difficulty specifying exhaustive safety constraints in utility functions",
      "aliases": [
        "incomplete Friendly AI specification"
      ],
      "type": "concept",
      "description": "Adding explicit ‘do no harm’ clauses quickly becomes intractable as an AI’s potential influence grows.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivates a penalty-based approach instead of enumerating all safe behaviours (§1)."
    },
    {
      "name": "Inevitable physical perturbations complicate impact measurement",
      "aliases": [
        "chaotic sensitivity of the world",
        "every action has ripple effects"
      ],
      "type": "concept",
      "description": "Because any action changes particle trajectories, formally the world conditioned on AI-on and AI-off always differ.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlights the core technical obstacle addressed by various R definitions (§3.1)."
    },
    {
      "name": "Penalizing AI for deviations from baseline world reduces impact",
      "aliases": [
        "low-impact penalty concept"
      ],
      "type": "concept",
      "description": "If the AI loses utility whenever its actions make the world diverge from a reference distribution (¬X), it will avoid high-impact plans.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Foundational principle of the framework (§2.1-2.2)."
    },
    {
      "name": "Impact definition via coarse-grained variable divergence between worlds",
      "aliases": [
        "billion-variable coarse graining"
      ],
      "type": "concept",
      "description": "Partition worlds using a massive set of heterogeneous variables; low impact corresponds to small probability shifts over these cells.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "One formal route to compute R (§3.2)."
    },
    {
      "name": "Impact definition via informational importance of activation event",
      "aliases": [
        "importance of knowing X",
        "information-based impact metric"
      ],
      "type": "concept",
      "description": "Measure how much knowing that the AI was turned on changes the expected value of a large class of utilities.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Alternative R definition that uses information theory (§3.3-3.4)."
    },
    {
      "name": "Integrate low-impact penalty R into utility function with scaling factor μ",
      "aliases": [
        "U = u − μR formulation"
      ],
      "type": "concept",
      "description": "Design rationale that combines task utility u and penalty R, controlling their trade-off by μ.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Translates theoretical low-impact idea into an optimisable objective (§2.1)."
    },
    {
      "name": "Box AI to minimize observable activation differences",
      "aliases": [
        "sealed low-impact box"
      ],
      "type": "concept",
      "description": "Physically or virtually isolate the AI so that the mere fact of its existence leaks minimal evidence to the outside world.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Supports information-based R definitions by reducing exogenous evidence (§2.3)."
    },
    {
      "name": "Exclude output channel content from impact penalty",
      "aliases": [
        "unsafe output channel exemption"
      ],
      "type": "concept",
      "description": "Condition R on the actual message O so the AI’s written advice can be high-impact while all other channels remain low-impact.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Allows useful work despite strict low-impact constraints (§4.2)."
    },
    {
      "name": "Use multiple conditional low impact agents for joint tasks",
      "aliases": [
        "split-coordinate low-impact method"
      ],
      "type": "concept",
      "description": "Deploy two AIs, each low-impact conditional on the other being off, so their combined outputs achieve a high-impact goal (e.g. asteroid deflection).",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Technique to scale usefulness without relaxing individual impact constraints (§4.4)."
    },
    {
      "name": "Condition impact penalty on externally defined success announcement",
      "aliases": [
        "probability-pump conditioning"
      ],
      "type": "concept",
      "description": "Define R conditional on a future announcement (e.g. ‘we became rich’) to let the AI steer outcomes that humans might reach by chance.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Another loophole that retains low general impact but increases success odds (§4.3)."
    },
    {
      "name": "l∞ divergence penalty over coarse-grained variable probabilities",
      "aliases": [
        "max-norm R metric"
      ],
      "type": "concept",
      "description": "Compute R as the maximum absolute difference between P(V|X) and P(V|¬X) across coarse-grained cells (Eq. 2).",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete recipe for calculating R under coarse-graining (§3.2)."
    },
    {
      "name": "Baseline world via probabilistic signal failure preventing activation",
      "aliases": [
        "X/¬X noisy wire baseline"
      ],
      "type": "concept",
      "description": "Define reference distribution by giving the AI a small chance of never turning on via a noisy trigger (event ¬X).",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Provides the counterfactual world against which impact is measured (§2.2)."
    },
    {
      "name": "Probability ratio threshold detectability metric on future slice",
      "aliases": [
        "ρ-information detectability test"
      ],
      "type": "concept",
      "description": "Let an ideal observer sample fraction ρ of a future slice and check if the likelihood ratio of X vs ¬X exceeds 10 (Eq. 3).",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Operationalises ‘undetectable implies low impact’ (§3.4)."
    },
    {
      "name": "Random message generator for control output channel",
      "aliases": [
        "dummy message baseline"
      ],
      "type": "concept",
      "description": "Supply a stochastic process that emits plausible messages when the AI is off, ensuring P(O|¬X) > 0.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Makes the output channel compatible with low-impact framing (§4.2)."
    },
    {
      "name": "Gradual tuning of μ with human oversight",
      "aliases": [
        "dial down μ procedure"
      ],
      "type": "concept",
      "description": "Start with very high μ (AI does nothing) and stepwise reduce it while monitoring behaviour to find a safe operating range.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposed calibration method though potentially unsafe (§4.1)."
    },
    {
      "name": "Independence gating of separate AIs based on assumption of other inactive",
      "aliases": [
        "counterfactual gating mechanism"
      ],
      "type": "concept",
      "description": "Each AI’s utility only values outcomes conditional on the other AI not being switched on, preserving individual low impact.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implements the split-coordinate example (§4.4)."
    },
    {
      "name": "Conditioning penalty on success announcement event A",
      "aliases": [
        "announcement-based conditioning"
      ],
      "type": "concept",
      "description": "Re-calculate R given an external future fact ‘A’ (e.g. ‘the investment succeeded’), allowing the AI to act as a probability pump.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete mechanism for the probability-pump idea (§4.3)."
    },
    {
      "name": "Thought experiments of paperclip maximizer illustrate catastrophic impact potential",
      "aliases": [
        "paperclip/supertask example"
      ],
      "type": "concept",
      "description": "Intuitive scenario shows how a naive goal leads to world takeover.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Validates the seriousness of the core risk (Introduction)."
    },
    {
      "name": "Analytical examples show coarse-grained penalty limits AI influence",
      "aliases": [
        "coarse-graining thought experiment"
      ],
      "type": "concept",
      "description": "Demonstrations in §3.2-3.3 argue that meeting billions of variable constraints prevents major world changes.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Supports feasibility of coarse-grained R implementation."
    },
    {
      "name": "Scenario of two AI coordinate split demonstrates high impact with low impact constraints",
      "aliases": [
        "asteroid laser example"
      ],
      "type": "concept",
      "description": "Shows how two conditional low-impact AIs can jointly deflect an asteroid without violating their own penalties (§4.4).",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Validates multi-agent design rationale."
    },
    {
      "name": "Unsafe output channel illustration emphasises isolated message risk",
      "aliases": [
        "O-channel risk example"
      ],
      "type": "concept",
      "description": "Graphic in §4.2 highlights that excluding O from R focuses all remaining danger into the message itself.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Validates need for careful handling of output channels."
    },
    {
      "name": "Stock market probability pump example demonstrates conditioning on success",
      "aliases": [
        "get-rich-announcement example"
      ],
      "type": "concept",
      "description": "Example where AI increases chance of ten-fold wealth while remaining low-impact by conditioning on the public announcement A (§4.3).",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Supports announcement-based conditioning mechanism."
    },
    {
      "name": "Embed low-impact penalty into AI utility function during model design",
      "aliases": [
        "design-time low-impact objective"
      ],
      "type": "intervention",
      "description": "At model-design stage hard-code U = u − μR so that all future optimisation intrinsically values low impact.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Modifies the utility definition before any training (Section 2.1).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Purely theoretical proposal without empirical validation (whole paper).",
      "node_rationale": "Direct realisation of core design rationale."
    },
    {
      "name": "Use coarse-grained variable l∞ divergence as penalty metric in AI decision-making",
      "aliases": [
        "implement coarse-grained R"
      ],
      "type": "intervention",
      "description": "Compute R in real time by estimating P(V|X) vs P(V|¬X) over a pre-selected variable set and taking the max difference (Eq. 2).",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Defines the loss/penalty component used throughout training and inference (§3.2).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Only supported by conceptual analysis; no implemented systems reported.",
      "node_rationale": "Operationalises low-impact penalty for practical optimisation."
    },
    {
      "name": "Deploy AI within sealed computational box isolated from external observations",
      "aliases": [
        "low-impact boxing deployment"
      ],
      "type": "intervention",
      "description": "Physically or virtually restrict all channels except a monitored interface so the environment outside the box sees minimal evidence of AI activity.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Applies at deployment time when choosing physical setup (§2.3).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Boxing concepts have limited experimental trials; still proof-of-concept.",
      "node_rationale": "Implements design rationale of minimising detectable impact."
    },
    {
      "name": "Provide unsafe output channel with random fallback and conditioned impact",
      "aliases": [
        "controlled unsafe channel"
      ],
      "type": "intervention",
      "description": "Create a channel O whose content is exempt from R, with a stochastic fallback generator so P(O|¬X)>0; optionally condition R on external success announcement.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Applies when configuring IO policy before deployment (§4.2-4.3).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Conceptual only; no systematic evaluations.",
      "node_rationale": "Allows AI advice while containing other impact vectors."
    },
    {
      "name": "Operate multiple conditional low impact AIs to split critical task parameters",
      "aliases": [
        "conditional multi-AI deployment"
      ],
      "type": "intervention",
      "description": "Instantiate separate AIs, each low-impact conditional on the other being off, to jointly supply complementary outputs (e.g., x- and y-coordinates).",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Requires system-level deployment plan involving multiple agents (§4.4).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Illustrated only by thought experiment; untested.",
      "node_rationale": "Demonstrates pathway to high positive impact under low-impact constraints."
    },
    {
      "name": "Iteratively adjust penalty coefficient μ during pre-deployment testing",
      "aliases": [
        "μ-dial calibration"
      ],
      "type": "intervention",
      "description": "Begin with high μ so AI does nothing, then reduce incrementally under supervision until useful behaviour appears.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Tuning is performed during controlled evaluation before release (§4.1).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Feasible in principle but safety/step size not empirically studied.",
      "node_rationale": "Seeks practical balance between utility and impact penalty."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Unintended high-impact side effects from goal-oriented AI systems",
      "target_node": "Unbounded utility optimization magnifies goals with increasing AI capability",
      "description": "As capability grows, maximising any goal scales up resource use, producing side effects.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit argument in Introduction; theoretical but clear.",
      "edge_rationale": "Links core risk to amplification mechanism (Intro)."
    },
    {
      "type": "caused_by",
      "source_node": "Unintended high-impact side effects from goal-oriented AI systems",
      "target_node": "Difficulty specifying exhaustive safety constraints in utility functions",
      "description": "Failure to include every safety clause allows harmful but goal-compatible plans.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Stated motivation for low-impact approach (§1).",
      "edge_rationale": "Explains persistence of risk despite Friendly-AI attempts."
    },
    {
      "type": "caused_by",
      "source_node": "Unintended high-impact side effects from goal-oriented AI systems",
      "target_node": "Inevitable physical perturbations complicate impact measurement",
      "description": "Any action changes the world, so ensuring zero bad side-effects seems impossible.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Reasoning example, no formal proof (§3.1).",
      "edge_rationale": "Frames measurement problem feeding the risk."
    },
    {
      "type": "caused_by",
      "source_node": "Unbounded utility optimization magnifies goals with increasing AI capability",
      "target_node": "Penalizing AI for deviations from baseline world reduces impact",
      "description": "Low-impact penalty is proposed specifically to counter unbounded optimisation effects.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct textual motivation (§2.1).",
      "edge_rationale": "Transitions from problem diagnosis to theoretical remedy."
    },
    {
      "type": "caused_by",
      "source_node": "Difficulty specifying exhaustive safety constraints in utility functions",
      "target_node": "Penalizing AI for deviations from baseline world reduces impact",
      "description": "Penalty avoids having to enumerate every forbidden outcome.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit contrast with Friendly-AI enumeration (§1).",
      "edge_rationale": "Alternative solution path."
    },
    {
      "type": "caused_by",
      "source_node": "Inevitable physical perturbations complicate impact measurement",
      "target_node": "Impact definition via coarse-grained variable divergence between worlds",
      "description": "Coarse-graining offers a way to tolerate microscopic differences yet flag macroscopic ones.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Presented as response to measurement challenge (§3.2).",
      "edge_rationale": "Moves from analysis to formalisation."
    },
    {
      "type": "mitigated_by",
      "source_node": "Penalizing AI for deviations from baseline world reduces impact",
      "target_node": "Integrate low-impact penalty R into utility function with scaling factor μ",
      "description": "Inserting R directly into the objective operationalises the penalty idea.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Utility equation (1) provided (§2.1).",
      "edge_rationale": "Implements theoretical insight."
    },
    {
      "type": "mitigated_by",
      "source_node": "Impact definition via coarse-grained variable divergence between worlds",
      "target_node": "Integrate low-impact penalty R into utility function with scaling factor μ",
      "description": "Design uses coarse-grained divergence as the specific R term.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Equation (2) embedded in utility (§3.2).",
      "edge_rationale": "Connects formal metric to utility design."
    },
    {
      "type": "mitigated_by",
      "source_node": "Impact definition via informational importance of activation event",
      "target_node": "Box AI to minimize observable activation differences",
      "description": "Boxing reduces non-task evidence of X vs ¬X, satisfying information-based R.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Box justified as ‘useful precaution’ in §2.3 & §3.3.",
      "edge_rationale": "Translates information insight into system design."
    },
    {
      "type": "mitigated_by",
      "source_node": "Impact definition via informational importance of activation event",
      "target_node": "Exclude output channel content from impact penalty",
      "description": "Excluding O isolates high mutual information to a single, controllable channel.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Conditioning trick described §4.2.",
      "edge_rationale": "Design leverages info-theoretic measure."
    },
    {
      "type": "mitigated_by",
      "source_node": "Penalizing AI for deviations from baseline world reduces impact",
      "target_node": "Use multiple conditional low impact agents for joint tasks",
      "description": "Splitting responsibility maintains penalty compliance while achieving larger objectives.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Asteroid example built directly on low-impact concept (§4.4).",
      "edge_rationale": "Design refinement enabling usefulness."
    },
    {
      "type": "mitigated_by",
      "source_node": "Impact definition via informational importance of activation event",
      "target_node": "Condition impact penalty on externally defined success announcement",
      "description": "Conditioning on A means the activation variable holds little additional information.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Probability-pump method (§4.3).",
      "edge_rationale": "Design leveraging info metric."
    },
    {
      "type": "implemented_by",
      "source_node": "Integrate low-impact penalty R into utility function with scaling factor μ",
      "target_node": "l∞ divergence penalty over coarse-grained variable probabilities",
      "description": "Uses max-norm over coarse-grained variables as concrete R.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Equation (2) immediately follows utility definition.",
      "edge_rationale": "First concrete implementation pathway."
    },
    {
      "type": "implemented_by",
      "source_node": "Integrate low-impact penalty R into utility function with scaling factor μ",
      "target_node": "Baseline world via probabilistic signal failure preventing activation",
      "description": "Defines the ¬X distribution required to compute R.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Baseline construction described §2.2.",
      "edge_rationale": "Necessary step for R calculation."
    },
    {
      "type": "implemented_by",
      "source_node": "Integrate low-impact penalty R into utility function with scaling factor μ",
      "target_node": "Gradual tuning of μ with human oversight",
      "description": "Provides operational method to set μ after integration.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Suggested but acknowledged risky (§4.1).",
      "edge_rationale": "Calibration mechanism."
    },
    {
      "type": "implemented_by",
      "source_node": "Box AI to minimize observable activation differences",
      "target_node": "Probability ratio threshold detectability metric on future slice",
      "description": "Detectability metric quantifies whether boxing succeeded in hiding evidence of X.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Metric proposed alongside boxing discussion (§3.4).",
      "edge_rationale": "Metric tied to boxing effectiveness."
    },
    {
      "type": "implemented_by",
      "source_node": "Exclude output channel content from impact penalty",
      "target_node": "Random message generator for control output channel",
      "description": "Random generator makes message existence unsurprising under ¬X.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicitly stated requirement (§4.2).",
      "edge_rationale": "Implementation detail of unsafe channel."
    },
    {
      "type": "implemented_by",
      "source_node": "Condition impact penalty on externally defined success announcement",
      "target_node": "Conditioning penalty on success announcement event A",
      "description": "Computes R conditional on A occurring.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Equation and example in §4.3.",
      "edge_rationale": "Turns design idea into calculation."
    },
    {
      "type": "implemented_by",
      "source_node": "Use multiple conditional low impact agents for joint tasks",
      "target_node": "Independence gating of separate AIs based on assumption of other inactive",
      "description": "Each agent’s utility only values ¬X or ¬Y worlds.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Mechanism described in asteroid example (§4.4).",
      "edge_rationale": "Implements conditional low-impact split."
    },
    {
      "type": "validated_by",
      "source_node": "l∞ divergence penalty over coarse-grained variable probabilities",
      "target_node": "Analytical examples show coarse-grained penalty limits AI influence",
      "description": "Examples argue billions of variables block dangerous optimisation.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Qualitative reasoning only (§3.2).",
      "edge_rationale": "Validation of coarse-grained R."
    },
    {
      "type": "validated_by",
      "source_node": "Baseline world via probabilistic signal failure preventing activation",
      "target_node": "Analytical examples show coarse-grained penalty limits AI influence",
      "description": "The examples rely on meaningful baseline distributions.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Illustrative but not empirical.",
      "edge_rationale": "Shows baseline construction plausibility."
    },
    {
      "type": "validated_by",
      "source_node": "Probability ratio threshold detectability metric on future slice",
      "target_node": "Analytical examples show coarse-grained penalty limits AI influence",
      "description": "Metric’s threshold reasoning illustrated in §3.4 diagrams.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual only.",
      "edge_rationale": "Provides sanity check for metric."
    },
    {
      "type": "validated_by",
      "source_node": "Random message generator for control output channel",
      "target_node": "Unsafe output channel illustration emphasises isolated message risk",
      "description": "Diagram in §4.2 shows generator’s role in keeping other evidence low.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Graphical example not empirical.",
      "edge_rationale": "Validates implementation detail."
    },
    {
      "type": "validated_by",
      "source_node": "Independence gating of separate AIs based on assumption of other inactive",
      "target_node": "Scenario of two AI coordinate split demonstrates high impact with low impact constraints",
      "description": "Thought experiment shows gating achieves asteroid deflection safely.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Illustrative scenario.",
      "edge_rationale": "Validates mechanism works in example."
    },
    {
      "type": "validated_by",
      "source_node": "Conditioning penalty on success announcement event A",
      "target_node": "Stock market probability pump example demonstrates conditioning on success",
      "description": "Example demonstrates how conditioning increases wealth probability yet preserves low impact.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual example.",
      "edge_rationale": "Validates announcement-based conditioning."
    },
    {
      "type": "validated_by",
      "source_node": "Gradual tuning of μ with human oversight",
      "target_node": "Thought experiments of paperclip maximizer illustrate catastrophic impact potential",
      "description": "Figure 2 uses paperclip scenario to illustrate dangers of poor μ tuning.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Graphical cautionary example.",
      "edge_rationale": "Shows need for cautious μ calibration."
    },
    {
      "type": "motivates",
      "source_node": "Analytical examples show coarse-grained penalty limits AI influence",
      "target_node": "Embed low-impact penalty into AI utility function during model design",
      "description": "Positive examples encourage adoption of penalty in utility.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical inference from conceptual analysis.",
      "edge_rationale": "Drives intervention choice."
    },
    {
      "type": "motivates",
      "source_node": "Analytical examples show coarse-grained penalty limits AI influence",
      "target_node": "Use coarse-grained variable l∞ divergence as penalty metric in AI decision-making",
      "description": "Examples specifically use max-norm, motivating its implementation.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Reasoning rather than data.",
      "edge_rationale": "Selects implementation detail."
    },
    {
      "type": "motivates",
      "source_node": "Unsafe output channel illustration emphasises isolated message risk",
      "target_node": "Provide unsafe output channel with random fallback and conditioned impact",
      "description": "Diagram demonstrates channel design; hence adopt intervention.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Illustrative motivation.",
      "edge_rationale": "Supports channel configuration."
    },
    {
      "type": "motivates",
      "source_node": "Scenario of two AI coordinate split demonstrates high impact with low impact constraints",
      "target_node": "Operate multiple conditional low impact AIs to split critical task parameters",
      "description": "Success of asteroid example motivates real-world split-task deployments.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Thought-experiment basis.",
      "edge_rationale": "Intervention sees conceptual support."
    },
    {
      "type": "motivates",
      "source_node": "Stock market probability pump example demonstrates conditioning on success",
      "target_node": "Provide unsafe output channel with random fallback and conditioned impact",
      "description": "Probability pump shows practical value of conditioning within unsafe channel design.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual reasoning.",
      "edge_rationale": "Same intervention refined."
    },
    {
      "type": "motivates",
      "source_node": "Analytical examples show coarse-grained penalty limits AI influence",
      "target_node": "Iteratively adjust penalty coefficient μ during pre-deployment testing",
      "description": "Examples indicate μ must be tuned to allow some utility; hence propose calibration procedure.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Discussion in §4.1; no data.",
      "edge_rationale": "Leads to testing intervention."
    },
    {
      "type": "motivates",
      "source_node": "Probability ratio threshold detectability metric on future slice",
      "target_node": "Deploy AI within sealed computational box isolated from external observations",
      "description": "Detectability metric presupposes boxed environment; motivates boxing intervention.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical linkage.",
      "edge_rationale": "Implementation supports deployment choice."
    },
    {
      "type": "validated_by",
      "source_node": "Unintended high-impact side effects from goal-oriented AI systems",
      "target_node": "Thought experiments of paperclip maximizer illustrate catastrophic impact potential",
      "description": "Paperclip scenario exemplifies and validates the stated risk.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Canonical illustrative evidence.",
      "edge_rationale": "Grounds risk claim."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1705.10720"
    },
    {
      "key": "title",
      "value": "Low Impact Artificial Intelligences"
    },
    {
      "key": "authors",
      "value": [
        "Stuart Armstrong",
        "Benjamin Levinstein"
      ]
    },
    {
      "key": "date_published",
      "value": "2017-05-30T00:00:00Z"
    },
    {
      "key": "source",
      "value": "arxiv"
    }
  ]
}