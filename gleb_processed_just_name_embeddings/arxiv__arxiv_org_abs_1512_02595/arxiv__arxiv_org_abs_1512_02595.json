{
  "nodes": [
    {
      "name": "High transcription error rate in ASR for varied speech conditions",
      "aliases": [
        "high WER in real world ASR",
        "ASR accuracy risk"
      ],
      "type": "concept",
      "description": "Automatic speech-recognition systems often fail on noisy, accented, far-field or cross-language input, leading to unacceptable word or character error rates.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in Introduction – motivating need for better accuracy."
    },
    {
      "name": "Fragmented hand-engineered ASR pipelines hinder robustness across languages and noise",
      "aliases": [
        "modular ASR brittleness",
        "pipeline component brittleness"
      ],
      "type": "concept",
      "description": "Traditional ASR stacks consist of many separately tuned modules (feature extraction, acoustic model, language model, pronunciation lexicon, etc.) that do not generalise well to new languages or acoustic conditions.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Introduction – explains cause of high error risk."
    },
    {
      "name": "End-to-end learning enables single model to generalize across speech variations",
      "aliases": [
        "end-to-end ASR insight",
        "single-network generalisation"
      ],
      "type": "concept",
      "description": "Replacing many modules with one deep neural network trained from raw spectrograms can learn representations that adapt to different speakers, noise conditions and languages.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Stated in Introduction and Related Work as fundamental assumption."
    },
    {
      "name": "Scale model depth, dataset size, and compute to improve end-to-end ASR accuracy",
      "aliases": [
        "scaling rationale for ASR",
        "depth-data-compute scaling"
      ],
      "type": "concept",
      "description": "Deep Speech 2 argues that deeper networks, much larger labelled datasets, and high-performance GPU training are required to fully realise end-to-end learning benefits.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3 Model Architecture and Section 5 Training Data."
    },
    {
      "name": "Deep bidirectional RNN with CTC loss for speech-to-text",
      "aliases": [
        "CTC deep RNN ASR",
        "bidirectional RNN CTC model"
      ],
      "type": "concept",
      "description": "Core network composed of convolutional front-end followed by multiple bidirectional recurrent layers and a softmax trained with Connectionist Temporal Classification.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in Section 3.1 Preliminaries."
    },
    {
      "name": "Sequence-wise batch normalization in RNN layers",
      "aliases": [
        "BatchNorm for RNN",
        "sequence BN"
      ],
      "type": "concept",
      "description": "Applying batch normalisation across the time-sequence for each recurrent unit accelerates convergence and lowers error, especially in very deep RNNs.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.2 Batch Normalization."
    },
    {
      "name": "SortaGrad curriculum based on utterance length in first epoch",
      "aliases": [
        "SortaGrad",
        "length-based curriculum"
      ],
      "type": "concept",
      "description": "Training starts with shorter utterances ordered by length for the first epoch before random batching thereafter, improving numerical stability and final accuracy.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.3 SortaGrad."
    },
    {
      "name": "2D time-frequency convolutional front-end with strides",
      "aliases": [
        "2-D conv spectrogram front-end",
        "time-frequency convolution"
      ],
      "type": "concept",
      "description": "Multiple convolutional layers along both time and frequency dimensions (with striding) capture speaker and noise invariance and reduce sequence length.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.5 Frequency Convolutions."
    },
    {
      "name": "Deep Speech 2 reduces English WER by 43 % versus Deep Speech 1 across benchmarks",
      "aliases": [
        "WER reduction evidence",
        "DS2 vs DS1 results"
      ],
      "type": "concept",
      "description": "Experiments show 43.4 % relative WER improvement on an internal 3.3k-utterance test set and superior human-level performance on several read-speech benchmarks.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 6.1 Results – Tables 11-15."
    },
    {
      "name": "WER decreases following power law with dataset size up to 12k hours",
      "aliases": [
        "data scaling evidence",
        "power-law WER trend"
      ],
      "type": "concept",
      "description": "Table 10 shows ~40 % relative WER reduction for each 10× increase in training hours across both regular and noisy dev sets.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 5.3 Scaling Data."
    },
    {
      "name": "High deployment latency in automatic speech recognition services",
      "aliases": [
        "ASR latency risk",
        "slow real-time ASR"
      ],
      "type": "concept",
      "description": "For interactive applications, long compute latency between user speech and transcription degrades usability.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 7 Deployment introduction."
    },
    {
      "name": "Bidirectional RNN architectures require full utterance and individual request processing lacks batching",
      "aliases": [
        "full-utterance requirement",
        "inefficient per-request processing"
      ],
      "type": "concept",
      "description": "Streaming cannot start decoding until the entire utterance is available; small per-request batches waste GPU bandwidth.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 7 Deployment – motivation."
    },
    {
      "name": "Limited future context and batch processing can maintain accuracy with reduced latency",
      "aliases": [
        "future context insight",
        "batching insight"
      ],
      "type": "concept",
      "description": "Only a few future frames are needed for good predictions, and assembling multiple user streams into batches improves compute efficiency without large delays.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Derived from Sections 3.7 and 7.1."
    },
    {
      "name": "Adopt unidirectional architectures with row convolution and Batch Dispatch for real-time ASR",
      "aliases": [
        "low-latency design rationale",
        "streaming ASR design"
      ],
      "type": "concept",
      "description": "Switch to forward-only RNN layers augmented with a small-window row convolution and run them under an eager batching scheduler on GPUs.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Sections 3.7 Row Convolution & 7.1 Batch Dispatch."
    },
    {
      "name": "Row convolution layer with small future context in unidirectional RNN",
      "aliases": [
        "row convolution τ=19",
        "future-context conv"
      ],
      "type": "concept",
      "description": "A convolution over time rows after recurrent layers gathers a fixed τ future frames (e.g., 19) enabling streaming while retaining bidirectional-like accuracy.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.7 and 7.4 deployment model."
    },
    {
      "name": "Batch Dispatch scheduler for dynamic request batching on GPU",
      "aliases": [
        "Batch Dispatch",
        "eager batching scheduler"
      ],
      "type": "concept",
      "description": "Server-side scheduler buffers concurrent audio streams just long enough to build batches, greatly increasing GPU utilisation with small latency impact.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 7.1 Batch Dispatch."
    },
    {
      "name": "Half-precision optimized matrix multiply kernels for deployment",
      "aliases": [
        "FP16 GEMM kernels",
        "half-precision inference"
      ],
      "type": "concept",
      "description": "Custom CUDA kernels perform small-batch GEMMs in FP16, achieving >90 % memory-bandwidth efficiency for batch ≤4, cutting latency and memory usage.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 7.2 Deployment Optimised Kernels."
    },
    {
      "name": "Deployment system median latency 44 ms and 98th percentile 67 ms with 5 % CER degradation",
      "aliases": [
        "latency evidence",
        "Batch Dispatch latency results"
      ],
      "type": "concept",
      "description": "On a Quadro K1200 GPU with 10 streams, the streaming model achieves 44 ms median and 67 ms P98 end-to-utterance latency, only 0.3 CER worse than research model.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Figures 5-6 and Section 7.4 Results."
    },
    {
      "name": "Prohibitively long training time for large end-to-end ASR models",
      "aliases": [
        "slow model exploration risk",
        "training time bottleneck"
      ],
      "type": "concept",
      "description": "Single-GPU training of 100 M-parameter RNNs on 12k h speech would take weeks, hindering architecture iteration and deployment.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 1 Introduction & Section 4 System Optimizations."
    },
    {
      "name": "Large model size and CPU-based CTC computation bottleneck training throughput",
      "aliases": [
        "CPU CTC bottleneck",
        "communication bottleneck"
      ],
      "type": "concept",
      "description": "CTC loss on CPU and standard MPI all-reduce introduce computation and communication overheads, preventing efficient multi-GPU scaling.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Sections 4.1 and 4.2."
    },
    {
      "name": "GPU optimisation and synchronous data-parallelism can accelerate deep RNN training",
      "aliases": [
        "training speed insight",
        "GPU CTC + sync SGD insight"
      ],
      "type": "concept",
      "description": "Moving CTC to GPU and using ring-all-reduce synchronous SGD promises linear scaling with easier debugging compared to asynchronous methods.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4 System Optimizations."
    },
    {
      "name": "Implement GPU CTC loss and fast all-reduce synchronous SGD",
      "aliases": [
        "training speed design rationale",
        "compute optimisation rationale"
      ],
      "type": "concept",
      "description": "Replace CPU CTC with CUDA version and craft a GPUDirect ring all-reduce to exchange gradients efficiently across 8–16 GPUs.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Sections 4.1–4.2."
    },
    {
      "name": "Custom GPU implementation of CTC loss function",
      "aliases": [
        "GPU CTC",
        "CUDA CTC"
      ],
      "type": "concept",
      "description": "Parallel algorithm refactor plus ModernGPU sorting achieves 12–29× speed-up over CPU CTC, saving up to 95 min per epoch.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4.2."
    },
    {
      "name": "Ring-based all-reduce with GPUDirect across GPUs",
      "aliases": [
        "custom ring all-reduce",
        "fast gradient exchange"
      ],
      "type": "concept",
      "description": "MPI send/recv with CUDA kernels avoid host copies, delivering 2–20× faster gradient aggregation than OpenMPI default on 4–16 GPUs.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4.1 – Table 7."
    },
    {
      "name": "Training time per epoch reduced by 10–20 % and scaling near-linear to 16 GPUs",
      "aliases": [
        "training speed evidence",
        "scaling evidence"
      ],
      "type": "concept",
      "description": "Figure 4 and Table 8 show epoch time almost halves with GPU count doubling; GPU CTC removes key bottleneck, cutting total epoch time markedly.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Sections 4.1–4.2."
    },
    {
      "name": "Train deep bidirectional RNN ASR models with sequence-wise BatchNorm, SortaGrad, and 2D convolutions on >10k hours labeled data",
      "aliases": [
        "DS2-style training intervention",
        "large-scale end-to-end ASR training"
      ],
      "type": "intervention",
      "description": "During model-development phase, employ deep convolutional-recurrent architecture, sequence-wise BatchNorm, SortaGrad curriculum and thousands of hours of noisy-augmented speech to maximise accuracy.",
      "concept_category": null,
      "intervention_lifecycle": "2",
      "intervention_lifecycle_rationale": "Covers core training procedure before any fine-tuning – Section 3 & 5.",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Demonstrated at 100 M parameter scale across two languages (Section 6) but not yet universal industry standard.",
      "node_rationale": "Actionable recipe derived from validated mechanisms."
    },
    {
      "name": "Integrate custom GPU CTC loss and ring all-reduce synchronous SGD during ASR training on 8–16 GPUs",
      "aliases": [
        "fast multi-GPU training intervention",
        "GPU CTC + sync SGD"
      ],
      "type": "intervention",
      "description": "Optimise the training stack by switching CTC computation to CUDA kernel and using GPUDirect ring all-reduce for gradient exchange, enabling 3–5 day training cycles.",
      "concept_category": null,
      "intervention_lifecycle": "2",
      "intervention_lifecycle_rationale": "Alters core optimisation loop during pre-training – Section 4.",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Implemented and benchmarked internally; not yet fully commoditised.",
      "node_rationale": "Directly addresses slow-training risk."
    },
    {
      "name": "Deploy unidirectional RNN ASR with row convolution, Batch Dispatch batching, and half-precision kernels on GPU servers for real-time transcription",
      "aliases": [
        "low-latency ASR deployment intervention",
        "streaming ASR stack"
      ],
      "type": "intervention",
      "description": "In production, serve an FP16 unidirectional 5-layer RNN+row-conv model; Batch Dispatch forms dynamic batches, and custom small-batch GEMM kernels run inference within 70 ms P98 latency.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Applies at deployment/runtime phase – Section 7.",
      "intervention_maturity": "4",
      "intervention_maturity_rationale": "Operational in Baidu internal production with measured latency statistics.",
      "node_rationale": "Provides actionable blueprint to mitigate latency risk."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "High transcription error rate in ASR for varied speech conditions",
      "target_node": "Fragmented hand-engineered ASR pipelines hinder robustness across languages and noise",
      "description": "Modular pipelines’ poor generalisation leads directly to elevated error rates.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit reasoning in Introduction.",
      "edge_rationale": "Paper states module tuning difficulty causes high WER."
    },
    {
      "type": "caused_by",
      "source_node": "Fragmented hand-engineered ASR pipelines hinder robustness across languages and noise",
      "target_node": "End-to-end learning enables single model to generalize across speech variations",
      "description": "The limitation of modularity motivates shift toward end-to-end models.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Stated motivation but not experimentally compared.",
      "edge_rationale": "Transition described in Related Work."
    },
    {
      "type": "mitigated_by",
      "source_node": "End-to-end learning enables single model to generalize across speech variations",
      "target_node": "Scale model depth, dataset size, and compute to improve end-to-end ASR accuracy",
      "description": "Scaling is proposed as concrete method to realise the end-to-end advantage.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Design argument throughout Section 3.",
      "edge_rationale": "Paper links bigger data/models to better generalisation."
    },
    {
      "type": "implemented_by",
      "source_node": "Scale model depth, dataset size, and compute to improve end-to-end ASR accuracy",
      "target_node": "Deep bidirectional RNN with CTC loss for speech-to-text",
      "description": "Chosen architecture embodies the scaling rationale.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct implementation details provided.",
      "edge_rationale": "Section 3 describes model as instantiation."
    },
    {
      "type": "refined_by",
      "source_node": "Deep bidirectional RNN with CTC loss for speech-to-text",
      "target_node": "Sequence-wise batch normalization in RNN layers",
      "description": "BatchNorm is a refinement improving optimisation of the base architecture.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Ablation table shows benefit.",
      "edge_rationale": "Table 1, Section 3.2."
    },
    {
      "type": "refined_by",
      "source_node": "Deep bidirectional RNN with CTC loss for speech-to-text",
      "target_node": "SortaGrad curriculum based on utterance length in first epoch",
      "description": "Curriculum further refines training stability of same architecture.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Table 2 evidence.",
      "edge_rationale": "Section 3.3."
    },
    {
      "type": "refined_by",
      "source_node": "Deep bidirectional RNN with CTC loss for speech-to-text",
      "target_node": "2D time-frequency convolutional front-end with strides",
      "description": "Adding 2-D conv layers refines front-end for noise robustness.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Table 4 shows clear gains.",
      "edge_rationale": "Section 3.5."
    },
    {
      "type": "validated_by",
      "source_node": "Deep bidirectional RNN with CTC loss for speech-to-text",
      "target_node": "Deep Speech 2 reduces English WER by 43 % versus Deep Speech 1 across benchmarks",
      "description": "Empirical results confirm architecture effectiveness.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Multiple controlled evaluations.",
      "edge_rationale": "Section 6.1."
    },
    {
      "type": "validated_by",
      "source_node": "Scale model depth, dataset size, and compute to improve end-to-end ASR accuracy",
      "target_node": "WER decreases following power law with dataset size up to 12k hours",
      "description": "Data-scaling experiment supports scaling rationale.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Systematic dataset-size sweep.",
      "edge_rationale": "Section 5.3."
    },
    {
      "type": "validated_by",
      "source_node": "Sequence-wise batch normalization in RNN layers",
      "target_node": "Deep Speech 2 reduces English WER by 43 % versus Deep Speech 1 across benchmarks",
      "description": "BatchNorm contributes to overall WER reduction.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Performance tables include BN on/off.",
      "edge_rationale": "Section 3.2."
    },
    {
      "type": "validated_by",
      "source_node": "SortaGrad curriculum based on utterance length in first epoch",
      "target_node": "Deep Speech 2 reduces English WER by 43 % versus Deep Speech 1 across benchmarks",
      "description": "Curriculum part of training recipe achieving final results.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Indirect evidence; moderate inference.",
      "edge_rationale": "Section 3.3."
    },
    {
      "type": "validated_by",
      "source_node": "2D time-frequency convolutional front-end with strides",
      "target_node": "Deep Speech 2 reduces English WER by 43 % versus Deep Speech 1 across benchmarks",
      "description": "2-D conv front-end improves noisy-speech accuracy contributing to headline WER.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Table 4 noisy dev improvements.",
      "edge_rationale": "Section 3.5."
    },
    {
      "type": "motivates",
      "source_node": "Deep Speech 2 reduces English WER by 43 % versus Deep Speech 1 across benchmarks",
      "target_node": "Train deep bidirectional RNN ASR models with sequence-wise BatchNorm, SortaGrad, and 2D convolutions on >10k hours labeled data",
      "description": "Empirical success motivates adopting this training recipe.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct link from evidence to recommended practice.",
      "edge_rationale": "Section 6 Conclusion."
    },
    {
      "type": "motivates",
      "source_node": "WER decreases following power law with dataset size up to 12k hours",
      "target_node": "Train deep bidirectional RNN ASR models with sequence-wise BatchNorm, SortaGrad, and 2D convolutions on >10k hours labeled data",
      "description": "Scaling evidence supports large-data intervention.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Clear quantitative trend.",
      "edge_rationale": "Section 5.3."
    },
    {
      "type": "caused_by",
      "source_node": "High deployment latency in automatic speech recognition services",
      "target_node": "Bidirectional RNN architectures require full utterance and individual request processing lacks batching",
      "description": "These architectural features directly create latency bottlenecks.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit in Deployment section.",
      "edge_rationale": "Section 7 intro."
    },
    {
      "type": "caused_by",
      "source_node": "Bidirectional RNN architectures require full utterance and individual request processing lacks batching",
      "target_node": "Limited future context and batch processing can maintain accuracy with reduced latency",
      "description": "Problem analysis leads to insight about minimal future context and batching.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoning chain in Sections 3.7 & 7.1.",
      "edge_rationale": "Derived from discussion preceding solutions."
    },
    {
      "type": "mitigated_by",
      "source_node": "Limited future context and batch processing can maintain accuracy with reduced latency",
      "target_node": "Adopt unidirectional architectures with row convolution and Batch Dispatch for real-time ASR",
      "description": "Design choice directly applies the insight.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Design stated as answer to insight.",
      "edge_rationale": "Sections 3.7, 7."
    },
    {
      "type": "implemented_by",
      "source_node": "Adopt unidirectional architectures with row convolution and Batch Dispatch for real-time ASR",
      "target_node": "Row convolution layer with small future context in unidirectional RNN",
      "description": "Row convolution realises limited future-context requirement.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Figure 3.",
      "edge_rationale": "Section 3.7."
    },
    {
      "type": "implemented_by",
      "source_node": "Adopt unidirectional architectures with row convolution and Batch Dispatch for real-time ASR",
      "target_node": "Batch Dispatch scheduler for dynamic request batching on GPU",
      "description": "Scheduler realises batching part of the design.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 7.1 details.",
      "edge_rationale": "Batch Dispatch described as implementation."
    },
    {
      "type": "implemented_by",
      "source_node": "Adopt unidirectional architectures with row convolution and Batch Dispatch for real-time ASR",
      "target_node": "Half-precision optimized matrix multiply kernels for deployment",
      "description": "Custom FP16 kernels further implement low-latency design.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Supportive but less central component.",
      "edge_rationale": "Section 7.2."
    },
    {
      "type": "validated_by",
      "source_node": "Row convolution layer with small future context in unidirectional RNN",
      "target_node": "Deployment system median latency 44 ms and 98th percentile 67 ms with 5 % CER degradation",
      "description": "Latency & accuracy metrics include row-conv model.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct measurement.",
      "edge_rationale": "Section 7.4."
    },
    {
      "type": "validated_by",
      "source_node": "Batch Dispatch scheduler for dynamic request batching on GPU",
      "target_node": "Deployment system median latency 44 ms and 98th percentile 67 ms with 5 % CER degradation",
      "description": "Figure 6 shows effectiveness of batching.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Empirical latency curves.",
      "edge_rationale": "Section 7.1."
    },
    {
      "type": "validated_by",
      "source_node": "Half-precision optimized matrix multiply kernels for deployment",
      "target_node": "Deployment system median latency 44 ms and 98th percentile 67 ms with 5 % CER degradation",
      "description": "FP16 kernels boost throughput contributing to latency figure.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Figure 7 benchmarks.",
      "edge_rationale": "Section 7.2."
    },
    {
      "type": "motivates",
      "source_node": "Deployment system median latency 44 ms and 98th percentile 67 ms with 5 % CER degradation",
      "target_node": "Deploy unidirectional RNN ASR with row convolution, Batch Dispatch batching, and half-precision kernels on GPU servers for real-time transcription",
      "description": "Positive latency evidence motivates adopting deployment recipe.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Operational metrics.",
      "edge_rationale": "Section 7.4."
    },
    {
      "type": "caused_by",
      "source_node": "Prohibitively long training time for large end-to-end ASR models",
      "target_node": "Large model size and CPU-based CTC computation bottleneck training throughput",
      "description": "CPU CTC & communication overheads directly lengthen training.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Problem explanation Section 4.",
      "edge_rationale": "Section 4."
    },
    {
      "type": "caused_by",
      "source_node": "Large model size and CPU-based CTC computation bottleneck training throughput",
      "target_node": "GPU optimisation and synchronous data-parallelism can accelerate deep RNN training",
      "description": "Analysis leads to insight about GPU CTC + sync SGD.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical derivation.",
      "edge_rationale": "Section 4."
    },
    {
      "type": "mitigated_by",
      "source_node": "GPU optimisation and synchronous data-parallelism can accelerate deep RNN training",
      "target_node": "Implement GPU CTC loss and fast all-reduce synchronous SGD",
      "description": "Design puts insight into practice.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit design description.",
      "edge_rationale": "Section 4."
    },
    {
      "type": "implemented_by",
      "source_node": "Implement GPU CTC loss and fast all-reduce synchronous SGD",
      "target_node": "Custom GPU implementation of CTC loss function",
      "description": "CUDA CTC is first part of implementation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Detailed algorithms.",
      "edge_rationale": "Section 4.2."
    },
    {
      "type": "implemented_by",
      "source_node": "Implement GPU CTC loss and fast all-reduce synchronous SGD",
      "target_node": "Ring-based all-reduce with GPUDirect across GPUs",
      "description": "Ring all-reduce is second part of implementation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 4.1.",
      "edge_rationale": "Section 4.1."
    },
    {
      "type": "validated_by",
      "source_node": "Custom GPU implementation of CTC loss function",
      "target_node": "Training time per epoch reduced by 10–20 % and scaling near-linear to 16 GPUs",
      "description": "Epoch-time savings quantify benefit of GPU CTC.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Table 8.",
      "edge_rationale": "Section 4.2."
    },
    {
      "type": "validated_by",
      "source_node": "Ring-based all-reduce with GPUDirect across GPUs",
      "target_node": "Training time per epoch reduced by 10–20 % and scaling near-linear to 16 GPUs",
      "description": "All-reduce speedups underpin near-linear scaling.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Table 7 & Figure 4.",
      "edge_rationale": "Section 4.1."
    },
    {
      "type": "motivates",
      "source_node": "Training time per epoch reduced by 10–20 % and scaling near-linear to 16 GPUs",
      "target_node": "Integrate custom GPU CTC loss and ring all-reduce synchronous SGD during ASR training on 8–16 GPUs",
      "description": "Measured speedups justify adopting this optimisation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Quantitative evidence.",
      "edge_rationale": "Section 4 Conclusion."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2015-12-08T00:00:00Z"
    },
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "title",
      "value": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin"
    },
    {
      "key": "authors",
      "value": [
        "Dario Amodei",
        "Rishita Anubhai",
        "Eric Battenberg",
        "Carl Case",
        "Jared Casper",
        "Bryan Catanzaro",
        "Jingdong Chen",
        "Mike Chrzanowski",
        "Adam Coates",
        "Greg Diamos",
        "Erich Elsen",
        "Jesse Engel",
        "Linxi Fan",
        "Christopher Fougner",
        "Tony Han",
        "Awni Hannun",
        "Billy Jun",
        "Patrick LeGresley",
        "Libby Lin",
        "Sharan Narang",
        "Andrew Ng",
        "Sherjil Ozair",
        "Ryan Prenger",
        "Jonathan Raiman",
        "Sanjeev Satheesh",
        "David Seetapun",
        "Shubho Sengupta",
        "Yi Wang",
        "Zhiqian Wang",
        "Chong Wang",
        "Bo Xiao",
        "Dani Yogatama",
        "Jun Zhan",
        "Zhenyao Zhu"
      ]
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1512.02595"
    }
  ]
}