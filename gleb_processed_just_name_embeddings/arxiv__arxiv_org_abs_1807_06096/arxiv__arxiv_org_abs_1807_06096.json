{
  "nodes": [
    {
      "name": "Safety violations during exploration in reinforcement learning agents",
      "aliases": [
        "unsafe exploration in RL",
        "dangerous actions by RL agents"
      ],
      "type": "concept",
      "description": "RL agents can execute unsafe actions while exploring, potentially causing collisions, damage or other unacceptable outcomes.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in Introduction section as the central safety risk motivating the work."
    },
    {
      "name": "Absence of runtime action-level safety constraints in RL environments",
      "aliases": [
        "no action filtering",
        "lack of safety guards in RL"
      ],
      "type": "concept",
      "description": "Standard RL maximises expected reward without an explicit, enforceable constraint that prevents specific unsafe actions at run-time.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Introduction and Related Work as a root cause of unsafe behaviour."
    },
    {
      "name": "Bounded-probability safety objectives balance exploration and safety in RL",
      "aliases": [
        "probabilistic safety constraints",
        "allow small violation probability"
      ],
      "type": "concept",
      "description": "Instead of forbidding violations absolutely, safety can be expressed as keeping the probability of reaching unsafe states below a chosen bound, enabling richer exploration.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposed in Introduction and Shield Construction as key insight enabling the method."
    },
    {
      "name": "Probabilistic δ-shield blocking high-risk actions in RL",
      "aliases": [
        "δ-shield",
        "probabilistic shield"
      ],
      "type": "concept",
      "description": "A design that consults pre-computed risk values and blocks any action whose probability of violating the safety property exceeds δ times the optimal minimal risk.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Defined formally in Shield Construction section (Definitions 2–4)."
    },
    {
      "name": "Adaptive δ-threshold tuning of shield strictness in RL",
      "aliases": [
        "iterative weakening",
        "on-the-fly δ adjustment"
      ],
      "type": "concept",
      "description": "The δ parameter can be increased or decreased online to trade off safety against performance, weakening the shield when learning stalls and strengthening it in critical states.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Presented in Shielding versus Performance subsection."
    },
    {
      "name": "Action valuation via probabilistic model checking of safety-relevant MDP",
      "aliases": [
        "model-checked action risk values",
        "safety-relevant MDP evaluation"
      ],
      "type": "concept",
      "description": "Model checking computes, for each state-action pair in a reduced MDP fragment, the minimal probability of violating the temporal-logic safety property, enabling shield look-up.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Technical mechanism detailed in Shield Construction section."
    },
    {
      "name": "Runtime δ adjustment algorithm based on performance progress in RL",
      "aliases": [
        "online shield weakening algorithm",
        "performance-aware δ control"
      ],
      "type": "concept",
      "description": "An algorithm observes learning progress; if performance plateaus, it lowers δ to permit more exploration, then restores δ once progress resumes.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation detail given in Shielding versus Performance subsection."
    },
    {
      "name": "Scalability issues of model checking for large safety-relevant MDPs",
      "aliases": [
        "state-space explosion in shielding",
        "verification scalability limits"
      ],
      "type": "concept",
      "description": "Even the reduced safety-relevant MDP can contain millions of states, causing memory and time bottlenecks in model checking.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Raised under Key Challenges and Scalable Shield Construction."
    },
    {
      "name": "Agent independence and finite horizon reduce shield computation complexity",
      "aliases": [
        "complexity reduction insight",
        "finite-horizon & independence"
      ],
      "type": "concept",
      "description": "Treating adversaries independently and analysing only a finite look-ahead allows shield construction to scale by shrinking the effective state space.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in Scalable Shield Construction subsection."
    },
    {
      "name": "Finite-horizon piecewise independent shield construction for RL safety",
      "aliases": [
        "piecewise shield computation",
        "parallel finite-horizon shielding"
      ],
      "type": "concept",
      "description": "Shield values are computed separately for each critical state and each adversary over a limited horizon, enabling parallel and memory-efficient computation.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation described in Scalable Shield Construction and Set-up sections."
    },
    {
      "name": "PAC-MAN and warehouse experiments show improved performance with shields",
      "aliases": [
        "experimental validation",
        "empirical results on shields"
      ],
      "type": "concept",
      "description": "Across hundreds of training episodes, shielded RL achieved higher scores and significantly fewer safety violations than unshielded baselines.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Results and Figures 2 & 3 in Implementation and Numerical Experiments."
    },
    {
      "name": "Deploy probabilistic δ-shield to block RL agent actions exceeding risk threshold",
      "aliases": [
        "apply δ-shield during learning",
        "integrate probabilistic shield"
      ],
      "type": "intervention",
      "description": "During both training and deployment, insert a decision-time filter that consults the pre-computed lookup table and prevents actions whose risk exceeds the chosen δ-scaled optimum.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Shield acts during exploration while the RL agent is fine-tuning (Shield Construction section).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Validated in prototype form in two case studies (Implementation and Numerical Experiments).",
      "node_rationale": "Primary actionable step derived from design; enables safety."
    },
    {
      "name": "Dynamically adjust δ threshold when RL performance stagnates",
      "aliases": [
        "iterative weakening intervention",
        "online δ tuning"
      ],
      "type": "intervention",
      "description": "Monitor reward trends; if no improvement is detected, decrease δ by ε to allow riskier actions, and reset δ once progress resumes, reducing shield invasiveness.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Executed during learning to influence exploration (Shielding versus Performance).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Proposed and demonstrated qualitatively; no systematic large-scale evaluation yet.",
      "node_rationale": "Provides a concrete control heuristic to implement the adaptive-δ design."
    },
    {
      "name": "Compute shields using finite-horizon piecewise independent-agent model checking before deployment",
      "aliases": [
        "parallel finite-horizon shield computation",
        "scalable shield synthesis"
      ],
      "type": "intervention",
      "description": "Before training or deployment, build shields by analysing each adversary separately over a limited time horizon, composing results via inclusion–exclusion and parallel model-checking jobs.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Performed in pre-deployment analysis to create lookup tables (Scalable Shield Construction, Set-up).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Implemented experimentally on medium-scale examples; not yet industry-scale.",
      "node_rationale": "Actionable technique to overcome scalability problem so shields can be produced in larger systems."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Safety violations during exploration in reinforcement learning agents",
      "target_node": "Absence of runtime action-level safety constraints in RL environments",
      "description": "Without any run-time filter, RL agents freely sample potentially dangerous actions, leading directly to safety violations.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicitly noted in Introduction as the motivating gap.",
      "edge_rationale": "Connects the observed risk to its primary cause."
    },
    {
      "type": "mitigated_by",
      "source_node": "Absence of runtime action-level safety constraints in RL environments",
      "target_node": "Bounded-probability safety objectives balance exploration and safety in RL",
      "description": "Introducing probabilistic (rather than absolute) safety objectives offers a pathway to add constraints without fully prohibiting exploration.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Paper argues qualitatively for this mitigation.",
      "edge_rationale": "Shows theoretical idea addressing the identified problem."
    },
    {
      "type": "implemented_by",
      "source_node": "Bounded-probability safety objectives balance exploration and safety in RL",
      "target_node": "Probabilistic δ-shield blocking high-risk actions in RL",
      "description": "The δ-shield is the concrete mechanism that enforces the bounded-probability objective.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Formal definitions link δ and probability bounds.",
      "edge_rationale": "Maps theory to design."
    },
    {
      "type": "implemented_by",
      "source_node": "Probabilistic δ-shield blocking high-risk actions in RL",
      "target_node": "Action valuation via probabilistic model checking of safety-relevant MDP",
      "description": "Shield requires pre-computing per-action risk values via model checking to decide which actions to block.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Implementation details directly follow design in Shield Construction.",
      "edge_rationale": "Links design to concrete implementation step."
    },
    {
      "type": "validated_by",
      "source_node": "Action valuation via probabilistic model checking of safety-relevant MDP",
      "target_node": "PAC-MAN and warehouse experiments show improved performance with shields",
      "description": "Experiments used model-checked valuations to build shields and observed resulting safety/performance gains.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Empirical evidence but limited to two domains.",
      "edge_rationale": "Shows empirical support for implementation mechanism."
    },
    {
      "type": "motivates",
      "source_node": "PAC-MAN and warehouse experiments show improved performance with shields",
      "target_node": "Deploy probabilistic δ-shield to block RL agent actions exceeding risk threshold",
      "description": "Positive experimental outcomes justify deploying the δ-shield in practice.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Clear quantitative improvements reported.",
      "edge_rationale": "Evidence supports taking the intervention."
    },
    {
      "type": "refined_by",
      "source_node": "Probabilistic δ-shield blocking high-risk actions in RL",
      "target_node": "Adaptive δ-threshold tuning of shield strictness in RL",
      "description": "Adaptive tuning refines the basic shield concept, making it less invasive when possible.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Described as refinement in Shielding versus Performance.",
      "edge_rationale": "Captures design evolution."
    },
    {
      "type": "implemented_by",
      "source_node": "Adaptive δ-threshold tuning of shield strictness in RL",
      "target_node": "Runtime δ adjustment algorithm based on performance progress in RL",
      "description": "The runtime algorithm realises the adaptive-δ concept by monitoring reward trends.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Algorithmic description provided but without full formal proof.",
      "edge_rationale": "Maps refined design to concrete algorithm."
    },
    {
      "type": "validated_by",
      "source_node": "Runtime δ adjustment algorithm based on performance progress in RL",
      "target_node": "PAC-MAN and warehouse experiments show improved performance with shields",
      "description": "Experiments include iterative weakening showing further score improvements.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Evidence is qualitative and limited; marked as weaker.",
      "edge_rationale": "Links algorithm to empirical results."
    },
    {
      "type": "caused_by",
      "source_node": "Action valuation via probabilistic model checking of safety-relevant MDP",
      "target_node": "Scalability issues of model checking for large safety-relevant MDPs",
      "description": "Model checking large MDPs leads to state-space explosion, creating scalability problems.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Paper explicitly cites scalability as a main challenge.",
      "edge_rationale": "Identifies secondary problem triggered by the implementation mechanism."
    },
    {
      "type": "mitigated_by",
      "source_node": "Scalability issues of model checking for large safety-relevant MDPs",
      "target_node": "Agent independence and finite horizon reduce shield computation complexity",
      "description": "Theoretical observation that finite horizon and agent independence lower computational load addresses scalability concerns.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Presented as reasoning, not yet formally proved optimal.",
      "edge_rationale": "Shows conceptual mitigation of the problem."
    },
    {
      "type": "implemented_by",
      "source_node": "Agent independence and finite horizon reduce shield computation complexity",
      "target_node": "Finite-horizon piecewise independent shield construction for RL safety",
      "description": "Concrete procedure builds shields in parallel for each agent and horizon-limited fragment.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Detailed implementation steps in Scalable Shield Construction.",
      "edge_rationale": "Connects insight to mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Finite-horizon piecewise independent shield construction for RL safety",
      "target_node": "PAC-MAN and warehouse experiments show improved performance with shields",
      "description": "Large-scale PM instance (10^12 states) was handled using piecewise construction, demonstrating feasibility.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Evidence from single-machine experiments; still medium strength.",
      "edge_rationale": "Empirical support for scalability mechanism."
    },
    {
      "type": "motivates",
      "source_node": "PAC-MAN and warehouse experiments show improved performance with shields",
      "target_node": "Compute shields using finite-horizon piecewise independent-agent model checking before deployment",
      "description": "Successful use in experiments motivates adopting the scalable shield synthesis procedure in practice.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Results demonstrate practicality but across limited domains.",
      "edge_rationale": "Drives the decision to perform the intervention."
    },
    {
      "type": "motivates",
      "source_node": "PAC-MAN and warehouse experiments show improved performance with shields",
      "target_node": "Dynamically adjust δ threshold when RL performance stagnates",
      "description": "Observed trade-offs between safety and performance encourage applying online δ adjustment.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Only qualitative indication; therefore lower strength.",
      "edge_rationale": "Connects evidence to adaptive intervention."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2018-07-16T00:00:00Z"
    },
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1807.06096"
    },
    {
      "key": "title",
      "value": "Safe Reinforcement Learning via Probabilistic Shields"
    },
    {
      "key": "authors",
      "value": [
        "Nils Jansen",
        "Bettina Könighofer",
        "Sebastian Junges",
        "Alexandru C. Serban",
        "Roderick Bloem"
      ]
    }
  ]
}