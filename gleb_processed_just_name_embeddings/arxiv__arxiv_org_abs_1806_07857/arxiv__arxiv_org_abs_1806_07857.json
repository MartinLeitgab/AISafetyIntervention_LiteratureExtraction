{
  "nodes": [
    {
      "name": "credit assignment failure in delayed reward reinforcement learning",
      "aliases": [
        "inefficient credit assignment with delayed rewards",
        "long-term credit assignment risk"
      ],
      "type": "concept",
      "description": "When reward signals arrive far after the actions that caused them, RL agents learn very slowly or converge to unsafe sub-optimal behaviour.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Stated in Introduction: learning long-term credit assignment for delayed rewards is a major challenge (Section 1 Introduction)."
    },
    {
      "name": "bias in temporal difference learning and high variance in Monte Carlo learning in delayed reward MDPs",
      "aliases": [
        "TD bias and MC variance under delayed rewards"
      ],
      "type": "concept",
      "description": "TD methods need exponentially many updates to correct bias, while MC estimates suffer exponentially growing variance when rewards are delayed.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in Section 1 Introduction and proofs A3 – the paper quantifies the exponential problems."
    },
    {
      "name": "eliminating expected future rewards simplifies Q-value estimation to immediate reward mean",
      "aliases": [
        "zero expected future reward principle",
        "Q-value equals immediate reward"
      ],
      "type": "concept",
      "description": "If the expected sum of future rewards is forced to zero, estimating Q-values reduces to computing an immediate reward average, removing bias/variance sources.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Formally introduced end of Section 1 and Theorem 3 (Reward Redistribution and Novel Learning Algorithms)."
    },
    {
      "name": "return-equivalent sequence-Markov decision processes preserve optimal policies",
      "aliases": [
        "return-equivalent SDP theory"
      ],
      "type": "concept",
      "description": "By allowing non-Markov rewards one can transform an MDP into an SDP with identical optimal policies but altered reward timing.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proved in Theorem 1 (Section 2 Reward Redistribution and Novel Learning Algorithms)."
    },
    {
      "name": "reward redistribution achieving zero expected future rewards in SDPs",
      "aliases": [
        "optimal reward redistribution design"
      ],
      "type": "concept",
      "description": "Design idea: shift delayed rewards backward along the trajectory so that each state-action pair’s expected future reward becomes zero while keeping overall return.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 2 introduces reward redistribution as main concept to push expected future rewards toward zero."
    },
    {
      "name": "return decomposition via contribution analysis for constructing reward redistribution",
      "aliases": [
        "return decomposition design",
        "contribution analysis for credit assignment"
      ],
      "type": "concept",
      "description": "Transform RL into a supervised regression task that predicts total return; use contribution analysis to attribute return to preceding state-action pairs.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3 Constructing Reward Redistributions by Return Decomposition."
    },
    {
      "name": "second-order Markov reward redistribution using differences of consecutive Q-values",
      "aliases": [
        "optimal second-order reward shift"
      ],
      "type": "concept",
      "description": "Implementation mechanism where redistributed reward at t+1 depends only on (s_{t-1},a_{t-1},s_t,a_t) and equals successive Q-value differences.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Derived in Theorem 2, Equation (1) (Section 2)."
    },
    {
      "name": "LSTM-based return prediction with contribution analysis (RUDDER implementation)",
      "aliases": [
        "RUDDER LSTM return decomposition"
      ],
      "type": "concept",
      "description": "Uses an LSTM to predict episode-wide return at every time step and assigns redistributed rewards via differences of consecutive predictions.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in Section 3 (Return Decomposition) and Section 3 ‘RUDDER: Return Decomposition using LSTM’."
    },
    {
      "name": "RUDDER speed-up over TD, MC, and MCTS on artificial delayed-reward tasks",
      "aliases": [
        "RUDDER artificial tasks evidence"
      ],
      "type": "concept",
      "description": "Experiments I–III show exponential speed-ups; statistically significant improvements (p<1e-12) over baselines as delay increases.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4 Experiments – Artificial Tasks."
    },
    {
      "name": "RUDDER improves PPO performance on delayed-reward Atari games",
      "aliases": [
        "RUDDER Atari evidence"
      ],
      "type": "concept",
      "description": "On Bowling, Solaris, Venture, Seaquest, RUDDER significantly boosts PPO scores; majority of 52 games show higher final scores.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4 Experiments – Atari Games, Table 1."
    },
    {
      "name": "integrate RUDDER reward redistribution during RL fine-tuning to mitigate delayed reward credit assignment",
      "aliases": [
        "apply RUDDER in RL training"
      ],
      "type": "intervention",
      "description": "During the fine-tuning / RL phase, replace original rewards with LSTM-based redistributed rewards so that expected future reward per step is near zero, leading to faster, lower-variance learning.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Implemented inside the learning loop after pre-training; see Section 2 ‘Novel Learning Algorithms’ description of combining reward redistribution with Q-learning, policy gradients.",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Demonstrated on multiple benchmarks but not yet an industry standard (Section 4 Experiments).",
      "node_rationale": "Represents the actionable procedure the paper proposes for practitioners."
    },
    {
      "name": "train an LSTM return decomposition model to automate reward redistribution in RL training",
      "aliases": [
        "LSTM return decomposition intervention"
      ],
      "type": "intervention",
      "description": "Before or during RL, fit an LSTM that predicts episode returns and uses contribution analysis (difference of predictions) to generate redistributed rewards online.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Part of the ongoing RL training pipeline (Section 3 ‘RUDDER using LSTM’).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Validated on artificial tasks and Atari but still prototype research software (Section 4 Experiments).",
      "node_rationale": "Concrete training recipe required to realise RUDDER in practice."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "credit assignment failure in delayed reward reinforcement learning",
      "target_node": "bias in temporal difference learning and high variance in Monte Carlo learning in delayed reward MDPs",
      "description": "Inefficient credit assignment arises because TD bias and MC variance grow exponentially with reward delay.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit mathematical proofs A8 & A10 (Appendix).",
      "edge_rationale": "Section 1 Introduction links the learning failure to these bias/variance mechanisms."
    },
    {
      "type": "mitigated_by",
      "source_node": "bias in temporal difference learning and high variance in Monte Carlo learning in delayed reward MDPs",
      "target_node": "eliminating expected future rewards simplifies Q-value estimation to immediate reward mean",
      "description": "If expected future reward is zero, neither TD bias nor MC variance terms appear, alleviating the problem.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Argument given in Section 1 without empirical proof.",
      "edge_rationale": "Theoretical reasoning immediately after bias/variance discussion."
    },
    {
      "type": "enabled_by",
      "source_node": "eliminating expected future rewards simplifies Q-value estimation to immediate reward mean",
      "target_node": "reward redistribution achieving zero expected future rewards in SDPs",
      "description": "Reward redistribution is proposed as the mechanism to enforce zero expected future reward.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Formalised in Theorem 2 (Section 2).",
      "edge_rationale": "Design choice directly follows theoretical goal."
    },
    {
      "type": "enabled_by",
      "source_node": "return-equivalent sequence-Markov decision processes preserve optimal policies",
      "target_node": "reward redistribution achieving zero expected future rewards in SDPs",
      "description": "Return equivalence guarantees that shifting rewards does not alter optimal policies, making redistribution viable.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicitly proved in Theorem 1.",
      "edge_rationale": "Section 2 argues this property as prerequisite for design."
    },
    {
      "type": "implemented_by",
      "source_node": "reward redistribution achieving zero expected future rewards in SDPs",
      "target_node": "second-order Markov reward redistribution using differences of consecutive Q-values",
      "description": "Optimal redistribution formula uses Q-value differences dependent only on two consecutive state-action pairs.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Mathematical equivalence in Theorem 2, Eq.(1).",
      "edge_rationale": "Section 2 derives this exact implementation."
    },
    {
      "type": "implemented_by",
      "source_node": "return decomposition via contribution analysis for constructing reward redistribution",
      "target_node": "LSTM-based return prediction with contribution analysis (RUDDER implementation)",
      "description": "RUDDER realises return decomposition using an LSTM and difference-of-prediction contribution analysis.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Algorithm fully detailed in Section 3.",
      "edge_rationale": "Section 3 describes LSTM as practical mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "second-order Markov reward redistribution using differences of consecutive Q-values",
      "target_node": "RUDDER speed-up over TD, MC, and MCTS on artificial delayed-reward tasks",
      "description": "Artificial tasks confirm that algorithms using this redistribution learn exponentially faster.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Controlled experiments with statistical tests (Section 4).",
      "edge_rationale": "Experiments I–III isolate the redistribution effect."
    },
    {
      "type": "validated_by",
      "source_node": "LSTM-based return prediction with contribution analysis (RUDDER implementation)",
      "target_node": "RUDDER speed-up over TD, MC, and MCTS on artificial delayed-reward tasks",
      "description": "The full LSTM implementation is what was tested in artificial tasks.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct empirical data.",
      "edge_rationale": "Section 4 reports LSTM-based RUDDER results."
    },
    {
      "type": "validated_by",
      "source_node": "LSTM-based return prediction with contribution analysis (RUDDER implementation)",
      "target_node": "RUDDER improves PPO performance on delayed-reward Atari games",
      "description": "Same implementation evaluated on Atari benchmark.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct experimental evidence.",
      "edge_rationale": "Section 4 Atari Games."
    },
    {
      "type": "motivates",
      "source_node": "RUDDER speed-up over TD, MC, and MCTS on artificial delayed-reward tasks",
      "target_node": "integrate RUDDER reward redistribution during RL fine-tuning to mitigate delayed reward credit assignment",
      "description": "Empirical gains encourage practitioners to adopt RUDDER in training pipelines.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Empirical but limited scale.",
      "edge_rationale": "Section 4 concludes with recommendation."
    },
    {
      "type": "motivates",
      "source_node": "RUDDER improves PPO performance on delayed-reward Atari games",
      "target_node": "integrate RUDDER reward redistribution during RL fine-tuning to mitigate delayed reward credit assignment",
      "description": "Atari results show real-world relevance, justifying the intervention.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Multiple games but research setting.",
      "edge_rationale": "Section 4 discussion."
    },
    {
      "type": "motivates",
      "source_node": "RUDDER speed-up over TD, MC, and MCTS on artificial delayed-reward tasks",
      "target_node": "train an LSTM return decomposition model to automate reward redistribution in RL training",
      "description": "Shows feasibility of learning‐based return decomposition.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Experimental success.",
      "edge_rationale": "Section 3–4 linkage."
    }
  ],
  "meta": [
    {
      "key": "authors",
      "value": [
        "Jose A. Arjona-Medina",
        "Michael Gillhofer",
        "Michael Widrich",
        "Thomas Unterthiner",
        "Johannes Brandstetter",
        "Sepp Hochreiter"
      ]
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1806.07857"
    },
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "title",
      "value": "RUDDER: Return Decomposition for Delayed Rewards"
    },
    {
      "key": "date_published",
      "value": "2018-06-20T00:00:00Z"
    }
  ]
}