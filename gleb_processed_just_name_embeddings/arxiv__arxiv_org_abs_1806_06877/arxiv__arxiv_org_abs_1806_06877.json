{
  "nodes": [
    {
      "name": "Incorrect reward inference in inverse reinforcement learning applications",
      "aliases": [
        "Reward misspecification in IRL",
        "Mislearned reward functions in IRL"
      ],
      "type": "concept",
      "description": "Risk that an autonomous agent trained via inverse reinforcement learning adopts a reward function that does not reflect the true preferences, potentially leading to unsafe or undesirable behaviour.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced throughout Section 3 Primary Challenges as the overarching danger of inaccurate reward learning."
    },
    {
      "name": "Ill-posed reward ambiguity from limited demonstrations in IRL",
      "aliases": [
        "Reward ambiguity in IRL",
        "Degenerate reward solutions"
      ],
      "type": "concept",
      "description": "Because many reward functions can explain a small set of demonstrations, IRL is ill-posed, leading to ambiguity and potential unsafe reward choices.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Section 3.1 Accurate Inference as the first major challenge."
    },
    {
      "name": "Noisy or suboptimal demonstrations impacting reward accuracy in IRL",
      "aliases": [
        "Perturbed demonstrations in IRL",
        "Sub-optimal input trajectories"
      ],
      "type": "concept",
      "description": "Sensor noise or sub-optimal expert actions corrupt demonstrations, degrading the accuracy of inferred rewards.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 5.1.1 Learning from Faulty Input analyses this difficulty."
    },
    {
      "name": "Incomplete model knowledge affecting reward inference in IRL",
      "aliases": [
        "Unknown dynamics in IRL",
        "Missing reward features"
      ],
      "type": "concept",
      "description": "When transition dynamics or relevant reward features are partially unknown, learned rewards may be biased or unsafe.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Sections 6.3.1â€“6.3.2 Incomplete Model detail this challenge."
    },
    {
      "name": "High dimensional state spaces increasing computational burden in IRL",
      "aliases": [
        "Curse of dimensionality in IRL",
        "Scalability issues in IRL"
      ],
      "type": "concept",
      "description": "Large or continuous state spaces make solving the inner MDP and IRL optimisation computationally expensive, hindering safety validation.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined in Section 3.4 Growth in Solution Complexity."
    },
    {
      "name": "High sample complexity requirements in IRL",
      "aliases": [
        "Demonstration inefficiency in IRL",
        "Large data needs for IRL"
      ],
      "type": "concept",
      "description": "Accurate IRL often needs many expert trajectories, which can be costly or unsafe to gather.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 5.2 discusses sample-complexity mitigation."
    },
    {
      "name": "Maximum entropy principle yields least-biased reward distribution in IRL",
      "aliases": [
        "MaxEnt principle for IRL",
        "Entropy regularisation in IRL"
      ],
      "type": "concept",
      "description": "Applying maximum entropy selects the distribution over trajectories/rewards that makes the fewest additional assumptions beyond matching feature expectations, reducing ambiguity.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in Section 4.2 Entropy Optimization."
    },
    {
      "name": "Bayesian posterior over reward functions captures uncertainty in IRL",
      "aliases": [
        "Bayesian IRL uncertainty modelling",
        "Posterior distribution of rewards"
      ],
      "type": "concept",
      "description": "Bayesian formulations treat demonstrations as observations updating a prior over reward functions, explicitly modelling uncertainty and noise.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4.3 Bayesian Update describes this principle."
    },
    {
      "name": "Active learning querying expert at high-entropy states reduces sample complexity in IRL",
      "aliases": [
        "Active BIRL entropy querying",
        "Selective expert querying in IRL"
      ],
      "type": "concept",
      "description": "Measuring state-wise entropy of the reward posterior identifies informative states; querying the expert there reduces data requirements.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Derived from Section 4.3 Active birl (Lopes 2009)."
    },
    {
      "name": "Automatic feature construction via regression boosting reduces manual bias in IRL",
      "aliases": [
        "FIRL feature learning",
        "Boosted feature construction for IRL"
      ],
      "type": "concept",
      "description": "Building reward features automatically (e.g., via regression trees) decreases dependence on handcrafted features and reduces modelling bias.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in Section 4.4 Feature construction using regression."
    },
    {
      "name": "State abstraction via bisimulation metrics enhances generalization in IRL",
      "aliases": [
        "Bisimulation-based abstraction",
        "Kernel methods with state equivalence"
      ],
      "type": "concept",
      "description": "Using bisimulation metrics to group similar states allows policy learning and reward inference to generalize with fewer samples.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4.3 birl via Bisimulation presents this idea."
    },
    {
      "name": "Use maximum entropy optimization to resolve reward ambiguity in IRL",
      "aliases": [
        "MaxEnt IRL design choice"
      ],
      "type": "concept",
      "description": "Design strategy that applies the MaxEnt principle to select a unique, least-biased reward function consistent with demonstrations.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivated in Section 4.2 through the MaxEnt IRL algorithm."
    },
    {
      "name": "Maintain reward uncertainty through Bayesian IRL framework",
      "aliases": [
        "Bayesian IRL design choice"
      ],
      "type": "concept",
      "description": "Design rationale to keep a posterior over rewards so the agent is aware of ambiguity and noisy data.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Justified in Section 4.3."
    },
    {
      "name": "Solicit informative demonstrations using active Bayesian IRL",
      "aliases": [
        "Active querying design"
      ],
      "type": "concept",
      "description": "Design choice that exploits entropy measures to ask the expert for new demonstrations at the most uncertain states.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4.3 Active birl."
    },
    {
      "name": "Automatically learn nonlinear reward features with regression trees",
      "aliases": [
        "Regression-tree reward modelling"
      ],
      "type": "concept",
      "description": "Design approach to generate richer, nonlinear reward features automatically to address missing-feature problems.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "From FIRL method described in Section 4.4."
    },
    {
      "name": "Apply bisimulation-based state aggregation to cut sample complexity",
      "aliases": [
        "Bisimulation aggregation design"
      ],
      "type": "concept",
      "description": "Design rationale to exploit state equivalence so learning need not see every state individually.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4.3 bisimulation approach."
    },
    {
      "name": "Convex entropy-constrained optimization of reward parameters",
      "aliases": [
        "MaxEnt IRL optimisation mechanism"
      ],
      "type": "concept",
      "description": "Implements MaxEnt IRL by solving a convex optimisation that maximises trajectory entropy subject to feature-expectation constraints.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in Eq 6 of Section 4.2."
    },
    {
      "name": "MCMC sampling of reward posterior distribution",
      "aliases": [
        "Random-walk posterior sampling in Bayesian IRL"
      ],
      "type": "concept",
      "description": "Implements Bayesian IRL by randomly walking in reward-function space to approximate the posterior.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation discussed in Ramachandran & Amir 2007, Section 4.3."
    },
    {
      "name": "Entropy-driven state query strategy for expert demonstrations",
      "aliases": [
        "High-entropy state querying"
      ],
      "type": "concept",
      "description": "Selects states with maximal posterior entropy and queries the expert for the correct action, operationalising active BIRL.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4.3 Active birl procedure."
    },
    {
      "name": "Regression tree-based feature generation algorithm (FIRL)",
      "aliases": [
        "FIRL feature generator",
        "Feature regression boosting"
      ],
      "type": "concept",
      "description": "Uses minimal regression trees to discover conjunction features that make the reward hypothesis consistent with demonstrations.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Algorithm steps in Section 4.4."
    },
    {
      "name": "Kernel regression over bisimulation metric for policy generalization",
      "aliases": [
        "Bisimulation kernel regression mechanism"
      ],
      "type": "concept",
      "description": "Applies a kernel built from the bisimulation metric to generalise demonstrated state-action pairs to unseen states.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mechanism described in S.Melo 2010, Section 4.3."
    },
    {
      "name": "Robust performance under noisy demonstrations in helicopter control tasks using MaxEnt IRL",
      "aliases": [
        "MaxEnt IRL helicopter study"
      ],
      "type": "concept",
      "description": "Empirical study shows MaxEnt IRL maintains performance despite sub-optimal or noisy demonstrations in helicopter manoeuvres.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Reported in Section 5.1.1 citing Ziebart et al."
    },
    {
      "name": "Accurate reward posterior in gridworld experiments with Bayesian IRL",
      "aliases": [
        "Bayesian IRL gridworld evidence"
      ],
      "type": "concept",
      "description": "Experiments demonstrate that Bayesian IRL recovers reward functions with low value loss in controlled gridworld settings.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Validation examples in Section 4.3."
    },
    {
      "name": "Lower sample complexity in active BIRL simulated domains",
      "aliases": [
        "Active BIRL sample efficiency evidence"
      ],
      "type": "concept",
      "description": "Simulation studies show that querying high-entropy states reduces the number of trajectories needed to reach a value-loss target.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Results presented in Lopes 2009, Section 5.2."
    },
    {
      "name": "Reduced imitation loss in path planning via automatic feature boosting",
      "aliases": [
        "FIRL path-planning evidence"
      ],
      "type": "concept",
      "description": "Applying boosted feature construction in path-planning tasks lowers the imitation loss versus linear-feature baselines.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Evidence illustrated in Figure 19 and Section 6.4."
    },
    {
      "name": "Improved generalization with fewer demonstrations using bisimulation IRL",
      "aliases": [
        "Bisimulation IRL evidence"
      ],
      "type": "concept",
      "description": "Empirical tests show that bisimulation-based IRL matches expert performance with markedly fewer demonstrations.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 5.2 Figure 10 and accompanying discussion."
    },
    {
      "name": "Implement maximum entropy IRL during reward inference for autonomous agents",
      "aliases": [
        "Deploy MaxEnt IRL"
      ],
      "type": "intervention",
      "description": "During fine-tuning / RLHF phases, fit the reward model via convex maximum-entropy optimisation matching feature counts to reduce ambiguity and improve robustness to noisy data.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "MaxEnt IRL is applied when learning from demonstrations or RL fine-tuning (Section 4.2 templates).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Widely validated in prototype studies such as helicopter control (Section 5.1.1).",
      "node_rationale": "Actionable strategy directly derived from the design and implementation nodes."
    },
    {
      "name": "Apply Bayesian IRL with MCMC to model reward uncertainty in training",
      "aliases": [
        "Deploy Bayesian IRL"
      ],
      "type": "intervention",
      "description": "Run Markov-chain sampling over reward parameters during fine-tuning to track posterior uncertainty and mitigate effects of noisy demonstrations.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Occurs during learning phase when demonstrations are processed (Section 4.3).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Experimental evidence mostly in controlled tasks (Section 4.3 validation).",
      "node_rationale": "Provides uncertainty-aware reward estimation for safety-critical systems."
    },
    {
      "name": "Collect demonstrations via entropy-based active learning queries",
      "aliases": [
        "Active BIRL querying"
      ],
      "type": "intervention",
      "description": "Before training, compute posterior entropy across states and interactively ask experts to act in the most informative states, reducing data needs.",
      "concept_category": null,
      "intervention_lifecycle": "2",
      "intervention_lifecycle_rationale": "Executed during data-collection/pre-training (Section 4.3 Active birl).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Proof-of-concept simulations show sample efficiency gains (Section 5.2).",
      "node_rationale": "Direct operationalisation of active-learning design."
    },
    {
      "name": "Integrate regression tree feature construction into reward modeling pipeline",
      "aliases": [
        "Deploy FIRL feature learning"
      ],
      "type": "intervention",
      "description": "During model-design/pre-training, run FIRL to generate boosted conjunction features and fit a nonlinear reward, reducing handcrafted feature bias.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Feature engineering happens at model design time (Section 4.4).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated in robotics & image-based navigation but not yet large-scale production (Section 6.4 evidence).",
      "node_rationale": "Addresses incomplete-features problem, improving safety."
    },
    {
      "name": "Deploy bisimulation-based state abstraction for efficient IRL training",
      "aliases": [
        "Use bisimulation abstraction"
      ],
      "type": "intervention",
      "description": "Before or during model design, compute bisimulation metrics and aggregate equivalent states, then perform IRL on the abstracted MDP to cut sample and compute cost.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "State-space design decision prior to training (Section 4.3).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Validated in simulation studies; not yet mainstream in large-scale systems (Section 5.2 evidence).",
      "node_rationale": "Mitigates high-dimensional risk pathway."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Incorrect reward inference in inverse reinforcement learning applications",
      "target_node": "Ill-posed reward ambiguity from limited demonstrations in IRL",
      "description": "Ambiguity in reward solutions is a primary reason why reward inference may be wrong.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicitly stated in Section 3.1.",
      "edge_rationale": "Section 3.1 identifies ambiguity as a core cause of inaccurate rewards."
    },
    {
      "type": "caused_by",
      "source_node": "Incorrect reward inference in inverse reinforcement learning applications",
      "target_node": "Noisy or suboptimal demonstrations impacting reward accuracy in IRL",
      "description": "Noisy or sub-optimal inputs distort inferred rewards, leading to wrong behaviour.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 5.1.1 details this linkage.",
      "edge_rationale": "Faulty demonstrations directly undermine reward accuracy."
    },
    {
      "type": "caused_by",
      "source_node": "Incorrect reward inference in inverse reinforcement learning applications",
      "target_node": "Incomplete model knowledge affecting reward inference in IRL",
      "description": "Missing dynamics or features can bias learnt rewards.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 6.3 describes failures due to incomplete models.",
      "edge_rationale": "Model incompleteness is declared a direct source of error."
    },
    {
      "type": "caused_by",
      "source_node": "Incorrect reward inference in inverse reinforcement learning applications",
      "target_node": "High dimensional state spaces increasing computational burden in IRL",
      "description": "Scalability limits sometimes force approximations that degrade reward accuracy.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Implied in Section 3.4 complexity discussion.",
      "edge_rationale": "Large spaces cause optimisation shortcuts, risking mis-learning."
    },
    {
      "type": "caused_by",
      "source_node": "Incorrect reward inference in inverse reinforcement learning applications",
      "target_node": "High sample complexity requirements in IRL",
      "description": "If insufficient demonstrations are available, reward inference errors rise.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Discussion of sample complexity in Sections 3.4 & 5.2.",
      "edge_rationale": "Data scarcity identified as risk driver."
    },
    {
      "type": "caused_by",
      "source_node": "Ill-posed reward ambiguity from limited demonstrations in IRL",
      "target_node": "Maximum entropy principle yields least-biased reward distribution in IRL",
      "description": "Ambiguity motivates the need for a principle (MaxEnt) to choose among many possible rewards.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical link made in Section 4.2 introducing MaxEnt to solve ambiguity.",
      "edge_rationale": "MaxEnt adopted specifically to address ambiguity."
    },
    {
      "type": "mitigated_by",
      "source_node": "Maximum entropy principle yields least-biased reward distribution in IRL",
      "target_node": "Use maximum entropy optimization to resolve reward ambiguity in IRL",
      "description": "Design rationale operationalises the theoretical insight.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct statement in Section 4.2.",
      "edge_rationale": "Design choice follows immediately from theory."
    },
    {
      "type": "implemented_by",
      "source_node": "Use maximum entropy optimization to resolve reward ambiguity in IRL",
      "target_node": "Convex entropy-constrained optimization of reward parameters",
      "description": "Convex optimisation mechanism realises the design.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Algorithmic details given in Eq 6.",
      "edge_rationale": "Mechanism is the concrete realisation."
    },
    {
      "type": "validated_by",
      "source_node": "Convex entropy-constrained optimization of reward parameters",
      "target_node": "Robust performance under noisy demonstrations in helicopter control tasks using MaxEnt IRL",
      "description": "Empirical study shows mechanism achieves intended robustness.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 5.1.1 provides case study evidence.",
      "edge_rationale": "Validation compares mechanism performance to noisy demo challenge."
    },
    {
      "type": "motivates",
      "source_node": "Robust performance under noisy demonstrations in helicopter control tasks using MaxEnt IRL",
      "target_node": "Implement maximum entropy IRL during reward inference for autonomous agents",
      "description": "Successful validation motivates the practical intervention.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Empirical success in Section 5.1.1 used as justification.",
      "edge_rationale": "Validation leads directly to deployment recommendation."
    },
    {
      "type": "caused_by",
      "source_node": "Noisy or suboptimal demonstrations impacting reward accuracy in IRL",
      "target_node": "Bayesian posterior over reward functions captures uncertainty in IRL",
      "description": "Uncertainty from noise motivates Bayesian treatment.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 4.3 frames Bayesian IRL as handling noisy data.",
      "edge_rationale": "Theoretical response to noisy demo problem."
    },
    {
      "type": "mitigated_by",
      "source_node": "Bayesian posterior over reward functions captures uncertainty in IRL",
      "target_node": "Maintain reward uncertainty through Bayesian IRL framework",
      "description": "Design applies Bayesian posterior to keep uncertainty.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 4.3 directly.",
      "edge_rationale": "Design flows from theory."
    },
    {
      "type": "implemented_by",
      "source_node": "Maintain reward uncertainty through Bayesian IRL framework",
      "target_node": "MCMC sampling of reward posterior distribution",
      "description": "Sampling implements the Bayesian design.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Implementation described in Ramachandran & Amir 2007.",
      "edge_rationale": "Mechanism realises design."
    },
    {
      "type": "validated_by",
      "source_node": "MCMC sampling of reward posterior distribution",
      "target_node": "Accurate reward posterior in gridworld experiments with Bayesian IRL",
      "description": "Experiments confirm accuracy of sampled posterior.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 4.3 reports results.",
      "edge_rationale": "Evidence supports mechanism."
    },
    {
      "type": "motivates",
      "source_node": "Accurate reward posterior in gridworld experiments with Bayesian IRL",
      "target_node": "Apply Bayesian IRL with MCMC to model reward uncertainty in training",
      "description": "Positive validation motivates adopting Bayesian IRL in practice.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Link made from empirical to intervention.",
      "edge_rationale": "Evidence underpins recommendation."
    },
    {
      "type": "caused_by",
      "source_node": "High sample complexity requirements in IRL",
      "target_node": "Active learning querying expert at high-entropy states reduces sample complexity in IRL",
      "description": "Sample complexity problem motivates active learning insight.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 5.2 positions active BIRL as sample-efficient.",
      "edge_rationale": "Theory addresses problem."
    },
    {
      "type": "mitigated_by",
      "source_node": "Active learning querying expert at high-entropy states reduces sample complexity in IRL",
      "target_node": "Solicit informative demonstrations using active Bayesian IRL",
      "description": "Design leverages entropy insight for querying.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Design steps in Lopes 2009.",
      "edge_rationale": "Design derived from theory."
    },
    {
      "type": "implemented_by",
      "source_node": "Solicit informative demonstrations using active Bayesian IRL",
      "target_node": "Entropy-driven state query strategy for expert demonstrations",
      "description": "Specific querying strategy implements the design.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Algorithm described in Section 4.3.",
      "edge_rationale": "Mechanism realises design."
    },
    {
      "type": "validated_by",
      "source_node": "Entropy-driven state query strategy for expert demonstrations",
      "target_node": "Lower sample complexity in active BIRL simulated domains",
      "description": "Simulation results confirm sample efficiency.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Evidence in Section 5.2.",
      "edge_rationale": "Empirical validation of mechanism."
    },
    {
      "type": "motivates",
      "source_node": "Lower sample complexity in active BIRL simulated domains",
      "target_node": "Collect demonstrations via entropy-based active learning queries",
      "description": "Validated gains motivate adoption of querying intervention.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical extension from results.",
      "edge_rationale": "Evidence supports practical step."
    },
    {
      "type": "caused_by",
      "source_node": "Incomplete model knowledge affecting reward inference in IRL",
      "target_node": "Automatic feature construction via regression boosting reduces manual bias in IRL",
      "description": "Missing or poor features inspire automated feature-learning insight.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 4.4 presents feature construction as solution.",
      "edge_rationale": "Insight directly addresses incomplete features."
    },
    {
      "type": "mitigated_by",
      "source_node": "Automatic feature construction via regression boosting reduces manual bias in IRL",
      "target_node": "Automatically learn nonlinear reward features with regression trees",
      "description": "Design chooses regression trees for automatic feature learning.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "FIRL algorithm description.",
      "edge_rationale": "Design instantiates insight."
    },
    {
      "type": "implemented_by",
      "source_node": "Automatically learn nonlinear reward features with regression trees",
      "target_node": "Regression tree-based feature generation algorithm (FIRL)",
      "description": "FIRL algorithm implements the design rationale.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Implementation described in Section 4.4.",
      "edge_rationale": "Algorithm is mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Regression tree-based feature generation algorithm (FIRL)",
      "target_node": "Reduced imitation loss in path planning via automatic feature boosting",
      "description": "Empirical results show performance gains using FIRL.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Figure 19 evidence.",
      "edge_rationale": "Evidence validates mechanism."
    },
    {
      "type": "motivates",
      "source_node": "Reduced imitation loss in path planning via automatic feature boosting",
      "target_node": "Integrate regression tree feature construction into reward modeling pipeline",
      "description": "Positive results motivate integrating FIRL into practice.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical follow-on from validation.",
      "edge_rationale": "Evidence supports intervention adoption."
    },
    {
      "type": "caused_by",
      "source_node": "High dimensional state spaces increasing computational burden in IRL",
      "target_node": "State abstraction via bisimulation metrics enhances generalization in IRL",
      "description": "High dimensionality motivates abstracting equivalent states.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 5.2 references bisimulation to combat complexity.",
      "edge_rationale": "Insight tackles complexity problem."
    },
    {
      "type": "mitigated_by",
      "source_node": "State abstraction via bisimulation metrics enhances generalization in IRL",
      "target_node": "Apply bisimulation-based state aggregation to cut sample complexity",
      "description": "Design applies abstraction insight.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Design discussion in Section 4.3.",
      "edge_rationale": "Design arises directly from insight."
    },
    {
      "type": "implemented_by",
      "source_node": "Apply bisimulation-based state aggregation to cut sample complexity",
      "target_node": "Kernel regression over bisimulation metric for policy generalization",
      "description": "Kernel regression mechanism realises the abstraction design.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Implementation details in S.Melo 2010.",
      "edge_rationale": "Mechanism instantiates design."
    },
    {
      "type": "validated_by",
      "source_node": "Kernel regression over bisimulation metric for policy generalization",
      "target_node": "Improved generalization with fewer demonstrations using bisimulation IRL",
      "description": "Experiments show generalisation benefits.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Figure 10 evidence.",
      "edge_rationale": "Evidence confirms mechanism effectiveness."
    },
    {
      "type": "motivates",
      "source_node": "Improved generalization with fewer demonstrations using bisimulation IRL",
      "target_node": "Deploy bisimulation-based state abstraction for efficient IRL training",
      "description": "Successful validation encourages adoption of bisimulation abstraction.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical conclusion from evidence.",
      "edge_rationale": "Evidence leads to recommended intervention."
    }
  ],
  "meta": [
    {
      "key": "authors",
      "value": [
        "Saurabh Arora",
        "Prashant Doshi"
      ]
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1806.06877"
    },
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "title",
      "value": "A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress"
    },
    {
      "key": "date_published",
      "value": "2018-06-18T00:00:00Z"
    }
  ]
}