Summary
Data source overview: The paper formalises the “corrupt-reward problem” for reinforcement-learning agents, proves that ordinary RL (incl. Bayesian and CR agents) suffers near-maximal true-reward regret even under strong simplifying assumptions, and develops two main solution families: (1) Decoupled feedback/value-learning frameworks (CIRL, SSRL, LVFS) that cross-check rewards across states, and (2) Quantilisation, which randomises over the top-quantile of apparently good policies/states to limit adversarial exploitation. Formal theorems (11,16,19,20,23) and grid-world experiments support each proposal.

EXTRACTION CONFIDENCE[83]

Inference strategy justification: All nodes are directly grounded in section headings, theorem statements or experiment sections. Moderate inference (confidence 2) was only used to derive implementation-stage intervention names from the formal concepts (e.g. mapping “δ–quantilising agent” to an actionable deployment-phase control).

Extraction completeness: 24 nodes capture every causal step from the top-level risk to the four actionable interventions, inter-linking through assumptions, design rationales, mechanisms and validation. All edges connect and no duplicates or satellites remain.

Key limitations: 1) Paper does not give implementation engineering details, so mechanism nodes are abstract; 2) maturity levels are estimated from theoretical/experimental status; 3) only main solution lines extracted, minor side remarks omitted.

JSON fabric below follows required schema.