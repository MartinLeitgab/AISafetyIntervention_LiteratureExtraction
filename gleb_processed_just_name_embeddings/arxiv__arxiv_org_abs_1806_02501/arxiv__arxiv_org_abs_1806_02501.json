{
  "nodes": [
    {
      "name": "Reward misspecification in robot planning across environments",
      "aliases": [
        "reward design errors in robotics",
        "proxy reward mismatch"
      ],
      "type": "concept",
      "description": "Robots optimising poorly-specified reward functions can behave undesirably or unsafely when deployed in novel environments.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in I Introduction as the core failure mode motivating the work."
    },
    {
      "name": "Designer overburden when specifying joint reward across diverse environments",
      "aliases": [
        "reward tuning burden",
        "joint reward design difficulty"
      ],
      "type": "concept",
      "description": "Specifying a single reward that yields correct behaviour in all training environments requires many frustrating tuning iterations.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in I Introduction describing iterative tuning pain-points."
    },
    {
      "name": "Proxy reward non-generalization to novel environments",
      "aliases": [
        "reward generalization failure",
        "training-test reward mismatch"
      ],
      "type": "concept",
      "description": "A reward that induces optimal behaviour in training environments can fail when test environments contain unseen conditions.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in II Related Work under Inverse Reward Design examples (e.g., wet road case)."
    },
    {
      "name": "Simpler per-environment reward specification reduces cognitive load",
      "aliases": [
        "independent reward ease",
        "single-environment reward simplicity"
      ],
      "type": "concept",
      "description": "Designers find it easier to express correct trade-offs when focusing on one environment at a time.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Posited in I Introduction with household motion-planning example."
    },
    {
      "name": "Independently specified proxy rewards provide conditional evidence of true reward",
      "aliases": [
        "proxies as observations",
        "conditional independence of proxies"
      ],
      "type": "concept",
      "description": "Each per-environment proxy reward is treated as an observation conditionally independent given the true reward parameters.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Formalised in III Independent Reward Design, Eq. (2)."
    },
    {
      "name": "Divide-and-conquer reward design combining independent proxies via Bayesian inference",
      "aliases": [
        "independent reward design strategy",
        "divide-and-conquer reward tuning"
      ],
      "type": "concept",
      "description": "Overall approach: specify separate rewards per environment and infer a common reward posterior, lowering design effort.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined at end of I Introduction and detailed in III."
    },
    {
      "name": "Treat designer as approximately optimal via Boltzmann observation model",
      "aliases": [
        "β-rational designer assumption",
        "Boltzmann observation for reward"
      ],
      "type": "concept",
      "description": "Assumes designers choose proxy rewards with probability exponential in true reward obtained by resulting optimal trajectory, parameterised by β.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Equation (3) in Section III defines this assumption."
    },
    {
      "name": "Bayesian posterior inference over true reward using IRD observation model and Monte Carlo sampling",
      "aliases": [
        "posterior over reward via IRD",
        "Monte-Carlo IRD inference"
      ],
      "type": "concept",
      "description": "Computes a posterior over reward parameters with an IRD-based likelihood, Monte-Carlo approximation of the partition function and Metropolis sampling.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation details in Section III (Eq. 4–6)."
    },
    {
      "name": "Expected reward planning with posterior mean",
      "aliases": [
        "planning with mean reward",
        "posterior mean optimisation"
      ],
      "type": "concept",
      "description": "Robot plans by maximising expected reward under the posterior, equivalent to optimising the mean reward parameters.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Equation (7) in Section III describes this planner."
    },
    {
      "name": "Risk-averse planning using reward posterior uncertainty",
      "aliases": [
        "posterior risk-averse planner",
        "reward uncertainty cautious planning"
      ],
      "type": "concept",
      "description": "Alternative planner uses the posterior to act conservatively in regions of high reward uncertainty.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mentioned in Section III referencing prior IRD work."
    },
    {
      "name": "Grid world user study showing 51% faster design and 69.8% lower regret",
      "aliases": [
        "grid-world study results",
        "user study 1"
      ],
      "type": "concept",
      "description": "30-participant within-subjects study demonstrated significant reductions in time and regret and higher ease ratings for independent design.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Results reported in Section V-B Analysis."
    },
    {
      "name": "Robot manipulator user study confirming independent design benefits",
      "aliases": [
        "Jaco arm study",
        "user study 2"
      ],
      "type": "concept",
      "description": "60-participant study on a 7-DOF arm showed lower regret, faster design time and higher subjective ease for independent reward design.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in Section VI-B Analysis."
    },
    {
      "name": "Sensitivity analysis across feasible sets, feature subsets and environment counts demonstrating robustness",
      "aliases": [
        "robustness sensitivity studies",
        "user study 3"
      ],
      "type": "concept",
      "description": "Three additional experiments varied feasible set size, features per environment, and number of environments, consistently finding advantages for independent design.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Summarised in Section VII Sensitivity Analysis."
    },
    {
      "name": "Adopt independent reward design workflow in model design phase for robot tasks",
      "aliases": [
        "use independent reward design",
        "divide-and-conquer reward process"
      ],
      "type": "intervention",
      "description": "During reward specification, have designers create separate proxy rewards per training environment instead of a single joint reward.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Occurs while the reward function is being initially designed (I Introduction, V-A).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Validated by two user studies but not yet deployed in industrial settings (Sections V & VI).",
      "node_rationale": "Central actionable recommendation derived from validation evidence."
    },
    {
      "name": "Apply Bayesian inference over independent proxy rewards before deployment",
      "aliases": [
        "compute reward posterior",
        "IRD-based reward inference"
      ],
      "type": "intervention",
      "description": "Infer a posterior distribution over true reward parameters using the IRD observation model, Monte-Carlo partition-function approximation and Metropolis sampling.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Performed after proxy rewards are collected and before final policy planning (Section III).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Implemented and empirically evaluated in controlled studies but limited real-world deployment.",
      "node_rationale": "Transforms independent proxy rewards into usable reward information."
    },
    {
      "name": "Plan robot behavior using mean of reward posterior during deployment",
      "aliases": [
        "posterior mean planning deployment",
        "expected reward execution"
      ],
      "type": "intervention",
      "description": "Deploy robots that optimise the mean reward parameters obtained from the posterior when acting in novel environments.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Policy execution at deployment relies on previously computed posterior (Section III Eq. 7).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Evaluated in simulation/user studies but not yet field-deployed.",
      "node_rationale": "Primary execution-time control scheme validated in the studies."
    },
    {
      "name": "Employ risk-averse planning against reward uncertainty in deployment",
      "aliases": [
        "risk-averse posterior planning",
        "uncertainty-aware planning"
      ],
      "type": "intervention",
      "description": "Use planning algorithms that explicitly penalise high-uncertainty regions of the reward posterior to improve safety in novel conditions.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Executed by the robot during task planning after posterior estimation (Section III discussion).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Mentioned as an option; lacking empirical validation in this paper.",
      "node_rationale": "Safety-oriented extension building on reward posterior."
    },
    {
      "name": "Select representative subset of training environments to maximize independent reward design effectiveness",
      "aliases": [
        "environment subset selection",
        "training environment curation"
      ],
      "type": "intervention",
      "description": "From a very large pool of candidate environments, choose a moderate-sized, diverse subset that keeps per-environment rewards simple and collective complexity high.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Part of experimental setup before designers create proxy rewards (VIII Discussion).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Proposed future work without current empirical support.",
      "node_rationale": "Addresses scalability limitation highlighted in Discussion."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Reward misspecification in robot planning across environments",
      "target_node": "Designer overburden when specifying joint reward across diverse environments",
      "description": "Complex joint reward tuning leads to errors that manifest as misspecified rewards.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Stated qualitatively in I Introduction; no quantitative proof.",
      "edge_rationale": "Introduction explains iterative tuning causes mis-specification."
    },
    {
      "type": "caused_by",
      "source_node": "Reward misspecification in robot planning across environments",
      "target_node": "Proxy reward non-generalization to novel environments",
      "description": "When proxies fail to generalise, the operational reward is effectively misspecified in new contexts.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Example of self-driving car in II Related Work.",
      "edge_rationale": "Mis-generalisation directly yields misspecification risk."
    },
    {
      "type": "mitigated_by",
      "source_node": "Designer overburden when specifying joint reward across diverse environments",
      "target_node": "Simpler per-environment reward specification reduces cognitive load",
      "description": "Splitting reward design alleviates the need to satisfy all constraints at once.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Rationale articulated in I Introduction; supported by time reductions in studies.",
      "edge_rationale": "Paper claims single-environment design is easier."
    },
    {
      "type": "mitigated_by",
      "source_node": "Proxy reward non-generalization to novel environments",
      "target_node": "Independently specified proxy rewards provide conditional evidence of true reward",
      "description": "Treating multiple proxies as evidence allows later correction and generalisation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section III formal argument; empirical support indirect.",
      "edge_rationale": "Combining evidence addresses generalisation failure."
    },
    {
      "type": "enabled_by",
      "source_node": "Simpler per-environment reward specification reduces cognitive load",
      "target_node": "Divide-and-conquer reward design combining independent proxies via Bayesian inference",
      "description": "Ease of per-environment specification motivates the overall divide-and-conquer strategy.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical connection explicitly stated in I Introduction.",
      "edge_rationale": "Strategy builds on the insight of easier per-environment design."
    },
    {
      "type": "enabled_by",
      "source_node": "Independently specified proxy rewards provide conditional evidence of true reward",
      "target_node": "Treat designer as approximately optimal via Boltzmann observation model",
      "description": "Observation model formalises how each proxy becomes probabilistic evidence.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Equation (3) directly uses conditional independence assumption.",
      "edge_rationale": "Boltzmann model operationalises the evidence concept."
    },
    {
      "type": "implemented_by",
      "source_node": "Divide-and-conquer reward design combining independent proxies via Bayesian inference",
      "target_node": "Bayesian posterior inference over true reward using IRD observation model and Monte Carlo sampling",
      "description": "The inference algorithm realises the divide-and-conquer design rationale.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Detailed algorithmic description in Section III.",
      "edge_rationale": "Algorithm is concrete implementation of rationale."
    },
    {
      "type": "implemented_by",
      "source_node": "Treat designer as approximately optimal via Boltzmann observation model",
      "target_node": "Bayesian posterior inference over true reward using IRD observation model and Monte Carlo sampling",
      "description": "Boltzmann likelihood forms the core of the Bayesian inference computation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Equation linkage explicit in Section III.",
      "edge_rationale": "Observation model feeds into posterior inference."
    },
    {
      "type": "validated_by",
      "source_node": "Bayesian posterior inference over true reward using IRD observation model and Monte Carlo sampling",
      "target_node": "Grid world user study showing 51% faster design and 69.8% lower regret",
      "description": "Study demonstrates algorithm yields lower regret and design time.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Statistical significance p<0.01 reported in Section V-B.",
      "edge_rationale": "Empirical evidence supports algorithm’s effectiveness."
    },
    {
      "type": "validated_by",
      "source_node": "Bayesian posterior inference over true reward using IRD observation model and Monte Carlo sampling",
      "target_node": "Robot manipulator user study confirming independent design benefits",
      "description": "Second study replicates benefits in a real robot domain.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Multiple p<0.0001 results in Section VI-B.",
      "edge_rationale": "Further validation across domain."
    },
    {
      "type": "validated_by",
      "source_node": "Expected reward planning with posterior mean",
      "target_node": "Grid world user study showing 51% faster design and 69.8% lower regret",
      "description": "Performance metrics are computed using planning with the posterior mean.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Method described in Section V-A experiment design.",
      "edge_rationale": "Study outcome depends on this planner."
    },
    {
      "type": "validated_by",
      "source_node": "Expected reward planning with posterior mean",
      "target_node": "Robot manipulator user study confirming independent design benefits",
      "description": "Manipulator study also uses posterior mean planning.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section VI-A states optimisation procedure.",
      "edge_rationale": "Planner integral to demonstrated benefit."
    },
    {
      "type": "validated_by",
      "source_node": "Bayesian posterior inference over true reward using IRD observation model and Monte Carlo sampling",
      "target_node": "Sensitivity analysis across feasible sets, feature subsets and environment counts demonstrating robustness",
      "description": "Sensitivity experiments measure regret after posterior inference.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Multiple ANOVA results reported in Section VII.",
      "edge_rationale": "Robustness testing validates inference mechanism."
    },
    {
      "type": "motivates",
      "source_node": "Grid world user study showing 51% faster design and 69.8% lower regret",
      "target_node": "Adopt independent reward design workflow in model design phase for robot tasks",
      "description": "Positive empirical results encourage adoption of independent design.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Statistically significant improvements support recommendation.",
      "edge_rationale": "Empirical evidence → practice change."
    },
    {
      "type": "motivates",
      "source_node": "Robot manipulator user study confirming independent design benefits",
      "target_node": "Adopt independent reward design workflow in model design phase for robot tasks",
      "description": "Real-robot results further motivate adoption.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Replicated benefits in second domain.",
      "edge_rationale": "Cross-domain validation strengthens case."
    },
    {
      "type": "motivates",
      "source_node": "Grid world user study showing 51% faster design and 69.8% lower regret",
      "target_node": "Plan robot behavior using mean of reward posterior during deployment",
      "description": "Low-regret outcomes arise when planning with posterior mean.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Outcome metrics computed with mean planner.",
      "edge_rationale": "Demonstrated efficacy motivates deployment."
    },
    {
      "type": "motivates",
      "source_node": "Robot manipulator user study confirming independent design benefits",
      "target_node": "Plan robot behavior using mean of reward posterior during deployment",
      "description": "Manipulator results indicate planner’s practical viability.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Lower regret achieved with mean planner.",
      "edge_rationale": "Empirical benefit motivates use."
    },
    {
      "type": "motivates",
      "source_node": "Sensitivity analysis across feasible sets, feature subsets and environment counts demonstrating robustness",
      "target_node": "Select representative subset of training environments to maximize independent reward design effectiveness",
      "description": "Analysis highlights scenarios where moderate environment sets work best, motivating environment selection.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section VII discussion suggests selecting subsets as future work.",
      "edge_rationale": "Evidence identifies optimisation opportunity."
    },
    {
      "type": "motivates",
      "source_node": "Bayesian posterior inference over true reward using IRD observation model and Monte Carlo sampling",
      "target_node": "Apply Bayesian inference over independent proxy rewards before deployment",
      "description": "Inference mechanism is necessary to realise the divide-and-conquer approach in practice.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Algorithmic success demonstrated in studies.",
      "edge_rationale": "Central implementation step."
    },
    {
      "type": "motivates",
      "source_node": "Risk-averse planning using reward posterior uncertainty",
      "target_node": "Employ risk-averse planning against reward uncertainty in deployment",
      "description": "Conceptual mechanism underlies the proposed safety-oriented planning intervention.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Mentioned without empirical validation; light inference.",
      "edge_rationale": "Theoretical suggestion leads to intervention."
    }
  ],
  "meta": [
    {
      "key": "title",
      "value": "Simplifying Reward Design through Divide-and-Conquer"
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1806.02501"
    },
    {
      "key": "date_published",
      "value": "2018-06-07T00:00:00Z"
    },
    {
      "key": "authors",
      "value": [
        "Ellis Ratner",
        "Dylan Hadfield-Menell",
        "Anca D. Dragan"
      ]
    },
    {
      "key": "source",
      "value": "arxiv"
    }
  ]
}