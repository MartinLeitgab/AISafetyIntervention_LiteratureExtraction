{
  "nodes": [
    {
      "name": "suboptimal exploration in universal reinforcement learning agents",
      "aliases": [
        "insufficient exploration in URL",
        "poor exploration behaviour"
      ],
      "type": "concept",
      "description": "URL agents like AIXI often fail to visit enough novel states, leading to low long-run reward and unsafe behaviour.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in 2 Literature Survey and highlighted in 4 Experiments where agents get stuck."
    },
    {
      "name": "performance degradation from bad model priors in Bayesian reinforcement learning",
      "aliases": [
        "prior misspecification effects",
        "bad priors harming performance"
      ],
      "type": "concept",
      "description": "Choice of an inappropriate prior (e.g. uniform over distant goals) can drive Bayesian RL agents to poor asymptotic reward.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Section 2.3 where AIXI can be made to perform badly by a poor prior."
    },
    {
      "name": "computational intractability of planning in partially observable environments for URL",
      "aliases": [
        "planning intractability in POMDPs",
        "high compute cost of expectimax"
      ],
      "type": "concept",
      "description": "Exact expectimax planning over long horizons is provably infeasible for URL agents in realistic POMDPs.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.3 ‘Planning’ quantifies ρUCT worst-case time O(κm|M||A|)."
    },
    {
      "name": "noise-induced distraction in entropy-seeking reinforcement learning agents",
      "aliases": [
        "hooked on noise problem",
        "entropy-seeker distraction"
      ],
      "type": "concept",
      "description": "Agents maximising percept entropy can become trapped by stochastic noise sources, ignoring task reward.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4 Experiments Figure 3 and text on Square/Shannon KSA."
    },
    {
      "name": "lack of asymptotic optimality in AIXI agent model",
      "aliases": [
        "AIXI non-optimality",
        "AIXI fails weak optimality"
      ],
      "type": "concept",
      "description": "AIXI does not satisfy weak asymptotic optimality, limiting long-term safety guarantees.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in 2.3 Algorithms before BayesExp/TS are introduced."
    },
    {
      "name": "insufficient intrinsic motivation design in AIXI",
      "aliases": [
        "AIXI lacks intrinsic motivation",
        "exploration gap in AIXI"
      ],
      "type": "concept",
      "description": "AIXI’s reward-only utility omits an exploration drive, causing under-exploration.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "2.3 Algorithms contrasts AIXI with knowledge-seeking agents."
    },
    {
      "name": "posterior collapse causing premature exploitation in mixture models",
      "aliases": [
        "early posterior collapse",
        "single-hypothesis fixation"
      ],
      "type": "concept",
      "description": "When the posterior rapidly focuses on one environment, the agent stops exploring even if better rewards exist elsewhere.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Observed in 4 Experiments where mixture model Mloc makes AIXI settle."
    },
    {
      "name": "Monte Carlo tree search horizon limits agent foresight",
      "aliases": [
        "short planning horizon",
        "finite-depth MCTS"
      ],
      "type": "concept",
      "description": "Finite MCTS depth m << effective horizon Hγ causes agents to ignore distant high-value states.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "3.3 Agent Approximations discusses practical horizon limits."
    },
    {
      "name": "entropy-seeking utility misaligned with information gain",
      "aliases": [
        "entropy vs info gain mis-alignment"
      ],
      "type": "concept",
      "description": "Utilities maximising Shannon/Square entropy reward noise rather than epistemic value, leading to unsafe policies.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 2.3 Definition 5 and later experiments."
    },
    {
      "name": "uniform prior over goal locations disperses probability mass",
      "aliases": [
        "uninformative spatial prior"
      ],
      "type": "concept",
      "description": "A flat prior treats distant goal positions as likely, biasing planning toward deep exploration and lower early reward.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in 4 Experiments (MDL vs uniform-prior AIξ)."
    },
    {
      "name": "weak asymptotic optimality via exploration schedules in Bayesian RL",
      "aliases": [
        "exploration schedule theory",
        "ε-schedule optimality"
      ],
      "type": "concept",
      "description": "By interleaving exploitation with scheduled exploration bursts an agent can provably achieve weak asymptotic optimality.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Derived in Section 2.3 for BayesExp and Thompson Sampling."
    },
    {
      "name": "information gain utility avoids noise attraction in stochastic environments",
      "aliases": [
        "IG utility robustness",
        "info-gain avoids noise"
      ],
      "type": "concept",
      "description": "Utility based on reduction in model entropy (KL-KSA) targets epistemic value, not percept entropy, preventing noise addiction.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in Definition 4 and evaluated in Section 4."
    },
    {
      "name": "Occam bias improves early performance through MDL principle",
      "aliases": [
        "MDL Occam bias",
        "simplicity bias benefit"
      ],
      "type": "concept",
      "description": "Selecting the simplest unfalsified model concentrates probability mass on nearby goals, accelerating reward acquisition.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 2.3 Algorithm 4 and experiments Fig 6."
    },
    {
      "name": "Dirichlet environment model increases epistemic uncertainty for exploration",
      "aliases": [
        "factorised Dirichlet model boosts exploration"
      ],
      "type": "concept",
      "description": "A factorised Dirichlet model maintains high posterior entropy over maze layout, promoting continued exploration.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.2 Models contrasts MDirichlet with Mloc."
    },
    {
      "name": "finite-horizon approximation acceptable for planning in MCTS",
      "aliases": [
        "effective horizon surrogate",
        "short-horizon acceptability"
      ],
      "type": "concept",
      "description": "Using planning horizon m as a proxy for Hγ can make planning tractable while retaining useful optimality properties.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.3 Effective Horizon explains approximation."
    },
    {
      "name": "information-gain-triggered exploration bursts in Bayes agents",
      "aliases": [
        "BayesExp exploration rationale"
      ],
      "type": "concept",
      "description": "Trigger exploration whenever expected information gain exceeds a decaying threshold εt, following BayesExp design.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Algorithm 2 BayesExp in 2.3 Algorithms."
    },
    {
      "name": "hypothesis commitment through posterior sampling",
      "aliases": [
        "Thompson sampling rationale",
        "posterior sampling design"
      ],
      "type": "concept",
      "description": "Commit to a single sampled hypothesis for one effective horizon to balance exploration and exploitation.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Algorithm 3 Thompson Sampling in 2.3 Algorithms."
    },
    {
      "name": "simplest unfalsified model selection via MDL",
      "aliases": [
        "MDL greedy model selection"
      ],
      "type": "concept",
      "description": "Greedily pick the model with minimum K complexity penalised by log-likelihood until falsified.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Algorithm 4 MDL Agent discussion."
    },
    {
      "name": "intrinsic information gain utility driving exploration",
      "aliases": [
        "KL-KSA design rationale"
      ],
      "type": "concept",
      "description": "Define utility as expected reduction in posterior entropy, incentivising epistemic exploration.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Definition 4 and comparative discussion."
    },
    {
      "name": "rich model class maintains posterior entropy for sustained exploration",
      "aliases": [
        "model class enrichment rationale"
      ],
      "type": "concept",
      "description": "Using a less-constrained model (Dirichlet) prevents early posterior collapse, sustaining uncertainty and exploration.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "3.2 Models motivation for MDirichlet."
    },
    {
      "name": "Monte Carlo tree search planning with mixture models for POMDPs",
      "aliases": [
        "ρUCT planning rationale"
      ],
      "type": "concept",
      "description": "Use ρUCT to approximate expectimax over Bayesian mixture despite perceptual aliasing.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "3.3 Planning describes rationale for ρUCT."
    },
    {
      "name": "BayesExp algorithm with ε-schedule and effective horizon planning",
      "aliases": [
        "BayesExp mechanism"
      ],
      "type": "concept",
      "description": "Concrete algorithm interleaving KL-KSA exploration bursts with Bayes exploitation as per Algorithm 2.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Algorithm 2 details."
    },
    {
      "name": "Thompson sampling with effective horizon Monte Carlo tree search",
      "aliases": [
        "TS mechanism",
        "posterior sampling implementation"
      ],
      "type": "concept",
      "description": "Implements posterior sampling for one effective horizon followed by MCTS planning within the sampled model.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Algorithm 3 implementation specifics."
    },
    {
      "name": "MDL agent with Kolmogorov-regularized likelihood selection",
      "aliases": [
        "MDL implementation"
      ],
      "type": "concept",
      "description": "Operationalises MDL by minimising K(ν)–λlog likelihood and following the selected model’s optimal policy.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Algorithm 4 step-by-step."
    },
    {
      "name": "KL-KSA agent utilizing information gain utility function",
      "aliases": [
        "KL knowledge-seeking implementation"
      ],
      "type": "concept",
      "description": "Agent computes utility = entropy drop of posterior, driving exploration without relying on extrinsic reward.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Definition 4 & implementation comment."
    },
    {
      "name": "Dirichlet gridworld model with factorized tile priors",
      "aliases": [
        "factorised Dirichlet model"
      ],
      "type": "concept",
      "description": "Environment model factorises across tiles with Beta priors, enabling O(1) updates and high uncertainty.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "3.2 Models elaboration."
    },
    {
      "name": "ρUCT Monte Carlo tree search planner with finite horizon",
      "aliases": [
        "ρUCT planner",
        "finite-horizon MCTS"
      ],
      "type": "concept",
      "description": "History-based UCT variant approximating expectimax; balances exploration via UCB and computational budget.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "3.3 Planning equation (9)."
    },
    {
      "name": "proof of weak asymptotic optimality for BayesExp in general environments",
      "aliases": [
        "BayesExp theoretical proof"
      ],
      "type": "concept",
      "description": "Mathematical result showing BayesExp achieves Vπ→V* in mean across all ν∈M.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Referenced in 2.3 Algorithms just after Algorithm 2."
    },
    {
      "name": "gridworld experiments showing KL-KSA outperforming entropy-seeking agents",
      "aliases": [
        "KL-KSA experimental evidence"
      ],
      "type": "concept",
      "description": "Section 4 Figures 3-4 demonstrate higher exploration fraction and robustness to noise.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "4 Experiments, ‘Intrinsic motivation’ subsection."
    },
    {
      "name": "experiments showing Dirichlet model improves AIXI exploration",
      "aliases": [
        "Dirichlet model evidence"
      ],
      "type": "concept",
      "description": "Figure 2 contrasts MC-AIXI vs MC-AIXI-Dirichlet with higher exploration and reward variance.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "4 Experiments ‘Model class’ results."
    },
    {
      "name": "experiments showing Thompson sampling underperforms AIξ under equal sample budget",
      "aliases": [
        "TS performance evidence"
      ],
      "type": "concept",
      "description": "Figure 5a reports lower mean reward and higher variance for TS when κ identical.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "4 Experiments ‘Exploration’ section."
    },
    {
      "name": "experiments showing MDL agent outperforms uniform-prior AIξ in deterministic gridworld",
      "aliases": [
        "MDL performance evidence"
      ],
      "type": "concept",
      "description": "Figure 6 left shows faster reward accumulation for MDL due to simplicity bias.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "4 Experiments ‘Occam bias’ subsection."
    },
    {
      "name": "fine-tune RL agents using BayesExp exploration bursts to achieve asymptotic optimality",
      "aliases": [
        "apply BayesExp during training"
      ],
      "type": "intervention",
      "description": "During fine-tuning/RL stage interleave KL-KSA exploration bursts according to ε-schedule and effective horizon.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Implements Algorithm 2 during the RL fine-tuning phase (2.3 Algorithms).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Supported by formal proofs but limited empirical evaluation (Section 2.3).",
      "node_rationale": "Targets lack of asymptotic optimality and poor exploration."
    },
    {
      "name": "train agents with KL-KSA intrinsic motivation to enhance exploration without noise attraction",
      "aliases": [
        "use KL knowledge seeking"
      ],
      "type": "intervention",
      "description": "Replace or augment reward with information-gain utility during RL fine-tuning to drive safe exploration.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Utility function is applied during policy optimisation (2.3 Definitions & 4 Experiments).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Demonstrated empirically in gridworlds (Fig 3-4).",
      "node_rationale": "Mitigates suboptimal exploration and noise distraction."
    },
    {
      "name": "design environment models with factorized Dirichlet priors to stimulate exploration during planning",
      "aliases": [
        "Dirichlet model adoption"
      ],
      "type": "intervention",
      "description": "At model-design time choose a factorised Dirichlet world model so posterior retains entropy and supports exploration.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Model structure is chosen before training begins (3.2 Models).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Validated in Section 4 Figure 2 but not yet deployed at scale.",
      "node_rationale": "Addresses premature exploitation from posterior collapse."
    },
    {
      "name": "adopt Thompson sampling for policy selection with adequate planning horizon adjustments",
      "aliases": [
        "apply TS with horizon tuning"
      ],
      "type": "intervention",
      "description": "Use posterior sampling but allocate compute budget to extend planning horizon m or time budget to maintain performance.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Sampling and planning occur during RL training (2.3 Algorithm 3).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Mixed empirical results in Section 4; requires further tuning.",
      "node_rationale": "Provides an asymptotically optimal exploration strategy if horizon issues are solved."
    },
    {
      "name": "integrate MDL-based model selection into Bayesian RL to bias towards simpler hypotheses",
      "aliases": [
        "apply MDL bias"
      ],
      "type": "intervention",
      "description": "During fine-tuning, greedily follow the simplest unfalsified model to speed up early reward acquisition.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Model selection occurs iteratively during training (Algorithm 4).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Empirically supported on deterministic tasks (Fig 6).",
      "node_rationale": "Mitigates performance loss from bad uniform priors."
    },
    {
      "name": "adopt Monte Carlo tree search with finite horizon approximation for POMDP planning in RL agents",
      "aliases": [
        "apply ρUCT planner"
      ],
      "type": "intervention",
      "description": "Embed ρUCT with carefully chosen horizon and sample budget to make planning tractable yet effective.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Planner choice is a model-design decision (3.3 Planning).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Widely used in academic prototypes; shown effective in this paper’s experiments.",
      "node_rationale": "Addresses planning intractability risk."
    },
    {
      "name": "restrict entropy-seeking agents from high-entropy noise sources during deployment",
      "aliases": [
        "shield entropy seekers from noise"
      ],
      "type": "intervention",
      "description": "At deployment time filter or down-weight observations from artificial noise channels to prevent distraction.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Implemented as a deployment-time observation filter.",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Proposed as safety mitigation; not experimentally tested here.",
      "node_rationale": "Reduces risk of noise-induced distraction identified in experiments."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "suboptimal exploration in universal reinforcement learning agents",
      "target_node": "insufficient intrinsic motivation design in AIXI",
      "description": "Missing intrinsic drive leads directly to poor exploration.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicitly discussed in 2.3 Algorithms.",
      "edge_rationale": "AIXI relies solely on extrinsic reward, causing insufficient exploration."
    },
    {
      "type": "caused_by",
      "source_node": "suboptimal exploration in universal reinforcement learning agents",
      "target_node": "posterior collapse causing premature exploitation in mixture models",
      "description": "Once posterior collapses the agent exploits a local optimum.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Observed empirically in Figure 2.",
      "edge_rationale": "Empirical evidence shows early exploitation following collapse."
    },
    {
      "type": "mitigated_by",
      "source_node": "insufficient intrinsic motivation design in AIXI",
      "target_node": "information gain utility avoids noise attraction in stochastic environments",
      "description": "Using information-gain utility supplies the missing intrinsic drive.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Argued conceptually in Section 2.3.",
      "edge_rationale": "KL-KSA adds intrinsic info-gain motivation."
    },
    {
      "type": "refined_by",
      "source_node": "information gain utility avoids noise attraction in stochastic environments",
      "target_node": "intrinsic information gain utility driving exploration",
      "description": "Insight is operationalised as a formal utility definition.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Definition 4 provides formalisation.",
      "edge_rationale": "Design rationale embodies theoretical insight."
    },
    {
      "type": "implemented_by",
      "source_node": "intrinsic information gain utility driving exploration",
      "target_node": "KL-KSA agent utilizing information gain utility function",
      "description": "Design realised as concrete KL-KSA algorithm.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Implementation described in 2.3.",
      "edge_rationale": "KL-KSA directly encodes info-gain utility."
    },
    {
      "type": "validated_by",
      "source_node": "KL-KSA agent utilizing information gain utility function",
      "target_node": "gridworld experiments showing KL-KSA outperforming entropy-seeking agents",
      "description": "Experiments confirm superior exploration behaviour.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Empirical data in Figures 3-4.",
      "edge_rationale": "Result linked to KL-KSA implementation."
    },
    {
      "type": "motivates",
      "source_node": "gridworld experiments showing KL-KSA outperforming entropy-seeking agents",
      "target_node": "train agents with KL-KSA intrinsic motivation to enhance exploration without noise attraction",
      "description": "Positive experimental results motivate adopting KL-KSA during training.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct experimental evidence.",
      "edge_rationale": "Demonstrated benefit encourages intervention."
    },
    {
      "type": "caused_by",
      "source_node": "noise-induced distraction in entropy-seeking reinforcement learning agents",
      "target_node": "entropy-seeking utility misaligned with information gain",
      "description": "Utility definition rewards high entropy not knowledge, enabling noise trap.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Analysed in Section 2.3.",
      "edge_rationale": "Mismatch leads to distraction risk."
    },
    {
      "type": "mitigated_by",
      "source_node": "entropy-seeking utility misaligned with information gain",
      "target_node": "information gain utility avoids noise attraction in stochastic environments",
      "description": "Switching utility to info-gain removes incentive for random noise.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Conceptual comparison in text.",
      "edge_rationale": "Information-gain aligned utility addresses mis-alignment."
    },
    {
      "type": "caused_by",
      "source_node": "performance degradation from bad model priors in Bayesian reinforcement learning",
      "target_node": "uniform prior over goal locations disperses probability mass",
      "description": "Uninformative uniform prior drives agent to over-explore distant hypotheses.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Discussed around Figure 6.",
      "edge_rationale": "Uniform prior identified as root cause."
    },
    {
      "type": "mitigated_by",
      "source_node": "uniform prior over goal locations disperses probability mass",
      "target_node": "Occam bias improves early performance through MDL principle",
      "description": "Occam bias re-weights prior toward simpler nearby hypotheses.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Algorithm 4 motivation.",
      "edge_rationale": "MDL counteracts dispersed uniform prior."
    },
    {
      "type": "refined_by",
      "source_node": "Occam bias improves early performance through MDL principle",
      "target_node": "simplest unfalsified model selection via MDL",
      "description": "Principle turned into concrete greedy model selection design.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Algorithm 4 provides steps.",
      "edge_rationale": "Design implements theoretical Occam bias."
    },
    {
      "type": "implemented_by",
      "source_node": "simplest unfalsified model selection via MDL",
      "target_node": "MDL agent with Kolmogorov-regularized likelihood selection",
      "description": "Design realized as MDL agent implementation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Algorithm 4 code details.",
      "edge_rationale": "MDL agent is implementation."
    },
    {
      "type": "validated_by",
      "source_node": "MDL agent with Kolmogorov-regularized likelihood selection",
      "target_node": "experiments showing MDL agent outperforms uniform-prior AIξ in deterministic gridworld",
      "description": "Experiments show faster reward with MDL under deterministic rewards.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Figure 6 data.",
      "edge_rationale": "Empirical validation connects to implementation."
    },
    {
      "type": "motivates",
      "source_node": "experiments showing MDL agent outperforms uniform-prior AIξ in deterministic gridworld",
      "target_node": "integrate MDL-based model selection into Bayesian RL to bias towards simpler hypotheses",
      "description": "Success motivates integrating MDL in practical agents.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct empirical support.",
      "edge_rationale": "Performance benefit informs intervention."
    },
    {
      "type": "caused_by",
      "source_node": "computational intractability of planning in partially observable environments for URL",
      "target_node": "Monte Carlo tree search horizon limits agent foresight",
      "description": "Limited horizon is a practical symptom of planning intractability.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 3.3 analysis.",
      "edge_rationale": "Finite horizon arises from compute constraints."
    },
    {
      "type": "mitigated_by",
      "source_node": "Monte Carlo tree search horizon limits agent foresight",
      "target_node": "finite-horizon approximation acceptable for planning in MCTS",
      "description": "Accepting finite surrogate horizon offers tractable compromise.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Approximation justified but not rigorously proved.",
      "edge_rationale": "Theoretical insight proposes mitigation."
    },
    {
      "type": "refined_by",
      "source_node": "finite-horizon approximation acceptable for planning in MCTS",
      "target_node": "Monte Carlo tree search planning with mixture models for POMDPs",
      "description": "Insight instantiated as rationale for using ρUCT.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "3.3 Planning explanation.",
      "edge_rationale": "Design applies theoretical insight."
    },
    {
      "type": "implemented_by",
      "source_node": "Monte Carlo tree search planning with mixture models for POMDPs",
      "target_node": "ρUCT Monte Carlo tree search planner with finite horizon",
      "description": "Design realised via concrete ρUCT algorithm.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Equation 9 and implementation details.",
      "edge_rationale": "ρUCT is implementation of design."
    },
    {
      "type": "validated_by",
      "source_node": "ρUCT Monte Carlo tree search planner with finite horizon",
      "target_node": "experiments showing Thompson sampling underperforms AIξ under equal sample budget",
      "description": "Experiment reveals performance sensitivity to horizon and sample budget.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Figure 5 analysis.",
      "edge_rationale": "Results informative for planner effectiveness."
    },
    {
      "type": "motivates",
      "source_node": "experiments showing Thompson sampling underperforms AIξ under equal sample budget",
      "target_node": "adopt Monte Carlo tree search with finite horizon approximation for POMDP planning in RL agents",
      "description": "Highlights need to tune horizon & budget when adopting MCTS planners.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Paper suggests time-budget compromise.",
      "edge_rationale": "Performance gap motivates refined planner adoption."
    },
    {
      "type": "caused_by",
      "source_node": "lack of asymptotic optimality in AIXI agent model",
      "target_node": "insufficient intrinsic motivation design in AIXI",
      "description": "Missing exploration impetus prevents asymptotic optimality.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "2.3 notes insufficient exploration as cause.",
      "edge_rationale": "Root cause relationship."
    },
    {
      "type": "mitigated_by",
      "source_node": "insufficient intrinsic motivation design in AIXI",
      "target_node": "weak asymptotic optimality via exploration schedules in Bayesian RL",
      "description": "Exploration schedules formally restore optimality.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Lattimore & Hutter results cited.",
      "edge_rationale": "Theory addresses motivation gap."
    },
    {
      "type": "refined_by",
      "source_node": "weak asymptotic optimality via exploration schedules in Bayesian RL",
      "target_node": "information-gain-triggered exploration bursts in Bayes agents",
      "description": "BayesExp embodies exploration schedule idea.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Algorithm 2 design.",
      "edge_rationale": "Design instantiates theory."
    },
    {
      "type": "implemented_by",
      "source_node": "information-gain-triggered exploration bursts in Bayes agents",
      "target_node": "BayesExp algorithm with ε-schedule and effective horizon planning",
      "description": "Implementation realises design.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Algorithm 2 code lines.",
      "edge_rationale": "BayesExp is concrete algorithm."
    },
    {
      "type": "validated_by",
      "source_node": "BayesExp algorithm with ε-schedule and effective horizon planning",
      "target_node": "proof of weak asymptotic optimality for BayesExp in general environments",
      "description": "Formal proof confirms algorithm’s property.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Referenced theorem in 2.3.",
      "edge_rationale": "Proof validates algorithm effectiveness."
    },
    {
      "type": "motivates",
      "source_node": "proof of weak asymptotic optimality for BayesExp in general environments",
      "target_node": "fine-tune RL agents using BayesExp exploration bursts to achieve asymptotic optimality",
      "description": "Proof motivates practical adoption during RL fine-tuning.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical extrapolation from theorem.",
      "edge_rationale": "Theoretical guarantee encourages intervention."
    },
    {
      "type": "mitigated_by",
      "source_node": "posterior collapse causing premature exploitation in mixture models",
      "target_node": "Dirichlet environment model increases epistemic uncertainty for exploration",
      "description": "Richer model prevents early posterior collapse.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "3.2 explanation and experiments.",
      "edge_rationale": "Insight directly addresses collapse problem."
    },
    {
      "type": "refined_by",
      "source_node": "Dirichlet environment model increases epistemic uncertainty for exploration",
      "target_node": "rich model class maintains posterior entropy for sustained exploration",
      "description": "Insight refined into design rationale to choose factorised model.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Design motivation in 3.2.",
      "edge_rationale": "Design response to insight."
    },
    {
      "type": "implemented_by",
      "source_node": "rich model class maintains posterior entropy for sustained exploration",
      "target_node": "Dirichlet gridworld model with factorized tile priors",
      "description": "Design realised as the MDirichlet implementation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Implementation details in 3.2.",
      "edge_rationale": "Concrete model implementation."
    },
    {
      "type": "validated_by",
      "source_node": "Dirichlet gridworld model with factorized tile priors",
      "target_node": "experiments showing Dirichlet model improves AIXI exploration",
      "description": "Experiments show higher exploration fraction.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Figure 2 data.",
      "edge_rationale": "Empirical validation of model benefit."
    },
    {
      "type": "motivates",
      "source_node": "experiments showing Dirichlet model improves AIXI exploration",
      "target_node": "design environment models with factorized Dirichlet priors to stimulate exploration during planning",
      "description": "Positive evidence encourages use of Dirichlet models.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct empirical support.",
      "edge_rationale": "Results motivate intervention."
    },
    {
      "type": "refined_by",
      "source_node": "weak asymptotic optimality via exploration schedules in Bayesian RL",
      "target_node": "hypothesis commitment through posterior sampling",
      "description": "Thompson sampling offers alternative realisation of exploration schedule.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Algorithm 3 design rationale.",
      "edge_rationale": "TS refines theoretical idea."
    },
    {
      "type": "implemented_by",
      "source_node": "hypothesis commitment through posterior sampling",
      "target_node": "Thompson sampling with effective horizon Monte Carlo tree search",
      "description": "Design implemented as TS algorithm.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Algorithm 3 code.",
      "edge_rationale": "Implementation of design."
    },
    {
      "type": "validated_by",
      "source_node": "Thompson sampling with effective horizon Monte Carlo tree search",
      "target_node": "experiments showing Thompson sampling underperforms AIξ under equal sample budget",
      "description": "Experiments assess performance of TS implementation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Figure 5.",
      "edge_rationale": "Empirical validation."
    },
    {
      "type": "motivates",
      "source_node": "experiments showing Thompson sampling underperforms AIξ under equal sample budget",
      "target_node": "adopt Thompson sampling for policy selection with adequate planning horizon adjustments",
      "description": "Results highlight need to adjust horizon for TS adoption.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Paper suggests time-budget tuning.",
      "edge_rationale": "Evidence directs intervention refinement."
    },
    {
      "type": "motivates",
      "source_node": "gridworld experiments showing KL-KSA outperforming entropy-seeking agents",
      "target_node": "restrict entropy-seeking agents from high-entropy noise sources during deployment",
      "description": "Findings on noise attraction justify deploying filters for entropy-seeking agents.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Moderate inference—filter not explicitly proposed.",
      "edge_rationale": "Safety mitigation inferred from experimental risk."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1705.10557"
    },
    {
      "key": "title",
      "value": "Universal Reinforcement Learning Algorithms: Survey and Experiments"
    },
    {
      "key": "authors",
      "value": [
        "John Aslanides",
        "Jan Leike",
        "Marcus Hutter"
      ]
    },
    {
      "key": "date_published",
      "value": "2017-05-30T00:00:00Z"
    },
    {
      "key": "source",
      "value": "arxiv"
    }
  ]
}