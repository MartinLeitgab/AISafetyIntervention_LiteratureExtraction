{
  "nodes": [
    {
      "name": "Unverifiable safe behavior in general AI systems",
      "aliases": [
        "inability to guarantee AGI safety",
        "lack of verifiable safety in AGI"
      ],
      "type": "concept",
      "description": "Existential and large-scale harms may arise because developers cannot be certain that a general AI will always behave in ways humans deem Good.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central risk framed in Introduction and Discussion sections."
    },
    {
      "name": "Non-computability of agent deontological compliance in AGI",
      "aliases": [
        "undecidable safety property",
        "verification non-computability"
      ],
      "type": "concept",
      "description": "Given any non-trivial, viable set of Good histories, deciding whether an agent always selects Good actions is formally undecidable.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Derived from Formalizing Specification and Verification section."
    },
    {
      "name": "Unpredictable physical world outcomes hindering validation in AGI",
      "aliases": [
        "world-model uncertainty",
        "validation infeasibility"
      ],
      "type": "concept",
      "description": "Accurately predicting real-world consequences of actions requires complete analytic physical and social models, which do not exist, making outcome validation futile.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Shown in Specifying and Validating a Deontology section."
    },
    {
      "name": "Manual verification burden for iterative agent development",
      "aliases": [
        "onerous theorem proving",
        "proof burden"
      ],
      "type": "concept",
      "description": "Every code revision, especially in learning systems, would require new formal proofs, making manual verification impractically costly.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Implementing Verification section."
    },
    {
      "name": "Rice's theorem constraints on behavioural verification in AGI",
      "aliases": [
        "Rice’s theorem application",
        "formal undecidability insight"
      ],
      "type": "concept",
      "description": "Because any non-trivial property of a partial computable function is undecidable, full behavioural verification of AGI is impossible.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proof presented in Formalizing Specification and Verification."
    },
    {
      "name": "Incomplete physical and sociological models limit consequence prediction in AGI",
      "aliases": [
        "model incompleteness insight",
        "chaos and non-analyticity limits"
      ],
      "type": "concept",
      "description": "Chaotic dynamics and poor human-behaviour models prevent reliable mapping from actions to outcomes.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in Specifying and Validating a Deontology."
    },
    {
      "name": "Employ deontological governor architecture to enforce rules in AGI",
      "aliases": [
        "DG design rationale",
        "computational superego idea"
      ],
      "type": "concept",
      "description": "Introduce an external module that inspects proposed actions plus context and only permits those conforming to a computable deontology.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposed in Implementing Verification section as partial mitigation."
    },
    {
      "name": "Apply manual formal proofs for each agent revision in AGI",
      "aliases": [
        "iterative proof strategy",
        "manual verification rationale"
      ],
      "type": "concept",
      "description": "Despite undecidability, developers could attempt hand-crafted proofs of compliance at every software iteration.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Implementing Verification section."
    },
    {
      "name": "Restrict agent capabilities and successors to retain decidable deontologies in AGI",
      "aliases": [
        "capability limitation rationale",
        "successor restriction idea"
      ],
      "type": "concept",
      "description": "Limiting what the agent and its self-improved descendants can do or represent may preserve a computable deontology.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Raised in Specifying and Validating a Deontology section."
    },
    {
      "name": "Computational superego deontological governor intercepting output stream in AGI",
      "aliases": [
        "DG implementation",
        "action-filter module"
      ],
      "type": "concept",
      "description": "A final-layer software filter examining perceptions, actions, and candidate next action; only forwards an action if it matches the computable rule set.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation details described in Implementing Verification."
    },
    {
      "name": "Iterative theorem proving in continuous integration pipeline for AGI code",
      "aliases": [
        "CI proof pipeline",
        "automated proof workflow"
      ],
      "type": "concept",
      "description": "Embed formal-methods tooling within CI so each commit triggers re-verification proofs before deployment.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Extrapolated from Implementing Verification and Continuous Integration analogy."
    },
    {
      "name": "Successor agents mandated to reuse same deontological governor and compatible IO",
      "aliases": [
        "DG inheritance rule",
        "successor capability constraint"
      ],
      "type": "concept",
      "description": "Require that any self-improved or spawned agent embed the original DG and preserve identical sensor/actuator semantics.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposed workaround in Specifying and Validating a Deontology."
    },
    {
      "name": "Formal undecidability proof of verification problem in AGI",
      "aliases": [
        "undecidability validation evidence",
        "non-computability proof"
      ],
      "type": "concept",
      "description": "Mathematical proof using Rice’s theorem shows no algorithm can decide if an arbitrary agent always acts Good.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Constructed in Formalizing Specification and Verification."
    },
    {
      "name": "Analysis of specification and validation burdens in AGI",
      "aliases": [
        "burden analysis evidence",
        "feasibility assessment"
      ],
      "type": "concept",
      "description": "Qualitative and formal reasoning demonstrates high effort and futility of guaranteeing Good outcomes.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "From Implementing Verification and Specifying and Validating a Deontology."
    },
    {
      "name": "Integrate verified deontological governor into AGI actuator interface",
      "aliases": [
        "deploy DG",
        "install computational superego"
      ],
      "type": "intervention",
      "description": "At deployment, add a previously formally verified deontological governor as the final gate between the agent’s chosen actions and physical actuators.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Deployment-phase action, referenced in Implementing Verification where DG is ‘installed as the very last step’.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Conceptual prototypes exist in academic literature but no large-scale operational systems; evidence from Implementing Verification.",
      "node_rationale": "Direct actionable step derived from DG design rationale."
    },
    {
      "name": "Conduct manual formal verification proofs after every AGI code revision",
      "aliases": [
        "continuous manual proofs",
        "proof-per-revision"
      ],
      "type": "intervention",
      "description": "Institute a development policy that blocks deployment until human experts complete new proofs of compliance for all modified components.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Occurs pre-deployment during testing/assurance, as discussed in Implementing Verification.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Feasible with existing theorem-proving tools but not widely practiced for AGI-scale systems.",
      "node_rationale": "Actionable mitigation for verification burden highlighted in the paper."
    },
    {
      "name": "Design AGI with restricted capabilities and enforce governor reuse for all successor agents",
      "aliases": [
        "capability-limited AGI design",
        "successor DG enforcement"
      ],
      "type": "intervention",
      "description": "Architect the initial agent and any self-improved descendants to operate only within a capability set for which the deontology is decidable and to embed the same verified governor.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Affects foundational model design choices before training.",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Purely theoretical at present, suggested as a possible workaround; no implementations exist.",
      "node_rationale": "Direct design action inferred from successor and decidability discussion."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Unverifiable safe behavior in general AI systems",
      "target_node": "Non-computability of agent deontological compliance in AGI",
      "description": "If compliance is undecidable, developers cannot verify safety, generating the overarching risk.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit causal link made in Formalizing Specification and Verification.",
      "edge_rationale": "Paper states undecidability directly prevents guarantees of safety."
    },
    {
      "type": "refined_by",
      "source_node": "Non-computability of agent deontological compliance in AGI",
      "target_node": "Rice's theorem constraints on behavioural verification in AGI",
      "description": "Rice’s theorem provides the formal underpinning that refines and explains the non-computability problem.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Strong formal proof provided in the text.",
      "edge_rationale": "Theorem is cited as the specific reason undecidability arises."
    },
    {
      "type": "mitigated_by",
      "source_node": "Rice's theorem constraints on behavioural verification in AGI",
      "target_node": "Employ deontological governor architecture to enforce rules in AGI",
      "description": "A DG sidesteps undecidable full verification by confining checks to a computable rule set at runtime.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Presented as partial workaround; theoretical argument only.",
      "edge_rationale": "Implementing Verification presents DG after proving undecidability."
    },
    {
      "type": "implemented_by",
      "source_node": "Employ deontological governor architecture to enforce rules in AGI",
      "target_node": "Computational superego deontological governor intercepting output stream in AGI",
      "description": "The computational superego is the concrete mechanism realising the DG design rationale.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct description of mechanism follows rationale in text.",
      "edge_rationale": "Implementation section gives details of action-filter module."
    },
    {
      "type": "validated_by",
      "source_node": "Computational superego deontological governor intercepting output stream in AGI",
      "target_node": "Analysis of specification and validation burdens in AGI",
      "description": "Burden analysis evaluates the practicality and limitations of using a DG for assurance.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Qualitative evaluation rather than empirical study.",
      "edge_rationale": "Specifying and Validating a Deontology critiques DG constraints."
    },
    {
      "type": "motivates",
      "source_node": "Analysis of specification and validation burdens in AGI",
      "target_node": "Integrate verified deontological governor into AGI actuator interface",
      "description": "Despite limitations, analysis motivates actually deploying a DG as a pragmatic safeguard.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical motivation, not empirical; medium confidence.",
      "edge_rationale": "Paper concludes DG is lower-burden than full proofs, prompting deployment."
    },
    {
      "type": "caused_by",
      "source_node": "Unverifiable safe behavior in general AI systems",
      "target_node": "Manual verification burden for iterative agent development",
      "description": "Need for guarantees pushes developers toward exhaustive manual proofs, creating heavy burden.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Argument explicit in Implementing Verification.",
      "edge_rationale": "Risk exists because automated verification impossible."
    },
    {
      "type": "mitigated_by",
      "source_node": "Rice's theorem constraints on behavioural verification in AGI",
      "target_node": "Apply manual formal proofs for each agent revision in AGI",
      "description": "Manual proofs are offered as one of the few remaining ways to partially overcome undecidability.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "The paper discusses manual proofs immediately after theorem.",
      "edge_rationale": "Manual approach suggested right after presenting undecidability."
    },
    {
      "type": "implemented_by",
      "source_node": "Apply manual formal proofs for each agent revision in AGI",
      "target_node": "Iterative theorem proving in continuous integration pipeline for AGI code",
      "description": "Embedding theorem-proving tools into CI implements the manual-proof rationale at scale.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference from CI analogy; not explicitly built in paper.",
      "edge_rationale": "Implementing Verification mentions CI; mapping to proof pipeline inferred."
    },
    {
      "type": "validated_by",
      "source_node": "Iterative theorem proving in continuous integration pipeline for AGI code",
      "target_node": "Analysis of specification and validation burdens in AGI",
      "description": "Burden analysis assesses feasibility of continuous proof obligations.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Qualitative assessment; weak but textual support present.",
      "edge_rationale": "Analysis section critiques ongoing proof workload."
    },
    {
      "type": "motivates",
      "source_node": "Analysis of specification and validation burdens in AGI",
      "target_node": "Conduct manual formal verification proofs after every AGI code revision",
      "description": "Recognising the burden, the paper still recommends proofs where feasible to increase confidence.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct recommendation with caveats.",
      "edge_rationale": "Implementing Verification describes necessity despite burden."
    },
    {
      "type": "caused_by",
      "source_node": "Unverifiable safe behavior in general AI systems",
      "target_node": "Unpredictable physical world outcomes hindering validation in AGI",
      "description": "Risk remains because outcome-level validation is impossible in an uncertain world.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Paper links inability to predict consequences to overall safety risk.",
      "edge_rationale": "Specifying and Validating a Deontology ties outcome uncertainty to risk."
    },
    {
      "type": "refined_by",
      "source_node": "Unpredictable physical world outcomes hindering validation in AGI",
      "target_node": "Incomplete physical and sociological models limit consequence prediction in AGI",
      "description": "Model incompleteness explains why outcomes are unpredictable.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Theoretical but explicit reasoning.",
      "edge_rationale": "Section elaborates physics and sociology model gaps."
    },
    {
      "type": "mitigated_by",
      "source_node": "Incomplete physical and sociological models limit consequence prediction in AGI",
      "target_node": "Restrict agent capabilities and successors to retain decidable deontologies in AGI",
      "description": "Capability limits aim to confine behaviour to domains where outcome prediction is tractable.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Suggested as partial workaround; no empirical backing.",
      "edge_rationale": "Specifying and Validating a Deontology proposes capability limits."
    },
    {
      "type": "implemented_by",
      "source_node": "Restrict agent capabilities and successors to retain decidable deontologies in AGI",
      "target_node": "Successor agents mandated to reuse same deontological governor and compatible IO",
      "description": "Successor-reuse rule operationalises the capability-restriction rationale.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inferred concrete rule from discussion; weak evidence.",
      "edge_rationale": "Text notes successor agents should embed same DG."
    },
    {
      "type": "validated_by",
      "source_node": "Successor agents mandated to reuse same deontological governor and compatible IO",
      "target_node": "Analysis of specification and validation burdens in AGI",
      "description": "Burden analysis evaluates feasibility and limits of enforcing governor reuse.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Qualitative reasoning only.",
      "edge_rationale": "Section questions practicality of successor restrictions."
    },
    {
      "type": "motivates",
      "source_node": "Analysis of specification and validation burdens in AGI",
      "target_node": "Design AGI with restricted capabilities and enforce governor reuse for all successor agents",
      "description": "Despite drawbacks, analysis motivates designing agents with explicit capability and governor-reuse constraints.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical recommendation; no empirical data.",
      "edge_rationale": "Paper notes restrictions may be only tractable path."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2016-04-23T00:00:00Z"
    },
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "title",
      "value": "Limits to Verification and Validation of Agentic Behavior"
    },
    {
      "key": "authors",
      "value": [
        "David J. Jilk"
      ]
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1604.06963"
    }
  ]
}