{
  "nodes": [
    {
      "name": "Inferior translation quality in neural machine translation systems",
      "aliases": [
        "low BLEU in NMT",
        "sub-optimal MT accuracy"
      ],
      "type": "concept",
      "description": "Neural MT models often fail to reach desired translation accuracy on standard benchmarks such as WMT’14, limiting practical usefulness.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Stated motivation for exploring better architectures in Introduction"
    },
    {
      "name": "Training instability in deep neural machine translation models",
      "aliases": [
        "non-finite gradients in NMT",
        "unstable convergence in MT"
      ],
      "type": "concept",
      "description": "Deep NMT models can diverge or produce non-finite gradients, halting training.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Unstable runs without layer norm reported in Ablation Experiments (§5)"
    },
    {
      "name": "Prolonged training time for NMT model convergence",
      "aliases": [
        "slow convergence in NMT",
        "long training runtime"
      ],
      "type": "concept",
      "description": "Existing NMT systems can require many epochs and days to reach acceptable accuracy, consuming resources.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivates investigation of synchronous training and hybrid models in §4 and §6"
    },
    {
      "name": "Absence of universally beneficial training techniques in baseline architectures",
      "aliases": [
        "missing modern tricks in baseline NMT",
        "incomplete training toolkit"
      ],
      "type": "concept",
      "description": "Baseline RNMT or ConvS2S models often omit label smoothing, multi-head attention, layer norm or synchronous updates, degrading quality.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Introduction and §4 that ‘bag of tricks’ is critical yet under-documented"
    },
    {
      "name": "Gradient explosions in recurrent networks during NMT training",
      "aliases": [
        "large gradient norms in RNMT",
        "exploding gradients"
      ],
      "type": "concept",
      "description": "Deep LSTM stacks are prone to extreme gradient values, leading to divergence.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Adaptive clipping mechanism described in §4.1 targets this problem"
    },
    {
      "name": "Sequential dependency limits parallelism in RNMT training",
      "aliases": [
        "lack of parallelism in RNN",
        "encoder-decoder sequential bottleneck"
      ],
      "type": "concept",
      "description": "The recurrent nature of RNMT hinders full parallel training, extending wall-clock time.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivates switch from asynchronous to synchronous replica training in §4"
    },
    {
      "name": "Suboptimal encoder-decoder component pairing in NMT models",
      "aliases": [
        "mismatched encoder/decoder strengths",
        "inefficient component pairing"
      ],
      "type": "concept",
      "description": "Quality suffers when encoders and decoders do not exploit their complementary representational strengths.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Examined through hybrid experiments in §6.1"
    },
    {
      "name": "Cross-architecture applicability of label smoothing, layer normalization, multi-head attention, synchronous training",
      "aliases": [
        "universally useful NMT techniques"
      ],
      "type": "concept",
      "description": "The authors hypothesize and later show that several techniques improve multiple architectures, not just the ones that introduced them.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Basis of ablation studies in §5"
    },
    {
      "name": "Layer normalization stabilizes RNN training with multi-head attention",
      "aliases": [
        "layer norm benefits for RNMT",
        "LN stabilisation insight"
      ],
      "type": "concept",
      "description": "Applying per-gate layer normalization inside LSTM cells counteracts training divergence introduced by deep stacks and attention.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted in §4.1 and failure cases in §5"
    },
    {
      "name": "Synchronous replica training with warmup improves convergence speed and quality",
      "aliases": [
        "sync training insight",
        "replica-scaled LR benefit"
      ],
      "type": "concept",
      "description": "Synchronizing gradients and using a tailored warm-up LR schedule yields faster and more stable convergence than asynchronous training.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in §4.1 and ablation results in §5"
    },
    {
      "name": "Hybridizing encoders and decoders leverages complementary strengths",
      "aliases": [
        "encoder-decoder hybrid insight"
      ],
      "type": "concept",
      "description": "Using a Transformer encoder (rich features) with an RNMT+ decoder (stateful generation) can outperform either model alone.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Demonstrated in hybrid experiments §6"
    },
    {
      "name": "Enhancing RNMT with modern techniques to form RNMT+",
      "aliases": [
        "RNMT+ design rationale"
      ],
      "type": "concept",
      "description": "The RNMT architecture is upgraded with multi-head attention, layer normalization, label smoothing and synchronous training to combine strengths of recent advances.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section §4.1 Model Architecture of RNMT+"
    },
    {
      "name": "Integrating per-gate layer normalization and adaptive gradient clipping inside LSTM cells",
      "aliases": [
        "stabilisation design for RNMT+"
      ],
      "type": "concept",
      "description": "Specific stabilisation measures are added to RNMT+ to prevent gradient explosion and training halts.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in §4.1 under regularisation and clipping"
    },
    {
      "name": "Adopting synchronous training regime with tailored learning rate schedule",
      "aliases": [
        "sync training design rationale"
      ],
      "type": "concept",
      "description": "Switches optimisation strategy from asynchronous to synchronous with a replica-scaled LR schedule to improve throughput and quality.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Learning-rate schedule equation (1) and training description in §4.1"
    },
    {
      "name": "Combining Transformer encoder with RNMT+ decoder in hybrid models",
      "aliases": [
        "encoder-decoder hybrid design"
      ],
      "type": "concept",
      "description": "Design choice to pair the richer self-attention encoder with a stateful RNN decoder.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section §6.1 Assessing Individual Encoders and Decoders"
    },
    {
      "name": "Augmenting encoder via cascaded or multi-column integration of Transformer and RNMT+ layers",
      "aliases": [
        "encoder mixing design rationale"
      ],
      "type": "concept",
      "description": "Encoders are vertically (cascaded) or horizontally (multi-column) mixed to enrich representations before decoding.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section §6.2 Assessing Encoder Combinations"
    },
    {
      "name": "RNMT+ architecture with 6 bidirectional LSTM layers, multi-head additive attention, label smoothing, synchronous training",
      "aliases": [
        "RNMT+ implementation details"
      ],
      "type": "concept",
      "description": "Concrete configuration that instantiates the RNMT+ design.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation specifics enumerated in §4.1"
    },
    {
      "name": "Adaptive gradient clipping using moving average thresholding",
      "aliases": [
        "adaptive clipping mechanism"
      ],
      "type": "concept",
      "description": "Training steps are aborted when gradient norm exceeds four std-dev above moving average of log-norms.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Stabilisation technique described near end of §4.1"
    },
    {
      "name": "Learning rate schedule with linear warmup and exponential decay scaled by replica count",
      "aliases": [
        "warmup LR schedule"
      ],
      "type": "concept",
      "description": "Exact schedule (Equation 1) used for synchronous training.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Equation (1) in §4.1"
    },
    {
      "name": "Hybrid cascaded encoder stacking transformer layers on pre-trained RNMT+ encoder",
      "aliases": [
        "cascaded encoder mechanism"
      ],
      "type": "concept",
      "description": "Transformer layers fine-tuned on top of a frozen RNMT+ encoder to enrich stateful representations.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation details in §6.2 and Appendix A.5"
    },
    {
      "name": "Hybrid multi-column encoder concatenating outputs of Transformer and RNMT+ encoders",
      "aliases": [
        "multi-column encoder mechanism"
      ],
      "type": "concept",
      "description": "Parallel encoders whose outputs are concatenated before decoding.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation details in §6.2 and Appendix A.6"
    },
    {
      "name": "BLEU improvement to 41.00 and 28.49 on WMT14 En→Fr/En→De for RNMT+",
      "aliases": [
        "RNMT+ benchmark results"
      ],
      "type": "concept",
      "description": "RNMT+ surpassed previous architectures by ≈2 BLEU on En→Fr and ≈3 BLEU on En→De.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Results tables 1 & 2 in §4.2"
    },
    {
      "name": "Ablation study showing quality drop without key techniques and instability without layer normalization",
      "aliases": [
        "ablation evidence"
      ],
      "type": "concept",
      "description": "Removing techniques such as label smoothing or layer norm reduced BLEU or caused training to halt.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Table 4 in §5"
    },
    {
      "name": "Faster convergence and stable performance with synchronous training",
      "aliases": [
        "sync training evidence"
      ],
      "type": "concept",
      "description": "Synchronous training with warmup achieved comparable wall-clock convergence to other models despite lower per-GPU throughput.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Training time comparison in Table 3 and ablation in Table 4"
    },
    {
      "name": "Hybrid models achieve 41.67 BLEU surpassing individual architectures",
      "aliases": [
        "hybrid benchmark evidence"
      ],
      "type": "concept",
      "description": "Cascaded and multi-column hybrids with RNMT+ decoder exceeded both RNMT+ and Transformer results.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Results in Table 6 §6.2"
    },
    {
      "name": "Deploy RNMT+ model incorporating multi-head attention, label smoothing, per-gate layer normalization, and synchronous training in NMT systems",
      "aliases": [
        "use RNMT+ in production"
      ],
      "type": "intervention",
      "description": "Design and train translation systems with the full RNMT+ architecture and training regime.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Architecture choice and initial model specification (§4.1)",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Validated on large benchmarks but not yet described as deployed (§4.2)",
      "node_rationale": "Main actionable recommendation of the paper"
    },
    {
      "name": "Apply per-gate layer normalization and adaptive gradient clipping during RNMT training",
      "aliases": [
        "stabilise RNMT training"
      ],
      "type": "intervention",
      "description": "Integrate layer norm inside each LSTM gate and abort steps with anomalous gradient norms.",
      "concept_category": null,
      "intervention_lifecycle": "2",
      "intervention_lifecycle_rationale": "Training-time stabilisation (§4.1)",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Demonstrated via ablation studies (§5)",
      "node_rationale": "Directly mitigates instability risk"
    },
    {
      "name": "Use synchronous replica training with warmup learning rate schedule for NMT",
      "aliases": [
        "sync training intervention"
      ],
      "type": "intervention",
      "description": "Train NMT models with synchronised gradient updates across replicas and LR schedule from Equation 1.",
      "concept_category": null,
      "intervention_lifecycle": "2",
      "intervention_lifecycle_rationale": "Optimization-phase method (§4.1)",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Validated across architectures and backed by timing evidence (§4.2, Table 3)",
      "node_rationale": "Addresses slow convergence and instability"
    },
    {
      "name": "Adopt hybrid NMT architecture with Transformer encoder and RNMT+ decoder using cascaded or multi-column encoders",
      "aliases": [
        "deploy hybrid encoder-decoder"
      ],
      "type": "intervention",
      "description": "Design NMT systems that pair self-attention encoders with stateful RNMT+ decoders, optionally using cascaded or multi-column encoder mixes.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "High-level architecture decision (§6)",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Prototype validation on WMT benchmarks (§6.2)",
      "node_rationale": "Yields best reported BLEU scores"
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Inferior translation quality in neural machine translation systems",
      "target_node": "Absence of universally beneficial training techniques in baseline architectures",
      "description": "Missing techniques in baseline models directly lead to lower BLEU.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Stated qualitatively in Introduction",
      "edge_rationale": "Authors argue performance gap arises when techniques are omitted"
    },
    {
      "type": "mitigated_by",
      "source_node": "Absence of universally beneficial training techniques in baseline architectures",
      "target_node": "Cross-architecture applicability of label smoothing, layer normalization, multi-head attention, synchronous training",
      "description": "Recognising universality of techniques addresses omission.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Supported by ablation and cross-model experiments (§5)",
      "edge_rationale": "Insight shows how to fix the omission problem"
    },
    {
      "type": "enabled_by",
      "source_node": "Cross-architecture applicability of label smoothing, layer normalization, multi-head attention, synchronous training",
      "target_node": "Enhancing RNMT with modern techniques to form RNMT+",
      "description": "General insight motivates upgrading RNMT.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical step documented in §4",
      "edge_rationale": "RNMT+ is proposed as embodiment of techniques"
    },
    {
      "type": "implemented_by",
      "source_node": "Enhancing RNMT with modern techniques to form RNMT+",
      "target_node": "RNMT+ architecture with 6 bidirectional LSTM layers, multi-head additive attention, label smoothing, synchronous training",
      "description": "Specific architectural instantiation of the design rationale.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit implementation details §4.1",
      "edge_rationale": "Shows how design is realised"
    },
    {
      "type": "validated_by",
      "source_node": "RNMT+ architecture with 6 bidirectional LSTM layers, multi-head additive attention, label smoothing, synchronous training",
      "target_node": "BLEU improvement to 41.00 and 28.49 on WMT14 En→Fr/En→De for RNMT+",
      "description": "Benchmark scores confirm effectiveness.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Quantitative results Tables 1-2",
      "edge_rationale": "Empirical validation"
    },
    {
      "type": "motivates",
      "source_node": "BLEU improvement to 41.00 and 28.49 on WMT14 En→Fr/En→De for RNMT+",
      "target_node": "Deploy RNMT+ model incorporating multi-head attention, label smoothing, per-gate layer normalization, and synchronous training in NMT systems",
      "description": "Demonstrated gains encourage adoption.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Strong empirical evidence",
      "edge_rationale": "Higher BLEU justifies intervention"
    },
    {
      "type": "caused_by",
      "source_node": "Training instability in deep neural machine translation models",
      "target_node": "Gradient explosions in recurrent networks during NMT training",
      "description": "Exploding gradients are a primary mechanism of instability.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Discussed in §4.1",
      "edge_rationale": "Gradient anomalies halt training"
    },
    {
      "type": "mitigated_by",
      "source_node": "Gradient explosions in recurrent networks during NMT training",
      "target_node": "Layer normalization stabilizes RNN training with multi-head attention",
      "description": "Layer norm reduces gradient scale issues.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Empirical observation in ablations §5",
      "edge_rationale": "Layer norm prevents divergence"
    },
    {
      "type": "enabled_by",
      "source_node": "Layer normalization stabilizes RNN training with multi-head attention",
      "target_node": "Integrating per-gate layer normalization and adaptive gradient clipping inside LSTM cells",
      "description": "Design leverages stabilisation insight.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit linkage in §4.1",
      "edge_rationale": "Design rationale derives from insight"
    },
    {
      "type": "implemented_by",
      "source_node": "Integrating per-gate layer normalization and adaptive gradient clipping inside LSTM cells",
      "target_node": "Adaptive gradient clipping using moving average thresholding",
      "description": "Gradient clipping is concrete mechanism included.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Implementation described in detail §4.1",
      "edge_rationale": "Realisation of design"
    },
    {
      "type": "validated_by",
      "source_node": "Adaptive gradient clipping using moving average thresholding",
      "target_node": "Ablation study showing quality drop without key techniques and instability without layer normalization",
      "description": "Ablation shows training halts without stabilisation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Table 4 results",
      "edge_rationale": "Evidence for effectiveness"
    },
    {
      "type": "motivates",
      "source_node": "Ablation study showing quality drop without key techniques and instability without layer normalization",
      "target_node": "Apply per-gate layer normalization and adaptive gradient clipping during RNMT training",
      "description": "Demonstrated necessity motivates intervention.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Strong experimental backing",
      "edge_rationale": "Practitioners should include stabilisation"
    },
    {
      "type": "caused_by",
      "source_node": "Prolonged training time for NMT model convergence",
      "target_node": "Sequential dependency limits parallelism in RNMT training",
      "description": "Sequential RNN operations slow training.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explained in §2.1 and §4",
      "edge_rationale": "Sequential nature causes time risk"
    },
    {
      "type": "mitigated_by",
      "source_node": "Sequential dependency limits parallelism in RNMT training",
      "target_node": "Synchronous replica training with warmup improves convergence speed and quality",
      "description": "Replica-level parallelism offsets sequential internal ops.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Discussed in §4.1",
      "edge_rationale": "Insight proposes solution"
    },
    {
      "type": "enabled_by",
      "source_node": "Synchronous replica training with warmup improves convergence speed and quality",
      "target_node": "Adopting synchronous training regime with tailored learning rate schedule",
      "description": "Design embodies insight.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section §4.1 transition",
      "edge_rationale": "Design follows insight"
    },
    {
      "type": "implemented_by",
      "source_node": "Adopting synchronous training regime with tailored learning rate schedule",
      "target_node": "Learning rate schedule with linear warmup and exponential decay scaled by replica count",
      "description": "Equation 1 specifies the mechanism.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit formula in §4.1",
      "edge_rationale": "Schedule realises design"
    },
    {
      "type": "validated_by",
      "source_node": "Learning rate schedule with linear warmup and exponential decay scaled by replica count",
      "target_node": "Faster convergence and stable performance with synchronous training",
      "description": "Empirical timing and stability results verify benefit.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Table 3 and ablation evidence",
      "edge_rationale": "Data confirms mechanism works"
    },
    {
      "type": "motivates",
      "source_node": "Faster convergence and stable performance with synchronous training",
      "target_node": "Use synchronous replica training with warmup learning rate schedule for NMT",
      "description": "Evidence encourages practitioners to adopt sync training.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Strong quantitative support",
      "edge_rationale": "Intervention derived from validation"
    },
    {
      "type": "caused_by",
      "source_node": "Inferior translation quality in neural machine translation systems",
      "target_node": "Suboptimal encoder-decoder component pairing in NMT models",
      "description": "Mismatch of component strengths reduces quality.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference from hybrid experiments",
      "edge_rationale": "Authors imply pairing matters (§6.1)"
    },
    {
      "type": "mitigated_by",
      "source_node": "Suboptimal encoder-decoder component pairing in NMT models",
      "target_node": "Hybridizing encoders and decoders leverages complementary strengths",
      "description": "Insight identifies better pairing strategy.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit comparison results §6.1",
      "edge_rationale": "Theory to solve component mismatch"
    },
    {
      "type": "enabled_by",
      "source_node": "Hybridizing encoders and decoders leverages complementary strengths",
      "target_node": "Combining Transformer encoder with RNMT+ decoder in hybrid models",
      "description": "Design realises hybrid insight.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Design described in §6.1",
      "edge_rationale": "Design follows insight"
    },
    {
      "type": "implemented_by",
      "source_node": "Combining Transformer encoder with RNMT+ decoder in hybrid models",
      "target_node": "Hybrid cascaded encoder stacking transformer layers on pre-trained RNMT+ encoder",
      "description": "Cascaded encoder is one concrete implementation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Implementation details §6.2",
      "edge_rationale": "Mechanism realises design (variant 1)"
    },
    {
      "type": "implemented_by",
      "source_node": "Combining Transformer encoder with RNMT+ decoder in hybrid models",
      "target_node": "Hybrid multi-column encoder concatenating outputs of Transformer and RNMT+ encoders",
      "description": "Multi-column encoder is another implementation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Implementation details §6.2",
      "edge_rationale": "Mechanism realises design (variant 2)"
    },
    {
      "type": "validated_by",
      "source_node": "Hybrid cascaded encoder stacking transformer layers on pre-trained RNMT+ encoder",
      "target_node": "Hybrid models achieve 41.67 BLEU surpassing individual architectures",
      "description": "Benchmark confirms cascaded hybrid gain.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Table 6",
      "edge_rationale": "Evidence for implementation efficacy"
    },
    {
      "type": "validated_by",
      "source_node": "Hybrid multi-column encoder concatenating outputs of Transformer and RNMT+ encoders",
      "target_node": "Hybrid models achieve 41.67 BLEU surpassing individual architectures",
      "description": "Benchmark confirms multi-column hybrid gain.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Table 6",
      "edge_rationale": "Evidence for implementation efficacy"
    },
    {
      "type": "motivates",
      "source_node": "Hybrid models achieve 41.67 BLEU surpassing individual architectures",
      "target_node": "Adopt hybrid NMT architecture with Transformer encoder and RNMT+ decoder using cascaded or multi-column encoders",
      "description": "Superior BLEU motivates adoption.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Strong benchmark results",
      "edge_rationale": "Intervention justified by validation"
    }
  ],
  "meta": [
    {
      "key": "authors",
      "value": [
        "Mia Xu Chen",
        "Orhan Firat",
        "Ankur Bapna",
        "Melvin Johnson",
        "Wolfgang Macherey",
        "George Foster",
        "Llion Jones",
        "Niki Parmar",
        "Mike Schuster",
        "Zhifeng Chen",
        "Yonghui Wu",
        "Macduff Hughes"
      ]
    },
    {
      "key": "date_published",
      "value": "2018-04-26T00:00:00Z"
    },
    {
      "key": "title",
      "value": "The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation"
    },
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1804.09849"
    }
  ]
}