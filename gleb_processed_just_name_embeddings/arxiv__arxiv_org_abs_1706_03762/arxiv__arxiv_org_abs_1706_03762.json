{
  "nodes": [
    {
      "name": "Inefficient sequential computation in RNN sequence models",
      "aliases": [
        "sequential computation inefficiency in RNNs",
        "training latency in recurrent language models"
      ],
      "type": "concept",
      "description": "Recurrent neural networks process tokens sequentially, limiting parallelisation and extending training time, which impedes scalability and resource efficiency in sequence tasks.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in Introduction as a key limitation motivating alternative architectures."
    },
    {
      "name": "Difficulty learning long-range dependencies in sequence transduction",
      "aliases": [
        "long-range dependency difficulty in RNNs",
        "gradient path length issues in sequence models"
      ],
      "type": "concept",
      "description": "When path lengths between tokens grow, RNNs struggle to propagate information, harming translation and parsing quality.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Section 4 'Why Self-Attention' as a core challenge."
    },
    {
      "name": "O(n) sequential operations requirement in recurrent layers",
      "aliases": [
        "linear sequential computation in RNNs",
        "time-step dependency in recurrence"
      ],
      "type": "concept",
      "description": "Each token position depends on the previous hidden state, forcing n sequential steps per layer and limiting hardware utilisation.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explicitly quantified in Table 1 and surrounding text in Section 4."
    },
    {
      "name": "Long maximum path length between tokens in RNNs",
      "aliases": [
        "long path lengths in RNNs",
        "dependency distance problem in recurrence"
      ],
      "type": "concept",
      "description": "Signals must traverse many timesteps, elongating forward and backward paths and impeding the learning of distant relationships.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted in Table 1 and discussed in Section 4."
    },
    {
      "name": "Self-attention layers enable constant-time dependency modeling in sequence transduction",
      "aliases": [
        "self-attention parallelism advantage",
        "O(1) path length via attention"
      ],
      "type": "concept",
      "description": "Self-attention connects all token positions with O(1) sequential steps and O(1) path length, allowing efficient global dependency capture.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Derived from theoretical comparison in Section 4 and motivates architectural change."
    },
    {
      "name": "Replacing recurrence with self-attention in sequence models",
      "aliases": [
        "recurrence replacement by attention",
        "attention-only sequence modeling"
      ],
      "type": "concept",
      "description": "Design choice to remove recurrent and convolutional components, relying solely on self-attention to exploit its computational benefits.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Stated in Section 3 'Model Architecture' introduction."
    },
    {
      "name": "Transformer architecture with multi-head self-attention in sequence transduction",
      "aliases": [
        "Transformer self-attention architecture",
        "attention-only encoder-decoder"
      ],
      "type": "concept",
      "description": "Concrete implementation: encoder-decoder stacks (N=6), multi-head scaled dot-product attention, feed-forward layers, residual connections, layer norm and positional encodings.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Fully detailed in Section 3; operationalises the design rationale."
    },
    {
      "name": "BLEU improvement and reduced training cost on WMT14 translation benchmarks",
      "aliases": [
        "WMT14 Transformer BLEU gains",
        "empirical validation of Transformer"
      ],
      "type": "concept",
      "description": "Transformer achieves 28.4 BLEU on EN-DE and 41.0 on EN-FR with 3-12× less FLOPs than prior state-of-the-art, demonstrating effectiveness.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Reported in Section 6.1 'Machine Translation' and Table 2."
    },
    {
      "name": "Design sequence models using Transformer self-attention architecture",
      "aliases": [
        "adopt Transformer architecture in model design",
        "use attention-only models"
      ],
      "type": "intervention",
      "description": "At the model-design phase, choose Transformer-style attention-only architectures instead of RNNs/ConvS2S to improve efficiency and long-range dependency handling.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Applies directly during Phase 1 (Model Design); see Section 3 'Model Architecture'.",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Supported by systematic validation across large-scale translation and parsing tasks in Section 6, indicating prototype/early operational maturity.",
      "node_rationale": "Logical actionable step derived from paper’s findings, enabling downstream adoption for safety-relevant systems."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Inefficient sequential computation in RNN sequence models",
      "target_node": "O(n) sequential operations requirement in recurrent layers",
      "description": "Sequential processing overhead directly produces the observed inefficiency risk.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit statement in Introduction and Section 4 comparing complexity.",
      "edge_rationale": "Draws causal link from performance risk to its mechanical cause."
    },
    {
      "type": "caused_by",
      "source_node": "Difficulty learning long-range dependencies in sequence transduction",
      "target_node": "Long maximum path length between tokens in RNNs",
      "description": "Extended path lengths prevent effective gradient flow, causing poor long-range learning.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Table 1 and accompanying discussion in Section 4 provide detailed reasoning.",
      "edge_rationale": "Maps capability risk to path-length mechanism."
    },
    {
      "type": "mitigated_by",
      "source_node": "O(n) sequential operations requirement in recurrent layers",
      "target_node": "Self-attention layers enable constant-time dependency modeling in sequence transduction",
      "description": "Replacing sequential dependence with self-attention removes the O(n) bottleneck.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Theoretical complexity analysis in Section 4; empirical evidence later supports.",
      "edge_rationale": "Shows how theoretical insight addresses computational bottleneck."
    },
    {
      "type": "mitigated_by",
      "source_node": "Long maximum path length between tokens in RNNs",
      "target_node": "Self-attention layers enable constant-time dependency modeling in sequence transduction",
      "description": "Self-attention’s O(1) path length directly shortens dependency paths.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Same Section 4 analysis concerning path length.",
      "edge_rationale": "Connects theoretical benefit to dependency-learning problem."
    },
    {
      "type": "mitigated_by",
      "source_node": "Self-attention layers enable constant-time dependency modeling in sequence transduction",
      "target_node": "Replacing recurrence with self-attention in sequence models",
      "description": "Insight motivates the architectural decision to remove recurrence.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Design decision described at start of Section 3 drawing on prior analysis.",
      "edge_rationale": "Transitions from theory to concrete design approach."
    },
    {
      "type": "implemented_by",
      "source_node": "Replacing recurrence with self-attention in sequence models",
      "target_node": "Transformer architecture with multi-head self-attention in sequence transduction",
      "description": "The Transformer realises the design rationale with specific layers and mechanisms.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Section 3 provides full implementation details; direct implementation link.",
      "edge_rationale": "Embeds design rationale into an executable architecture."
    },
    {
      "type": "validated_by",
      "source_node": "Transformer architecture with multi-head self-attention in sequence transduction",
      "target_node": "BLEU improvement and reduced training cost on WMT14 translation benchmarks",
      "description": "Extensive experiments show the Transformer meets or exceeds state-of-the-art accuracy at lower cost.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Quantitative results in Section 6.1 and Table 2 with clear comparisons.",
      "edge_rationale": "Provides empirical support for the implementation mechanism."
    },
    {
      "type": "motivates",
      "source_node": "BLEU improvement and reduced training cost on WMT14 translation benchmarks",
      "target_node": "Design sequence models using Transformer self-attention architecture",
      "description": "Demonstrated benefits motivate adoption of Transformer designs in future projects.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Results generalised by authors in Conclusion; moderate inference to actionable step.",
      "edge_rationale": "Links validation evidence to an actionable intervention."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1706.03762"
    },
    {
      "key": "title",
      "value": "Attention Is All You Need"
    },
    {
      "key": "authors",
      "value": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan N. Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ]
    },
    {
      "key": "date_published",
      "value": "2017-06-12T00:00:00Z"
    },
    {
      "key": "source",
      "value": "arxiv"
    }
  ]
}