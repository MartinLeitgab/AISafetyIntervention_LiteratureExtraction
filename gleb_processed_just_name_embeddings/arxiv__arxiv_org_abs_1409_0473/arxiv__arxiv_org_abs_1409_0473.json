{
  "nodes": [
    {
      "name": "translation quality degradation for long sentences in neural machine translation",
      "aliases": [
        "poor long-sentence NMT performance",
        "length-related NMT accuracy drop"
      ],
      "type": "concept",
      "description": "Neural encoder-decoder models exhibit sharp BLEU score declines as source sentences exceed training lengths, harming overall translation quality.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in Introduction and Results as the core failure the new architecture tackles."
    },
    {
      "name": "incorrect word alignment and reordering in NMT outputs",
      "aliases": [
        "mis-alignment in translations",
        "wrong reordering of source words"
      ],
      "type": "concept",
      "description": "Encoder-decoder models without alignment often mistranslate phrases requiring non-monotonic reordering, leading to semantic errors.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Alignment and Qualitative sections as a major translation failure the attention mechanism addresses."
    },
    {
      "name": "fixed-length context vector bottleneck in encoder-decoder NMT",
      "aliases": [
        "single-vector information bottleneck",
        "context compression limit"
      ],
      "type": "concept",
      "description": "Standard encoder-decoder forces all source information into one vector, limiting capacity to represent long or complex sentences.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Identified in Introduction and Conclusion as root cause of long-sentence degradation."
    },
    {
      "name": "encoder burden to compress full source information",
      "aliases": [
        "overloaded encoder responsibility",
        "full-sentence compression requirement"
      ],
      "type": "concept",
      "description": "Without external attention, the encoder solely must embed every detail of the source, leading to information loss.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted throughout Background and Learning sections as a systemic challenge addressed by attention."
    },
    {
      "name": "absence of explicit alignment mechanism in encoder-decoder NMT",
      "aliases": [
        "no alignment modelling",
        "latent alignment omission"
      ],
      "type": "concept",
      "description": "Traditional encoder-decoder architectures generate translations without modelling which source positions relate to each target word.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Stated in Alignment subsection as cause of reordering errors."
    },
    {
      "name": "adaptive attention over source annotations can dynamically focus on relevant parts",
      "aliases": [
        "attention mechanism to relieve bottleneck",
        "dynamic focus on source words"
      ],
      "type": "concept",
      "description": "Allowing the decoder to weight encoder annotations per target word lets the model access detailed source information only when needed.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Derived from Learning to Align and Translate section explaining context vector ci."
    },
    {
      "name": "soft alignment supports non-monotonic reordering",
      "aliases": [
        "probabilistic alignment for phrase reordering",
        "soft attention enables jumps"
      ],
      "type": "concept",
      "description": "Using soft probabilistic weights over all source positions allows the model to align out-of-order phrases without hard constraints.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in Alignment subsection with examples of adjective/noun reorderings."
    },
    {
      "name": "joint training of alignment and translation to share gradients and improve performance",
      "aliases": [
        "end-to-end training with attention",
        "combined alignment-translation objective"
      ],
      "type": "concept",
      "description": "The model learns alignment weights and translation probabilities simultaneously, enabling mutual reinforcement and direct optimisation.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Stated in Decoder description and Conclusion as key architectural decision."
    },
    {
      "name": "use bidirectional RNN encoder for contextual annotations",
      "aliases": [
        "BiRNN encoder for annotations",
        "forward-backward encoder states"
      ],
      "type": "concept",
      "description": "A forward and backward RNN produce concatenated hidden states so each annotation summarises both preceding and following words.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in Encoder: Bidirectional RNN section as necessary to supply rich annotations."
    },
    {
      "name": "attention-weighted context vector per target word",
      "aliases": [
        "ci weighted annotation sum",
        "per-token attention context"
      ],
      "type": "concept",
      "description": "For each target token, the model computes ci as the weighted sum of source annotations using learned αij probabilities.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central mechanism described in Decoder equations (3–5)."
    },
    {
      "name": "feedforward alignment model producing softmax weights",
      "aliases": [
        "neural alignment scorer a(s,h)",
        "FFN alignment energy"
      ],
      "type": "concept",
      "description": "A small neural network scores compatibility between previous decoder state and each source annotation, softmaxed to αij probabilities.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Equation defining e_ij = a(s_{i-1}, h_j)."
    },
    {
      "name": "bidirectional RNN encoder concatenating forward and backward hidden states as annotations",
      "aliases": [
        "BiRNN annotation generation",
        "concatenated encoder states"
      ],
      "type": "concept",
      "description": "Separate forward and backward RNNs read the source in opposite directions; their hidden states are concatenated into h_j annotations.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation step from Encoder section enabling context-rich annotations."
    },
    {
      "name": "RNNsearch BLEU improvements over RNNencdec on WMT14 En-Fr",
      "aliases": [
        "quantitative BLEU evidence",
        "BLEU score gains"
      ],
      "type": "concept",
      "description": "RNNsearch-50 achieves BLEU 26.75 (all) vs 17.82 for RNNencdec-50; further tuned model reaches 28.45, rivaling phrase-based Moses.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Table of Results provides primary quantitative validation."
    },
    {
      "name": "performance robustness across sentence lengths with RNNsearch",
      "aliases": [
        "length-robust accuracy",
        "no degradation on 50+ tokens"
      ],
      "type": "concept",
      "description": "BLEU vs length plot shows little performance drop for RNNsearch even on 50-word sentences unlike steep decline for RNNencdec.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Figure 3 (bleu.pdf) demonstrates length-robustness."
    },
    {
      "name": "attention visualisations show plausible alignments",
      "aliases": [
        "heat-map alignment evidence",
        "qualitative alignment plots"
      ],
      "type": "concept",
      "description": "Soft-alignment matrices exhibit diagonals and correct non-monotonic mappings (e.g., adjective-noun swaps, ‘the man’ → ‘l’homme’).",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Alignment subsection and Figure 4 provide qualitative proof."
    },
    {
      "name": "integrate attention-based bidirectional encoder-decoder architecture (RNNsearch) during NMT model design",
      "aliases": [
        "apply attention NMT architecture",
        "design NMT with RNNsearch"
      ],
      "type": "intervention",
      "description": "During model design and training, replace fixed-vector encoder-decoder with a bidirectional encoder plus attention-based decoder jointly trained end-to-end.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Model choice is made at architecture definition (Experiment Settings).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Paper presents prototype with systematic validation on large benchmark (Results).",
      "node_rationale": "Primary actionable step to mitigate identified risks."
    },
    {
      "name": "inspect attention weight visualisations during pre-deployment testing to ensure alignment quality",
      "aliases": [
        "visual debug of attention maps",
        "qualitative attention check"
      ],
      "type": "intervention",
      "description": "During pre-deployment evaluation, generate attention heat-maps for representative sentences; flag models whose alignments diverge from linguistic expectations.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Action occurs after training, before release (Qualitative Analysis section).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Suggested by qualitative evidence but not yet formalised as standard practice.",
      "node_rationale": "Provides practical safety check against mis-alignments leading to mistranslations."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "translation quality degradation for long sentences in neural machine translation",
      "target_node": "fixed-length context vector bottleneck in encoder-decoder NMT",
      "description": "The degradation is attributed to the inability of a single vector to encode long sentences.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicitly stated in Introduction and Fig.3 analyses.",
      "edge_rationale": "Introduced as main hypothesis behind architecture change."
    },
    {
      "type": "caused_by",
      "source_node": "translation quality degradation for long sentences in neural machine translation",
      "target_node": "encoder burden to compress full source information",
      "description": "Overloading the encoder with complete information compression leads to information loss.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoning explained but not experimentally isolated.",
      "edge_rationale": "Background section discusses encoder limitations."
    },
    {
      "type": "caused_by",
      "source_node": "incorrect word alignment and reordering in NMT outputs",
      "target_node": "absence of explicit alignment mechanism in encoder-decoder NMT",
      "description": "Without alignment modelling, the model lacks guidance for ordering target words.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Alignment section attributes reordering errors to missing mechanism.",
      "edge_rationale": "Qualitative examples demonstrate misalignments."
    },
    {
      "type": "mitigated_by",
      "source_node": "fixed-length context vector bottleneck in encoder-decoder NMT",
      "target_node": "adaptive attention over source annotations can dynamically focus on relevant parts",
      "description": "Attention removes the need to store all information in one vector.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoning provided; direct causal test not isolated.",
      "edge_rationale": "Learning to Align section provides conceptual argument."
    },
    {
      "type": "mitigated_by",
      "source_node": "absence of explicit alignment mechanism in encoder-decoder NMT",
      "target_node": "soft alignment supports non-monotonic reordering",
      "description": "Soft alignment supplies the missing mechanism enabling flexible word ordering.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Supported by qualitative plots and discussion.",
      "edge_rationale": "Alignment subsection links theory to problem."
    },
    {
      "type": "enabled_by",
      "source_node": "adaptive attention over source annotations can dynamically focus on relevant parts",
      "target_node": "joint training of alignment and translation to share gradients and improve performance",
      "description": "Joint optimisation operationalises the attention insight within training.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Design described as direct realisation of attention insight.",
      "edge_rationale": "Decoder description emphasises joint training."
    },
    {
      "type": "enabled_by",
      "source_node": "adaptive attention over source annotations can dynamically focus on relevant parts",
      "target_node": "use bidirectional RNN encoder for contextual annotations",
      "description": "Bidirectional encoder supplies rich annotations required for effective attention.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Implied necessity; not empirically ablated.",
      "edge_rationale": "Encoder section motivates BiRNN for annotation quality."
    },
    {
      "type": "implemented_by",
      "source_node": "joint training of alignment and translation to share gradients and improve performance",
      "target_node": "attention-weighted context vector per target word",
      "description": "The context vector realises the jointly trained attention mechanism.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Equation-level mapping in Decoder section.",
      "edge_rationale": "ci definition embodies attention output."
    },
    {
      "type": "implemented_by",
      "source_node": "joint training of alignment and translation to share gradients and improve performance",
      "target_node": "feedforward alignment model producing softmax weights",
      "description": "FFN alignment scorer provides the trainable parameters for joint optimisation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit in model formulation.",
      "edge_rationale": "Alignment model ‘a’ is trained jointly."
    },
    {
      "type": "implemented_by",
      "source_node": "use bidirectional RNN encoder for contextual annotations",
      "target_node": "bidirectional RNN encoder concatenating forward and backward hidden states as annotations",
      "description": "The concatenation of forward and backward states realises the BiRNN design.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct implementation detail in Encoder section.",
      "edge_rationale": "Provides h_j vectors consumed by attention."
    },
    {
      "type": "validated_by",
      "source_node": "attention-weighted context vector per target word",
      "target_node": "RNNsearch BLEU improvements over RNNencdec on WMT14 En-Fr",
      "description": "Superior BLEU scores demonstrate effectiveness of attention mechanism.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Quantitative table compares architectures.",
      "edge_rationale": "BLEU improvements attributed to attention design."
    },
    {
      "type": "validated_by",
      "source_node": "attention-weighted context vector per target word",
      "target_node": "performance robustness across sentence lengths with RNNsearch",
      "description": "Stable performance on long sentences confirms alleviation of bottleneck.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Length-wise BLEU plot evidences robustness.",
      "edge_rationale": "Directly addresses long-sentence risk."
    },
    {
      "type": "validated_by",
      "source_node": "feedforward alignment model producing softmax weights",
      "target_node": "attention visualisations show plausible alignments",
      "description": "Heat-maps reveal the alignment model produces linguistically sensible weights.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Qualitative but convincing observations.",
      "edge_rationale": "Plots originate from α_ij produced by FFN alignment."
    },
    {
      "type": "motivates",
      "source_node": "RNNsearch BLEU improvements over RNNencdec on WMT14 En-Fr",
      "target_node": "integrate attention-based bidirectional encoder-decoder architecture (RNNsearch) during NMT model design",
      "description": "Demonstrated BLEU gains encourage adoption of the new architecture.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Strong quantitative results.",
      "edge_rationale": "BLEU is standard decision metric for model choice."
    },
    {
      "type": "motivates",
      "source_node": "performance robustness across sentence lengths with RNNsearch",
      "target_node": "integrate attention-based bidirectional encoder-decoder architecture (RNNsearch) during NMT model design",
      "description": "Length-robustness makes the architecture attractive for diverse inputs.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Empirical robustness is key requirement.",
      "edge_rationale": "Supports intervention’s efficacy claim."
    },
    {
      "type": "motivates",
      "source_node": "attention visualisations show plausible alignments",
      "target_node": "inspect attention weight visualisations during pre-deployment testing to ensure alignment quality",
      "description": "Clear visual alignment examples suggest using the same tool for sanity checks.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Qualitative suggestion, not formally tested.",
      "edge_rationale": "Authors use visualisations to evaluate model quality."
    },
    {
      "type": "required_by",
      "source_node": "attention-weighted context vector per target word",
      "target_node": "bidirectional RNN encoder concatenating forward and backward hidden states as annotations",
      "description": "Attention computation depends on availability of annotation vectors from the BiRNN encoder.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical dependency explained in Encoder/Decoder linkage.",
      "edge_rationale": "h_j annotations are inputs for α_ij weighting."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2014-09-01T00:00:00Z"
    },
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "title",
      "value": "Neural Machine Translation by Jointly Learning to Align and Translate"
    },
    {
      "key": "authors",
      "value": [
        "Dzmitry Bahdanau",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ]
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1409.0473"
    }
  ]
}