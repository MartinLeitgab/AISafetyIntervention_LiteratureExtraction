Summary:
The paper introduces “RNNsearch”, the first attention-based, bidirectional encoder-decoder architecture for neural machine translation (NMT). It diagnoses two central risks of the earlier fixed-vector encoder-decoder approach—severe translation quality drop on long sentences and faulty word re-ordering/alignment. It explains that these risks stem from forcing the encoder to compress all source information into a single vector and from the absence of an explicit alignment mechanism. The authors propose the theoretical insight that dynamically focusing (“attention”) on source annotations would remove the bottleneck and enable non-monotonic alignments. They justify a design that jointly trains alignment and translation and uses a bidirectional RNN encoder. Implementation details include an attention-weighted context vector per target word, a feed-forward alignment network, and concatenated forward/backward encoder states. Extensive quantitative (BLEU), length-robustness and qualitative (attention visualisation) evidence validate the design and motivate two interventions: (1) adopt attention-based bidirectional encoder-decoder architectures during model design/fine-tuning, and (2) routinely visualise attention weights during pre-deployment testing to catch alignment failures.

EXTRACTION CONFIDENCE[87]
The nodes/edges follow all naming, granularity and connectivity rules; every node is referenced by at least one edge and no risk–intervention shortcuts exist. Remaining uncertainty (13 %) comes from moderate inference in two low-confidence (edge = 2) links.  
Prompt improvement: explicitly allow grouping highly overlapping validation evidences to fewer nodes when numerous metrics are reported; this would reduce node duplication while preserving completeness.

Inference strategy justification:
Moderate inference (confidence 2) was used only to connect the encoder-burden problem to the adaptive-attention insight and to link soft-alignment theory to overall joint-training design—relationships strongly implied but not mathematically proved in the text.

Extraction completeness explanation:
All causal chains from the two top-level risks to both actionable interventions are captured, including every distinct claim, method, finding and mechanism referenced in quantitative or qualitative sections. Node aliases ensure cross-paper merging (e.g. “attention mechanism” vs “adaptive focus”).

Key limitations:
1. Paper does not provide systematic ablation studies; validation evidence strength therefore capped at 4.
2. No explicit discussion of deployment safety monitoring, so Intervention 2 is partly inferred and marked low maturity.
3. Unknown-word handling risk mentioned in conclusion not elaborated; prudently left out to avoid isolated nodes.

JSON knowledge fabric: