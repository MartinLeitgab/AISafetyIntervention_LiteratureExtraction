{
  "nodes": [
    {
      "name": "Unethical decision making in reinforcement learning agents deployed in real-world contexts",
      "aliases": [
        "unethical behaviour of RL agents",
        "AI ethical violations in RL systems"
      ],
      "type": "concept",
      "description": "AI systems trained with reinforcement learning can reach task goals by taking actions that violate widely-held moral norms, leading to harm (e.g., ignoring injured people, running over animals).",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central risk highlighted in Introduction: agents optimise goals while ignoring ethics."
    },
    {
      "name": "Side effects and dangerous exploration in reinforcement learning systems",
      "aliases": [
        "unintended side-effects in RL",
        "dangerous exploration by RL agents"
      ],
      "type": "concept",
      "description": "RL agents may cause collateral damage or explore hazardous actions during training or deployment because ethical constraints are missing from the reward signal.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explicitly called out in Introduction and Conclusions as problems ethics shaping aims to fix."
    },
    {
      "name": "Reward misspecification leading to unethical behavior in RL training",
      "aliases": [
        "incomplete reward functions in RL",
        "goal-only reward design"
      ],
      "type": "concept",
      "description": "When rewards optimise only task performance, agents find high-return behaviours that inadvertently violate ethical norms.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Identified in §1 as root cause of unethical actions."
    },
    {
      "name": "High cost of enumerating ethical scenarios for reward design in RL",
      "aliases": [
        "costly ethical reward engineering",
        "enumeration burden for ethics"
      ],
      "type": "concept",
      "description": "Designing explicit reward terms for every possible ethical contingency is prohibitively expensive and often impossible because norms are context-dependent.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in §1 as major practical barrier."
    },
    {
      "name": "Limitations of inverse reinforcement learning for learning ethics",
      "aliases": [
        "IRL drawbacks for ethics",
        "IRL data bias and complexity issues"
      ],
      "type": "concept",
      "description": "IRL needs large, goal-aligned human demonstrations, can inherit human sub-optimality, and struggles with temporally complex norms.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Enumerated in §1 and §3 as why pure IRL is insufficient."
    },
    {
      "name": "Sparse reward signals causing dangerous exploration in RL agents",
      "aliases": [
        "reward sparsity leads to unsafe exploration"
      ],
      "type": "concept",
      "description": "When only occasional task rewards are provided, RL agents explore randomly, increasing the chance of harmful side-effects.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Linked in §4 experiments (dangerous exploration alleviated by shaping)."
    },
    {
      "name": "Majority human behavior reflects universal ethical code",
      "aliases": [
        "human data as ethics prior",
        "common human ethics assumption"
      ],
      "type": "concept",
      "description": "Under ordinary circumstances most humans act ethically, so large, diverse human behaviour data encode normative patterns.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Key assumption stated in §1 and §3 enabling the approach."
    },
    {
      "name": "Policy divergence between RL agent and human indicates ethical discrepancy",
      "aliases": [
        "agent–human policy gap as ethics signal"
      ],
      "type": "concept",
      "description": "If the agent prefers an action humans rarely take, the action is likely unethical; conversely, if humans strongly prefer an action the agent avoids, it may reflect a positive ethical duty.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Formalised via KL-divergence in §3."
    },
    {
      "name": "Reward shaping alignment of RL policy with human ethics using non-task-aligned data",
      "aliases": [
        "ethics shaping design rationale",
        "KL-based reward shaping for ethics"
      ],
      "type": "concept",
      "description": "Rather than crafting explicit ethical rewards, bias the learning process by adding shaping rewards that minimise policy divergence from aggregated human behaviour.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined in §3 as the core solution concept."
    },
    {
      "name": "Penalize negative and reward positive ethical divergences in RL policy",
      "aliases": [
        "asymmetric shaping for ethics",
        "negative vs positive ethics rewards"
      ],
      "type": "concept",
      "description": "Apply penalties when the agent is more likely than humans to take low-probability actions (negative ethics) and bonuses when humans prefer actions the agent overlooks (positive ethics), controlled by thresholds τn, τp and weights cn, cp.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in equation 6 (§3)."
    },
    {
      "name": "Stochastic human policy inferred from diverse human action data via binomial aggregation",
      "aliases": [
        "integrated human policy",
        "human policy extraction"
      ],
      "type": "concept",
      "description": "Convert state-action counts from human trajectories into a probabilistic policy using Griffith et al.’s binomial method with confidence parameter C.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "First implementation step (equation 5, §3)."
    },
    {
      "name": "Kullback–Leibler divergence between agent and human policy with thresholds τn, τp and weights cn, cp",
      "aliases": [
        "KL divergence ethics metric",
        "policy divergence computation"
      ],
      "type": "concept",
      "description": "Compute D_KL(Pr_Q || Pr_H) and apply asymmetric thresholds to decide whether to issue penalties or rewards.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Equation 6, §3."
    },
    {
      "name": "Additional shaping reward H integrated into base reward during RL training",
      "aliases": [
        "H shaping term",
        "reward function augmentation"
      ],
      "type": "concept",
      "description": "The computed shaping value H(s,a) is added to the original reward R(s,a), influencing SARSA (or other RL) updates in every time-step.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Final implementation step described in §3 and used in §4 experiments."
    },
    {
      "name": "Grab a Milk experiment shows reduced baby harm and increased assistance without performance loss",
      "aliases": [
        "Grab-a-Milk validation",
        "baby crossing results"
      ],
      "type": "concept",
      "description": "Ethics shaping cut non-crying baby crossings and increased crying-baby soothing while keeping episode length comparable (Figures 1–3).",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Empirical support in §4.1."
    },
    {
      "name": "Driving and Avoiding experiment shows reduced cat harm with maintained performance",
      "aliases": [
        "Driving-Avoiding validation",
        "cat avoidance results"
      ],
      "type": "concept",
      "description": "Shaping keeps cumulative reward and collision count similar to baseline while greatly decreasing cats hit (Figures 4–6).",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Empirical support in §4.2."
    },
    {
      "name": "Driving and Rescuing experiment shows increased elder rescue with comparable performance",
      "aliases": [
        "Driving-Rescuing validation",
        "elder rescue results"
      ],
      "type": "concept",
      "description": "Ethics shaping rescues more elders without significantly increasing collisions or lowering reward (Figures 7–9).",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Empirical support in §4.3."
    },
    {
      "name": "Ethics shaping reduces side effects and dangerous exploration across tasks",
      "aliases": [
        "dangerous exploration reduction evidence"
      ],
      "type": "concept",
      "description": "Across all experiments, the number of unethical actions during learning drops, indicating less hazardous exploration (§4.3 discussion).",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Aggregated evidence summarised in §4 and Conclusion."
    },
    {
      "name": "Integrate KL-divergence-based ethics shaping reward into RL training using diverse human behavior data",
      "aliases": [
        "apply ethics shaping during RL",
        "KL reward shaping intervention"
      ],
      "type": "intervention",
      "description": "During fine-tuning/RL, compute KL divergence between current agent policy and pre-computed human policy each step, apply asymmetric shaping reward H(s,a), and update with SARSA/other RL algorithm.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Implements shaping during the RL learning loop (section 3 ‘Ethics Shaping’).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Evaluated only in simulated domains (§4), indicating proof-of-concept status.",
      "node_rationale": "Primary actionable method proposed to mitigate ethical risks."
    },
    {
      "name": "Collect and preprocess diverse human action data to derive ethical policy for shaping",
      "aliases": [
        "gather human trajectories for ethics",
        "human behaviour dataset acquisition"
      ],
      "type": "intervention",
      "description": "Record large amounts of ordinary human state-action pairs in varied contexts, aggregate them with binomial integration to form the stochastic human policy used for shaping.",
      "concept_category": null,
      "intervention_lifecycle": "2",
      "intervention_lifecycle_rationale": "Data acquisition and preprocessing occur before RL training starts.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated with synthetic data only; real-world data collection remains experimental.",
      "node_rationale": "Necessary preparatory step enabling the human-policy implementation mechanism."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Unethical decision making in reinforcement learning agents deployed in real-world contexts",
      "target_node": "Reward misspecification leading to unethical behavior in RL training",
      "description": "Poorly specified goal-only rewards lead agents to immoral yet high-reward behaviours.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicitly argued in §1 Introduction.",
      "edge_rationale": "Links main risk to its root problem analysis."
    },
    {
      "type": "caused_by",
      "source_node": "Unethical decision making in reinforcement learning agents deployed in real-world contexts",
      "target_node": "High cost of enumerating ethical scenarios for reward design in RL",
      "description": "Because enumerating all ethical contingencies is infeasible, designers omit them, enabling unethical outcomes.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Stated concern in §1; causal link plausible.",
      "edge_rationale": "Shows how design cost feeds into unethical results."
    },
    {
      "type": "caused_by",
      "source_node": "Unethical decision making in reinforcement learning agents deployed in real-world contexts",
      "target_node": "Limitations of inverse reinforcement learning for learning ethics",
      "description": "IRL’s shortcomings leave designers without practical alternatives, sustaining the risk.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Discussed in §1 and §3.",
      "edge_rationale": "Connects risk to inadequacy of existing solution."
    },
    {
      "type": "caused_by",
      "source_node": "Side effects and dangerous exploration in reinforcement learning systems",
      "target_node": "Sparse reward signals causing dangerous exploration in RL agents",
      "description": "With few intrinsic rewards, agents must explore widely, producing harmful side-effects.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implied link in §4; moderate inference.",
      "edge_rationale": "Explains origin of dangerous exploration."
    },
    {
      "type": "caused_by",
      "source_node": "Reward misspecification leading to unethical behavior in RL training",
      "target_node": "High cost of enumerating ethical scenarios for reward design in RL",
      "description": "When enumerating ethics is costly, reward functions remain misspecified.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical derivation from §1 discussion.",
      "edge_rationale": "Clarifies dependency between problems."
    },
    {
      "type": "mitigated_by",
      "source_node": "High cost of enumerating ethical scenarios for reward design in RL",
      "target_node": "Majority human behavior reflects universal ethical code",
      "description": "Using readily available human data avoids costly manual enumeration.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference from §1 & §3; stated motivation but not empirically measured.",
      "edge_rationale": "Shows theoretical insight addressing cost issue."
    },
    {
      "type": "enabled_by",
      "source_node": "Majority human behavior reflects universal ethical code",
      "target_node": "Policy divergence between RL agent and human indicates ethical discrepancy",
      "description": "If human data is ethical, divergence becomes a signal of unethical actions.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoning in §3.",
      "edge_rationale": "Connects two theoretical insights."
    },
    {
      "type": "mitigated_by",
      "source_node": "Policy divergence between RL agent and human indicates ethical discrepancy",
      "target_node": "Reward shaping alignment of RL policy with human ethics using non-task-aligned data",
      "description": "Shaping rewards based on divergence directly reduces discrepancies.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Central argument of §3.",
      "edge_rationale": "Transitions from theory to design."
    },
    {
      "type": "refined_by",
      "source_node": "Reward shaping alignment of RL policy with human ethics using non-task-aligned data",
      "target_node": "Penalize negative and reward positive ethical divergences in RL policy",
      "description": "Asymmetric treatment of negative vs positive ethics makes shaping more precise.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Equation 6 details, §3.",
      "edge_rationale": "Specialises the general design principle."
    },
    {
      "type": "implemented_by",
      "source_node": "Reward shaping alignment of RL policy with human ethics using non-task-aligned data",
      "target_node": "Stochastic human policy inferred from diverse human action data via binomial aggregation",
      "description": "Deriving a human policy is the first concrete step to realise shaping.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Implementation Section 3.",
      "edge_rationale": "Design → first implementation mechanism."
    },
    {
      "type": "required_by",
      "source_node": "Stochastic human policy inferred from diverse human action data via binomial aggregation",
      "target_node": "Collect and preprocess diverse human action data to derive ethical policy for shaping",
      "description": "High-coverage human trajectories are needed to estimate the policy.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicitly mentioned need in §1; experimental data synthetic.",
      "edge_rationale": "Connects implementation step to practical intervention."
    },
    {
      "type": "enables",
      "source_node": "Stochastic human policy inferred from diverse human action data via binomial aggregation",
      "target_node": "Kullback–Leibler divergence between agent and human policy with thresholds τn, τp and weights cn, cp",
      "description": "Having both agent and human policy allows KL divergence computation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Equation 6 derivation, §3.",
      "edge_rationale": "Implementation dependency."
    },
    {
      "type": "enables",
      "source_node": "Kullback–Leibler divergence between agent and human policy with thresholds τn, τp and weights cn, cp",
      "target_node": "Additional shaping reward H integrated into base reward during RL training",
      "description": "The KL result feeds directly into the shaping reward H(s,a).",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Implementation pipeline in §3.",
      "edge_rationale": "Final step before reward integration."
    },
    {
      "type": "validated_by",
      "source_node": "Additional shaping reward H integrated into base reward during RL training",
      "target_node": "Grab a Milk experiment shows reduced baby harm and increased assistance without performance loss",
      "description": "Experiment demonstrates shaping works in navigation with vulnerable entities.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Strong controlled simulation, §4.1.",
      "edge_rationale": "Implementation success evidence."
    },
    {
      "type": "validated_by",
      "source_node": "Additional shaping reward H integrated into base reward during RL training",
      "target_node": "Driving and Avoiding experiment shows reduced cat harm with maintained performance",
      "description": "Driving simulation validates method in safety-critical domain.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Figures 4–6, §4.2.",
      "edge_rationale": "Further evidence."
    },
    {
      "type": "validated_by",
      "source_node": "Additional shaping reward H integrated into base reward during RL training",
      "target_node": "Driving and Rescuing experiment shows increased elder rescue with comparable performance",
      "description": "Demonstrates positive ethical decisions can be encouraged.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Figures 7–9, §4.3.",
      "edge_rationale": "Evidence for positive ethics shaping."
    },
    {
      "type": "validated_by",
      "source_node": "Additional shaping reward H integrated into base reward during RL training",
      "target_node": "Ethics shaping reduces side effects and dangerous exploration across tasks",
      "description": "Aggregate analysis shows fewer unethical explorations during learning.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Qualitative claim in §4.3 and Conclusion.",
      "edge_rationale": "Evidence relating to exploration risk."
    },
    {
      "type": "motivates",
      "source_node": "Grab a Milk experiment shows reduced baby harm and increased assistance without performance loss",
      "target_node": "Integrate KL-divergence-based ethics shaping reward into RL training using diverse human behavior data",
      "description": "Successful results suggest adopting the intervention.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct empirical success, §4.1.",
      "edge_rationale": "Evidence → practical action."
    },
    {
      "type": "motivates",
      "source_node": "Driving and Avoiding experiment shows reduced cat harm with maintained performance",
      "target_node": "Integrate KL-divergence-based ethics shaping reward into RL training using diverse human behavior data",
      "description": "Confirms broad applicability, encouraging implementation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Empirical support, §4.2.",
      "edge_rationale": "Evidence → practical action."
    },
    {
      "type": "motivates",
      "source_node": "Driving and Rescuing experiment shows increased elder rescue with comparable performance",
      "target_node": "Integrate KL-divergence-based ethics shaping reward into RL training using diverse human behavior data",
      "description": "Shows positive ethics benefits, strengthening case for intervention.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Empirical support, §4.3.",
      "edge_rationale": "Evidence → practical action."
    },
    {
      "type": "motivates",
      "source_node": "Ethics shaping reduces side effects and dangerous exploration across tasks",
      "target_node": "Integrate KL-divergence-based ethics shaping reward into RL training using diverse human behavior data",
      "description": "Reduction in exploration hazards underscores safety value of intervention.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Summarised evidence across tasks.",
      "edge_rationale": "Evidence → practical action."
    },
    {
      "type": "mitigated_by",
      "source_node": "Sparse reward signals causing dangerous exploration in RL agents",
      "target_node": "Reward shaping alignment of RL policy with human ethics using non-task-aligned data",
      "description": "Adding shaping rewards densifies feedback, guiding safer exploration.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoned in Conclusion; moderate inference.",
      "edge_rationale": "Shows design also tackles exploration problem."
    },
    {
      "type": "mitigated_by",
      "source_node": "Limitations of inverse reinforcement learning for learning ethics",
      "target_node": "Reward shaping alignment of RL policy with human ethics using non-task-aligned data",
      "description": "Ethics shaping avoids IRL’s need for optimal goal-aligned demonstrations.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Argument in §3.",
      "edge_rationale": "Shows alternative approach."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1712.04172"
    },
    {
      "key": "title",
      "value": "A Low-Cost Ethics Shaping Approach for Designing Reinforcement Learning Agents"
    },
    {
      "key": "authors",
      "value": [
        "Yueh-Hua Wu",
        "Shou-De Lin"
      ]
    },
    {
      "key": "date_published",
      "value": "2017-12-12T00:00:00Z"
    },
    {
      "key": "source",
      "value": "arxiv"
    }
  ]
}