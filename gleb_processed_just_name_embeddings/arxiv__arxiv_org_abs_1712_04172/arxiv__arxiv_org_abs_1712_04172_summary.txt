Summary:
This paper introduces “Ethics Shaping”, a low-cost framework for making reinforcement-learning (RL) agents behave ethically. The key idea is to use large, task-agnostic human behaviour data to derive a stochastic “human policy”. During RL learning, a shaping reward based on the KL-divergence between the agent’s policy and the human policy is added: penalising actions humans rarely choose (negative ethics) and rewarding actions humans often choose (positive ethics). Experiments in three domains (Grab-a-Milk, Driving-and-Avoiding, Driving-and-Rescuing) show large reductions in unethical outcomes without harming task performance, and a reduction of dangerous exploration side-effects.

EXTRACTION CONFIDENCE[87]
The fabric follows the required node categories and inter-connections, no isolated nodes remain, and both interventions are fully justified by concept and evidence paths. Instructions could be improved by clarifying whether multiple design-rationale nodes are preferred over a single composite one and by giving explicit guidance on handling data-collection interventions.  

Inference strategy justification:
Moderate inference (confidence 2) was used only where the paper implicitly connects concepts (e.g., sparse rewards → dangerous exploration) or where intervention lifecycle/maturity had to be estimated.

Extraction completeness:
All causal paths from the two top-level risks (unethical decisions; side-effects & dangerous exploration) to the two actionable interventions are captured via 18 concept nodes and 20 edges, reflecting every distinct reasoning step, algorithmic mechanism and validation result discussed.

Key limitations:
• Real-world deployment evidence is still missing (maturity = 2).  
• The framework assumes most humans act ethically, which may not hold in adversarial settings.  
• Only synthetic human data were used; generalisation to real noisy data is untested.