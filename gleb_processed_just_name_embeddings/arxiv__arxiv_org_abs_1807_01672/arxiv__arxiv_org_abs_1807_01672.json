{
  "nodes": [
    {
      "name": "low-quality solutions in combinatorial optimization for industrial tasks",
      "aliases": [
        "suboptimal combinatorial optimisation outcomes",
        "inefficient industrial optimisation"
      ],
      "type": "concept",
      "description": "Industrial settings rely on NP-hard optimisation (e.g., bin packing); poor solution quality leads to wasted resources and safety or economic risks.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced implicitly in Introduction as motivation for improved optimisation methods."
    },
    {
      "name": "insufficient self-play curriculum in single-player reinforcement learning",
      "aliases": [
        "lack of curriculum in single-player RL",
        "no self-play signal for solo games"
      ],
      "type": "concept",
      "description": "Unlike two-player games, single-player RL lacks an automatically balanced opponent, often causing slow or plateaued learning.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Section 2.1 as the key bottleneck preventing AlphaZero-style success in single-player domains."
    },
    {
      "name": "relative performance ranking emulates self-play in single-player settings",
      "aliases": [
        "performance ranking as virtual opponent",
        "ranked reward self-play analogy"
      ],
      "type": "concept",
      "description": "Comparing each episode’s raw reward against recent performance percentiles creates an adaptive target analogous to an ever-improving opponent.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4.1 explains how ranked rewards reproduce self-play benefits."
    },
    {
      "name": "curriculum via ranked rewards exceeding recent performance threshold",
      "aliases": [
        "threshold-based reward curriculum",
        "percentile-driven curriculum"
      ],
      "type": "concept",
      "description": "Reward is set to 1 when the agent beats the α-percentile of its recent raw scores, forcing continuous improvement regardless of absolute skill.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in Algorithm 1 and Section 4.1 as core design decision."
    },
    {
      "name": "ranked reward mechanism integrated with neural policy-value network and MCTS",
      "aliases": [
        "R2 algorithm integration",
        "policy-value net + MCTS + ranked reward"
      ],
      "type": "concept",
      "description": "A 420 k-parameter visual CNN outputs policy and value; MCTS uses these estimates, and network weights are updated from MCTS-enhanced policies and ranked rewards.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4.2 elaborates network and search integration realising the design."
    },
    {
      "name": "bin packing experiments show 10% performance improvement and 72% optimality at 75% threshold",
      "aliases": [
        "empirical gains of R2 at 75 %",
        "experimental validation – 72 % optimal"
      ],
      "type": "concept",
      "description": "On ten-item 2-D bin-packing, R2 with α = 75 % achieved mean reward 0.97, surpassing plain MCTS by >10 % and solving 72 % of instances optimally.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Results reported in Section 5.1, Table 1 and Figure 5."
    },
    {
      "name": "learning instability at 90% ranking threshold",
      "aliases": [
        "performance degradation at high α",
        "unstable learning with 90 % threshold"
      ],
      "type": "concept",
      "description": "Threshold α = 90 % showed bursts of improvement followed by many low-reward games, yielding lower final performance than 75 %.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Documented in Section 5.2 and Figure 6d."
    },
    {
      "name": "Use ranked reward mechanism with 75th percentile threshold during RL training for single-player optimization tasks",
      "aliases": [
        "apply 75 % ranked reward curriculum",
        "train with α = 0.75 ranked rewards"
      ],
      "type": "intervention",
      "description": "During the fine-tuning/RL phase, compute recent-reward 75-percentile and assign binary ranked reward; update policy-value network accordingly.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Training configuration step (Section 5 Experiments).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Validated only on experimental benchmark – proof-of-concept.",
      "node_rationale": "Direct actionable step implied by best empirical results."
    },
    {
      "name": "Calibrate ranking threshold between 50% and 75% during pre-deployment testing to avoid learning instability",
      "aliases": [
        "threshold calibration 50-75 %",
        "avoid α > 0.9 instability"
      ],
      "type": "intervention",
      "description": "Before deployment, sweep α values and select range giving stable learning curves, avoiding observed instabilities at α = 90 %.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Executed after training algorithm design but before real-world release (Section 5.2).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Proposed based on limited evidence – theoretical guideline.",
      "node_rationale": "Addresses instability evidence, turning it into a concrete testing protocol."
    },
    {
      "name": "Implement the R2 algorithm combining policy-value network, MCTS, and ranked reward for combinatorial optimization problems",
      "aliases": [
        "deploy R2 for optimisation",
        "apply policy-value + MCTS + ranked rewards"
      ],
      "type": "intervention",
      "description": "Adopt the full R2 training loop (Algorithm 1) and associated CNN-MCTS architecture for industrial combinatorial-optimisation tasks.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Represents actual operational deployment to solve optimisation tasks.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated on a single benchmark; broader validation pending.",
      "node_rationale": "Primary actionable outcome of the research."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "low-quality solutions in combinatorial optimization for industrial tasks",
      "target_node": "insufficient self-play curriculum in single-player reinforcement learning",
      "description": "Without a curriculum, RL agents fail to reach high-quality solutions, producing the observed optimisation risk.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Inference supported by Section 2.1’s discussion of curriculum importance.",
      "edge_rationale": "Connects identified performance risk to its analysed mechanism."
    },
    {
      "type": "addressed_by",
      "source_node": "insufficient self-play curriculum in single-player reinforcement learning",
      "target_node": "relative performance ranking emulates self-play in single-player settings",
      "description": "Relative ranking supplies the missing adaptive difficulty, directly tackling the curriculum gap.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit logical argument in Section 4.1 but limited empirical isolation.",
      "edge_rationale": "Maps problem analysis to proposed theoretical remedy."
    },
    {
      "type": "implemented_by",
      "source_node": "relative performance ranking emulates self-play in single-player settings",
      "target_node": "curriculum via ranked rewards exceeding recent performance threshold",
      "description": "The insight is operationalised by converting raw rewards to binary signals based on percentile thresholds.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct description in Algorithm 1; no alternative implementation tested.",
      "edge_rationale": "Shows concrete design derived from theory."
    },
    {
      "type": "implemented_by",
      "source_node": "curriculum via ranked rewards exceeding recent performance threshold",
      "target_node": "ranked reward mechanism integrated with neural policy-value network and MCTS",
      "description": "The curriculum is embedded into a policy-value CNN guiding MCTS and into the loss function used for training.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Detailed architecture and algorithm steps in Sections 4.1–4.2.",
      "edge_rationale": "Transitions from design rationale to full algorithm mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "ranked reward mechanism integrated with neural policy-value network and MCTS",
      "target_node": "bin packing experiments show 10% performance improvement and 72% optimality at 75% threshold",
      "description": "Experimental results demonstrate significant gains over baselines, confirming mechanism effectiveness.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Controlled benchmark with quantitative metrics (Section 5.1).",
      "edge_rationale": "Provides empirical support for implementation."
    },
    {
      "type": "validated_by",
      "source_node": "ranked reward mechanism integrated with neural policy-value network and MCTS",
      "target_node": "learning instability at 90% ranking threshold",
      "description": "Same experiments revealed degraded stability at higher thresholds, highlighting sensitivity.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Empirical observation but limited sample size (Section 5.2).",
      "edge_rationale": "Captures negative validation outcome."
    },
    {
      "type": "motivates",
      "source_node": "bin packing experiments show 10% performance improvement and 72% optimality at 75% threshold",
      "target_node": "Use ranked reward mechanism with 75th percentile threshold during RL training for single-player optimization tasks",
      "description": "Best empirical performance at 75 % directly informs recommended training practice.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Evidence limited to one domain but quantitatively clear.",
      "edge_rationale": "Turns validation evidence into concrete intervention."
    },
    {
      "type": "motivates",
      "source_node": "learning instability at 90% ranking threshold",
      "target_node": "Calibrate ranking threshold between 50% and 75% during pre-deployment testing to avoid learning instability",
      "description": "Observed instability under high thresholds justifies calibration before deployment.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Empirical observation; broader generality not yet proven.",
      "edge_rationale": "Links negative evidence to a preventive intervention."
    },
    {
      "type": "motivates",
      "source_node": "bin packing experiments show 10% performance improvement and 72% optimality at 75% threshold",
      "target_node": "Implement the R2 algorithm combining policy-value network, MCTS, and ranked reward for combinatorial optimization problems",
      "description": "Overall superior performance suggests adopting the full R2 pipeline in practice.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Single-benchmark evidence; extrapolation required.",
      "edge_rationale": "Evidential basis for recommending algorithm deployment."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2018-07-04T00:00:00Z"
    },
    {
      "key": "title",
      "value": "Ranked Reward: Enabling Self-Play Reinforcement Learning for Combinatorial Optimization"
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1807.01672"
    },
    {
      "key": "authors",
      "value": [
        "Alexandre Laterre",
        "Yunguan Fu",
        "Mohamed Khalil Jabri",
        "Alain-Sam Cohen",
        "David Kas",
        "Karl Hajjar",
        "Torbjorn S. Dahl",
        "Amine Kerkeni",
        "Karim Beguir"
      ]
    },
    {
      "key": "source",
      "value": "arxiv"
    }
  ]
}