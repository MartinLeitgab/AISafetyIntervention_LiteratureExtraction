{
  "nodes": [
    {
      "name": "Research resource misallocation due to performance-only AI benchmarks",
      "aliases": [
        "misallocation of AI research effort",
        "performance-only benchmarking misalignment"
      ],
      "type": "concept",
      "description": "When progress is judged mainly by scores such as accuracy or Elo, funding and talent flow toward benchmark chasing instead of socially valuable research (Introduction, Background).",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Primary risk motivating the paper’s proposals (Introduction)."
    },
    {
      "name": "Unaccounted societal externalities from AI system deployment",
      "aliases": [
        "negative AI externalities",
        "hidden AI societal costs"
      ],
      "type": "concept",
      "description": "Costs such as environmental footprint, user frustration or skill atrophy that are rarely captured by performance metrics (Introduction, Full range of accounting).",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Second top-level risk highlighted by the authors (Full range of accounting)."
    },
    {
      "name": "Overemphasis on performance metrics neglecting resource costs in AI progress assessment",
      "aliases": [
        "performance-centric evaluation neglects costs"
      ],
      "type": "concept",
      "description": "Benchmarks focus on raw scores while ignoring compute, data, time and other resources (Background).",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Key mechanism causing both identified risks (Background)."
    },
    {
      "name": "Benchmark specialization reducing solution generality in AI research",
      "aliases": [
        "overfitting to benchmarks",
        "specialization problem"
      ],
      "type": "concept",
      "description": "Systems are tuned to narrow tasks, limiting adaptability to new domains (Background – specialization).",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Another mechanism leading to resource misallocation (Background)."
    },
    {
      "name": "Insufficient reproducibility and replicability data in AI publications",
      "aliases": [
        "reproducibility gap in AI",
        "replicability deficit"
      ],
      "type": "concept",
      "description": "Lack of shared code/data impedes independent verification and reuse (Background – reproducibility).",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Contributes to both misallocation and externalities (Background)."
    },
    {
      "name": "Utility functions integrating performance and resource costs align evaluation with stakeholder value",
      "aliases": [
        "comprehensive AI utility functions"
      ],
      "type": "concept",
      "description": "Formalising benefits and resource costs in a single utility captures diverse stakeholder preferences (Components and integration, Eq. 1).",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central theoretical proposal that addresses neglect of costs (Components and integration)."
    },
    {
      "name": "Pareto-front analysis exposes trade-offs across performance and resources",
      "aliases": [
        "Pareto surface evaluation",
        "multi-dimensional Pareto analysis"
      ],
      "type": "concept",
      "description": "Plotting systems in multi-dimensional space reveals efficient trade-offs rather than single winners (Exploring the Pareto front).",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Operationalises the utility concept for empirical comparison (Section 5)."
    },
    {
      "name": "Develop cost-sensitive AI evaluation frameworks covering full resource life cycle",
      "aliases": [
        "cost-sensitive AI evaluation design"
      ],
      "type": "concept",
      "description": "Frameworks should measure resource use during conception, production, reproduction and replication (Full range of accounting, Conclusions).",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Design motivation to implement theoretical insights (Sections 4, 7)."
    },
    {
      "name": "Promote comprehensive resource reporting standards for AI research",
      "aliases": [
        "resource reporting guidelines"
      ],
      "type": "concept",
      "description": "Authors should publish compute time, hardware, data size, etc. to enable full accounting (Conclusions – recommendations).",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Design step targeting reproducibility and cost visibility (Conclusions)."
    },
    {
      "name": "Use open leaderboards with code sharing to facilitate replicability",
      "aliases": [
        "open leaderboards for AI",
        "code-required leaderboards"
      ],
      "type": "concept",
      "description": "Platforms where participants submit code enable others to modify and resubmit, improving replicability (Background – reproducibility).",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Design rationale directly addressing reproducibility deficit (Background)."
    },
    {
      "name": "Resource taxonomy rd rk rs rh rm rc rn rt for AI system accounting",
      "aliases": [
        "eight-resource taxonomy",
        "rd-rk-rs taxonomy"
      ],
      "type": "concept",
      "description": "Table 1 lists data, knowledge, software, hardware, manipulation, computation, network and time as resource dimensions (Components and integration).",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete mechanism to realise cost-sensitive evaluation (Table 1)."
    },
    {
      "name": "Multi-dimensional benchmark plots of performance versus resource consumption",
      "aliases": [
        "Pareto plots with resource axes",
        "multi-axis benchmark visualisation"
      ],
      "type": "concept",
      "description": "Figures 1, 3 and 4 plot error vs. power or compute, illustrating how to surface Pareto fronts (Components and integration; Case studies).",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Practical tool enabling Pareto analysis (Figures 1,3,4)."
    },
    {
      "name": "Policy requirement to include compute and data metrics in academic submission templates",
      "aliases": [
        "mandatory resource reporting policy"
      ],
      "type": "concept",
      "description": "Institutional policy can require authors to disclose resource use, extending current reproducibility checklists (Conclusions – recommendations).",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implements reporting standards design rationale (Conclusions)."
    },
    {
      "name": "Leaderboard platforms enforcing code submission",
      "aliases": [
        "code-submission enforced benchmarks"
      ],
      "type": "concept",
      "description": "Systems like ‘open leaderboards’ require runnable code, allowing others to extend entries (Background – open leaderboards).",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implements open-leaderboard design rationale (Background)."
    },
    {
      "name": "Alpha* case study Pareto analysis showing compute-performance trade-offs",
      "aliases": [
        "AlphaGo Pareto evidence"
      ],
      "type": "concept",
      "description": "Comparison of AlphaGo variants demonstrates how adding resource axes changes which systems are Pareto optimal (Case studies – Alpha*).",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Empirical evidence supporting multi-dimensional evaluation (Case studies)."
    },
    {
      "name": "ALE case study Pareto analysis demonstrating efficiency focus shift",
      "aliases": [
        "Atari ALE Pareto evidence"
      ],
      "type": "concept",
      "description": "Longitudinal examination of Atari agents shows research shifting toward compute efficiency once plotted against resources (Case studies – ALE).",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Second empirical evidence node reinforcing conclusions (Case studies)."
    },
    {
      "name": "MNIST compute-performance Pareto front figure evidencing hidden trade-offs",
      "aliases": [
        "MNIST power vs error evidence"
      ],
      "type": "concept",
      "description": "Figure 1 shows many MNIST solutions off the Pareto surface when power consumption is considered (Components and integration).",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Additional evidence generalising the argument (Figure 1)."
    },
    {
      "name": "Define multi-dimensional AI benchmarks including resource costs",
      "aliases": [
        "create cost-aware benchmarks"
      ],
      "type": "intervention",
      "description": "Benchmark suites that, by design, report and weight multiple resource dimensions (compute, data, energy, time) alongside performance.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Benchmarks are used during evaluation before deployment (Conclusions – recommendations).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Existing prototypes (e.g., DAWNBench) but not yet standard (Background, Conclusions).",
      "node_rationale": "Direct actionable step derived from framework to mitigate misallocation."
    },
    {
      "name": "Mandate reporting of resource consumption metrics in AI research publications",
      "aliases": [
        "require resource disclosures"
      ],
      "type": "intervention",
      "description": "Conference/journal policy change requiring authors to list compute hours, data volumes, hardware types and human labour used.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "Policy intervention affecting the publication process (Conclusions – recommendations).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Some venues piloting checklists; not widespread (Background – reproducibility).",
      "node_rationale": "Implements resource reporting standards to tackle hidden costs."
    },
    {
      "name": "Adopt Pareto-front analysis to guide AI research portfolio optimization",
      "aliases": [
        "use Pareto gradient for research planning"
      ],
      "type": "intervention",
      "description": "Funding bodies and labs evaluate project value by movement of the Pareto surface rather than single benchmark scores.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "Strategic planning tool outside model training lifecycle (Exploring the Pareto front).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Conceptual proposal with limited institutional adoption (Section 5).",
      "node_rationale": "Addresses resource misallocation by changing incentive structures."
    },
    {
      "name": "Establish open leaderboards with mandatory code submission for replicability",
      "aliases": [
        "deploy code-verified leaderboards"
      ],
      "type": "intervention",
      "description": "Operational platforms where entries are ranked only when executable code is provided, enabling community replication and extensions.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Leaderboard services run continually after model development (Background – open leaderboards).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Existing examples in some subfields (e.g., OpenML) but not universal (Background).",
      "node_rationale": "Mitigates reproducibility problems and aids full accounting."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Research resource misallocation due to performance-only AI benchmarks",
      "target_node": "Overemphasis on performance metrics neglecting resource costs in AI progress assessment",
      "description": "Focusing only on performance metrics directs effort away from other valuable research dimensions.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit causal language in Background section.",
      "edge_rationale": "Text states misalignment results from measuring only performance (Background)."
    },
    {
      "type": "caused_by",
      "source_node": "Research resource misallocation due to performance-only AI benchmarks",
      "target_node": "Benchmark specialization reducing solution generality in AI research",
      "description": "Benchmark chasing incentivizes narrow, specialized solutions.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct discussion but without quantitative data (Background – specialization).",
      "edge_rationale": "Authors link specialization to resource misallocation."
    },
    {
      "type": "caused_by",
      "source_node": "Research resource misallocation due to performance-only AI benchmarks",
      "target_node": "Insufficient reproducibility and replicability data in AI publications",
      "description": "Performance-only focus discourages full code and data sharing needed for replication.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Argumentative inference based on discussion of incentives (Background).",
      "edge_rationale": "Authors note resource reporting is neglected when only performance matters."
    },
    {
      "type": "caused_by",
      "source_node": "Unaccounted societal externalities from AI system deployment",
      "target_node": "Overemphasis on performance metrics neglecting resource costs in AI progress assessment",
      "description": "Ignoring resource costs hides externalities such as energy use or user time.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit examples in Introduction and Section 4.",
      "edge_rationale": "Externalities framed as hidden costs due to performance focus."
    },
    {
      "type": "caused_by",
      "source_node": "Unaccounted societal externalities from AI system deployment",
      "target_node": "Insufficient reproducibility and replicability data in AI publications",
      "description": "Limited transparency makes it hard to quantify or internalize societal costs.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical link inferred from text (Section 4).",
      "edge_rationale": "Authors mention opacity hinders accounting for externalities."
    },
    {
      "type": "mitigated_by",
      "source_node": "Overemphasis on performance metrics neglecting resource costs in AI progress assessment",
      "target_node": "Utility functions integrating performance and resource costs align evaluation with stakeholder value",
      "description": "Adding resource costs into utility counters the overemphasis on performance.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Equation 1 directly proposed as remedy (Components and integration).",
      "edge_rationale": "Theoretical insight presented as solution."
    },
    {
      "type": "mitigated_by",
      "source_node": "Overemphasis on performance metrics neglecting resource costs in AI progress assessment",
      "target_node": "Pareto-front analysis exposes trade-offs across performance and resources",
      "description": "Pareto analysis makes resource–performance trade-offs explicit.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 5 frames Pareto analysis as response.",
      "edge_rationale": "Explicit linkage in Exploring the Pareto front section."
    },
    {
      "type": "mitigated_by",
      "source_node": "Benchmark specialization reducing solution generality in AI research",
      "target_node": "Pareto-front analysis exposes trade-offs across performance and resources",
      "description": "Plotting many dimensions discourages narrow specialization.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Argumentatic claim without empirical proof.",
      "edge_rationale": "Authors argue Pareto surfaces encourage diversity (Section 5)."
    },
    {
      "type": "mitigated_by",
      "source_node": "Insufficient reproducibility and replicability data in AI publications",
      "target_node": "Utility functions integrating performance and resource costs align evaluation with stakeholder value",
      "description": "Including resource costs incentivises sharing necessary data to measure them.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Reasoned implication (Background).",
      "edge_rationale": "Transparency required to fill utility function."
    },
    {
      "type": "refined_by",
      "source_node": "Utility functions integrating performance and resource costs align evaluation with stakeholder value",
      "target_node": "Pareto-front analysis exposes trade-offs across performance and resources",
      "description": "Pareto analysis operationalises the utility concept when utility cannot be collapsed to a single scalar.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct textual explanation in Section 3.",
      "edge_rationale": "Theoretical refinement relation stated."
    },
    {
      "type": "mitigated_by",
      "source_node": "Utility functions integrating performance and resource costs align evaluation with stakeholder value",
      "target_node": "Develop cost-sensitive AI evaluation frameworks covering full resource life cycle",
      "description": "Framework design applies the utility concept across stages.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Design recommendation follows theory (Section 4).",
      "edge_rationale": "Design rationale derived from theoretical insight."
    },
    {
      "type": "mitigated_by",
      "source_node": "Utility functions integrating performance and resource costs align evaluation with stakeholder value",
      "target_node": "Promote comprehensive resource reporting standards for AI research",
      "description": "Reporting standards provide data required for utility calculation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Stated as recommendation in Conclusions.",
      "edge_rationale": "Design rationale supports theoretical utility concept."
    },
    {
      "type": "mitigated_by",
      "source_node": "Utility functions integrating performance and resource costs align evaluation with stakeholder value",
      "target_node": "Use open leaderboards with code sharing to facilitate replicability",
      "description": "Open leaderboards supply replicated resource-performance data feeding utility assessment.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Connection implied, not explicit.",
      "edge_rationale": "Leaderboards considered tool for wider accounting (Background)."
    },
    {
      "type": "mitigated_by",
      "source_node": "Pareto-front analysis exposes trade-offs across performance and resources",
      "target_node": "Develop cost-sensitive AI evaluation frameworks covering full resource life cycle",
      "description": "Frameworks must generate Pareto surfaces over lifecycle resources.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 5 flows into recommendations in Section 7.",
      "edge_rationale": "Design rationale follows from analysis."
    },
    {
      "type": "mitigated_by",
      "source_node": "Pareto-front analysis exposes trade-offs across performance and resources",
      "target_node": "Promote comprehensive resource reporting standards for AI research",
      "description": "Full resource reporting is prerequisite for accurate Pareto plotting.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical requirement stated in Conclusions.",
      "edge_rationale": "Text notes need to report resources to plot surfaces."
    },
    {
      "type": "implemented_by",
      "source_node": "Develop cost-sensitive AI evaluation frameworks covering full resource life cycle",
      "target_node": "Resource taxonomy rd rk rs rh rm rc rn rt for AI system accounting",
      "description": "Taxonomy provides concrete dimensions used by evaluation frameworks.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Taxonomy introduced as implementation detail (Table 1).",
      "edge_rationale": "Implementation mechanism realises design."
    },
    {
      "type": "implemented_by",
      "source_node": "Develop cost-sensitive AI evaluation frameworks covering full resource life cycle",
      "target_node": "Multi-dimensional benchmark plots of performance versus resource consumption",
      "description": "Plots operationalise frameworks by visually presenting trade-offs.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Figures used as illustration in same section.",
      "edge_rationale": "Implementation mechanism demonstrates framework."
    },
    {
      "type": "implemented_by",
      "source_node": "Promote comprehensive resource reporting standards for AI research",
      "target_node": "Policy requirement to include compute and data metrics in academic submission templates",
      "description": "Policy change is the concrete mechanism enforcing reporting standards.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Recommendation stated, specific policy mechanism inferred.",
      "edge_rationale": "Implementation mechanism derives from design rationale."
    },
    {
      "type": "implemented_by",
      "source_node": "Use open leaderboards with code sharing to facilitate replicability",
      "target_node": "Leaderboard platforms enforcing code submission",
      "description": "Platforms enforce the code-sharing requirement.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Open leaderboards described as requiring code (Background).",
      "edge_rationale": "Direct implementation mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Resource taxonomy rd rk rs rh rm rc rn rt for AI system accounting",
      "target_node": "Alpha* case study Pareto analysis showing compute-performance trade-offs",
      "description": "Alpha* analysis uses compute and manipulation dimensions from taxonomy.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Taxonomy applied in case study (Section 6).",
      "edge_rationale": "Evidence node shows mechanism in practice."
    },
    {
      "type": "validated_by",
      "source_node": "Multi-dimensional benchmark plots of performance versus resource consumption",
      "target_node": "MNIST compute-performance Pareto front figure evidencing hidden trade-offs",
      "description": "Figure 1 visualises compute vs error, confirming effectiveness of plots.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Figure directly exemplifies mechanism.",
      "edge_rationale": "Validation evidence uses the mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Multi-dimensional benchmark plots of performance versus resource consumption",
      "target_node": "Alpha* case study Pareto analysis showing compute-performance trade-offs",
      "description": "Alpha* visualisations further validate plotting mechanism.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Case study employs plots (Figure 4).",
      "edge_rationale": "Evidence demonstrates mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Multi-dimensional benchmark plots of performance versus resource consumption",
      "target_node": "ALE case study Pareto analysis demonstrating efficiency focus shift",
      "description": "ALE plots confirm utility of multi-dimensional visualisation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Figure 4 right shows ALE plot.",
      "edge_rationale": "Evidence demonstrates mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Policy requirement to include compute and data metrics in academic submission templates",
      "target_node": "Alpha* case study Pareto analysis showing compute-performance trade-offs",
      "description": "Need for missing compute data in Alpha* papers illustrates policy necessity.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Authors note missing information; indirect validation.",
      "edge_rationale": "Evidence supports call for policy."
    },
    {
      "type": "validated_by",
      "source_node": "Leaderboard platforms enforcing code submission",
      "target_node": "ALE case study Pareto analysis demonstrating efficiency focus shift",
      "description": "ALE progress measurement project relies on public code to compare agents, validating platform usefulness.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "EFF project referenced with open code requirement.",
      "edge_rationale": "Case study confirms mechanism benefits."
    },
    {
      "type": "motivates",
      "source_node": "Alpha* case study Pareto analysis showing compute-performance trade-offs",
      "target_node": "Define multi-dimensional AI benchmarks including resource costs",
      "description": "Alpha* example shows why future benchmarks must include resource axes.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Case study explicitly discussed as motivation (Section 6).",
      "edge_rationale": "Validation evidence feeds into intervention."
    },
    {
      "type": "motivates",
      "source_node": "ALE case study Pareto analysis demonstrating efficiency focus shift",
      "target_node": "Define multi-dimensional AI benchmarks including resource costs",
      "description": "ALE case exhibits shift in research gradient once resource costs are plotted, motivating new benchmarks.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 6 uses ALE to exemplify necessity.",
      "edge_rationale": "Evidence informs intervention."
    },
    {
      "type": "motivates",
      "source_node": "MNIST compute-performance Pareto front figure evidencing hidden trade-offs",
      "target_node": "Define multi-dimensional AI benchmarks including resource costs",
      "description": "MNIST figure reveals unseen inefficiencies, supporting benchmark redesign.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Figure 1 directly supports argument.",
      "edge_rationale": "Evidence informs intervention."
    },
    {
      "type": "motivates",
      "source_node": "Alpha* case study Pareto analysis showing compute-performance trade-offs",
      "target_node": "Mandate reporting of resource consumption metrics in AI research publications",
      "description": "Missing resource data in Alpha* papers underscores need for mandatory reporting.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Observation noted in case study; indirect motivation.",
      "edge_rationale": "Evidence supports intervention."
    },
    {
      "type": "motivates",
      "source_node": "ALE case study Pareto analysis demonstrating efficiency focus shift",
      "target_node": "Mandate reporting of resource consumption metrics in AI research publications",
      "description": "Comparative analysis required uniform compute reporting, motivating policy.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Case study references compute estimates; direct support.",
      "edge_rationale": "Evidence supports intervention."
    },
    {
      "type": "motivates",
      "source_node": "Alpha* case study Pareto analysis showing compute-performance trade-offs",
      "target_node": "Adopt Pareto-front analysis to guide AI research portfolio optimization",
      "description": "Alpha* gradient analysis demonstrates utility of Pareto-based portfolio decisions.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 6 uses gradient arrows to discuss progress planning.",
      "edge_rationale": "Evidence leads to strategic intervention."
    },
    {
      "type": "motivates",
      "source_node": "ALE case study Pareto analysis demonstrating efficiency focus shift",
      "target_node": "Adopt Pareto-front analysis to guide AI research portfolio optimization",
      "description": "ALE shows how funding could shift when Pareto surfaces change, motivating adoption.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Case study illustrates planning benefit.",
      "edge_rationale": "Evidence leads to strategic intervention."
    },
    {
      "type": "motivates",
      "source_node": "Alpha* case study Pareto analysis showing compute-performance trade-offs",
      "target_node": "Establish open leaderboards with mandatory code submission for replicability",
      "description": "Need for variant exploration in Alpha* suggests value of leaderboard with code sharing.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Motivation inferred rather than stated.",
      "edge_rationale": "Evidence implies leaderboard usefulness."
    }
  ],
  "meta": [
    {
      "key": "title",
      "value": "Between Progress and Potential Impact of AI: the Neglected Dimensions"
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1806.00610"
    },
    {
      "key": "date_published",
      "value": "2018-06-02T00:00:00Z"
    },
    {
      "key": "authors",
      "value": [
        "Fernando Martínez-Plumed",
        "Shahar Avin",
        "Miles Brundage",
        "Allan Dafoe",
        "Sean Ó hÉigeartaigh",
        "José Hernández-Orallo"
      ]
    },
    {
      "key": "source",
      "value": "arxiv"
    }
  ]
}