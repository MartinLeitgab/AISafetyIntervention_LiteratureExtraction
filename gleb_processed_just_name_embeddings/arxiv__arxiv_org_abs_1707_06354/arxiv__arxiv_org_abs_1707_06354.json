{
  "nodes": [
    {
      "name": "Objective misalignment in human-robot collaboration",
      "aliases": [
        "human-robot value misalignment",
        "robot misunderstanding of human objectives"
      ],
      "type": "concept",
      "description": "Robots that fail to infer the true human objective may pursue actions counter to human intent, jeopardising usefulness and safety in both near-term assistance tasks and long-term autonomous operation.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced as the central challenge in Section 1 Introduction."
    },
    {
      "name": "Coordination injury in shared manipulation tasks",
      "aliases": [
        "physical harm from poor human-robot coordination",
        "collision risk in collaborative manipulation"
      ],
      "type": "concept",
      "description": "If a robot misinterprets human actions the pair may concurrently manipulate the same object, incurring high cost or injury, especially in household manipulation scenarios with blades or hot surfaces.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explicitly mentioned in Section 3 A Proof-of-Concept where simultaneous work ‘incurs a high cost (e.g., injury)’. "
    },
    {
      "name": "Literal interpretation of human actions by IRL robots",
      "aliases": [
        "standard IRL literalism",
        "non-pragmatic IRL assumption"
      ],
      "type": "concept",
      "description": "Traditional inverse reinforcement learning treats observed human behaviour as if the robot already knew the true objective, leading to incorrect inferences when humans act pedagogically.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Contrasted with pragmatic interpretation in Figure 1 and Section 2.2."
    },
    {
      "name": "Infinite belief hierarchy in asymmetric information structures",
      "aliases": [
        "unbounded mutual modelling",
        "recursive belief explosion"
      ],
      "type": "concept",
      "description": "Because the human knows the objective and the robot does not, naïve modelling would require each agent to reason about the other’s nested beliefs ad infinitum, making planning intractable.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Section 2.2 when motivating the need for tractable approximations."
    },
    {
      "name": "Humans behave pedagogically to inform collaborators",
      "aliases": [
        "pedagogic human behaviour",
        "human teaching actions"
      ],
      "type": "concept",
      "description": "Cognitive-science results show people choose actions that deliberately reveal their goals to assist a learner, providing an informative signal that robots can exploit.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Key insight imported from Shafto 2014 in Section 1 and 2.2."
    },
    {
      "name": "Robot pragmatic interpretation leverages human pedagogy",
      "aliases": [
        "pragmatic robot listener",
        "goal-directed pragmatic inference"
      ],
      "type": "concept",
      "description": "If the robot assumes the human acts pedagogically, it can interpret actions as intentional signals, leading to faster and more accurate objective inference.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined in Section 2.2 as the complementary role to human pedagogy."
    },
    {
      "name": "Noisy rationality modeling of humans avoids infinite recursion",
      "aliases": [
        "Boltzmann rational human model",
        "soft-max pedagogic policy assumption"
      ],
      "type": "concept",
      "description": "By treating the human as a noisily-rational decision maker whose action likelihood follows a Boltzmann distribution over expected value, the interaction reduces to a fixed-point rather than an infinite belief ladder.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Presented formally in Eq. (1) of Section 2.3."
    },
    {
      "name": "Pragmatic-pedagogic equilibrium for value alignment",
      "aliases": [
        "pragmatic-pedagogic fixed-point",
        "CIRL pragmatic equilibrium"
      ],
      "type": "concept",
      "description": "A game-theoretic equilibrium where the human’s pedagogic policy and the robot’s pragmatic inference reach a self-consistent fixed point, ensuring aligned behaviour under asymmetric objective knowledge.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Defined in Section 2.3 as the desired solution concept."
    },
    {
      "name": "Formulating CIRL as POMDP for tractable planning",
      "aliases": [
        "CIRL-POMDP reduction",
        "belief-state formulation of value alignment"
      ],
      "type": "concept",
      "description": "By embedding the pragmatic-pedagogic equilibrium into a partially-observable Markov decision process, existing POMDP solvers can be used to approximate optimal policies.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained at the end of Section 2.3."
    },
    {
      "name": "Bayesian belief update with Boltzmann human action model",
      "aliases": [
        "pedagogic Bayesian update",
        "belief dynamics under Eq.(2)"
      ],
      "type": "concept",
      "description": "The robot updates its belief over objectives by weighting prior belief with the likelihood of the observed human action under the Boltzmann pedagogic policy.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Equation (2) provides the deterministic update rule in Section 2.3."
    },
    {
      "name": "Feedback Stackelberg information structure in CIRL algorithms",
      "aliases": [
        "human observes robot prefix",
        "sequential Stackelberg feedback"
      ],
      "type": "concept",
      "description": "The game assumes the human sees the robot’s actions before acting, creating a feedback-Stackelberg structure that captures realistic human anticipation in collaboration.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivated in Section 2.3 when selecting an information structure."
    },
    {
      "name": "Value iteration solution for pragmatic CIRL POMDP",
      "aliases": [
        "belief-state value iteration",
        "CIRL POMDP planning algorithm"
      ],
      "type": "concept",
      "description": "For discount factor γ<1 the fixed-point dynamics allow value iteration over belief states, enabling approximate policy computation with existing POMDP techniques.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Derived at the end of Section 2.3."
    },
    {
      "name": "Meal-preparation simulation 86% success with pragmatic-pedagogic CIRL",
      "aliases": [
        "86 % success pragmatic robot",
        "proof-of-concept success rate"
      ],
      "type": "concept",
      "description": "In a household meal-prep scenario with four recipes, the pragmatic-pedagogic approach allowed the team to cook the correct recipe in 86 % of trials, far outperforming the baseline.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Reported in Section 3 A Proof-of-Concept."
    },
    {
      "name": "Meal-preparation simulation 15% success with literal IRL baseline",
      "aliases": [
        "15 % success literal robot",
        "baseline failure rate"
      ],
      "type": "concept",
      "description": "Under a literal IRL interpretation without pedagogy or pragmatics, the human-robot team only achieved the desired meal in 15 % of trials, illustrating the severity of misalignment.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Presented alongside the main result in Section 3."
    },
    {
      "name": "Train collaborative robots with pragmatic-pedagogic CIRL policies via POMDP value iteration",
      "aliases": [
        "pragmatic-pedagogic CIRL training",
        "deploy POMDP-derived CIRL policies"
      ],
      "type": "intervention",
      "description": "During the fine-tuning/RL phase, learn robot policies that assume pedagogic human behaviour, update beliefs with the Boltzmann model, and compute actions by value iteration in the CIRL-derived POMDP before deployment in collaborative tasks.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Policies are learned after pre-training but before field deployment, aligning with RL fine-tuning (Section 3 Proof-of-Concept describes offline planning/training).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Evidence is a simulation proof-of-concept without real-world trials (Section 3), placing maturity at experimental.",
      "node_rationale": "Only actionable step directly supported by validation evidence, enabling alignment in future systems."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Objective misalignment in human-robot collaboration",
      "target_node": "Literal interpretation of human actions by IRL robots",
      "description": "Robots that interpret actions literally often infer the wrong objective, leading to misaligned behaviour.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Stated qualitatively in Figure 1 and Section 1.",
      "edge_rationale": "Links central risk to key failure mode described early in the paper."
    },
    {
      "type": "caused_by",
      "source_node": "Coordination injury in shared manipulation tasks",
      "target_node": "Literal interpretation of human actions by IRL robots",
      "description": "Misinterpretation can cause robot and human to work on the same object simultaneously, risking injury.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit cost model in Section 3.",
      "edge_rationale": "Connects physical-safety risk to same failure mechanism."
    },
    {
      "type": "caused_by",
      "source_node": "Literal interpretation of human actions by IRL robots",
      "target_node": "Infinite belief hierarchy in asymmetric information structures",
      "description": "Literal IRL arises partly as a simplification to avoid unmanageable recursive belief modelling.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implied reasoning in Section 2.2; light inference.",
      "edge_rationale": "Explains underlying mechanism for literal approach."
    },
    {
      "type": "mitigated_by",
      "source_node": "Literal interpretation of human actions by IRL robots",
      "target_node": "Humans behave pedagogically to inform collaborators",
      "description": "Recognising human pedagogy provides an informative signal that counteracts literal misinterpretation.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Causal claim argued conceptually in Section 2.2.",
      "edge_rationale": "Introduces first theoretical remedy."
    },
    {
      "type": "enabled_by",
      "source_node": "Robot pragmatic interpretation leverages human pedagogy",
      "target_node": "Humans behave pedagogically to inform collaborators",
      "description": "Pragmatic interpretation is feasible because humans generate pedagogic signals.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Mutual dependence formalised in fixed-point Eq. (1-2).",
      "edge_rationale": "Shows interdependence of the two theoretical insights."
    },
    {
      "type": "motivates",
      "source_node": "Robot pragmatic interpretation leverages human pedagogy",
      "target_node": "Pragmatic-pedagogic equilibrium for value alignment",
      "description": "A self-consistent equilibrium is required to operationalise pragmatic interpretation with pedagogy.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct logical progression in Section 2.3.",
      "edge_rationale": "Moves from insight to solution concept."
    },
    {
      "type": "mitigated_by",
      "source_node": "Infinite belief hierarchy in asymmetric information structures",
      "target_node": "Noisy rationality modeling of humans avoids infinite recursion",
      "description": "Assuming soft-max noisy rationality collapses the infinite hierarchy to a fixed-point.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Formal argument preceding Eq. (1).",
      "edge_rationale": "Provides concrete theoretical fix."
    },
    {
      "type": "motivates",
      "source_node": "Noisy rationality modeling of humans avoids infinite recursion",
      "target_node": "Formulating CIRL as POMDP for tractable planning",
      "description": "With beliefs expressible finitely, the problem maps to a POMDP.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 2.3 reduction discussion.",
      "edge_rationale": "Connects theory to design choice."
    },
    {
      "type": "implemented_by",
      "source_node": "Pragmatic-pedagogic equilibrium for value alignment",
      "target_node": "Bayesian belief update with Boltzmann human action model",
      "description": "Belief update realises the equilibrium dynamics between actions and beliefs.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Equation-level derivation in Section 2.3.",
      "edge_rationale": "Maps design to concrete mechanism."
    },
    {
      "type": "implemented_by",
      "source_node": "Pragmatic-pedagogic equilibrium for value alignment",
      "target_node": "Feedback Stackelberg information structure in CIRL algorithms",
      "description": "Assuming feedback Stackelberg turns human observation of robot into part of equilibrium.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit modelling choice in Section 2.3.",
      "edge_rationale": "Second mechanism required for equilibrium."
    },
    {
      "type": "implemented_by",
      "source_node": "Formulating CIRL as POMDP for tractable planning",
      "target_node": "Value iteration solution for pragmatic CIRL POMDP",
      "description": "Value iteration is the chosen planning method over belief states in the POMDP.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct statement at end of Section 2.3.",
      "edge_rationale": "Details the algorithmic solution."
    },
    {
      "type": "refined_by",
      "source_node": "Feedback Stackelberg information structure in CIRL algorithms",
      "target_node": "Value iteration solution for pragmatic CIRL POMDP",
      "description": "Value iteration refines the information-structured model into a computable policy.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Link inferred as algorithm follows information structure.",
      "edge_rationale": "Clarifies implementation sequencing."
    },
    {
      "type": "required_by",
      "source_node": "Value iteration solution for pragmatic CIRL POMDP",
      "target_node": "Bayesian belief update with Boltzmann human action model",
      "description": "Belief updates define the transition dynamics that value iteration operates over.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Standard POMDP planning assumption; equations (2) feed into Bellman update.",
      "edge_rationale": "Shows dependency between mechanisms."
    },
    {
      "type": "validated_by",
      "source_node": "Value iteration solution for pragmatic CIRL POMDP",
      "target_node": "Meal-preparation simulation 86% success with pragmatic-pedagogic CIRL",
      "description": "Simulation applies value-iteration-derived policy, demonstrating high success.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Quantitative result in Section 3.",
      "edge_rationale": "Evidence shows mechanism’s effectiveness."
    },
    {
      "type": "validated_by",
      "source_node": "Literal interpretation of human actions by IRL robots",
      "target_node": "Meal-preparation simulation 15% success with literal IRL baseline",
      "description": "Low success rate empirically confirms the problem posed by literal interpretation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Quantitative comparison in Section 3.",
      "edge_rationale": "Supports severity of problem analysis."
    },
    {
      "type": "motivates",
      "source_node": "Meal-preparation simulation 86% success with pragmatic-pedagogic CIRL",
      "target_node": "Train collaborative robots with pragmatic-pedagogic CIRL policies via POMDP value iteration",
      "description": "Positive empirical results motivate adopting the training approach in practice.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct logical conclusion from proof-of-concept to proposed application.",
      "edge_rationale": "Connects evidence to actionable intervention."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1707.06354"
    },
    {
      "key": "title",
      "value": "Pragmatic-Pedagogic Value Alignment"
    },
    {
      "key": "authors",
      "value": [
        "Jaime F. Fisac",
        "Monica A. Gates",
        "Jessica B. Hamrick",
        "Chang Liu",
        "Dylan Hadfield-Menell",
        "Malayandi Palaniappan",
        "Dhruv Malik",
        "S. Shankar Sastry",
        "Thomas L. Griffiths",
        "Anca D. Dragan"
      ]
    },
    {
      "key": "date_published",
      "value": "2017-07-20T00:00:00Z"
    },
    {
      "key": "source",
      "value": "arxiv"
    }
  ]
}