{
  "nodes": [
    {
      "name": "Exploration failure in sparse-reward reinforcement learning environments",
      "aliases": [
        "insufficient exploration in sparse reward RL",
        "exploration-exploitation failure in sparse tasks"
      ],
      "type": "concept",
      "description": "RL agents often fail to reach states that yield high but rare rewards, resulting in poor policies and potential safety/performance issues.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 1 Introduction frames the difficulty of exploration in games like Montezuma’s Revenge."
    },
    {
      "name": "Insufficient exploitation of past high-return experiences in RL agents",
      "aliases": [
        "failure to leverage past good trajectories",
        "under-use of high-reward episodes"
      ],
      "type": "concept",
      "description": "Even when an agent occasionally encounters a rewarding trajectory, standard algorithms do not reliably reproduce and build on it, preventing deeper exploration.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Figure 1 and Section 1: baseline A2C picks up key occasionally but cannot consistently exploit it."
    },
    {
      "name": "Imitating trajectories with returns exceeding value estimates drives deeper exploration",
      "aliases": [
        "positive-advantage imitation aids exploration",
        "learn from better-than-expected episodes"
      ],
      "type": "concept",
      "description": "If the agent purposely repeats actions from episodes whose observed return is higher than its current value prediction, it is pushed into regions of state space nearer distal rewards.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivated in Sections 3 & 4: exploiting good experiences can indirectly foster exploration."
    },
    {
      "name": "Off-policy actor-critic imitation without importance sampling exploits good experiences",
      "aliases": [
        "importance-sampling-free imitation rationale",
        "design rationale for SIL objective"
      ],
      "type": "concept",
      "description": "Designs an objective that replays high-return state–action pairs without the instability of importance sampling, ensuring applicability to discrete and continuous control.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3 derives the SIL loss, emphasising no importance weights unlike ACER."
    },
    {
      "name": "Self-Imitation Learning objective with positive-advantage weighting",
      "aliases": [
        "SIL loss",
        "L_sil policy and value terms"
      ],
      "type": "concept",
      "description": "Loss L_sil updates policy/value only for (R − V) > 0, equivalent to lower-bound soft Q-learning when entropy weight → 0.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Equations 1–3 and Section 4.3 provide mechanism details."
    },
    {
      "name": "Prioritized experience replay by clipped advantage for SIL sampling",
      "aliases": [
        "advantage-based prioritized replay",
        "SIL sampling priority (R−V)+ "
      ],
      "type": "concept",
      "description": "Transitions are sampled proportionally to (R − V)+, increasing the fraction of useful imitation examples.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3 “Prioritized Replay” explains this mechanism."
    },
    {
      "name": "Montezuma's Revenge performance improvement with A2C+SIL",
      "aliases": [
        "Montezuma SIL result",
        "A2C vs A2C+SIL key-door evidence"
      ],
      "type": "concept",
      "description": "Learning curves show A2C+SIL rapidly learns to pick the key and open the door, unlike baseline A2C.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Figure 1 and Section 5.3 present these empirical results."
    },
    {
      "name": "A2C+SIL outperforming count-based exploration on hard Atari games",
      "aliases": [
        "SIL vs exploration bonus evidence",
        "Table 1 SIL rankings"
      ],
      "type": "concept",
      "description": "Across 6 of 7 hard-exploration Atari games, A2C+SIL achieves higher scores than state-of-the-art exploration-bonus agents.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Table 1 and Figure 3 in Section 5.3 provide quantitative support."
    },
    {
      "name": "Synergy of SIL and exploration bonus in Apple-Key-Door-Treasure domain",
      "aliases": [
        "A2C+SIL+EXP grid-world evidence"
      ],
      "type": "concept",
      "description": "In a grid-world with distractor apple rewards, only the combined method consistently reaches the treasure.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 5.2 reports this complementary effect."
    },
    {
      "name": "PPO+SIL gains on MuJoCo delayed-reward tasks",
      "aliases": [
        "MuJoCo delayed reward SIL evidence"
      ],
      "type": "concept",
      "description": "Adding SIL to PPO significantly widens the performance gap under delayed-reward conditions, indicating robustness.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 5.6 and Figure 5 show these continuous-control results."
    },
    {
      "name": "Integrate self-imitation learning updates into actor-critic training during RL fine-tuning",
      "aliases": [
        "apply SIL to A2C",
        "actor-critic with SIL"
      ],
      "type": "intervention",
      "description": "During fine-tuning/RL, alternate on-policy actor-critic updates with L_sil updates sampled from a replay buffer of high-return transitions.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Algorithm 1 (Section 3) occurs during training, i.e., fine-tuning/RL phase.",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Validated across 49 Atari games and several MuJoCo tasks (Sections 5.3–5.6), representing systematic prototype evidence.",
      "node_rationale": "Central actionable step proposed throughout the paper."
    },
    {
      "name": "Combine self-imitation learning with count-based exploration bonuses during RL training",
      "aliases": [
        "SIL plus exploration bonus",
        "A2C+SIL+EXP intervention"
      ],
      "type": "intervention",
      "description": "Add intrinsic exploration bonuses (e.g., pseudo-count rewards) while simultaneously applying SIL updates, leveraging their complementarity.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Used during training in Key-Door-Treasure experiments (Section 5.2).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated in toy domains; evidence limited to preliminary studies, hence experimental level.",
      "node_rationale": "Authors highlight complementary benefits; actionable for safety researchers seeking robust exploration."
    },
    {
      "name": "Apply prioritized replay based on positive advantage for self-imitation learning updates",
      "aliases": [
        "advantage-prioritized SIL replay",
        "priority (R−V)+ replay"
      ],
      "type": "intervention",
      "description": "Sample from the replay buffer with probability proportional to (R − V)+ to favour informative transitions for SIL.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Implemented inside the training loop (Section 3 Prioritized Replay).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Empirically validated in multiple Atari tasks (Sections 5.3–5.4).",
      "node_rationale": "Key practical step to make SIL efficient, directly suggested by authors."
    },
    {
      "name": "Augment PPO training with self-imitation learning objective for continuous control tasks",
      "aliases": [
        "PPO+SIL intervention",
        "SIL for PPO"
      ],
      "type": "intervention",
      "description": "During PPO optimisation, perform additional SIL updates on stored trajectories to enhance learning speed and robustness in continuous control.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Implemented during fine-tuning of PPO agents as per Section 5.6.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Tested on a limited set of MuJoCo tasks; evidence still at proof-of-concept stage.",
      "node_rationale": "Extends SIL beyond A2C, broadening applicability."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Exploration failure in sparse-reward reinforcement learning environments",
      "target_node": "Insufficient exploitation of past high-return experiences in RL agents",
      "description": "Agents fail to explore deeply because they do not capitalise on occasional high-reward episodes.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicitly argued in Section 1 and Figure 1; moderate empirical support.",
      "edge_rationale": "The paper links poor exploitation to failure to reach distal rewards."
    },
    {
      "type": "mitigated_by",
      "source_node": "Insufficient exploitation of past high-return experiences in RL agents",
      "target_node": "Imitating trajectories with returns exceeding value estimates drives deeper exploration",
      "description": "The theoretical insight offers a mitigation by repeating successful trajectories.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 3 hypothesis; supported by later results but not formally proven.",
      "edge_rationale": "Insight directly addresses the identified problem."
    },
    {
      "type": "implemented_by",
      "source_node": "Imitating trajectories with returns exceeding value estimates drives deeper exploration",
      "target_node": "Off-policy actor-critic imitation without importance sampling exploits good experiences",
      "description": "The design rationale operationalises the insight within an actor-critic framework.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 3 derives the objective from the insight.",
      "edge_rationale": "Shows concrete design translating theory to practice."
    },
    {
      "type": "implemented_by",
      "source_node": "Off-policy actor-critic imitation without importance sampling exploits good experiences",
      "target_node": "Self-Imitation Learning objective with positive-advantage weighting",
      "description": "The SIL loss is the concrete mechanism realising the design rationale.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Exact equations provided; direct derivation.",
      "edge_rationale": "Objective embodies the rationale."
    },
    {
      "type": "enabled_by",
      "source_node": "Self-Imitation Learning objective with positive-advantage weighting",
      "target_node": "Prioritized experience replay by clipped advantage for SIL sampling",
      "description": "Prioritised replay increases the proportion of transitions that satisfy (R−V)+, enhancing SIL effectiveness.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 3 Prioritized Replay explains necessity; empirically supported.",
      "edge_rationale": "Mechanism supports efficient implementation of objective."
    },
    {
      "type": "validated_by",
      "source_node": "Self-Imitation Learning objective with positive-advantage weighting",
      "target_node": "Montezuma's Revenge performance improvement with A2C+SIL",
      "description": "Empirical results show SIL leads to faster mastery of key-door reward sequence.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Clear improvement curve in Figure 1.",
      "edge_rationale": "Demonstrates success of mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Self-Imitation Learning objective with positive-advantage weighting",
      "target_node": "A2C+SIL outperforming count-based exploration on hard Atari games",
      "description": "Large-scale benchmarks confirm effectiveness across multiple tasks.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Table 1 quantitative comparisons.",
      "edge_rationale": "Provides broader evidence."
    },
    {
      "type": "validated_by",
      "source_node": "Prioritized experience replay by clipped advantage for SIL sampling",
      "target_node": "Synergy of SIL and exploration bonus in Apple-Key-Door-Treasure domain",
      "description": "Replay prioritisation plus SIL shows quick exploitation once treasure is found.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Single controlled grid-world experiment.",
      "edge_rationale": "Shows benefit of mechanism in complementary setup."
    },
    {
      "type": "validated_by",
      "source_node": "Self-Imitation Learning objective with positive-advantage weighting",
      "target_node": "PPO+SIL gains on MuJoCo delayed-reward tasks",
      "description": "Results indicate mechanism remains effective in continuous control settings.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Limited to six tasks; consistent but moderate evidence.",
      "edge_rationale": "Extends validation beyond Atari."
    },
    {
      "type": "motivates",
      "source_node": "Montezuma's Revenge performance improvement with A2C+SIL",
      "target_node": "Integrate self-imitation learning updates into actor-critic training during RL fine-tuning",
      "description": "Success on Montezuma encourages adoption of SIL within actor-critic training.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Strong empirical performance improvement.",
      "edge_rationale": "Direct empirical support for intervention."
    },
    {
      "type": "motivates",
      "source_node": "A2C+SIL outperforming count-based exploration on hard Atari games",
      "target_node": "Integrate self-imitation learning updates into actor-critic training during RL fine-tuning",
      "description": "Broad superiority suggests SIL integration is generally beneficial.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Multiple tasks, strong comparative evidence.",
      "edge_rationale": "Reinforces need for intervention."
    },
    {
      "type": "motivates",
      "source_node": "A2C+SIL outperforming count-based exploration on hard Atari games",
      "target_node": "Combine self-imitation learning with count-based exploration bonuses during RL training",
      "description": "While SIL outperforms, exploration bonuses sometimes still help (e.g., Venture), motivating combination.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Paper notes complementary strengths.",
      "edge_rationale": "Justifies hybrid intervention."
    },
    {
      "type": "motivates",
      "source_node": "Synergy of SIL and exploration bonus in Apple-Key-Door-Treasure domain",
      "target_node": "Combine self-imitation learning with count-based exploration bonuses during RL training",
      "description": "Empirical synergy shows combined method achieves optimal policy fastest.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Controlled experiment directly comparing methods.",
      "edge_rationale": "Strong evidence for proposed combo."
    },
    {
      "type": "motivates",
      "source_node": "Synergy of SIL and exploration bonus in Apple-Key-Door-Treasure domain",
      "target_node": "Apply prioritized replay based on positive advantage for self-imitation learning updates",
      "description": "Effectiveness depends on replay sampling that surfaces high-return transitions for imitation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Indirect but logical link shown in Section 5.2 discussion.",
      "edge_rationale": "Highlights importance of sampling strategy."
    },
    {
      "type": "motivates",
      "source_node": "PPO+SIL gains on MuJoCo delayed-reward tasks",
      "target_node": "Augment PPO training with self-imitation learning objective for continuous control tasks",
      "description": "Demonstrated improvements motivate adding SIL to PPO agents.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Evidence from Figure 5; limited but positive.",
      "edge_rationale": "Supports intervention extension."
    }
  ],
  "meta": [
    {
      "key": "title",
      "value": "Self-Imitation Learning"
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1806.05635"
    },
    {
      "key": "date_published",
      "value": "2018-06-14T00:00:00Z"
    },
    {
      "key": "authors",
      "value": [
        "Junhyuk Oh",
        "Yijie Guo",
        "Satinder Singh",
        "Honglak Lee"
      ]
    },
    {
      "key": "source",
      "value": "arxiv"
    }
  ]
}