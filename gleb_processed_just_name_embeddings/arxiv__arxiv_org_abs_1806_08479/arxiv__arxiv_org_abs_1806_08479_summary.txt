Summary  
Data source overview: The paper introduces “Human-Interactive Inverse Reinforcement Learning (HI-IRL)”, a training framework where a human expert supplies a small set of full demonstrations, names critical subgoal states, and afterwards only gives additional (partial) demonstrations on subtasks where the agent fails. The agent’s own failure rollouts are also used as negative evidence via the “Deep IRL from Failure (IRLFF)” optimisation. Experiments on grid-worlds and a car-parking simulator show that HI-IRL achieves near-expert performance with far fewer human-provided steps than baselines.  

EXTRACTION CONFIDENCE[85]  
The fabric captures every causal chain in the paper from top-level efficiency and reward-mis-specification risks to the concrete HI-IRL training interventions. All nodes are connected, granular, and named for easy merging. JSON structure validated with a linter.  

Inference strategy justification  
Where the paper implicitly motivates interventions (e.g. “collect partial demonstrations only where the agent struggles”), moderate inference (edge-confidence 2) was applied to make the training action explicit as an intervention node.

Extraction completeness explanation  
Three distinct risks, three problem causes, three theoretical insights, three design rationales, three implementation mechanisms, three validation evidences and three intervention nodes were extracted (total 21 nodes, ≈15 per 5 k words). These cover all reasoning segments described in Introduction, Method, and Experiments, and inter-connect into one fabric.

Key limitations  
1. Generalisability of grid-world and simulator experiments to real robots is not addressed in the source, so validation confidence is set to 3.  
2. Some edges rely on moderate inference because the paper presents results descriptively rather than as explicit causal tests.  
3. The fabric does not include broader AI-safety concerns (e.g. deception), as they are absent from the data source.



Improvements to instruction set  
1. Clarify whether multiple closely related interventions (e.g. HI-IRL vs its sub-components) should be separate nodes or merged; guidance would reduce inference variability.  
2. Provide explicit examples of edge confidences for qualitative user studies vs numerical ablations to standardise scoring.  
3. Add a short glossary in the prompt mapping lifecycle stages to common ML pipelines (e.g. interactive RL training) to avoid ambiguity.