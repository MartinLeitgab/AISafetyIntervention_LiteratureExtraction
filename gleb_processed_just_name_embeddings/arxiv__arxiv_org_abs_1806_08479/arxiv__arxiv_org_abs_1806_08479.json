{
  "nodes": [
    {
      "name": "Inefficient reward learning in complex sequential decision tasks",
      "aliases": [
        "data-inefficient IRL",
        "slow reward function acquisition"
      ],
      "type": "concept",
      "description": "Learning a reward model for long-horizon tasks like parking or navigation typically needs many demonstrations and iterations.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted in Introduction as a central challenge."
    },
    {
      "name": "Excessive human demonstration burden in robot training",
      "aliases": [
        "high-cost human feedback",
        "constant human interaction"
      ],
      "type": "concept",
      "description": "Providing full trajectories throughout training is costly and often impractical for human experts.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduction section lists human burden as a key problem."
    },
    {
      "name": "Reward misspecification causing subgoal failure in IRL",
      "aliases": [
        "subgoal misalignment",
        "critical state failure"
      ],
      "type": "concept",
      "description": "Sparse or misaligned reward estimates lead agents to miss critical states, preventing task completion.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Described in car-parking example where the car misses the turning point."
    },
    {
      "name": "Data sparsity of expert demonstrations in critical states",
      "aliases": [
        "missing coverage of key states"
      ],
      "type": "concept",
      "description": "Full-task demonstrations often visit critical subgoal states too rarely for IRL to learn correct rewards.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Enumerated as challenge (1) Data Sparsity in Introduction."
    },
    {
      "name": "Demonstration redundancy from full-task trajectories",
      "aliases": [
        "unnecessary repeated states",
        "redundant data collection"
      ],
      "type": "concept",
      "description": "Many parts of full demonstrations are already mastered by the agent, wasting human effort.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Challenge (3) Data Redundancy in Introduction."
    },
    {
      "name": "Lack of human high-level structure in IRL training",
      "aliases": [
        "missing subgoal guidance"
      ],
      "type": "concept",
      "description": "Standard IRL treats demonstrations as flat trajectories without leveraging humans’ hierarchical task insights.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivating paragraph on divide-and-conquer strategies."
    },
    {
      "name": "Human subgoal decomposition can guide efficient learning",
      "aliases": [
        "divide-and-conquer insight",
        "critical-state focus hypothesis"
      ],
      "type": "concept",
      "description": "If the agent is told which subgoal states must be visited, learning can focus on fewer subtasks.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Theory presented in 4.1 HI-IRL – Step 1."
    },
    {
      "name": "Failure trajectories provide informative negative signal",
      "aliases": [
        "learning from failure",
        "negative demonstrations"
      ],
      "type": "concept",
      "description": "Agent rollouts that fail highlight what not to do and improve reward inference when combined with successes.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3 ‘IRL from Failure’ and Step 4 in HI-IRL."
    },
    {
      "name": "Emphasizing critical subgoals increases reward alignment",
      "aliases": [
        "subgoal reward boosting"
      ],
      "type": "concept",
      "description": "Theoretical derivation (Eq. 20–22) shows states frequently demonstrated receive higher learned rewards.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Optimality of Subgoal Selection, Section 4.2."
    },
    {
      "name": "Structured human-agent interaction using subtasks",
      "aliases": [
        "subgoal-based interaction protocol"
      ],
      "type": "concept",
      "description": "Training loop where agent attempts subtasks sequentially and requests help only where it struggles.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Algorithm 2 HI-IRL description."
    },
    {
      "name": "Integrating failure experience with demonstration data",
      "aliases": [
        "dual positive–negative reward learning"
      ],
      "type": "concept",
      "description": "Combining successful and failed trajectories in optimisation encourages correct reward gradients.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Design choice stated in Step 4 and Algorithm 1."
    },
    {
      "name": "Deep neural reward networks for raw-state representation",
      "aliases": [
        "visual feature reward mapping"
      ],
      "type": "concept",
      "description": "Convolutional layers with batch-norm and ReLU extract features before linear reward heads.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Background section ‘Maximum Entropy Deep IRL’."
    },
    {
      "name": "HI-IRL algorithm with subgoal specification and partial demonstrations",
      "aliases": [
        "Human-Interactive IRL procedure"
      ],
      "type": "concept",
      "description": "Concrete multi-step training algorithm (Step 1-4) leveraging human subgoals and failure data.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Algorithm 2 in Section 4.1."
    },
    {
      "name": "Deep IRL from Failure gradient updates",
      "aliases": [
        "IRLFF optimisation"
      ],
      "type": "concept",
      "description": "Optimisation equations (12-16) updating reward parameters using Lagrange multipliers for failure gaps.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Algorithm 1 and associated equations."
    },
    {
      "name": "Neural conv-BN-ReLU-FC architecture to map images to reward",
      "aliases": [
        "CNN reward mapper"
      ],
      "type": "concept",
      "description": "3-layer (or 2-layer) CNN followed by FC layers producing scalar reward and FC1 feature vector.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in Step 4 Learning Reward Function paragraph."
    },
    {
      "name": "Grid-world and car-parking experiments show reduced demonstrations",
      "aliases": [
        "demo-efficiency experimental result"
      ],
      "type": "concept",
      "description": "Plots (Figure 4) demonstrate fewer human steps required for HI-IRL to reach test goals.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 5.1 Results."
    },
    {
      "name": "Performance curves indicate near-oracle success with fewer steps",
      "aliases": [
        "HI-IRL performance evidence"
      ],
      "type": "concept",
      "description": "HI-IRL matches or exceeds baselines after limited interaction, validating design rationale.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Analysis paragraph in Results."
    },
    {
      "name": "Reward maps show higher values at critical subgoal states",
      "aliases": [
        "subgoal reward visualization evidence"
      ],
      "type": "concept",
      "description": "Visualisations confirm that learned reward peaks align with human-defined subgoals.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Optimality proof and accompanying figures."
    },
    {
      "name": "Fine-tune agents with human-interactive subgoal-supervised IRL incorporating failure learning",
      "aliases": [
        "train with HI-IRL",
        "apply HI-IRL framework"
      ],
      "type": "intervention",
      "description": "During fine-tuning/RL, implement the HI-IRL algorithm: initial demonstrations, subgoal specification, subtask trials, partial demos on failures, and IRLFF updates.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Algorithm executed after initial model exists; classified as Fine-Tuning/RL (Section 4.1).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Evaluated only in simulation studies (Section 5 Experiments).",
      "node_rationale": "Primary actionable method proposed."
    },
    {
      "name": "Collect partial demonstrations only for struggling subtasks during training",
      "aliases": [
        "on-demand subtask demos"
      ],
      "type": "intervention",
      "description": "Request human demonstrations solely for subtasks where the agent fails, avoiding redundant full-task demos.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Part of interactive fine-tuning loop (Algorithm 2).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Tested in controlled experiments, not deployed (Section 5).",
      "node_rationale": "Key human-efficiency practice derived from design."
    },
    {
      "name": "Specify critical subgoals upfront to structure learning process",
      "aliases": [
        "human subgoal annotation"
      ],
      "type": "intervention",
      "description": "At training start, a human labels high-probability critical states so subtasks can be generated automatically.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Occurs during interactive fine-tuning setup.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Evaluated in simulations only.",
      "node_rationale": "Enables the subgoal-based interaction protocol."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Inefficient reward learning in complex sequential decision tasks",
      "target_node": "Data sparsity of expert demonstrations in critical states",
      "description": "Insufficient coverage of critical states slows reward estimation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicitly stated in Introduction challenge (1).",
      "edge_rationale": "Section 1 describes data sparsity leading to difficulty."
    },
    {
      "type": "caused_by",
      "source_node": "Inefficient reward learning in complex sequential decision tasks",
      "target_node": "Demonstration redundancy from full-task trajectories",
      "description": "Redundant state visits waste demonstration budget, leaving less coverage for hard areas.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Discussed qualitatively in Introduction challenge (3).",
      "edge_rationale": "Same paragraph as redundancy."
    },
    {
      "type": "caused_by",
      "source_node": "Reward misspecification causing subgoal failure in IRL",
      "target_node": "Data sparsity of expert demonstrations in critical states",
      "description": "Sparse data around subgoals leads the learnt reward to undervalue them.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Equation-based proof in Section 4.2.",
      "edge_rationale": "Optimality of Subgoal Selection derivation."
    },
    {
      "type": "caused_by",
      "source_node": "Excessive human demonstration burden in robot training",
      "target_node": "Demonstration redundancy from full-task trajectories",
      "description": "Full trajectories include parts the agent already knows, increasing human effort unnecessarily.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Stated directly in Introduction challenge (3).",
      "edge_rationale": "Introduction."
    },
    {
      "type": "caused_by",
      "source_node": "Lack of human high-level structure in IRL training",
      "target_node": "Inefficient reward learning in complex sequential decision tasks",
      "description": "Without subgoal hints, the agent explores blindly, worsening data efficiency.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoning in Section 4.1 Step 1.",
      "edge_rationale": "Step 1 explanation."
    },
    {
      "type": "mitigated_by",
      "source_node": "Data sparsity of expert demonstrations in critical states",
      "target_node": "Human subgoal decomposition can guide efficient learning",
      "description": "Subgoal hints channel limited demos to critical areas.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Theoretical claim, not directly empirically isolated.",
      "edge_rationale": "Section 4.1 rationale."
    },
    {
      "type": "mitigated_by",
      "source_node": "Demonstration redundancy from full-task trajectories",
      "target_node": "Human subgoal decomposition can guide efficient learning",
      "description": "Subtasks eliminate need for redundant early-stage states.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explained qualitatively in same section.",
      "edge_rationale": "Design description."
    },
    {
      "type": "mitigated_by",
      "source_node": "Lack of human high-level structure in IRL training",
      "target_node": "Failure trajectories provide informative negative signal",
      "description": "Using failure data adds structure contrastive to success.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Moderate inference; paper links failure to improved learning broadly.",
      "edge_rationale": "Section 3 IRL from Failure."
    },
    {
      "type": "enabled_by",
      "source_node": "Human subgoal decomposition can guide efficient learning",
      "target_node": "Structured human-agent interaction using subtasks",
      "description": "The insight motivates designing an interaction protocol around subtasks.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct design motivation.",
      "edge_rationale": "Section 4.1."
    },
    {
      "type": "enabled_by",
      "source_node": "Failure trajectories provide informative negative signal",
      "target_node": "Integrating failure experience with demonstration data",
      "description": "Negative signal inclusion stems from recognising value of failures.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 3 formalises learning from failure.",
      "edge_rationale": "Background IRL from Failure."
    },
    {
      "type": "enabled_by",
      "source_node": "Emphasizing critical subgoals increases reward alignment",
      "target_node": "Structured human-agent interaction using subtasks",
      "description": "Ensuring demonstrations focus on subgoals is central to the interaction design.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Equation-backed optimality proof.",
      "edge_rationale": "Section 4.2 leads into protocol."
    },
    {
      "type": "implemented_by",
      "source_node": "Structured human-agent interaction using subtasks",
      "target_node": "HI-IRL algorithm with subgoal specification and partial demonstrations",
      "description": "Algorithm 2 operationalises the interaction scheme.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Algorithm explicitly implements rationale.",
      "edge_rationale": "Algorithm 2 heading."
    },
    {
      "type": "implemented_by",
      "source_node": "Integrating failure experience with demonstration data",
      "target_node": "Deep IRL from Failure gradient updates",
      "description": "IRLFF equations update reward using both positive and negative samples.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Mathematical formulation given.",
      "edge_rationale": "Equations 12-16."
    },
    {
      "type": "implemented_by",
      "source_node": "Deep neural reward networks for raw-state representation",
      "target_node": "Neural conv-BN-ReLU-FC architecture to map images to reward",
      "description": "Specific CNN architecture instantiates the deep reward model.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Architecture described in detail.",
      "edge_rationale": "Learning Reward Function paragraph."
    },
    {
      "type": "validated_by",
      "source_node": "HI-IRL algorithm with subgoal specification and partial demonstrations",
      "target_node": "Grid-world and car-parking experiments show reduced demonstrations",
      "description": "Empirical results show the algorithm meets efficiency goals.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Controlled experiments with baselines.",
      "edge_rationale": "Figure 4 discussion."
    },
    {
      "type": "validated_by",
      "source_node": "Deep IRL from Failure gradient updates",
      "target_node": "Performance curves indicate near-oracle success with fewer steps",
      "description": "Adding failure learning contributes to performance gains.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Evidence indirect but supported by ablations in text.",
      "edge_rationale": "Results analysis referencing failures."
    },
    {
      "type": "validated_by",
      "source_node": "Neural conv-BN-ReLU-FC architecture to map images to reward",
      "target_node": "Reward maps show higher values at critical subgoal states",
      "description": "Visualisations confirm network correctly emphasises subgoals.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Qualitative visual evidence.",
      "edge_rationale": "Figure 3 & associated text."
    },
    {
      "type": "motivates",
      "source_node": "Grid-world and car-parking experiments show reduced demonstrations",
      "target_node": "Fine-tune agents with human-interactive subgoal-supervised IRL incorporating failure learning",
      "description": "Successful trials motivate adopting the full HI-IRL training procedure.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct outcome of experiments.",
      "edge_rationale": "Conclusions section."
    },
    {
      "type": "refined_by",
      "source_node": "Fine-tune agents with human-interactive subgoal-supervised IRL incorporating failure learning",
      "target_node": "Collect partial demonstrations only for struggling subtasks during training",
      "description": "Partial demos detail refines the broader HI-IRL intervention.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Moderate inference to separate sub-action.",
      "edge_rationale": "Algorithm 2 steps."
    },
    {
      "type": "required_by",
      "source_node": "Fine-tune agents with human-interactive subgoal-supervised IRL incorporating failure learning",
      "target_node": "Specify critical subgoals upfront to structure learning process",
      "description": "Subgoal specification is prerequisite for subtask generation in HI-IRL.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Algorithm requires subgoal set at start.",
      "edge_rationale": "Step 1 of HI-IRL."
    }
  ],
  "meta": [
    {
      "key": "authors",
      "value": [
        "Xinlei Pan",
        "Eshed Ohn-Bar",
        "Nicholas Rhinehart",
        "Yan Xu",
        "Yilin Shen",
        "Kris M. Kitani"
      ]
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1806.08479"
    },
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "title",
      "value": "Human-Interactive Subgoal Supervision for Efficient Inverse Reinforcement Learning"
    },
    {
      "key": "date_published",
      "value": "2018-06-22T00:00:00Z"
    }
  ]
}