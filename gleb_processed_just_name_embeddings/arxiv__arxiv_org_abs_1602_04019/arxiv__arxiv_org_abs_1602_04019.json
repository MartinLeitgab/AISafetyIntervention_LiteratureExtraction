{
  "nodes": [
    {
      "name": "Inadequate AI safety preparedness from overestimated AI energy requirements",
      "aliases": [
        "AI safety complacency due to energy misconceptions",
        "Delayed safety measures from perceived AI power infeasibility"
      ],
      "type": "concept",
      "description": "Risk that stakeholders postpone or down-prioritise alignment and governance work because they believe advanced AI would need unrealistically high power budgets.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central risk repeatedly highlighted when critiquing Krauss’s claim; see Introduction & Conclusion."
    },
    {
      "name": "Overestimation of AI energy needs via neuron-equivalent scaling",
      "aliases": [
        "Inflated AI power estimates from brain emulation assumptions",
        "Neuron-level scaling energy estimate error"
      ],
      "type": "concept",
      "description": "Problem of computing AI power budgets by multiplying brain-level operations (e.g. spikes) by present device joules per flop, leading to 10-12× overestimates.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explicitly criticised throughout ‘Computer and brain emulation energy use’ section."
    },
    {
      "name": "Misapplied neuron-level energy scaling inflates AI feasibility judgements",
      "aliases": [
        "Neuron-equivalent assumption is unsuitable for de-novo AI",
        "Brain emulation baseline misapplication"
      ],
      "type": "concept",
      "description": "The underlying assumption that de-novo AI must replicate each neuronal operation rather than higher-level abstractions.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Causal mechanism for the overestimation identified in ‘Computer and brain emulation energy use’."
    },
    {
      "name": "Higher-level representation processing enables lower computational requirements than neural emulation",
      "aliases": [
        "Task-level abstraction reduces operations",
        "Cognitive tasks need far fewer operations than spikes"
      ],
      "type": "concept",
      "description": "Insight that cognition can be executed on compressed, symbolic or vector representations, avoiding billions of spikes per second.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in ‘Energy of AI’ section as the primary reason energy comparisons are misleading."
    },
    {
      "name": "Koomey's law indicates energy per operation halves every 1.57 years",
      "aliases": [
        "Historical energy efficiency trend",
        "Computing joules/op improvement trend"
      ],
      "type": "concept",
      "description": "Empirical trend of continuous improvement in electrical efficiency of computing hardware, outpacing the 3-year figure cited by Krauss.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Quantitative evidence from ‘Computer and brain emulation energy use’ subsection."
    },
    {
      "name": "Base AI energy feasibility estimates on algorithmic-level computational costs",
      "aliases": [
        "Algorithmic abstraction energy estimation approach",
        "Top-down AI power modelling rationale"
      ],
      "type": "concept",
      "description": "Design principle: estimate energy needs from the cost of processing high-level representations rather than neuron firing counts.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposed remedy to the misestimation problem, articulated in ‘Energy of AI’ and ‘Conclusion’."
    },
    {
      "name": "Leverage hardware specialization to reduce AI power requirements",
      "aliases": [
        "Use GPUs/ASICs/analog accelerators for efficient AI",
        "Specialised hardware energy-efficiency rationale"
      ],
      "type": "concept",
      "description": "Design choice to exploit hardware whose operations/energy ratios are 1–3 orders of magnitude better than general-purpose CPUs.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted in ‘Energy of AI’ citing GPUs and prototype analog chips."
    },
    {
      "name": "Abstraction-based AI computational cost modelling tools",
      "aliases": [
        "Task-level energy forecasting models",
        "High-level AI power estimation calculators"
      ],
      "type": "concept",
      "description": "Practical mechanism: analytic or simulation tools that take algorithmic workloads and hardware joules/op curves to yield realistic power needs.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation path suggested by design rationale; draws on methods in ‘Energy of AI’."
    },
    {
      "name": "GPU/ASIC/analog deep learning accelerators achieving sub-nJ per flop",
      "aliases": [
        "Energy-efficient AI hardware accelerators",
        "Specialised low-power DL chips"
      ],
      "type": "concept",
      "description": "Concrete hardware (e.g. NVIDIA GPUs at 0.04 nJ/flop, analog 1 TOPS/W chips) delivering orders-of-magnitude better joules/op.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Cited across ‘Computer and brain emulation energy use’ and ‘Energy of AI’ sections."
    },
    {
      "name": "Empirical deep learning benchmarks on GPUs show low energy consumption for human-relevant subtasks",
      "aliases": [
        "Speech/ImageNet GPU energy evidence",
        "Benchmark evidence for low-power AI"
      ],
      "type": "concept",
      "description": "Findings: end-to-end speech recognition and ImageNet training are achieved on ~100 W GPUs within days, far below brain-level power.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Evidence block combining Catanzaro 2015 and Liu 2016 examples."
    },
    {
      "name": "Prototype analog deep learning chip demonstrates 1 TOPS/W efficiency",
      "aliases": [
        "Analog DL chip energy benchmark",
        "Lu et al. 2015 efficiency evidence"
      ],
      "type": "concept",
      "description": "Validation that specialised silicon can deliver extremely high throughput per watt, supporting hardware specialisation claims.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Drawn from Lu et al. 2015 citation in ‘Energy of AI’."
    },
    {
      "name": "Integrate abstraction-based energy forecasting into AI risk timelines",
      "aliases": [
        "Apply realistic AI power models in safety planning",
        "Energy-aware AI scenario analysis"
      ],
      "type": "intervention",
      "description": "Action: update forecasting, governance and alignment road-mapping efforts to use algorithmic-level energy models rather than neuron-equivalent estimates, reducing complacency about timelines.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "Applies at policy/strategic forecasting stage discussed implicitly in ‘Conclusion’.",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Only foundational conceptual work exists; no systematic tools deployed yet.",
      "node_rationale": "Directly follows from validation evidence contradicting high-energy claims."
    },
    {
      "name": "Adopt specialised energy-efficient hardware and algorithmic abstraction when designing AI systems",
      "aliases": [
        "Design AI with GPUs/ASICs & high-level representations",
        "Energy-efficient AI system design practice"
      ],
      "type": "intervention",
      "description": "Action: during model design choose architectures and dedicated accelerators that minimise joules per operation, enabling faster progress without prohibitive power use.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Taken at initial architecture and hardware co-design stage.",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Prototype chips (Lu et al. 2015) and GPU pipelines already demonstrate feasibility but lack wide operational deployment.",
      "node_rationale": "Operationalises the hardware specialisation design rationale."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Inadequate AI safety preparedness from overestimated AI energy requirements",
      "target_node": "Overestimation of AI energy needs via neuron-equivalent scaling",
      "description": "Safety complacency stems from believing AI would need impractically high power.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit logical link in Introduction; no quantitative study so medium.",
      "edge_rationale": "Introduces causal connection identified in early discussion."
    },
    {
      "type": "caused_by",
      "source_node": "Overestimation of AI energy needs via neuron-equivalent scaling",
      "target_node": "Misapplied neuron-level energy scaling inflates AI feasibility judgements",
      "description": "The problematic estimates originate in assuming each spike/action potential must be simulated.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Argument detailed in ‘Computer and brain emulation energy use’.",
      "edge_rationale": "Shows mechanistic origin of the problem."
    },
    {
      "type": "addressed_by",
      "source_node": "Overestimation of AI energy needs via neuron-equivalent scaling",
      "target_node": "Higher-level representation processing enables lower computational requirements than neural emulation",
      "description": "Recognising algorithmic-level representations corrects the inflated estimates.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoning in ‘Energy of AI’; qualitative but well-argued.",
      "edge_rationale": "Maps problem to corrective insight."
    },
    {
      "type": "addressed_by",
      "source_node": "Overestimation of AI energy needs via neuron-equivalent scaling",
      "target_node": "Koomey's law indicates energy per operation halves every 1.57 years",
      "description": "Historical efficiency improvements further undermine pessimistic power claims.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Cited quantitative trend; applies directly.",
      "edge_rationale": "Adds empirical counter-argument to the problem."
    },
    {
      "type": "mitigated_by",
      "source_node": "Higher-level representation processing enables lower computational requirements than neural emulation",
      "target_node": "Leverage hardware specialization to reduce AI power requirements",
      "description": "Using specialised hardware capitalises on representational efficiencies.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Coupled argument in ‘Energy of AI’.",
      "edge_rationale": "Connects insight to design approach."
    },
    {
      "type": "mitigated_by",
      "source_node": "Koomey's law indicates energy per operation halves every 1.57 years",
      "target_node": "Leverage hardware specialization to reduce AI power requirements",
      "description": "Hardware specialisation aligns with the documented efficiency trend.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Light inference; trend supports but is not solely about specialisation.",
      "edge_rationale": "Links empirical trend to design choice."
    },
    {
      "type": "mitigated_by",
      "source_node": "Misapplied neuron-level energy scaling inflates AI feasibility judgements",
      "target_node": "Base AI energy feasibility estimates on algorithmic-level computational costs",
      "description": "Switching estimation methodology removes reliance on neuron-level counts.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct prescription in argumentative text.",
      "edge_rationale": "Design rationale corrects faulty theoretical assumption."
    },
    {
      "type": "implemented_by",
      "source_node": "Base AI energy feasibility estimates on algorithmic-level computational costs",
      "target_node": "Abstraction-based AI computational cost modelling tools",
      "description": "Practical tools carry out the algorithmic-level estimation approach.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Implementation implied though not detailed; medium support.",
      "edge_rationale": "Links design rationale to concrete mechanism."
    },
    {
      "type": "implemented_by",
      "source_node": "Leverage hardware specialization to reduce AI power requirements",
      "target_node": "GPU/ASIC/analog deep learning accelerators achieving sub-nJ per flop",
      "description": "Specialised accelerators embody the hardware-focused design principle.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Multiple hardware examples cited.",
      "edge_rationale": "Establishes mechanism for design rationale."
    },
    {
      "type": "validated_by",
      "source_node": "Abstraction-based AI computational cost modelling tools",
      "target_node": "Empirical deep learning benchmarks on GPUs show low energy consumption for human-relevant subtasks",
      "description": "Benchmark results support the accuracy of abstraction-based estimates.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Quantitative task performance and wattage reported in cited work.",
      "edge_rationale": "Evidence backs implementation mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "GPU/ASIC/analog deep learning accelerators achieving sub-nJ per flop",
      "target_node": "Prototype analog deep learning chip demonstrates 1 TOPS/W efficiency",
      "description": "Prototype chip measurements verify high efficiency claims.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Peer-reviewed hardware result.",
      "edge_rationale": "Empirical validation for mechanism."
    },
    {
      "type": "motivates",
      "source_node": "Empirical deep learning benchmarks on GPUs show low energy consumption for human-relevant subtasks",
      "target_node": "Integrate abstraction-based energy forecasting into AI risk timelines",
      "description": "Evidence encourages policy actors to update forecasting tools.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical extrapolation from data to planning; medium support.",
      "edge_rationale": "Shows why the intervention is warranted."
    },
    {
      "type": "motivates",
      "source_node": "Prototype analog deep learning chip demonstrates 1 TOPS/W efficiency",
      "target_node": "Adopt specialised energy-efficient hardware and algorithmic abstraction when designing AI systems",
      "description": "Hardware validation motivates practical adoption in system design.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Prototype performance provides credible incentive but limited deployment.",
      "edge_rationale": "Connects validation evidence to design intervention."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2016-02-12T00:00:00Z"
    },
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "title",
      "value": "Energetics of the brain and AI"
    },
    {
      "key": "authors",
      "value": [
        "Anders Sandberg"
      ]
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1602.04019"
    }
  ]
}