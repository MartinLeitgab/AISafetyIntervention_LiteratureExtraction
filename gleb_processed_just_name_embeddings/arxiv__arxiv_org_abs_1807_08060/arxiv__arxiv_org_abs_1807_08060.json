{
  "nodes": [
    {
      "name": "Catastrophic state visitation during exploration in reinforcement learning",
      "aliases": [
        "unsafe state visitation in RL",
        "exploration-induced catastrophes in RL"
      ],
      "type": "concept",
      "description": "During exploratory learning, RL agents can enter states that incur large negative rewards or irreversible damage, posing safety risks.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in Introduction section as the main safety concern addressed by the Safe Option-Critic framework."
    },
    {
      "name": "Unconstrained exploration in hierarchical option learning",
      "aliases": [
        "unsafe exploration in options",
        "lack of safety constraints in option-critic"
      ],
      "type": "concept",
      "description": "Option-based hierarchical RL traditionally optimises only cumulative reward, giving no guidance to avoid dangerous states while exploring.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 1 and 3 identify that vanilla option-critic lacks any explicit safety or risk constraints."
    },
    {
      "name": "Variance of temporal difference error as uncertainty metric in state-option pairs",
      "aliases": [
        "TD error variance as uncertainty",
        "controllability via TD error variance"
      ],
      "type": "concept",
      "description": "A high variance in the TD error signals uncertainty about the true value of a state-option pair; defining controllability as the negative variance provides a quantitative risk measure.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3 formulates controllability C = –Var(δ) as the core theoretical insight connecting uncertainty to safety."
    },
    {
      "name": "Incorporate controllability regularization into option policies to reduce TD error variance",
      "aliases": [
        "controllability regularization in options",
        "risk-aware option policy objective"
      ],
      "type": "concept",
      "description": "Augment the option-critic objective with ψ·C so each option maximises reward while penalising high TD-variance, encouraging risk-averse behaviour.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Derived in Safe Option-Critic Model section as the central design choice translating the insight into a training objective."
    },
    {
      "name": "Safe option-critic algorithm with controllability regularization",
      "aliases": [
        "Safe-OC algorithm",
        "controllability-augmented option-critic"
      ],
      "type": "concept",
      "description": "Algorithm 1 modifies intra-option and termination gradients to include controllability, implemented for tabular and linear function approximation.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Algorithm details provided in Section 3; this is the concrete mechanism executing the design rationale."
    },
    {
      "name": "Safe asynchronous advantage option-critic integrating controllability and deliberation",
      "aliases": [
        "Safe-A2OC algorithm",
        "controllability-regularized A2OC"
      ],
      "type": "concept",
      "description": "Extension of A2OC to deep RL: adds controllability term to the intra-option policy gradient while retaining deliberation cost and asynchronous updates.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4.3 describes how controllability is integrated into deep A2OC for Atari games."
    },
    {
      "name": "Hyperparameter tuning of controllability regularizer ψ",
      "aliases": [
        "ψ parameter selection",
        "risk aversion degree tuning"
      ],
      "type": "concept",
      "description": "Grid-searching ψ balances exploration and safety; too high ψ (>0.3) yields excessive risk aversion, while 0.05–0.1 delivers best performance.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Parameter study detailed in Sections 4.1–4.3 motivates systematic tuning as part of the method."
    },
    {
      "name": "Reduced unsafe state visitation and return variance in Four-Rooms with Safe-OC",
      "aliases": [
        "Four-Rooms validation Safe-OC",
        "gridworld reduced variance evidence"
      ],
      "type": "concept",
      "description": "Safe-OC agent avoided slippery ‘frozen’ states and showed lower return variance compared to vanilla option-critic over 200 trials.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Empirical results presented in Figures 2-4 of Section 4.1."
    },
    {
      "name": "Faster learning and lower variance in CartPole with ψ=0.25 in Safe-OC",
      "aliases": [
        "CartPole validation Safe-OC",
        "cartpole reduced variance evidence"
      ],
      "type": "concept",
      "description": "Safe-OC achieved quicker convergence and smaller standard-deviation of returns than baselines in 50-trial experiments.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Reported in Figure 5, Section 4.2."
    },
    {
      "name": "Improved Atari scores in MsPacman, Amidar, Q*Bert with ψ 0.05-0.10 in Safe-A2OC",
      "aliases": [
        "Atari validation Safe-A2OC",
        "ALE controllability evidence"
      ],
      "type": "concept",
      "description": "Safe-A2OC surpassed vanilla A2OC and several DQN variants, and beat primitive-action baselines in two of three games after 80 M frames.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Learning curves and Table 1 in Section 4.3 provide this evidence."
    },
    {
      "name": "Optimal ψ between 0.05 and 0.1 yields best safety-performance trade-off across tasks",
      "aliases": [
        "ψ sweet spot evidence",
        "parameter tuning validation"
      ],
      "type": "concept",
      "description": "Across GridWorld, CartPole and Atari, mid-range ψ produced the best convergence and safety, while high ψ >0.3 harmed performance.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Synthesised from hyper-parameter analyses in Sections 4.1–4.3."
    },
    {
      "name": "Fine-tune RL agents with safe option-critic framework using controllability regularization",
      "aliases": [
        "apply Safe-OC during training",
        "controllability-regularized option learning"
      ],
      "type": "intervention",
      "description": "During fine-tuning/RL phase, replace vanilla option-critic with the Safe-OC objective (reward plus ψ·C) and corresponding gradient updates.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Implemented during the training loop (Algorithm 1, Section 3).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Validated across multiple benchmark tasks but not yet deployed in real-world systems.",
      "node_rationale": "Central actionable step proposed by the paper to mitigate unsafe exploration."
    },
    {
      "name": "Conduct grid search to select controllability regularizer ψ between 0.05-0.1 during Safe-OC training",
      "aliases": [
        "ψ tuning during training",
        "select risk-aversion coefficient"
      ],
      "type": "intervention",
      "description": "Before or during fine-tuning, evaluate several ψ values (e.g., 0, 0.05, 0.1, 0.15) and choose the range that balances exploration and safety.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Executed alongside training hyper-parameter optimisation (Sections 4.1–4.3).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated experimentally but not yet systematic across domains.",
      "node_rationale": "Necessary to operationalise the controllability concept without excessive risk-aversion."
    },
    {
      "name": "Deploy Safe-A2OC with controllability regularization in deep RL agents for high-variance environments",
      "aliases": [
        "apply Safe-A2OC in Atari-scale tasks",
        "deep Safe-OC deployment"
      ],
      "type": "intervention",
      "description": "Use Safe-A2OC (Equation 22) with deliberation cost and ψ≈0.05-0.1 in production-like image-based RL settings.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Applies the trained Safe-A2OC models during live interaction with complex environments (Section 4.3).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Prototype-level validation on Atari; not yet field-deployed but beyond proof-of-concept.",
      "node_rationale": "Represents the paper’s recommended path for practical, large-scale application."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Catastrophic state visitation during exploration in reinforcement learning",
      "target_node": "Unconstrained exploration in hierarchical option learning",
      "description": "Lack of safety constraints in option policies leads to exploration that can enter catastrophic states.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicitly stated in Introduction that unsafe exploration arises because agents are unaware of dangerous states.",
      "edge_rationale": "Text links unsafe behaviours to unconstrained exploration without safety terms."
    },
    {
      "type": "mitigated_by",
      "source_node": "Unconstrained exploration in hierarchical option learning",
      "target_node": "Variance of temporal difference error as uncertainty metric in state-option pairs",
      "description": "Using TD-error variance as an uncertainty signal provides a basis to control risky exploration.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 3 proposes TD-variance as the remedy for the identified problem.",
      "edge_rationale": "Theoretical insight offers a mitigation path for the problem analysis."
    },
    {
      "type": "mitigated_by",
      "source_node": "Variance of temporal difference error as uncertainty metric in state-option pairs",
      "target_node": "Incorporate controllability regularization into option policies to reduce TD error variance",
      "description": "Regularising option objectives with controllability leverages the TD-variance insight to mitigate uncertainty.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Design rationale directly follows the theoretical insight in Section 3.",
      "edge_rationale": "Shows transition from insight to concrete solution principle."
    },
    {
      "type": "implemented_by",
      "source_node": "Incorporate controllability regularization into option policies to reduce TD error variance",
      "target_node": "Safe option-critic algorithm with controllability regularization",
      "description": "Safe-OC algorithm realises the regularised objective through modified gradients (Algorithm 1).",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Full mathematical derivation and algorithm listing provided.",
      "edge_rationale": "Implementation mechanism faithfully encodes the design rationale."
    },
    {
      "type": "validated_by",
      "source_node": "Safe option-critic algorithm with controllability regularization",
      "target_node": "Reduced unsafe state visitation and return variance in Four-Rooms with Safe-OC",
      "description": "GridWorld experiments confirm Safe-OC lessens visits to unsafe frozen states and lowers return variance.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Controlled comparison over 200 trials in Section 4.1.",
      "edge_rationale": "Empirical evidence supports the algorithm’s effectiveness."
    },
    {
      "type": "validated_by",
      "source_node": "Safe option-critic algorithm with controllability regularization",
      "target_node": "Faster learning and lower variance in CartPole with ψ=0.25 in Safe-OC",
      "description": "CartPole results show quicker convergence and reduced variance using Safe-OC.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "50-trial study; smaller scale than GridWorld but still systematic.",
      "edge_rationale": "Further validation for Safe-OC in a different domain."
    },
    {
      "type": "implemented_by",
      "source_node": "Incorporate controllability regularization into option policies to reduce TD error variance",
      "target_node": "Safe asynchronous advantage option-critic integrating controllability and deliberation",
      "description": "Safe-A2OC extends the same regularised objective to a deep, asynchronous setting.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 4.3 describes the modification of A2OC with the new gradient term.",
      "edge_rationale": "Shows alternative implementation path of the design rationale."
    },
    {
      "type": "validated_by",
      "source_node": "Safe asynchronous advantage option-critic integrating controllability and deliberation",
      "target_node": "Improved Atari scores in MsPacman, Amidar, Q*Bert with ψ 0.05-0.10 in Safe-A2OC",
      "description": "Atari benchmarks demonstrate that Safe-A2OC outperforms baselines.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "80 M-frame experiments with multiple seeds; strong but limited to three games.",
      "edge_rationale": "Provides empirical support for the deep implementation."
    },
    {
      "type": "implemented_by",
      "source_node": "Incorporate controllability regularization into option policies to reduce TD error variance",
      "target_node": "Hyperparameter tuning of controllability regularizer ψ",
      "description": "Practical application of the design requires selecting an appropriate ψ value.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Based on empirical tuning advice; not formally derived.",
      "edge_rationale": "Parameter selection is part of implementing the design rationale."
    },
    {
      "type": "validated_by",
      "source_node": "Hyperparameter tuning of controllability regularizer ψ",
      "target_node": "Optimal ψ between 0.05 and 0.1 yields best safety-performance trade-off across tasks",
      "description": "Cross-domain experiments identify a narrow ψ range that balances risk and reward.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Consistent empirical observations in several environments.",
      "edge_rationale": "Evidence supports the tuning mechanism."
    },
    {
      "type": "motivates",
      "source_node": "Reduced unsafe state visitation and return variance in Four-Rooms with Safe-OC",
      "target_node": "Fine-tune RL agents with safe option-critic framework using controllability regularization",
      "description": "Successful GridWorld results motivate adopting Safe-OC during training.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Clear performance gains documented in Section 4.1.",
      "edge_rationale": "Validation evidence drives the proposed intervention."
    },
    {
      "type": "motivates",
      "source_node": "Faster learning and lower variance in CartPole with ψ=0.25 in Safe-OC",
      "target_node": "Fine-tune RL agents with safe option-critic framework using controllability regularization",
      "description": "CartPole findings reinforce the motivation for Safe-OC fine-tuning.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Consistent but smaller-scale evidence.",
      "edge_rationale": "Additional empirical support."
    },
    {
      "type": "motivates",
      "source_node": "Improved Atari scores in MsPacman, Amidar, Q*Bert with ψ 0.05-0.10 in Safe-A2OC",
      "target_node": "Deploy Safe-A2OC with controllability regularization in deep RL agents for high-variance environments",
      "description": "Atari improvements motivate applying Safe-A2OC in real-world high-variance tasks.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Prototype-level evidence across multiple games.",
      "edge_rationale": "Shows practical benefit for deployment."
    },
    {
      "type": "motivates",
      "source_node": "Optimal ψ between 0.05 and 0.1 yields best safety-performance trade-off across tasks",
      "target_node": "Conduct grid search to select controllability regularizer ψ between 0.05-0.1 during Safe-OC training",
      "description": "Identified sweet-spot motivates systematic ψ tuning during training.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Empirical parameter study across several environments.",
      "edge_rationale": "Directly informs the intervention to choose ψ."
    }
  ],
  "meta": [
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "authors",
      "value": [
        "Arushi Jain",
        "Khimya Khetarpal",
        "Doina Precup"
      ]
    },
    {
      "key": "title",
      "value": "Safe Option-Critic: Learning Safety in the Option-Critic Architecture"
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1807.08060"
    },
    {
      "key": "date_published",
      "value": "2018-07-21T00:00:00Z"
    }
  ]
}