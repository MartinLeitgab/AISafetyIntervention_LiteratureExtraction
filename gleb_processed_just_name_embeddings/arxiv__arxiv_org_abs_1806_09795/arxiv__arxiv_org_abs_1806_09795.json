{
  "nodes": [
    {
      "name": "Reward misalignment from strategic interactions in multi-agent contexts",
      "aliases": [
        "reward model failure in multi-agent settings",
        "mis-specified rewards with interacting agents"
      ],
      "type": "concept",
      "description": "Risk that inverse-reinforcement learning trained on demonstrations that ignore other agents yields policies that do not respect actual multi-agent incentives, leading to unsafe behaviour.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted in Introduction: IRL treats others as environment, causing potential mis-alignment when agents interact."
    },
    {
      "name": "Ambiguous reward inference due to non-unique equilibria in stochastic games",
      "aliases": [
        "equilibrium non-uniqueness ambiguity",
        "multiple CE/NE ambiguity"
      ],
      "type": "concept",
      "description": "Risk that many reward functions are compatible with an observed equilibrium when that equilibrium is not unique, undermining safety-critical reward specification.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 1 & 4 note MIRL ill-posedness and CE/NE non-uniqueness causing inference ambiguity."
    },
    {
      "name": "Ill-posed MIRL under multiple reward-consistent equilibria",
      "aliases": [
        "MIRL inverse problem ill-posedness",
        "multiple rewards give same policy"
      ],
      "type": "concept",
      "description": "Problem that for any observed bi-policy there can exist infinitely many reward vectors that rationalise it, making safe reward inference difficult.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Sections 1–2 as the core analytical obstacle."
    },
    {
      "name": "Non-unique equilibrium likelihood specification challenge in MIRL",
      "aliases": [
        "likelihood undefined under multiple equilibria",
        "CE/NE selection problem"
      ],
      "type": "concept",
      "description": "When several equilibria exist, defining a likelihood for Bayesian MIRL becomes unclear, obstructing inference.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 1 last paragraph before Section 2 describes issue for uCE/uNE."
    },
    {
      "name": "Unique equilibrium selection constrains reward inference search space",
      "aliases": [
        "equilibrium uniqueness constraint",
        "using uCS/advE/cooE for identifiability"
      ],
      "type": "concept",
      "description": "Insight that focusing on equilibrium concepts that are unique (uCS, advE, cooE) provides necessary & sufficient linear constraints that shrink the feasible reward set.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Developed in Section 4.2–4.4 theorems."
    },
    {
      "name": "Local reduced gap objective approximates utilitarian equilibrium selection",
      "aliases": [
        "local gap minimisation for uCE/uNE",
        "total game value gap heuristic"
      ],
      "type": "concept",
      "description": "Observation that among correlated/Nash equilibria, the utilitarian one minimises a specially defined local reduced gap to the cooperative strategy, enabling selection.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in Section 4.5, Theorem 4.5."
    },
    {
      "name": "Bayesian MAP estimation with equilibrium linear constraints",
      "aliases": [
        "MAP-constrained MIRL",
        "convex Bayesian MIRL"
      ],
      "type": "concept",
      "description": "Design approach: maximise posterior over rewards subject to linear constraints guaranteeing the observed bi-policy is the unique equilibrium.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Sections 4.2–4.4 present MAP-formulations for uCS/advE/cooE."
    },
    {
      "name": "Linear programming for utilitarian CE and NE reward recovery",
      "aliases": [
        "uCE/uNE LP formulation",
        "local gap LP"
      ],
      "type": "concept",
      "description": "Design rationale for cases with non-unique equilibria: form an LP that maximises total game value while minimising local reduced gap under CE or NE constraints.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4.5 and 4.6 derive the LP objective and constraints."
    },
    {
      "name": "Linear constraint formulation using Bπ and Dπ matrices",
      "aliases": [
        "B_pi D_pi constraint system",
        "matrix constraints for MIRL"
      ],
      "type": "concept",
      "description": "Implementation mechanism that encodes equilibrium conditions as (B_π − B_a)D_π r ≥/≤ 0 style linear inequalities enabling convex optimisation.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Developed through Theorems 4.1–4.3."
    },
    {
      "name": "Local reduced gap linear program with y variables and regularisation",
      "aliases": [
        "gap-LP with social value term",
        "uCE/uNE LP implementation"
      ],
      "type": "concept",
      "description": "Implementation mechanism introducing auxiliary y variables, λ regulariser and L1 sparsity penalties to solve the LP for uCE/uNE MIRL.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Equations 53–55 describe concrete LP."
    },
    {
      "name": "GridWorld MIRL NRMSE 1e-3 outperforming IRL",
      "aliases": [
        "GridWorld validation of MIRL",
        "GG1/GG2 empirical results"
      ],
      "type": "concept",
      "description": "Validation evidence: in two 72-state grid games MIRL achieved normalised RMSE ≈1×10⁻³ while IRL/d-MIRL errors >0.06, demonstrating accuracy gains.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 5, Tables 1–2."
    },
    {
      "name": "Soccer simulation shows advE-MIRL competitive with zero-sum MIRL and better than d-MIRL",
      "aliases": [
        "soccer game advE validation",
        "Monte-Carlo competitive evaluation"
      ],
      "type": "concept",
      "description": "In abstract soccer, advE-MIRL agents matched zero-sum MIRL and beat d-MIRL/BIRL across β∈{0,0.4,1} settings, supporting practical effectiveness.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 6, Tables 4–7."
    },
    {
      "name": "Robustness study shows gradual degradation with missing policy states",
      "aliases": [
        "incomplete bi-policy robustness evidence",
        "imputation NRMSE analysis"
      ],
      "type": "concept",
      "description": "When up to 15/72 states missing, uNE-MIRL NRMSE rose from 0 to 0.1168, indicating graceful degradation under partial observability.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 7, Table 8."
    },
    {
      "name": "Deploy Bayesian MIRL with unique equilibrium constraints to model multi-agent rewards during AI training",
      "aliases": [
        "apply constrained MIRL in reward modelling",
        "use uCS/advE/cooE MIRL"
      ],
      "type": "intervention",
      "description": "Before fine-tuning or RLHF, infer reward functions from multi-agent demonstrations using the Bayesian MAP formulation with Bπ/Dπ constraints to reduce mis-alignment.",
      "concept_category": null,
      "intervention_lifecycle": "2",
      "intervention_lifecycle_rationale": "Reward modelling is performed prior to training; see Section 4 Model Development.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated on toy domains only (Sections 5–6).",
      "node_rationale": "Intervention naturally follows from validated Bayesian MIRL method."
    },
    {
      "name": "Use local reduced gap LP to infer utilitarian equilibria for multi-agent reward alignment",
      "aliases": [
        "apply uCE/uNE LP reward inference",
        "gap-LP reward modelling"
      ],
      "type": "intervention",
      "description": "Run the LP with local reduced gap objective (Equations 53–55) to estimate human-aligned utilitarian rewards when equilibrium is not unique.",
      "concept_category": null,
      "intervention_lifecycle": "2",
      "intervention_lifecycle_rationale": "Executed during reward-model creation before policy optimisation (Section 4.5).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Validated experimentally but not yet deployed.",
      "node_rationale": "Direct operationalisation of theoretical LP approach."
    },
    {
      "name": "Adopt advE-MIRL approach for competitive environment reward modeling",
      "aliases": [
        "foe equilibrium MIRL application",
        "competitive MIRL reward inference"
      ],
      "type": "intervention",
      "description": "In adversarial or zero-sum-like settings, infer rewards via advE-MIRL constraints to capture strategic opponent modelling and avoid unsafe extrapolation.",
      "concept_category": null,
      "intervention_lifecycle": "2",
      "intervention_lifecycle_rationale": "Reward inference precedes subsequent training (Section 4.3).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Evaluated in Section 6 simulations only.",
      "node_rationale": "Leverages validation that advE-MIRL matches zero-sum MIRL while requiring weaker assumptions."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Reward misalignment from strategic interactions in multi-agent contexts",
      "target_node": "Ill-posed MIRL under multiple reward-consistent equilibria",
      "description": "Misalignment stems from the ill-posed nature of MIRL where many rewards are consistent with observations.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicitly discussed in Introduction.",
      "edge_rationale": "Section 1 explains that multiple reward explanations lead to misalignment."
    },
    {
      "type": "caused_by",
      "source_node": "Ambiguous reward inference due to non-unique equilibria in stochastic games",
      "target_node": "Non-unique equilibrium likelihood specification challenge in MIRL",
      "description": "Ambiguity arises because likelihood cannot be defined when equilibria are not unique.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 1 last paragraphs.",
      "edge_rationale": "Paper links non-uniqueness to inference ambiguity."
    },
    {
      "type": "mitigated_by",
      "source_node": "Ill-posed MIRL under multiple reward-consistent equilibria",
      "target_node": "Unique equilibrium selection constrains reward inference search space",
      "description": "Selecting unique equilibria reduces the set of admissible reward functions.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical mitigation outlined in Section 4.2–4.4.",
      "edge_rationale": "Theorems show uniqueness removes ambiguity."
    },
    {
      "type": "specified_by",
      "source_node": "Unique equilibrium selection constrains reward inference search space",
      "target_node": "Bayesian MAP estimation with equilibrium linear constraints",
      "description": "Theoretical uniqueness insight leads to the MAP design using those constraints.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Sections 4.2–4.4 transition from theory to design.",
      "edge_rationale": "Design derives directly from insight."
    },
    {
      "type": "implemented_by",
      "source_node": "Bayesian MAP estimation with equilibrium linear constraints",
      "target_node": "Linear constraint formulation using Bπ and Dπ matrices",
      "description": "Implementation encodes the MAP constraints with Bπ and Dπ matrices.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Equations in Theorems 4.1–4.3.",
      "edge_rationale": "Matrices operationalise the design."
    },
    {
      "type": "validated_by",
      "source_node": "Linear constraint formulation using Bπ and Dπ matrices",
      "target_node": "GridWorld MIRL NRMSE 1e-3 outperforming IRL",
      "description": "GridWorld experiments test the constraint-based implementation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Tables 1–2 directly measure performance.",
      "edge_rationale": "Empirical evidence verifies implementation."
    },
    {
      "type": "motivates",
      "source_node": "GridWorld MIRL NRMSE 1e-3 outperforming IRL",
      "target_node": "Deploy Bayesian MIRL with unique equilibrium constraints to model multi-agent rewards during AI training",
      "description": "Strong empirical gains motivate deploying the Bayesian MIRL intervention.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Performance improvement provides moderate support.",
      "edge_rationale": "Better alignment justifies the intervention."
    },
    {
      "type": "validated_by",
      "source_node": "Linear constraint formulation using Bπ and Dπ matrices",
      "target_node": "Soccer simulation shows advE-MIRL competitive with zero-sum MIRL and better than d-MIRL",
      "description": "Implementation applied to advE-MIRL validated in soccer domain.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 6 results.",
      "edge_rationale": "Shows generality across domains."
    },
    {
      "type": "motivates",
      "source_node": "Soccer simulation shows advE-MIRL competitive with zero-sum MIRL and better than d-MIRL",
      "target_node": "Adopt advE-MIRL approach for competitive environment reward modeling",
      "description": "Empirical success encourages using advE-MIRL in adversarial setups.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Simulation evidence but limited scale.",
      "edge_rationale": "Evidence directly supports intervention adoption."
    },
    {
      "type": "mitigated_by",
      "source_node": "Non-unique equilibrium likelihood specification challenge in MIRL",
      "target_node": "Local reduced gap objective approximates utilitarian equilibrium selection",
      "description": "Local reduced gap provides a criterion to pick one equilibrium, mitigating non-uniqueness.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Theorem 4.5 establishes relationship.",
      "edge_rationale": "Gap objective selects a unique utilitarian equilibrium."
    },
    {
      "type": "specified_by",
      "source_node": "Local reduced gap objective approximates utilitarian equilibrium selection",
      "target_node": "Linear programming for utilitarian CE and NE reward recovery",
      "description": "Insight leads to LP design embodying the gap objective.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 4.5 formulates LP from insight.",
      "edge_rationale": "Design instantiates the theoretical idea."
    },
    {
      "type": "implemented_by",
      "source_node": "Linear programming for utilitarian CE and NE reward recovery",
      "target_node": "Local reduced gap linear program with y variables and regularisation",
      "description": "Specific LP with y variables realises the design.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Equations 53–55 give full implementation.",
      "edge_rationale": "Concrete optimisation form."
    },
    {
      "type": "validated_by",
      "source_node": "Local reduced gap linear program with y variables and regularisation",
      "target_node": "GridWorld MIRL NRMSE 1e-3 outperforming IRL",
      "description": "uCE/uNE results in GridWorld validate LP implementation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Tables 1–2 include uCE/uNE metrics.",
      "edge_rationale": "Empirical confirmation."
    },
    {
      "type": "motivates",
      "source_node": "GridWorld MIRL NRMSE 1e-3 outperforming IRL",
      "target_node": "Use local reduced gap LP to infer utilitarian equilibria for multi-agent reward alignment",
      "description": "Success supports adopting LP-based utilitarian inference.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Performance gain provides moderate evidence.",
      "edge_rationale": "Better alignment via utilitarian selection."
    },
    {
      "type": "validated_by",
      "source_node": "Local reduced gap linear program with y variables and regularisation",
      "target_node": "Robustness study shows gradual degradation with missing policy states",
      "description": "Robustness experiment applies LP under missing data.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 7 experiment.",
      "edge_rationale": "Shows implementation behaviour under partial observability."
    },
    {
      "type": "motivates",
      "source_node": "Robustness study shows gradual degradation with missing policy states",
      "target_node": "Deploy Bayesian MIRL with unique equilibrium constraints to model multi-agent rewards during AI training",
      "description": "Demonstrates need for robust MIRL methods in practice, motivating wider deployment with caution.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Indirect inference from robustness findings.",
      "edge_rationale": "Practical robustness encourages adoption."
    }
  ],
  "meta": [
    {
      "key": "authors",
      "value": [
        "Xiaomin Lin",
        "Stephen C. Adams",
        "Peter A. Beling"
      ]
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1806.09795"
    },
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "title",
      "value": "Multi-agent Inverse Reinforcement Learning for Certain General-sum Stochastic Games"
    },
    {
      "key": "date_published",
      "value": "2018-06-26T00:00:00Z"
    }
  ]
}