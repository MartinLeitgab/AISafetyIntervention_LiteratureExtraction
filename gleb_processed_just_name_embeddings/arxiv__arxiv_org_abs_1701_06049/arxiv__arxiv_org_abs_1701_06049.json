{
  "nodes": [
    {
      "name": "Unintended behavioral loops in robots trained with policy-independent human feedback",
      "aliases": [
        "unintended robot behaviour from misinterpreted feedback",
        "positive reward cycles in HCRL"
      ],
      "type": "concept",
      "description": "Robots trained with algorithms that treat human feedback as stationary reward often develop cycles of behaviour not intended by the trainer, leading to unsafe or useless actions. Section III Human-centered Reinforcement Learning highlights this phenomenon.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Represents the primary safety risk motivating the work (Section III)."
    },
    {
      "name": "High trainer burden and inefficient feedback in human-centered reinforcement learning",
      "aliases": [
        "trainer fatigue in HCRL",
        "inefficient interactive training"
      ],
      "type": "concept",
      "description": "Because existing algorithms demand dense, consistent feedback, human trainers tire quickly, limiting the practicality of teaching complex behaviours. Section IV-B Training Strategies discusses this burden.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Captures the second central risk motivating new design (Section IV-B)."
    },
    {
      "name": "Assumption of policy-independent feedback in human-centered RL algorithms",
      "aliases": [
        "policy-independent feedback assumption",
        "stationary feedback model"
      ],
      "type": "concept",
      "description": "Many HCRL methods assume that the numeric feedback given by humans is independent of the learner’s current policy, restricting usable training strategies. Section III details this assumption.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Direct mechanism causing unintended behaviour risk (Section III)."
    },
    {
      "name": "Reward-maximization interpretation of human feedback inducing positive reward cycles",
      "aliases": [
        "feedback as reward exploits",
        "reward hacking from human signals"
      ],
      "type": "concept",
      "description": "Treating feedback numerically as reward to be maximised can create exploit loops where agents repeat actions solely to elicit feedback, producing unsafe behaviour. Section III cites prior evidence.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explains mechanistic pathway from assumption to risk (Section III)."
    },
    {
      "name": "Existing HCRL algorithms lack support for diminishing returns, differential feedback and policy shaping",
      "aliases": [
        "missing advanced training strategies",
        "inflexible feedback handling"
      ],
      "type": "concept",
      "description": "Algorithms like TAMER, SABL and Policy Shaping cannot exploit training strategies that reduce trainer effort or guide learning gradually. Section IV-B analyses these gaps.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Links trainer burden risk to algorithmic limitations (Section IV-B)."
    },
    {
      "name": "Human feedback is policy-dependent and aligns with advantage function in RL",
      "aliases": [
        "policy-dependent feedback finding",
        "feedback corresponds to advantage"
      ],
      "type": "concept",
      "description": "Empirical evidence shows feedback sign and magnitude depend on how behaviour compares to current policy; mathematically this matches the RL advantage Aπ(s,a). Sections IV-A and V-A.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Core insight justifying new algorithm (Sections IV-A, V-A)."
    },
    {
      "name": "Policy-dependent feedback enables differential feedback, diminishing returns and policy shaping strategies",
      "aliases": [
        "feedback enables advanced schedules",
        "advantage-driven training strategies"
      ],
      "type": "concept",
      "description": "Because feedback maps to advantage, trainers can naturally scale, taper or reverse feedback to guide learning efficiently. Section IV-B and V-A elaborate these strategies.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explains how insight addresses trainer burden (Section IV-B)."
    },
    {
      "name": "Use advantage-function-aligned policy updates to learn safely from policy-dependent feedback",
      "aliases": [
        "advantage-aligned learning design",
        "feedback-driven policy gradient rationale"
      ],
      "type": "concept",
      "description": "Designing an algorithm that treats feedback as an unbiased estimate of advantage ensures convergence and avoids reward exploitation. Section V-B Convergence.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Translates theoretical insight into concrete algorithmic goal (Section V-B)."
    },
    {
      "name": "Reduce trainer burden via diminishing returns and eligibility tracing mechanisms",
      "aliases": [
        "trainer-effort reduction rationale",
        "feedback efficiency design"
      ],
      "type": "concept",
      "description": "Incorporating mechanisms that naturally taper feedback and credit prior actions reduces the amount and precision of feedback required. Section V-C Real-time COACH.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Design driver addressing risk of trainer fatigue (Section V-C)."
    },
    {
      "name": "TD-error as unbiased advantage estimate for direct policy updates in COACH",
      "aliases": [
        "TD-error advantage mechanism",
        "direct actor update via human feedback"
      ],
      "type": "concept",
      "description": "COACH replaces critic output with human feedback, using the temporal-difference error as an unbiased advantage proxy, ensuring convergence. Section V-B.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Key internal mechanism realising design rationale (Section V-B)."
    },
    {
      "name": "Real-time COACH with eligibility traces and reward aggregation for delayed sparse feedback",
      "aliases": [
        "eligibility-trace reward-aggregation mechanism",
        "real-time COACH interface"
      ],
      "type": "concept",
      "description": "Maintains multiple eligibility traces with different decay rates and aggregates sequential button presses to handle human reaction delay and variable magnitude feedback. Section V-C.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implements trainer-effort reduction rationale (Section V-C)."
    },
    {
      "name": "Softmax visual feature-based policy on TurtleBot within COACH framework",
      "aliases": [
        "TurtleBot softmax policy implementation",
        "image-feature linear policy"
      ],
      "type": "concept",
      "description": "Uses linear preferences over hand-crafted image features within a softmax policy to allow fast 33 ms decisions on noisy vision inputs. Section VII Robotics Case Study.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Shows scalability of mechanism to real robot (Section VII)."
    },
    {
      "name": "AMT gridworld study shows sign of feedback varies with learner policy",
      "aliases": [
        "AMT policy-dependent feedback evidence",
        "online user study feedback sign"
      ],
      "type": "concept",
      "description": "Statistical analysis across three conditions (bad, alright, good) demonstrated significant differences in feedback direction, confirming policy dependence (p < 0.05–0.01). Section IV-A Empirical Results.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Validates theoretical insight about feedback nature (Section IV-A)."
    },
    {
      "name": "TurtleBot case study demonstrates fast learning of five behaviours with COACH",
      "aliases": [
        "robotics case evidence for COACH",
        "COACH TurtleBot success"
      ],
      "type": "concept",
      "description": "COACH trained five distinct behaviours in under two minutes each, including compositional tasks, outperforming TAMER. Section VII-A Results.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Validates implementation mechanisms and design rationales (Section VII-A)."
    },
    {
      "name": "TAMER comparison reveals unlearning when feedback is misinterpreted as reward",
      "aliases": [
        "negative evidence for reward-interpretation",
        "TAMER unlearning scenario"
      ],
      "type": "concept",
      "description": "A controlled scenario showed TAMER reversed previously learned behaviour after a weaker reward, illustrating dangers of reward-maximization interpretation. Section VII-A.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Empirically confirms problem analysis of reward cycles (Section VII-A)."
    },
    {
      "name": "Fine-tune robots using COACH algorithm to integrate policy-dependent human feedback",
      "aliases": [
        "apply COACH in robot training",
        "policy-dependent feedback fine-tuning"
      ],
      "type": "intervention",
      "description": "During reinforcement-learning fine-tuning, replace conventional reward modeling with COACH’s advantage-aligned update steps to exploit policy-dependent human feedback.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Applies at fine-tuning/RL stage when human trainers shape behaviour (Section V Convergent Actor-Critic by Humans).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Validated through prototype robotics study (Section VII-A).",
      "node_rationale": "Actionable method directly derived from implementation mechanisms."
    },
    {
      "name": "Integrate advantage-function-based feedback mapping into human-in-the-loop RL training interfaces",
      "aliases": [
        "advantage feedback mapping interface",
        "policy-aware feedback UI"
      ],
      "type": "intervention",
      "description": "Update trainer interfaces (sliders, buttons) so that back-end interprets feedback as advantage estimates, enabling differential, diminishing and shaping strategies without code changes by trainers.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Implemented during RL training tool design or fine-tuning (Section V-C Real-time COACH).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated in controlled AMT interface; broader deployment untested (Section IV-A).",
      "node_rationale": "Transforms theoretical insight into practical tooling improvements."
    },
    {
      "name": "Implement eligibility traces with variable decay and reward aggregation in real-time RL systems",
      "aliases": [
        "variable eligibility trace deployment",
        "reward aggregation mechanism deployment"
      ],
      "type": "intervention",
      "description": "Add multiple eligibility traces with trainer-selectable decay rates and aggregate sequential feedback pulses to handle delay and sparse signals in real-time learning agents.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Modifies learning algorithm during RL stage (Section V-C Real-time COACH).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Prototype validated on TurtleBot; not yet large-scale (Section VII-A).",
      "node_rationale": "Directly operationalises burden-reduction design rationale."
    },
    {
      "name": "Adopt compositional and differential training strategies supported by policy-dependent feedback during robot instruction",
      "aliases": [
        "compositional feedback strategy adoption",
        "differential feedback teaching"
      ],
      "type": "intervention",
      "description": "Structure training sessions so trainers can successively reinforce sub-behaviours and vary feedback magnitude, leveraging COACH’s support for these strategies to speed learning and avoid unlearning.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Takes place during live deployment or advanced trainer sessions (Section VII-A Results).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated informally by a single trainer; needs broader study (Section VII-A).",
      "node_rationale": "Uses validated strategies to mitigate trainer burden in practice."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Unintended behavioral loops in robots trained with policy-independent human feedback",
      "target_node": "Assumption of policy-independent feedback in human-centered RL algorithms",
      "description": "Incorrect assumption disconnects feedback semantics from policy, enabling exploit loops.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoned link supported by Section III discussion but not quantified.",
      "edge_rationale": "Section III explains how the assumption leads to unintended behaviours."
    },
    {
      "type": "caused_by",
      "source_node": "Unintended behavioral loops in robots trained with policy-independent human feedback",
      "target_node": "Reward-maximization interpretation of human feedback inducing positive reward cycles",
      "description": "Reward-maximization of raw feedback causes loops of self-reinforcing actions.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Based on prior cited work in Section III.",
      "edge_rationale": "Section III references studies showing positive reward cycles."
    },
    {
      "type": "caused_by",
      "source_node": "High trainer burden and inefficient feedback in human-centered reinforcement learning",
      "target_node": "Existing HCRL algorithms lack support for diminishing returns, differential feedback and policy shaping",
      "description": "Lack of these strategies forces dense, continuous feedback.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Derived from qualitative analysis in Section IV-B.",
      "edge_rationale": "Section IV-B argues missing strategies increase trainer effort."
    },
    {
      "type": "addressed_by",
      "source_node": "Assumption of policy-independent feedback in human-centered RL algorithms",
      "target_node": "Human feedback is policy-dependent and aligns with advantage function in RL",
      "description": "Empirical and theoretical insight contradicts and resolves the assumption.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Supported by significant AMT study results (Section IV-A).",
      "edge_rationale": "Section IV-A provides evidence refuting policy-independence."
    },
    {
      "type": "addressed_by",
      "source_node": "Existing HCRL algorithms lack support for diminishing returns, differential feedback and policy shaping",
      "target_node": "Policy-dependent feedback enables differential feedback, diminishing returns and policy shaping strategies",
      "description": "Recognising policy dependence unlocks these strategies.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Backed by formal advantage-function analysis (Section V-A).",
      "edge_rationale": "Section V-A demonstrates how advantage captures the strategies."
    },
    {
      "type": "mitigated_by",
      "source_node": "Human feedback is policy-dependent and aligns with advantage function in RL",
      "target_node": "Use advantage-function-aligned policy updates to learn safely from policy-dependent feedback",
      "description": "Aligning updates with advantage exploits the feedback property.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Mathematical convergence proof in Section V-B.",
      "edge_rationale": "Section V-B derives gradient update using advantage."
    },
    {
      "type": "mitigated_by",
      "source_node": "Policy-dependent feedback enables differential feedback, diminishing returns and policy shaping strategies",
      "target_node": "Reduce trainer burden via diminishing returns and eligibility tracing mechanisms",
      "description": "Design rationale leverages these strategies to ease trainer effort.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Argumentative link in Section V-C, no quantitative test.",
      "edge_rationale": "Section V-C explains how eligibility traces operationalise diminishing returns."
    },
    {
      "type": "implemented_by",
      "source_node": "Use advantage-function-aligned policy updates to learn safely from policy-dependent feedback",
      "target_node": "TD-error as unbiased advantage estimate for direct policy updates in COACH",
      "description": "TD-error provides implementable signal for advantage.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Derivation in Section V-B.",
      "edge_rationale": "Section V-B shows TD-error equals unbiased advantage."
    },
    {
      "type": "implemented_by",
      "source_node": "Reduce trainer burden via diminishing returns and eligibility tracing mechanisms",
      "target_node": "Real-time COACH with eligibility traces and reward aggregation for delayed sparse feedback",
      "description": "Real-time COACH incorporates eligibility traces and reward aggregation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Algorithm 1 implementation detail (Section V-C).",
      "edge_rationale": "Section V-C specifies multiple traces and aggregation."
    },
    {
      "type": "implemented_by",
      "source_node": "Use advantage-function-aligned policy updates to learn safely from policy-dependent feedback",
      "target_node": "Softmax visual feature-based policy on TurtleBot within COACH framework",
      "description": "Softmax policy instantiation realises the update rule in robotics domain.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Engineering choice described in Section VII.",
      "edge_rationale": "Section VII details policy representation consistent with COACH."
    },
    {
      "type": "validated_by",
      "source_node": "TD-error as unbiased advantage estimate for direct policy updates in COACH",
      "target_node": "AMT gridworld study shows sign of feedback varies with learner policy",
      "description": "Study confirms prerequisite policy-dependent feedback assumption required by TD-error approach.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Statistical significance (p<0.05–0.01) in Section IV-A.",
      "edge_rationale": "Section IV-A data supports mechanism validity."
    },
    {
      "type": "validated_by",
      "source_node": "TD-error as unbiased advantage estimate for direct policy updates in COACH",
      "target_node": "TurtleBot case study demonstrates fast learning of five behaviours with COACH",
      "description": "Robot results show mechanism works in complex, noisy domain.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Empirical success across five tasks (Section VII-A).",
      "edge_rationale": "Section VII-A reports learning speed and robustness."
    },
    {
      "type": "validated_by",
      "source_node": "Real-time COACH with eligibility traces and reward aggregation for delayed sparse feedback",
      "target_node": "TurtleBot case study demonstrates fast learning of five behaviours with COACH",
      "description": "Real-time components enabled rapid trainer interaction.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Qualitative evidence section VII-A.",
      "edge_rationale": "Section VII-A credits eligibility traces for compositional tasks."
    },
    {
      "type": "validated_by",
      "source_node": "Softmax visual feature-based policy on TurtleBot within COACH framework",
      "target_node": "TurtleBot case study demonstrates fast learning of five behaviours with COACH",
      "description": "Implementation successfully controlled real hardware.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Informal observation in Section VII-A.",
      "edge_rationale": "Section VII-A documents successful behaviours."
    },
    {
      "type": "validated_by",
      "source_node": "Reward-maximization interpretation of human feedback inducing positive reward cycles",
      "target_node": "TAMER comparison reveals unlearning when feedback is misinterpreted as reward",
      "description": "Unlearning scenario empirically demonstrates the risk.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Repeated experimental failure reported in Section VII-A.",
      "edge_rationale": "Section VII-A describes TAMER unlearning due to reward cycles."
    },
    {
      "type": "motivates",
      "source_node": "AMT gridworld study shows sign of feedback varies with learner policy",
      "target_node": "Integrate advantage-function-based feedback mapping into human-in-the-loop RL training interfaces",
      "description": "Study highlights need for interfaces that interpret advantage, motivating interface change.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Empirical evidence directly linked (Section IV-A).",
      "edge_rationale": "Section IV-A findings imply interface redesign."
    },
    {
      "type": "motivates",
      "source_node": "TurtleBot case study demonstrates fast learning of five behaviours with COACH",
      "target_node": "Fine-tune robots using COACH algorithm to integrate policy-dependent human feedback",
      "description": "Successful prototype encourages broader adoption in robot fine-tuning.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Demonstrated across multiple behaviours (Section VII-A).",
      "edge_rationale": "Section VII-A suggests COACH generalises."
    },
    {
      "type": "motivates",
      "source_node": "TurtleBot case study demonstrates fast learning of five behaviours with COACH",
      "target_node": "Implement eligibility traces with variable decay and reward aggregation in real-time RL systems",
      "description": "Results show eligibility traces critical, motivating their deployment.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Qualitative trainer feedback (Section VII-A).",
      "edge_rationale": "Section VII-A credits traces for performance."
    },
    {
      "type": "motivates",
      "source_node": "TurtleBot case study demonstrates fast learning of five behaviours with COACH",
      "target_node": "Adopt compositional and differential training strategies supported by policy-dependent feedback during robot instruction",
      "description": "Case study proves compositional strategies feasible, motivating their adoption.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Successful alternate and cylinder navigation tasks (Section VII-A).",
      "edge_rationale": "Section VII-A highlights effectiveness of differential and compositional strategies."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2017-01-21T00:00:00Z"
    },
    {
      "key": "title",
      "value": "Interactive Learning from Policy-Dependent Human Feedback"
    },
    {
      "key": "authors",
      "value": [
        "James MacGlashan",
        "Mark K Ho",
        "Robert Loftin",
        "Bei Peng",
        "Guan Wang",
        "David Roberts",
        "Matthew E. Taylor",
        "Michael L. Littman"
      ]
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1701.06049"
    },
    {
      "key": "source",
      "value": "arxiv"
    }
  ]
}