Summary
Data source overview:
The paper “Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles” introduces a non-Bayesian recipe—proper scoring-rule training, adversarial data augmentation, and independently initialised neural-network ensembles—for producing calibrated predictive uncertainties that remain reliable under dataset shift. Extensive experiments (toy regression, nine real-world regression datasets, MNIST, SVHN, ImageNet, out-of-distribution tests) show large improvements in negative log-likelihood, Brier score, entropy on unseen classes, and accuracy-versus-confidence curves compared with MC-dropout and probabilistic back-propagation.

Inference strategy justification:
All nodes reflect explicit reasoning steps or experimentally validated mechanisms in the paper. Moderate inference (edge-confidence 2) is used only where the text implies, but does not state, a causal link (e.g. using entropy-thresholding to decide when to defer).

Extraction completeness:
18 concept nodes cover risks→problem analyses→theoretical insights→design rationales→implementation mechanisms→validation evidence. Two actionable interventions are extracted, giving full causal-interventional chains with no isolated nodes. Each node name follows the mandated granularity pattern and can merge with identical concepts extracted from other sources.

Key limitations:
• The paper studies supervised settings; extensions to reinforcement learning are inferred but not validated.
• Only two deployment-phase interventions are identifiable; others (e.g. distillation) are suggested but not evaluated, so excluded here.
• Experimental evidence comes from benchmarks, not real safety-critical domains, so edge confidences are capped at 4.

Extraction confidence[88]

JSON knowledge fabric


How to improve the instruction set:
1. Clarify directionality of “mitigated_by”: many users mis-interpret whether the arrow should start at the problem or the mitigation. A schematic example would help.
2. Allow combining several closely related implementation details (e.g. averaging formula) into one node to reduce verbosity where appropriate.
3. Provide explicit guidance on handling multiple parallel validation evidences that support the same intervention to avoid redundant edge work.

Key limitations remain around subjective confidence scoring, requirement to pick single lifecycle numbers for hybrid interventions, and ambiguity about when an edge confidence of “5” is justified outside formal statistical significance.