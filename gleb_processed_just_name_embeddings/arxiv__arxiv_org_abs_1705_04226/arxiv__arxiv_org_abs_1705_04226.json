{
  "nodes": [
    {
      "name": "misspecified robot reward functions in human-robot interaction",
      "aliases": [
        "reward misspecification in human-robot collaboration",
        "incorrect robot objective functions with humans"
      ],
      "type": "concept",
      "description": "Robots optimising incorrectly specified reward functions can generate unsafe or unwanted behaviour when operating around people.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Stated as a central safety problem in I Introduction and IV Using Human Behavior to Infer Human Internal State."
    },
    {
      "name": "inefficient human-robot coordination in shared physical tasks",
      "aliases": [
        "sub-optimal collaboration with humans",
        "coordination inefficiency between robots and people"
      ],
      "type": "concept",
      "description": "Robots that fail to anticipate or influence human actions slow teams down and may create unsafe traffic patterns.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted throughout III Human Behavior During Interaction (e.g. conservative autonomous cars that never merge)."
    },
    {
      "name": "misinterpretation of robot intentions by humans in interaction",
      "aliases": [
        "human misunderstanding of robot goals",
        "ambiguity of robot intent"
      ],
      "type": "concept",
      "description": "When humans cannot correctly infer a robot’s goals or internal state, their responses can produce unsafe or inefficient outcomes.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in V Human Inferences about the Robot as a motivation for transparency-oriented behaviour."
    },
    {
      "name": "robots assuming fixed reward functions without uncertainty in human-robot interaction",
      "aliases": [
        "objective taken as given",
        "static robot reward assumption"
      ],
      "type": "concept",
      "description": "Robots normally treat their reward as immutable, ignoring designer errors and user-specific preferences.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Problem framed in IV Using Human Behavior to Infer Human Internal State."
    },
    {
      "name": "robots treating humans as moving obstacles in planning",
      "aliases": [
        "obstacle-based human model",
        "people as dynamic obstacles"
      ],
      "type": "concept",
      "description": "Traditional planners ignore human agency, leading to overly conservative or unsafe robot motion.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explicitly criticised in III Human Behavior During Interaction (autonomous driving example)."
    },
    {
      "name": "opaque robot behavior reducing predictability to humans",
      "aliases": [
        "low t-predictability",
        "non-transparent robot motion"
      ],
      "type": "concept",
      "description": "Efficient but ambiguous trajectories leave humans uncertain about future robot actions.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Problem motivating t-predictability in V-A Humans Expecting Robot Behavior."
    },
    {
      "name": "human behavior as information about true reward functions in human-robot interaction",
      "aliases": [
        "behavior reveals preferences",
        "actions as objective evidence"
      ],
      "type": "concept",
      "description": "Assumes human physical actions, corrections, orders, and reward specifications are informative observations of latent preferences.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Theoretical foundation in IV Using Human Behavior to Infer Human Internal State."
    },
    {
      "name": "humans modelled as rational agents best-responding to robot actions in interaction",
      "aliases": [
        "best-response human model",
        "rational reaction model"
      ],
      "type": "concept",
      "description": "Humans optimise their own reward assuming the robot’s trajectory is fixed, simplifying full game-theoretic reasoning.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in III Human Behavior During Interaction (non-collaborative driving)."
    },
    {
      "name": "human myopic decision making in collaborative tasks",
      "aliases": [
        "greedy human action model",
        "bounded-rational myopia"
      ],
      "type": "concept",
      "description": "People often choose actions that look locally good rather than globally optimal.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Used in III Human Behavior During Interaction to justify guidance planners."
    },
    {
      "name": "human expectations of robot rationality for action prediction",
      "aliases": [
        "principle of rational action expectation",
        "predictive human observer model"
      ],
      "type": "concept",
      "description": "Humans assume robots act approximately to maximise their reward and use that assumption to predict future moves.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Described in V-A Humans Expecting Robot Behavior."
    },
    {
      "name": "maintain uncertainty over robot reward and update via human guidance",
      "aliases": [
        "reward uncertainty framework",
        "objective as latent state"
      ],
      "type": "concept",
      "description": "Design principle that robots should represent a belief distribution over possible reward functions and refine it using human signals.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Advocated in IV Using Human Behavior to Infer Human Internal State."
    },
    {
      "name": "robot action planning accounting for human best responses",
      "aliases": [
        "best-response-aware planning",
        "human-reaction-aware planning"
      ],
      "type": "concept",
      "description": "Plan robot trajectories that maximise robot reward once the predicted best-response of the human is taken into account.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivated by best-response insight in III Human Behavior During Interaction."
    },
    {
      "name": "robot actions guiding myopic humans toward global optimal plans",
      "aliases": [
        "guidance for bounded-rational humans",
        "incentivising good human plans"
      ],
      "type": "concept",
      "description": "Robots choose actions that make the human’s locally greedy response yield high global reward.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained with handover example in III Human Behavior During Interaction."
    },
    {
      "name": "embed communication objectives in robot planning via predictability",
      "aliases": [
        "predictability-oriented planning",
        "explicit communication in motion"
      ],
      "type": "concept",
      "description": "Plans are chosen to maximise how well partial execution communicates remaining trajectory or hidden state to humans.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Design principle in V-A and V-B sections."
    },
    {
      "name": "Bayesian inverse reinforcement learning from demonstrations, corrections, and orders",
      "aliases": [
        "IRL with rich human guidance",
        "posterior over reward via mixed signals"
      ],
      "type": "concept",
      "description": "Implementation technique that treats diverse human inputs as observations in a Bayesian update over reward parameters.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed across IV Using Human Behavior…, esp. physical corrections and shutdown orders."
    },
    {
      "name": "best-response planning for autonomous driving",
      "aliases": [
        "coordination planner for AVs",
        "underactuated interaction planner"
      ],
      "type": "concept",
      "description": "Robot car plans trajectory assuming surrounding drivers compute best responses, enabling merges and intersection negotiation.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation given in III Human Behavior During Interaction (driving simulator study)."
    },
    {
      "name": "myopic collaborative planning for object handover tasks",
      "aliases": [
        "grasp-guiding handover planner",
        "myopic-human handover algorithm"
      ],
      "type": "concept",
      "description": "Robot selects grasp poses that make the human’s greedy grasp still near-optimal at the task goal.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mechanism described with Fig.3 in III Human Behavior During Interaction."
    },
    {
      "name": "online centralized replanning with perfect collaborative model",
      "aliases": [
        "reactive centralized planning",
        "continual optimal replanning"
      ],
      "type": "concept",
      "description": "Assumes shared reward, replans jointly at every timestep to adapt to human deviations.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Reactive condition in Fig.2, III Human Behavior During Interaction."
    },
    {
      "name": "active information gathering via robot probing actions",
      "aliases": [
        "information-maximising interaction",
        "probing human styles"
      ],
      "type": "concept",
      "description": "Robot trades off task reward with expected information gain about human reward parameters by selecting informative actions.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mechanism in IV Active Online Inference using Robot Physical Actions."
    },
    {
      "name": "t-predictable trajectory optimization",
      "aliases": [
        "partial-trajectory predictability",
        "t-step predictability planner"
      ],
      "type": "concept",
      "description": "Optimises complete trajectory so that after t steps the remainder is maximally predictable to human observers.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Formulated in V-A Humans Expecting Robot Behavior."
    },
    {
      "name": "exaggerated motion to disambiguate robot goal",
      "aliases": [
        "goal-exaggerating trajectory",
        "intent-expressive motion"
      ],
      "type": "concept",
      "description": "Robot intentionally deviates towards goal direction to increase human inference accuracy of its goal.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Shown in Fig.8 within V-B Humans Using Robot Behavior to Infer Robot Internal State."
    },
    {
      "name": "reduced negative consequences through reward inference experiments",
      "aliases": [
        "reward-design inversion validation",
        "misspecification mitigation evidence"
      ],
      "type": "concept",
      "description": "Experimental results (Hadfield-Menell 2017) showing treating designed reward as evidence decreases reward hacking and side-effects.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Validation in IV Using Human Behavior… (reward design inversion)."
    },
    {
      "name": "improved merge negotiation performance in driving simulator study",
      "aliases": [
        "best-response driving evidence",
        "coordinated AV study"
      ],
      "type": "concept",
      "description": "Simulator user study showed higher robot reward when planner assumes human best response instead of obstacle model.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "III Human Behavior During Interaction driving results."
    },
    {
      "name": "lower ergonomic cost in handover experiments",
      "aliases": [
        "myopic guidance evidence",
        "handover ergonomic study"
      ],
      "type": "concept",
      "description": "Human-robot handover experiments showed robot grasps guiding myopic humans reduced joint ergonomic cost.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Results accompanying Fig.3 in III Human Behavior During Interaction."
    },
    {
      "name": "faster task completion with reactive replanning study",
      "aliases": [
        "centralised replanning evidence",
        "collaborative TSP user study"
      ],
      "type": "concept",
      "description": "234-subject online study showed reactive robot replanning cut completion time and improved subjective ratings.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Evidence in Fig.2 of III Human Behavior During Interaction."
    },
    {
      "name": "increased human coordination success in t-predictability user studies",
      "aliases": [
        "predictability validation",
        "t-predictability evidence"
      ],
      "type": "concept",
      "description": "Online and in-person studies showed people coordinate better with robots following t-predictable trajectories.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Reported in V-A Humans Expecting Robot Behavior."
    },
    {
      "name": "faster human goal inference from exaggerated motion studies",
      "aliases": [
        "goal-exaggeration evidence",
        "intent expression validation"
      ],
      "type": "concept",
      "description": "Manipulator experiments demonstrated humans identify robot goals sooner when trajectories are exaggerated.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Validation for Fig.8 in V-B Humans Using Robot Behavior to Infer Robot Internal State."
    },
    {
      "name": "integrate reward uncertainty and human guidance into robot objective learning workflows",
      "aliases": [
        "reward-uncertainty integration",
        "Bayesian objective inference pipeline"
      ],
      "type": "intervention",
      "description": "During model-design and fine-tuning, represent reward parameters as latent variables and update them online from demonstrations, corrections, orders, and designed reward functions.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Targets the model-design phase where reward functions are specified (IV section).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Multiple proof-of-concept studies but no large-scale deployment reported.",
      "node_rationale": "Actionable method directly implied by Bayesian IRL mechanism and validation evidence."
    },
    {
      "name": "deploy best-response planning algorithms in autonomous vehicles for road coordination",
      "aliases": [
        "best-response AV deployment",
        "coordination-aware AV planner"
      ],
      "type": "intervention",
      "description": "Integrate planners that assume surrounding drivers compute best responses, allowing safe merges and intersection negotiation before deployment.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Implemented at the planning algorithm design stage (III section).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Validated only in simulator studies.",
      "node_rationale": "Concrete deployment suggestion drawn from best-response planning mechanism."
    },
    {
      "name": "implement guidance-oriented myopic collaborative planners in collaborative manipulation robots",
      "aliases": [
        "myopic guidance deployment",
        "ergonomic handover planner"
      ],
      "type": "intervention",
      "description": "Adopt planners that select grasps/postures to steer myopic human actions toward globally optimal task outcomes during fine-tuning.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Implemented during fine-tuning/behaviour shaping (III handover study).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated in lab user studies only.",
      "node_rationale": "Directly motivated by handover mechanism and evidence."
    },
    {
      "name": "apply online centralized replanning in collaborative tasks for adaptive support",
      "aliases": [
        "reactive centralized planner deployment",
        "continual replan support"
      ],
      "type": "intervention",
      "description": "Continuously recompute shared optimal plans at runtime to keep assisting humans even as they deviate.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Algorithm runs during task execution (fine-tuning/RL phase).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Validated via online study but not yet fielded.",
      "node_rationale": "Suggested by reactive study results."
    },
    {
      "name": "use information-gathering probing actions to personalise robot plans to individual users",
      "aliases": [
        "interactive probing actions",
        "information gain probing"
      ],
      "type": "intervention",
      "description": "Inject actions (e.g. lane nudges) chosen to maximise expected information gain about user style before or during deployment.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Executed during pre-deployment testing to learn user model quickly.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated in simulation; no large-scale trials.",
      "node_rationale": "Derived from active information gathering mechanism."
    },
    {
      "name": "optimize robot trajectories for t-predictability to enhance human predictability",
      "aliases": [
        "t-predictable trajectory deployment",
        "predictability-oriented motion"
      ],
      "type": "intervention",
      "description": "During task planning, select trajectories whose first t steps render the remainder highly predictable to humans.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Implemented in fine-tuning/behaviour layer (V section).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Backed by lab studies but not production systems.",
      "node_rationale": "Actionable output of t-predictable optimization mechanism."
    },
    {
      "name": "exaggerate robot motion paths to communicate goals to humans",
      "aliases": [
        "goal exaggeration deployment",
        "intent-expressive motion deployment"
      ],
      "type": "intervention",
      "description": "Plan slightly inefficient but goal-disambiguating trajectories, improving human goal inference during deployment.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Applied during behaviour generation (V-B section).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Validated in user studies on manipulators.",
      "node_rationale": "Derived from exaggerated motion mechanism and validation."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "misspecified robot reward functions in human-robot interaction",
      "target_node": "robots assuming fixed reward functions without uncertainty in human-robot interaction",
      "description": "Taking the reward as fixed leads directly to misspecification risk.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit causal link in IV section.",
      "edge_rationale": "Problem analysis explains origin of risk."
    },
    {
      "type": "addressed_by",
      "source_node": "robots assuming fixed reward functions without uncertainty in human-robot interaction",
      "target_node": "human behavior as information about true reward functions in human-robot interaction",
      "description": "Viewing behaviour as information tackles the fixed-reward assumption.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Supported theoretically but no formal proof.",
      "edge_rationale": "Theoretical insight directly targets the problem."
    },
    {
      "type": "mitigated_by",
      "source_node": "human behavior as information about true reward functions in human-robot interaction",
      "target_node": "maintain uncertainty over robot reward and update via human guidance",
      "description": "Design rationale operationalises the insight to reduce misspecification.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Design principle articulated in IV section.",
      "edge_rationale": "Rationale stems from insight."
    },
    {
      "type": "implemented_by",
      "source_node": "maintain uncertainty over robot reward and update via human guidance",
      "target_node": "Bayesian inverse reinforcement learning from demonstrations, corrections, and orders",
      "description": "Bayesian IRL is the concrete method realising the design rationale.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Direct implementation described with equations.",
      "edge_rationale": "Implementation mechanism builds the rationale."
    },
    {
      "type": "implemented_by",
      "source_node": "maintain uncertainty over robot reward and update via human guidance",
      "target_node": "active information gathering via robot probing actions",
      "description": "Probing actions accelerate the reward-uncertainty update.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Algorithm and examples provided.",
      "edge_rationale": "Second concrete mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Bayesian inverse reinforcement learning from demonstrations, corrections, and orders",
      "target_node": "reduced negative consequences through reward inference experiments",
      "description": "Experiments show IRL lowers side-effects and reward hacking.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Empirical studies cited.",
      "edge_rationale": "Evidence supports mechanism efficacy."
    },
    {
      "type": "motivates",
      "source_node": "reduced negative consequences through reward inference experiments",
      "target_node": "integrate reward uncertainty and human guidance into robot objective learning workflows",
      "description": "Positive findings motivate adopting the workflow intervention.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Inference from experimental success.",
      "edge_rationale": "Evidence leads to intervention adoption."
    },
    {
      "type": "caused_by",
      "source_node": "inefficient human-robot coordination in shared physical tasks",
      "target_node": "robots treating humans as moving obstacles in planning",
      "description": "Ignoring human agency produces coordination failures.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Directly argued in III section.",
      "edge_rationale": "Problem causes risk."
    },
    {
      "type": "addressed_by",
      "source_node": "robots treating humans as moving obstacles in planning",
      "target_node": "humans modelled as rational agents best-responding to robot actions in interaction",
      "description": "Best-response model replaces obstacle view.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Model proposed to fix issue.",
      "edge_rationale": "Insight addresses problem."
    },
    {
      "type": "mitigated_by",
      "source_node": "humans modelled as rational agents best-responding to robot actions in interaction",
      "target_node": "robot action planning accounting for human best responses",
      "description": "Planning with best-response mitigates coordination inefficiency.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Design rationale follows from insight.",
      "edge_rationale": "Design aims to mitigate."
    },
    {
      "type": "implemented_by",
      "source_node": "robot action planning accounting for human best responses",
      "target_node": "best-response planning for autonomous driving",
      "description": "Autonomous driving planner realises best-response design.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Detailed algorithm and simulation.",
      "edge_rationale": "Mechanism implements design."
    },
    {
      "type": "implemented_by",
      "source_node": "robot action planning accounting for human best responses",
      "target_node": "online centralized replanning with perfect collaborative model",
      "description": "Centralised replanning is another concrete mechanism for coordination.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Mechanism explained; assumes collaboration.",
      "edge_rationale": "Alternative implementation."
    },
    {
      "type": "validated_by",
      "source_node": "best-response planning for autonomous driving",
      "target_node": "improved merge negotiation performance in driving simulator study",
      "description": "Simulator results validate planner efficacy.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Controlled user study data.",
      "edge_rationale": "Evidence supports mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "online centralized replanning with perfect collaborative model",
      "target_node": "faster task completion with reactive replanning study",
      "description": "TSP user study validates reactive replanning.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Significant empirical improvement.",
      "edge_rationale": "Evidence supports mechanism."
    },
    {
      "type": "motivates",
      "source_node": "improved merge negotiation performance in driving simulator study",
      "target_node": "deploy best-response planning algorithms in autonomous vehicles for road coordination",
      "description": "Positive simulator evidence motivates deployment.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Inference from study to deployment.",
      "edge_rationale": "Evidence → intervention link."
    },
    {
      "type": "motivates",
      "source_node": "faster task completion with reactive replanning study",
      "target_node": "apply online centralized replanning in collaborative tasks for adaptive support",
      "description": "Study suggests practical value of reactive replanning.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Inference based on empirical gains.",
      "edge_rationale": "Evidence → intervention."
    },
    {
      "type": "addressed_by",
      "source_node": "robots treating humans as moving obstacles in planning",
      "target_node": "human myopic decision making in collaborative tasks",
      "description": "Recognising myopia refines human model further.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Paper introduces myopic model as alternative.",
      "edge_rationale": "Adds detail to analysis."
    },
    {
      "type": "mitigated_by",
      "source_node": "human myopic decision making in collaborative tasks",
      "target_node": "robot actions guiding myopic humans toward global optimal plans",
      "description": "Design uses guidance to counteract myopia.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Design directly explained.",
      "edge_rationale": "Design mitigates problem."
    },
    {
      "type": "implemented_by",
      "source_node": "robot actions guiding myopic humans toward global optimal plans",
      "target_node": "myopic collaborative planning for object handover tasks",
      "description": "Handover planner realises the guidance strategy.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Specific algorithm and study.",
      "edge_rationale": "Mechanism implements design."
    },
    {
      "type": "validated_by",
      "source_node": "myopic collaborative planning for object handover tasks",
      "target_node": "lower ergonomic cost in handover experiments",
      "description": "Experiments validate ergonomic benefit.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Quantitative user study.",
      "edge_rationale": "Evidence supports mechanism."
    },
    {
      "type": "motivates",
      "source_node": "lower ergonomic cost in handover experiments",
      "target_node": "implement guidance-oriented myopic collaborative planners in collaborative manipulation robots",
      "description": "Positive results encourage adoption.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoned extrapolation.",
      "edge_rationale": "Evidence → intervention."
    },
    {
      "type": "caused_by",
      "source_node": "misinterpretation of robot intentions by humans in interaction",
      "target_node": "opaque robot behavior reducing predictability to humans",
      "description": "Ambiguous behaviour causes misunderstanding.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explained in V section.",
      "edge_rationale": "Problem causes risk."
    },
    {
      "type": "addressed_by",
      "source_node": "opaque robot behavior reducing predictability to humans",
      "target_node": "human expectations of robot rationality for action prediction",
      "description": "Exploiting humans’ rationality expectations addresses opacity.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Proposed as basis for predictability.",
      "edge_rationale": "Insight addresses problem."
    },
    {
      "type": "mitigated_by",
      "source_node": "human expectations of robot rationality for action prediction",
      "target_node": "embed communication objectives in robot planning via predictability",
      "description": "Design uses expectation model to craft communicative plans.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Design explicitly derived.",
      "edge_rationale": "Design mitigates risk."
    },
    {
      "type": "implemented_by",
      "source_node": "embed communication objectives in robot planning via predictability",
      "target_node": "t-predictable trajectory optimization",
      "description": "t-Predictable optimisation is the concrete method.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Algorithm detailed with formula.",
      "edge_rationale": "Mechanism implements design."
    },
    {
      "type": "implemented_by",
      "source_node": "embed communication objectives in robot planning via predictability",
      "target_node": "exaggerated motion to disambiguate robot goal",
      "description": "Goal-exaggeration is another communicative method.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Mechanism described with example.",
      "edge_rationale": "Alternate implementation."
    },
    {
      "type": "validated_by",
      "source_node": "t-predictable trajectory optimization",
      "target_node": "increased human coordination success in t-predictability user studies",
      "description": "User studies confirm improved coordination.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Empirical data reported.",
      "edge_rationale": "Evidence supports mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "exaggerated motion to disambiguate robot goal",
      "target_node": "faster human goal inference from exaggerated motion studies",
      "description": "Lab studies show quicker goal inference.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Quantitative user study.",
      "edge_rationale": "Evidence supports mechanism."
    },
    {
      "type": "motivates",
      "source_node": "increased human coordination success in t-predictability user studies",
      "target_node": "optimize robot trajectories for t-predictability to enhance human predictability",
      "description": "Validated benefit motivates implementing predictability optimisation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Inference from evidence to practice.",
      "edge_rationale": "Evidence → intervention."
    },
    {
      "type": "motivates",
      "source_node": "faster human goal inference from exaggerated motion studies",
      "target_node": "exaggerate robot motion paths to communicate goals to humans",
      "description": "Success of exaggeration motivates deployment.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoned link.",
      "edge_rationale": "Evidence → intervention."
    },
    {
      "type": "motivates",
      "source_node": "active information gathering via robot probing actions",
      "target_node": "use information-gathering probing actions to personalise robot plans to individual users",
      "description": "Implementation mechanism naturally suggests probing intervention.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Moderate inference from mechanism to practice.",
      "edge_rationale": "Mechanism → intervention."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2017-05-11T00:00:00Z"
    },
    {
      "key": "title",
      "value": "Robot Planning with Mathematical Models of Human State and Action"
    },
    {
      "key": "authors",
      "value": [
        "Anca D. Dragan"
      ]
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1705.04226"
    },
    {
      "key": "source",
      "value": "arxiv"
    }
  ]
}