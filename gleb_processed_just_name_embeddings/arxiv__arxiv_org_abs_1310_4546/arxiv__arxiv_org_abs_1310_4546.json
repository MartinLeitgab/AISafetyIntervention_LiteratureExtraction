{
  "nodes": [
    {
      "name": "Semantic misunderstanding in NLP applications due to poor embeddings",
      "aliases": [
        "inaccurate language understanding in NLP",
        "semantic errors from weak word vectors"
      ],
      "type": "concept",
      "description": "Downstream AI systems that rely on word vectors can misinterpret meaning, produce incorrect answers, or propagate bias when the underlying embeddings are low quality.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivating concern mentioned implicitly in Introduction as the reason for pursuing better word representations."
    },
    {
      "name": "Excessive computational cost in large-scale word embedding training",
      "aliases": [
        "high training time for word vectors",
        "compute inefficiency in embedding learning"
      ],
      "type": "concept",
      "description": "Full-softmax neural language models require evaluating every vocabulary item, making large-corpus training prohibitively slow and expensive.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted in Introduction and §2 as a barrier that Skip-gram seeks to overcome."
    },
    {
      "name": "Inability to model idiomatic phrases in NLP systems",
      "aliases": [
        "missing phrase semantics",
        "phrase representation gap"
      ],
      "type": "concept",
      "description": "Word-level embeddings cannot capture non-compositional expressions such as ‘New York Times’, leading to meaning loss in downstream tasks.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Introduction and §4 Learning Phrases as a limitation of word-only models."
    },
    {
      "name": "Low-quality distributed word representations in legacy neural language models",
      "aliases": [
        "weak legacy word vectors",
        "poor previous embeddings"
      ],
      "type": "concept",
      "description": "Earlier neural models grouped similar words but failed to encode many linguistic regularities or handle rare entities effectively.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Problem statement motivating Skip-gram, §1 Introduction."
    },
    {
      "name": "Full softmax output layer complexity scaling with vocabulary size",
      "aliases": [
        "softmax computational bottleneck",
        "O(V) softmax cost"
      ],
      "type": "concept",
      "description": "Computing gradients for the conventional softmax requires operations proportional to the vocabulary (10^5-10^7 terms), inflating training time.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in §2 Skip-gram Model as the efficiency barrier."
    },
    {
      "name": "Word-level tokenization ignores multi-word expressions with unique meanings",
      "aliases": [
        "single-token limitation for phrases",
        "missing multiword units"
      ],
      "type": "concept",
      "description": "Treating each word as an independent token prevents the model from learning representations for phrases whose meanings are not compositional.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in §4 Learning Phrases."
    },
    {
      "name": "Linear vector space structure allows analogical reasoning from embeddings",
      "aliases": [
        "embedding linear regularities",
        "linearity of semantic relationships"
      ],
      "type": "concept",
      "description": "Word vectors trained with the Skip-gram objective organize such that linguistic relations correspond to linear translations, enabling analogy solving.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Demonstrated in §1 Introduction and §3 Empirical Results with examples like Spain→Madrid."
    },
    {
      "name": "Approximate softmax objectives can reduce computation without losing representation quality",
      "aliases": [
        "softmax approximation theory",
        "efficient objective functions"
      ],
      "type": "concept",
      "description": "Re-framing the learning objective as hierarchical softmax or noise-contrastive variants retains embedding quality while avoiding full softmax costs.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined in §2.1 and §2.2 with supporting math."
    },
    {
      "name": "Data-driven phrase token identification enhances representational expressiveness",
      "aliases": [
        "phrase mining improves embeddings",
        "phrase token theory"
      ],
      "type": "concept",
      "description": "Automatically detecting and merging frequent bigrams/phrases allows the model to learn dedicated vectors that capture idiomatic meaning.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivating insight of §4 Learning Phrases."
    },
    {
      "name": "Context prediction training objective in Skip-gram model for efficient embedding learning",
      "aliases": [
        "skip-gram context prediction",
        "predict surrounding words"
      ],
      "type": "concept",
      "description": "Skip-gram maximizes the probability of surrounding words given a center word, producing representations that capture useful distributional information with simple architecture.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Described in §2 The Skip-gram Model."
    },
    {
      "name": "Use of negative sampling and hierarchical softmax to approximate softmax efficiently",
      "aliases": [
        "efficient softmax design choices",
        "NEG/HS objective selection"
      ],
      "type": "concept",
      "description": "Selecting either negative sampling or hierarchical softmax provides tractable learning signals that match full softmax quality with far less compute.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Design choices compared empirically in §3 Empirical Results."
    },
    {
      "name": "Incorporating phrase tokens into training corpus to capture idiomatic meaning",
      "aliases": [
        "phrase-token training strategy",
        "phrase-aware embedding design"
      ],
      "type": "concept",
      "description": "Treat phrases detected in preprocessing as single tokens so that Skip-gram can learn dedicated vectors for them, improving semantic coverage.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposed in §4 Learning Phrases."
    },
    {
      "name": "Negative sampling with k=5–15 negative draws per positive sample",
      "aliases": [
        "NEG objective",
        "noise contrastive negative sampling"
      ],
      "type": "concept",
      "description": "Replaces each softmax term with a logistic regression that separates the true context word from k noise samples, greatly lowering computational load.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation details in §2.2 Negative Sampling."
    },
    {
      "name": "Hierarchical softmax with binary Huffman tree",
      "aliases": [
        "HS-Huffman",
        "binary tree softmax"
      ],
      "type": "concept",
      "description": "Organises vocabulary leaves in a Huffman tree so that frequent words have short codes, reducing computation to O(log V) per update.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation mechanism in §2.1 Hierarchical Softmax."
    },
    {
      "name": "Subsampling of frequent words with probability P(w)=1−√(t/f(w))",
      "aliases": [
        "frequency subsampling",
        "discard frequent tokens"
      ],
      "type": "concept",
      "description": "Randomly discards extremely common words during training to speed up learning and improve rare word representations, with typical threshold t=1e-5.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in §2.3 Subsampling of Frequent Words."
    },
    {
      "name": "Data-driven bigram score thresholding to create phrase tokens",
      "aliases": [
        "bigram scoring δ method",
        "phrase extraction algorithm"
      ],
      "type": "concept",
      "description": "Uses count(w_i w_j) − δ · count(w_i)·count(w_j) to select bigrams/phrases for tokenisation; multiple passes form longer phrases.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Algorithm described in §4 Learning Phrases."
    },
    {
      "name": "Analogical reasoning accuracy improvements over baselines",
      "aliases": [
        "analogy benchmark gains",
        "linear analogy results"
      ],
      "type": "concept",
      "description": "Skip-gram with negative sampling achieved up to 61 % accuracy on word analogies, outperforming hierarchical softmax and prior published vectors.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Results in Table 1 §3 Empirical Results."
    },
    {
      "name": "Training speed improvements (2x–10x) with subsampling and negative sampling",
      "aliases": [
        "faster training timings",
        "Skip-gram efficiency gains"
      ],
      "type": "concept",
      "description": "Optimised implementations process >100 B words/day; subsampling and NEG cut runtime from 97 min to 36 min in experiments.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Timing data in Table 1 and narrative in §3 Empirical Results."
    },
    {
      "name": "Phrase analogy accuracy improvement to 72% with phrase embeddings",
      "aliases": [
        "phrase analogy benchmark result",
        "72 % phrase accuracy"
      ],
      "type": "concept",
      "description": "Hierarchical-softmax Skip-gram trained on 33 B words plus phrase tokens solved 72 % of 3 218 phrase analogies, far above baselines.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Reported in §4.1 Phrase Skip-Gram Results (Table 3)."
    },
    {
      "name": "Pre-train word embeddings with Skip-gram using negative sampling and subsampling",
      "aliases": [
        "apply NEG+subsample Skip-gram",
        "efficient Skip-gram embedding pre-training"
      ],
      "type": "intervention",
      "description": "During pre-training, employ the Skip-gram objective with k=5–15 negative samples per positive instance and discard high-frequency tokens using the 1−√(t/f) rule to obtain accurate, efficient embeddings.",
      "concept_category": null,
      "intervention_lifecycle": "2",
      "intervention_lifecycle_rationale": "Pre-Training phase – Section §2 defines training objective and §3 evaluates pre-training embeddings.",
      "intervention_maturity": "4",
      "intervention_maturity_rationale": "Technique is widely deployed (word2vec, fastText); results validated across billions of tokens – evidence in §3 and §7 Conclusion.",
      "node_rationale": "Direct actionable step derived from validated NEG + subsampling mechanisms."
    },
    {
      "name": "Pre-process corpus to detect bigram and n-gram phrases and train phrase-level Skip-gram embeddings",
      "aliases": [
        "phrase token pre-processing",
        "phrase-aware Skip-gram training"
      ],
      "type": "intervention",
      "description": "Run multiple passes of bigram score thresholding with decreasing thresholds to merge frequent phrases into single tokens, then train Skip-gram embeddings including those tokens.",
      "concept_category": null,
      "intervention_lifecycle": "2",
      "intervention_lifecycle_rationale": "Phrase tokens are generated before model training – detailed in §4.",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Demonstrated to work (72 % accuracy) but not yet universal in production pipelines.",
      "node_rationale": "Addresses idiomatic phrase risk using validated phrase analogy evidence."
    },
    {
      "name": "Train word embeddings with hierarchical softmax using Huffman codes for computational efficiency",
      "aliases": [
        "HS-Huffman embedding training",
        "hierarchical softmax pre-training"
      ],
      "type": "intervention",
      "description": "Replace full softmax with a Huffman-coded binary tree; update only log₂|V| nodes per training step to obtain embeddings when negative sampling is unsuitable.",
      "concept_category": null,
      "intervention_lifecycle": "2",
      "intervention_lifecycle_rationale": "Alternative objective chosen during pre-training (§2.1).",
      "intervention_maturity": "4",
      "intervention_maturity_rationale": "Hierarchical softmax is a standard option in word2vec and other open-source toolkits; performance validated in §3.",
      "node_rationale": "Primary intervention for compute-cost risk when negative sampling can’t be applied."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Semantic misunderstanding in NLP applications due to poor embeddings",
      "target_node": "Low-quality distributed word representations in legacy neural language models",
      "description": "Inadequate legacy embeddings fail to encode necessary linguistic information, leading to misunderstanding.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implicit causal link inferred from Introduction; not experimentally quantified.",
      "edge_rationale": "Section 1 states that improved representations raise NLP performance."
    },
    {
      "type": "mitigated_by",
      "source_node": "Low-quality distributed word representations in legacy neural language models",
      "target_node": "Linear vector space structure allows analogical reasoning from embeddings",
      "description": "Recognising that embeddings can encode linear relations offers a pathway to higher-quality representations.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Paper presents examples but not a formal causal test.",
      "edge_rationale": "Examples of Madrid-Spain→Paris in §1."
    },
    {
      "type": "implemented_by",
      "source_node": "Linear vector space structure allows analogical reasoning from embeddings",
      "target_node": "Context prediction training objective in Skip-gram model for efficient embedding learning",
      "description": "Skip-gram operationalises the insight by predicting context words, which encourages linear relationships.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Directly described in §2.",
      "edge_rationale": "Model architecture explained in Figure 1."
    },
    {
      "type": "implemented_by",
      "source_node": "Context prediction training objective in Skip-gram model for efficient embedding learning",
      "target_node": "Negative sampling with k=5–15 negative draws per positive sample",
      "description": "Negative sampling is one realisation of the Skip-gram objective that replaces the costly softmax.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit definition in §2.2.",
      "edge_rationale": "Equation 4."
    },
    {
      "type": "refined_by",
      "source_node": "Negative sampling with k=5–15 negative draws per positive sample",
      "target_node": "Subsampling of frequent words with probability P(w)=1−√(t/f(w))",
      "description": "Subsampling further optimises NEG training by down-weighting high-frequency words.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Mechanism interaction described in §2.3.",
      "edge_rationale": "Heuristic justification under Subsampling section."
    },
    {
      "type": "validated_by",
      "source_node": "Subsampling of frequent words with probability P(w)=1−√(t/f(w))",
      "target_node": "Training speed improvements (2x–10x) with subsampling and negative sampling",
      "description": "Experiments show that subsampling accelerates training several-fold.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Quantitative timings in Table 1.",
      "edge_rationale": "§3 Empirical Results."
    },
    {
      "type": "motivates",
      "source_node": "Training speed improvements (2x–10x) with subsampling and negative sampling",
      "target_node": "Pre-train word embeddings with Skip-gram using negative sampling and subsampling",
      "description": "Demonstrated efficiency encourages practitioners to adopt NEG + subsampling in pre-training.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Application inference; paper does not prescribe deployment.",
      "edge_rationale": "Conclusion notes practical speed benefits."
    },
    {
      "type": "validated_by",
      "source_node": "Negative sampling with k=5–15 negative draws per positive sample",
      "target_node": "Analogical reasoning accuracy improvements over baselines",
      "description": "NEG models outperform HS and NCE on analogy accuracy.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Table 1 direct comparison.",
      "edge_rationale": "§3 Empirical Results."
    },
    {
      "type": "motivates",
      "source_node": "Analogical reasoning accuracy improvements over baselines",
      "target_node": "Pre-train word embeddings with Skip-gram using negative sampling and subsampling",
      "description": "Accuracy gains make the NEG+subsampling configuration attractive for deployment.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference from performance to adoption.",
      "edge_rationale": "Discussion in §7 Conclusion."
    },
    {
      "type": "caused_by",
      "source_node": "Excessive computational cost in large-scale word embedding training",
      "target_node": "Full softmax output layer complexity scaling with vocabulary size",
      "description": "Compute explosion arises because plain softmax touches every vocabulary entry.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Well-known algorithmic fact stated in §2.",
      "edge_rationale": "Equation 2 discussion."
    },
    {
      "type": "mitigated_by",
      "source_node": "Full softmax output layer complexity scaling with vocabulary size",
      "target_node": "Approximate softmax objectives can reduce computation without losing representation quality",
      "description": "Approximate objectives address the softmax bottleneck.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Paper presents HS/NEG as solutions.",
      "edge_rationale": "§2.1, §2.2."
    },
    {
      "type": "implemented_by",
      "source_node": "Approximate softmax objectives can reduce computation without losing representation quality",
      "target_node": "Use of negative sampling and hierarchical softmax to approximate softmax efficiently",
      "description": "Design choice operationalises the approximation insight.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct mapping in text.",
      "edge_rationale": "§3 comparison."
    },
    {
      "type": "implemented_by",
      "source_node": "Use of negative sampling and hierarchical softmax to approximate softmax efficiently",
      "target_node": "Hierarchical softmax with binary Huffman tree",
      "description": "Hierarchical softmax realises one branch of the design rationale.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Implementation specifics in §2.1.",
      "edge_rationale": "Tree description and Huffman coding."
    },
    {
      "type": "implemented_by",
      "source_node": "Use of negative sampling and hierarchical softmax to approximate softmax efficiently",
      "target_node": "Negative sampling with k=5–15 negative draws per positive sample",
      "description": "Negative sampling realises the alternative branch of the same design.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Implementation in §2.2.",
      "edge_rationale": "Equation 4."
    },
    {
      "type": "validated_by",
      "source_node": "Hierarchical softmax with binary Huffman tree",
      "target_node": "Training speed improvements (2x–10x) with subsampling and negative sampling",
      "description": "HS also contributes to lower training time compared with full softmax.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Timing comparison rows in Table 1.",
      "edge_rationale": "§3 Empirical Results."
    },
    {
      "type": "motivates",
      "source_node": "Training speed improvements (2x–10x) with subsampling and negative sampling",
      "target_node": "Train word embeddings with hierarchical softmax using Huffman codes for computational efficiency",
      "description": "Practitioners may select HS when NEG is unsuitable, due to demonstrated speed benefits.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference; adoption step not explicit.",
      "edge_rationale": "Performance table and conclusion."
    },
    {
      "type": "caused_by",
      "source_node": "Inability to model idiomatic phrases in NLP systems",
      "target_node": "Word-level tokenization ignores multi-word expressions with unique meanings",
      "description": "Ignoring multi-word expressions leads directly to loss of phrase meaning.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicitly stated in §4.",
      "edge_rationale": "Opening paragraph of §4."
    },
    {
      "type": "mitigated_by",
      "source_node": "Word-level tokenization ignores multi-word expressions with unique meanings",
      "target_node": "Data-driven phrase token identification enhances representational expressiveness",
      "description": "Identifying phrases offers a mitigation path.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Proposed solution in §4.",
      "edge_rationale": "Phrase identification discussion."
    },
    {
      "type": "implemented_by",
      "source_node": "Data-driven phrase token identification enhances representational expressiveness",
      "target_node": "Incorporating phrase tokens into training corpus to capture idiomatic meaning",
      "description": "Design rationale operationalises the theoretical insight.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Mechanism description in §4.",
      "edge_rationale": "Replacement of ‘New York Times’ with single token."
    },
    {
      "type": "required_by",
      "source_node": "Context prediction training objective in Skip-gram model for efficient embedding learning",
      "target_node": "Incorporating phrase tokens into training corpus to capture idiomatic meaning",
      "description": "Phrase-token training still relies on the core Skip-gram objective.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Implicit dependency stated across §2 and §4.",
      "edge_rationale": "Phrase models are trained with the same Skip-gram setup."
    },
    {
      "type": "implemented_by",
      "source_node": "Incorporating phrase tokens into training corpus to capture idiomatic meaning",
      "target_node": "Data-driven bigram score thresholding to create phrase tokens",
      "description": "Bigram scoring algorithm realises the phrase incorporation strategy.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Algorithm defined in Equation 6.",
      "edge_rationale": "§4 Learning Phrases."
    },
    {
      "type": "validated_by",
      "source_node": "Data-driven bigram score thresholding to create phrase tokens",
      "target_node": "Phrase analogy accuracy improvement to 72% with phrase embeddings",
      "description": "Phrase analogy benchmark confirms effectiveness of phrase token approach.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "72 % accuracy reported in §4.1.",
      "edge_rationale": "Table 3 and discussion."
    },
    {
      "type": "motivates",
      "source_node": "Phrase analogy accuracy improvement to 72% with phrase embeddings",
      "target_node": "Pre-process corpus to detect bigram and n-gram phrases and train phrase-level Skip-gram embeddings",
      "description": "High phrase analogy accuracy encourages adoption of phrase token pre-processing.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Deployment inference.",
      "edge_rationale": "§7 Conclusion notes advantages of phrase tokens."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2013-10-16T00:00:00Z"
    },
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "title",
      "value": "Distributed Representations of Words and Phrases and their Compositionality"
    },
    {
      "key": "authors",
      "value": [
        "Tomas Mikolov",
        "Ilya Sutskever",
        "Kai Chen",
        "Greg Corrado",
        "Jeffrey Dean"
      ]
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1310.4546"
    }
  ]
}