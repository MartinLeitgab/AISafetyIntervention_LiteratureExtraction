{
  "nodes": [
    {
      "name": "Unreliable real-world applicability assessment of semi-supervised learning algorithms",
      "aliases": [
        "Misleading SSL real-world evaluation",
        "SSL applicability uncertainty"
      ],
      "type": "concept",
      "description": "Risk that published semi-supervised learning (SSL) methods appear better than they actually are in practice because evaluation protocols do not mirror real-world constraints.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central risk discussed across Introduction and Conclusions sections."
    },
    {
      "name": "Unrealistic evaluation protocols in SSL research",
      "aliases": [
        "Standard SSL evaluation shortcomings",
        "Artificial SSL benchmarks"
      ],
      "type": "concept",
      "description": "Current practice discards labels from large datasets, uses mismatched validation sizes, differing architectures, and ignores class-distribution mismatch, leading to skewed results.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Enumerated in Section 2 ‘Improved Evaluation’ as causes of unreliable assessment."
    },
    {
      "name": "Standardised evaluation frameworks mitigate protocol bias",
      "aliases": [
        "Need for shared implementation",
        "Framework standardisation insight"
      ],
      "type": "concept",
      "description": "Using identical model architectures, training procedures, and realistic dataset splits removes confounding factors, allowing true comparison of SSL methods.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Justified in Section 2 (‘A Shared Implementation’) and motivates later methodology."
    },
    {
      "name": "Comprehensive evaluation methodology for SSL realism",
      "aliases": [
        "Improved SSL evaluation design",
        "Methodology integrating multiple tests"
      ],
      "type": "concept",
      "description": "Design that combines shared implementation, strong supervised & transfer baselines, class-distribution mismatch tests, data-sensitivity studies, and small validation sets.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined at end of Section 2 and detailed in Conclusions."
    },
    {
      "name": "Unified modular SSL evaluation software on Wide-ResNet with large-scale hyperparameter optimisation",
      "aliases": [
        "Shared SSL codebase",
        "WRN-based SSL testbed"
      ],
      "type": "concept",
      "description": "Implementation uses WRN-28-2, common training hyper-parameters, 1000-trial Bayesian tuning per method, enabling fair, reproducible experiments.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Described in Section 4.1 ‘Reproduction’ as concrete realisation of the design."
    },
    {
      "name": "Experiments show tuned supervised baseline narrows SSL advantage",
      "aliases": [
        "Small SSL-vs-supervised gap evidence"
      ],
      "type": "concept",
      "description": "Across CIFAR-10 and SVHN, fully-supervised model matched to SSL architecture reduced previously reported performance gaps.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Reported in Section 4.2."
    },
    {
      "name": "Transfer learning fine-tuning outperforms SSL on CIFAR-10",
      "aliases": [
        "Transfer baseline evidence"
      ],
      "type": "concept",
      "description": "WRN-28-2 pretrained on ImageNet32 and fine-tuned with 4k labels achieved 12% error, beating all SSL methods.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4.3 ‘Transfer Learning’."
    },
    {
      "name": "Class distribution mismatch degrades SSL performance",
      "aliases": [
        "Mismatch harm evidence"
      ],
      "type": "concept",
      "description": "CIFAR-10 animal-class experiment shows unlabeled data with unseen classes can worsen accuracy compared with no unlabeled data.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4.4 ‘Class Distribution Mismatch’."
    },
    {
      "name": "SSL performance sensitivity to labelled-unlabelled data amounts",
      "aliases": [
        "Data scaling evidence"
      ],
      "type": "concept",
      "description": "Experiments varying label counts and SVHN-extra size show method-dependent scaling and plateau after ~80k unlabeled samples.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4.5 ‘Varying Data Amounts’."
    },
    {
      "name": "Small validation sets hinder reliable hyperparameter selection",
      "aliases": [
        "Validation-set size evidence"
      ],
      "type": "concept",
      "description": "Empirical variance analysis and Hoeffding bound show >10× larger sets needed to differentiate SSL methods at 1% accuracy.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4.6 ‘Small Validation Sets’."
    },
    {
      "name": "Unified reimplementation reproduces relative SSL rankings across datasets",
      "aliases": [
        "Reproducibility evidence"
      ],
      "type": "concept",
      "description": "Despite having half the parameters of prior work, the shared WRN framework replicated the ordering (VAT & Mean-Teacher best).",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4.1 table and discussion."
    },
    {
      "name": "Adopt shared implementation framework when comparing SSL methods",
      "aliases": [
        "Use common SSL testbed"
      ],
      "type": "intervention",
      "description": "Researchers should evaluate all SSL algorithms on the same architecture, training pipeline, and codebase (e.g., the provided WRN-based framework).",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Evaluation stage prior to deployment; Section 2 ‘A Shared Implementation’.",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Prototype software released (Section 4.1).",
      "node_rationale": "Direct recommendation in Conclusions."
    },
    {
      "name": "Include high-quality supervised and transfer learning baselines in SSL evaluations",
      "aliases": [
        "Baseline reporting recommendation"
      ],
      "type": "intervention",
      "description": "Allocate equal hyper-parameter tuning budget to fully-supervised models and, where possible, transfer-learning setups to benchmark true SSL benefit.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Pre-deployment benchmarking; Section 2 item ‘High-Quality Supervised Baseline’.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Guideline demonstrated experimentally but not yet standard practice (Sections 4.2–4.3).",
      "node_rationale": "Explicit recommendation; motivated by evidence."
    },
    {
      "name": "Evaluate SSL robustness under class-distribution mismatch scenarios",
      "aliases": [
        "Class mismatch testing recommendation"
      ],
      "type": "intervention",
      "description": "Construct test splits where unlabeled data contains classes absent from labeled data to detect negative transfer effects.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Testing robustness pre-deployment; Section 2 ‘Consider Class Distribution Mismatch’.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Proposed and shown in one study; needs broader validation (Section 4.4).",
      "node_rationale": "Recommendation in Conclusions."
    },
    {
      "name": "Systematically vary labelled and unlabeled data amounts during SSL evaluation",
      "aliases": [
        "Data amount sensitivity tests"
      ],
      "type": "intervention",
      "description": "Benchmark each SSL method across multiple labelled-data regimes and with increasing unlabeled pools (e.g., SVHN-extra).",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Part of evaluation; Section 2 ‘Varying the Amount of Labeled and Unlabeled Data’.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated experimentally for two datasets (Section 4.5).",
      "node_rationale": "Recommendation to reveal method sensitivity."
    },
    {
      "name": "Use realistically small validation sets or cross-validation for hyperparameter tuning in SSL",
      "aliases": [
        "Validation set realism"
      ],
      "type": "intervention",
      "description": "Restrict validation size to a fraction of training labels or employ cross-validation to avoid over-fitting hyper-parameters on unrealistically large held-out data.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Hyper-parameter selection step before deployment; Section 2 ‘Realistically Small Validation Sets’.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Evidence provided but no large-scale adoption yet (Section 4.6).",
      "node_rationale": "Key recommendation in Conclusions."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Unreliable real-world applicability assessment of semi-supervised learning algorithms",
      "target_node": "Unrealistic evaluation protocols in SSL research",
      "description": "Flawed protocols directly lead to over-optimistic claims about SSL effectiveness.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit narrative in Introduction.",
      "edge_rationale": "Section 1 discusses how current evaluation masks real-world issues."
    },
    {
      "type": "mitigated_by",
      "source_node": "Unrealistic evaluation protocols in SSL research",
      "target_node": "Standardised evaluation frameworks mitigate protocol bias",
      "description": "Adopting shared frameworks addresses inconsistencies and bias.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoned argument in Section 2.",
      "edge_rationale": "Section 2 item ‘A Shared Implementation’ provides rationale."
    },
    {
      "type": "mitigated_by",
      "source_node": "Standardised evaluation frameworks mitigate protocol bias",
      "target_node": "Comprehensive evaluation methodology for SSL realism",
      "description": "Insight informs the design of a broader methodology covering additional real-world factors.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical flow from insight to design laid out in Section 2.",
      "edge_rationale": "Section 2 enumerates methodology components derived from the insight."
    },
    {
      "type": "implemented_by",
      "source_node": "Comprehensive evaluation methodology for SSL realism",
      "target_node": "Unified modular SSL evaluation software on Wide-ResNet with large-scale hyperparameter optimisation",
      "description": "Software realises the methodological principles allowing empirical study.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Concrete implementation described in Section 4.1.",
      "edge_rationale": "Section 4.1 ‘Reproduction’ details code and setup."
    },
    {
      "type": "validated_by",
      "source_node": "Unified modular SSL evaluation software on Wide-ResNet with large-scale hyperparameter optimisation",
      "target_node": "Experiments show tuned supervised baseline narrows SSL advantage",
      "description": "Framework produced experimental evidence of smaller gaps.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Empirical results in Section 4.2.",
      "edge_rationale": "Table 1 shows reduced gap."
    },
    {
      "type": "validated_by",
      "source_node": "Unified modular SSL evaluation software on Wide-ResNet with large-scale hyperparameter optimisation",
      "target_node": "Transfer learning fine-tuning outperforms SSL on CIFAR-10",
      "description": "Same framework used to benchmark transfer learning.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Experiment Section 4.3.",
      "edge_rationale": "Section 4.3 reports 12% error."
    },
    {
      "type": "validated_by",
      "source_node": "Unified modular SSL evaluation software on Wide-ResNet with large-scale hyperparameter optimisation",
      "target_node": "Class distribution mismatch degrades SSL performance",
      "description": "Framework facilitated controlled mismatch experiments.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Figures in Section 4.4.",
      "edge_rationale": "Figure 2 shows degradation."
    },
    {
      "type": "validated_by",
      "source_node": "Unified modular SSL evaluation software on Wide-ResNet with large-scale hyperparameter optimisation",
      "target_node": "SSL performance sensitivity to labelled-unlabelled data amounts",
      "description": "Framework allowed label/unlabel quantity sweeps.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Results in Section 4.5.",
      "edge_rationale": "Figure 3 and 4."
    },
    {
      "type": "validated_by",
      "source_node": "Unified modular SSL evaluation software on Wide-ResNet with large-scale hyperparameter optimisation",
      "target_node": "Small validation sets hinder reliable hyperparameter selection",
      "description": "Framework generated empirical variance curves for different validation sizes.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 4.6 data.",
      "edge_rationale": "Figure 5."
    },
    {
      "type": "validated_by",
      "source_node": "Unified modular SSL evaluation software on Wide-ResNet with large-scale hyperparameter optimisation",
      "target_node": "Unified reimplementation reproduces relative SSL rankings across datasets",
      "description": "Cross-dataset consistency confirms implementation validity.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Qualitative claim in Section 4.1.",
      "edge_rationale": "Discussion around Table 1."
    },
    {
      "type": "motivates",
      "source_node": "Unified reimplementation reproduces relative SSL rankings across datasets",
      "target_node": "Adopt shared implementation framework when comparing SSL methods",
      "description": "Demonstrated reproducibility supports recommendation to use shared frameworks.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical implication; explicit recommendation.",
      "edge_rationale": "Conclusions highlight shared framework value."
    },
    {
      "type": "motivates",
      "source_node": "Experiments show tuned supervised baseline narrows SSL advantage",
      "target_node": "Include high-quality supervised and transfer learning baselines in SSL evaluations",
      "description": "Evidence that strong baselines are crucial motivates inclusion.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct empirical result.",
      "edge_rationale": "Section 4.2 discussion."
    },
    {
      "type": "motivates",
      "source_node": "Transfer learning fine-tuning outperforms SSL on CIFAR-10",
      "target_node": "Include high-quality supervised and transfer learning baselines in SSL evaluations",
      "description": "Outperformance shows necessity of reporting transfer baselines.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Experimental evidence Section 4.3.",
      "edge_rationale": "Section 4.3 emphasises comparison."
    },
    {
      "type": "motivates",
      "source_node": "Class distribution mismatch degrades SSL performance",
      "target_node": "Evaluate SSL robustness under class-distribution mismatch scenarios",
      "description": "Observed degradation motivates testing mismatch routinely.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Quantitative results Figure 2.",
      "edge_rationale": "Section 4.4 and Conclusions."
    },
    {
      "type": "motivates",
      "source_node": "SSL performance sensitivity to labelled-unlabelled data amounts",
      "target_node": "Systematically vary labelled and unlabeled data amounts during SSL evaluation",
      "description": "Different scaling behaviour across methods motivates systematic variation during evaluation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Experiments Figures 3 & 4.",
      "edge_rationale": "Section 4.5 findings."
    },
    {
      "type": "motivates",
      "source_node": "Small validation sets hinder reliable hyperparameter selection",
      "target_node": "Use realistically small validation sets or cross-validation for hyperparameter tuning in SSL",
      "description": "Large variance with small sets motivates realistic validation strategies.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Empirical and theoretical evidence Section 4.6.",
      "edge_rationale": "Section 4.6 discussion."
    }
  ],
  "meta": [
    {
      "key": "authors",
      "value": [
        "Avital Oliver",
        "Augustus Odena",
        "Colin Raffel",
        "Ekin D. Cubuk",
        "Ian J. Goodfellow"
      ]
    },
    {
      "key": "date_published",
      "value": "2018-04-24T00:00:00Z"
    },
    {
      "key": "title",
      "value": "Realistic Evaluation of Deep Semi-Supervised Learning Algorithms"
    },
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1804.09170"
    }
  ]
}