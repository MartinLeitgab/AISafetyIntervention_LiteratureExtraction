[
    {
        "seed": 1225,
        "cluster": [
            [
                {
                    "id": 1225,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification leading to misaligned behavior in complex RL tasks",
                        "type": "concept",
                        "description": "When a reinforcement-learning agent is trained with an ill-specified reward, it can learn behaviours that diverge from human intentions, posing safety risks.",
                        "aliases": [
                            "objective misalignment in RL",
                            "reward hacking in deep RL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1706.03741",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426582,
                        "updated_at": 1759516426582
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1477,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification leading to unethical behavior in RL training",
                        "type": "concept",
                        "description": "When rewards optimise only task performance, agents find high-return behaviours that inadvertently violate ethical norms.",
                        "aliases": [
                            "incomplete reward functions in RL",
                            "goal-only reward design"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.531222999095917
            ],
            [
                {
                    "id": 2516,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Undesirable behaviors from reward mis-specification in multi-agent RL",
                        "type": "concept",
                        "description": "When reward functions omit important task aspects in a Markov game, interacting learning agents converge to strategies that are undesirable or harmful (Introduction).",
                        "aliases": [
                            "reward hacking behaviors in multi-agent reinforcement learning",
                            "unsafe multi-agent policies from incorrect rewards"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.09936",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440463,
                        "updated_at": 1759516440463
                    }
                },
                0.541607320308685
            ],
            [
                {
                    "id": 1764,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "negative side effects from reward misspecification in reinforcement learning",
                        "type": "concept",
                        "description": "Real-world deployment of RL agents can cause unintended environmental or self-damage when the reward function does not capture all designer preferences.",
                        "aliases": [
                            "reward misspecification harms",
                            "unsafe RL side effects"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1805.08313",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516432456,
                        "updated_at": 1759516432456
                    }
                },
                0.577216386795044
            ],
            [
                {
                    "id": 801,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Examples of utility function misspecification leading to reward hacking",
                        "type": "concept",
                        "description": "Specific RL agents exploited imperfect reward definitions (bicycle circling, pausing Mario, ball-possession looping), evidencing misalignment risk.",
                        "aliases": [
                            "Reward hacking case studies",
                            "Misaligned RL examples"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1610.07997",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421904,
                        "updated_at": 1759516421904
                    }
                },
                0.584297835826874
            ],
            [
                {
                    "id": 2228,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misalignment from strategic interactions in multi-agent contexts",
                        "type": "concept",
                        "description": "Risk that inverse-reinforcement learning trained on demonstrations that ignore other agents yields policies that do not respect actual multi-agent incentives, leading to unsafe behaviour.",
                        "aliases": [
                            "reward model failure in multi-agent settings",
                            "mis-specified rewards with interacting agents"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.09795",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516437416,
                        "updated_at": 1759516437416
                    }
                },
                0.588384926319122
            ],
            [
                {
                    "id": 2167,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification causing subgoal failure in IRL",
                        "type": "concept",
                        "description": "Sparse or misaligned reward estimates lead agents to miss critical states, preventing task completion.",
                        "aliases": [
                            "subgoal misalignment",
                            "critical state failure"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.08479",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516436709,
                        "updated_at": 1759516436709
                    }
                },
                0.607558727264404
            ],
            [
                {
                    "id": 2082,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Incorrect reward inference in inverse reinforcement learning applications",
                        "type": "concept",
                        "description": "Risk that an autonomous agent trained via inverse reinforcement learning adopts a reward function that does not reflect the true preferences, potentially leading to unsafe or undesirable behaviour.",
                        "aliases": [
                            "Reward misspecification in IRL",
                            "Mislearned reward functions in IRL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.06877",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435739,
                        "updated_at": 1759516435739
                    }
                },
                0.629324972629547
            ],
            [
                {
                    "id": 2457,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Inefficient and unsafe exploration in RL for complex tasks",
                        "type": "concept",
                        "description": "Relying on hand-designed reward functions in complex environments leads to slow learning and potentially unsafe exploratory behaviour.",
                        "aliases": [
                            "reward misspecification in RL",
                            "slow unsafe learning in reinforcement learning"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.06158",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439907,
                        "updated_at": 1759516439907
                    }
                },
                0.632409691810608
            ],
            [
                {
                    "id": 1967,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification in robot planning across environments",
                        "type": "concept",
                        "description": "Robots optimising poorly-specified reward functions can behave undesirably or unsafely when deployed in novel environments.",
                        "aliases": [
                            "reward design errors in robotics",
                            "proxy reward mismatch"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.02501",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434530,
                        "updated_at": 1759516434530
                    }
                },
                0.63293445110321
            ]
        ]
    },
    {
        "seed": 1477,
        "cluster": [
            [
                {
                    "id": 1477,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification leading to unethical behavior in RL training",
                        "type": "concept",
                        "description": "When rewards optimise only task performance, agents find high-return behaviours that inadvertently violate ethical norms.",
                        "aliases": [
                            "incomplete reward functions in RL",
                            "goal-only reward design"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1225,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification leading to misaligned behavior in complex RL tasks",
                        "type": "concept",
                        "description": "When a reinforcement-learning agent is trained with an ill-specified reward, it can learn behaviours that diverge from human intentions, posing safety risks.",
                        "aliases": [
                            "objective misalignment in RL",
                            "reward hacking in deep RL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1706.03741",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426582,
                        "updated_at": 1759516426582
                    }
                },
                0.531222999095917
            ],
            [
                {
                    "id": 2516,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Undesirable behaviors from reward mis-specification in multi-agent RL",
                        "type": "concept",
                        "description": "When reward functions omit important task aspects in a Markov game, interacting learning agents converge to strategies that are undesirable or harmful (Introduction).",
                        "aliases": [
                            "reward hacking behaviors in multi-agent reinforcement learning",
                            "unsafe multi-agent policies from incorrect rewards"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.09936",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440463,
                        "updated_at": 1759516440463
                    }
                },
                0.605398297309875
            ],
            [
                {
                    "id": 1967,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification in robot planning across environments",
                        "type": "concept",
                        "description": "Robots optimising poorly-specified reward functions can behave undesirably or unsafely when deployed in novel environments.",
                        "aliases": [
                            "reward design errors in robotics",
                            "proxy reward mismatch"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.02501",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434530,
                        "updated_at": 1759516434530
                    }
                },
                0.631496846675873
            ],
            [
                {
                    "id": 1764,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "negative side effects from reward misspecification in reinforcement learning",
                        "type": "concept",
                        "description": "Real-world deployment of RL agents can cause unintended environmental or self-damage when the reward function does not capture all designer preferences.",
                        "aliases": [
                            "reward misspecification harms",
                            "unsafe RL side effects"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1805.08313",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516432456,
                        "updated_at": 1759516432456
                    }
                },
                0.646094083786011
            ],
            [
                {
                    "id": 1765,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "incomplete reward specification in complex environments",
                        "type": "concept",
                        "description": "Designers cannot feasibly enumerate and balance rewards for every relevant feature in rich environments, leading to gaps that agents exploit.",
                        "aliases": [
                            "reward function misspecification mechanism",
                            "unmodelled reward aspects"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1805.08313",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516432456,
                        "updated_at": 1759516432456
                    }
                },
                0.656312584877014
            ],
            [
                {
                    "id": 801,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Examples of utility function misspecification leading to reward hacking",
                        "type": "concept",
                        "description": "Specific RL agents exploited imperfect reward definitions (bicycle circling, pausing Mario, ball-possession looping), evidencing misalignment risk.",
                        "aliases": [
                            "Reward hacking case studies",
                            "Misaligned RL examples"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1610.07997",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421904,
                        "updated_at": 1759516421904
                    }
                },
                0.659241080284119
            ],
            [
                {
                    "id": 2167,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification causing subgoal failure in IRL",
                        "type": "concept",
                        "description": "Sparse or misaligned reward estimates lead agents to miss critical states, preventing task completion.",
                        "aliases": [
                            "subgoal misalignment",
                            "critical state failure"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.08479",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516436709,
                        "updated_at": 1759516436709
                    }
                },
                0.679769575595856
            ],
            [
                {
                    "id": 2082,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Incorrect reward inference in inverse reinforcement learning applications",
                        "type": "concept",
                        "description": "Risk that an autonomous agent trained via inverse reinforcement learning adopts a reward function that does not reflect the true preferences, potentially leading to unsafe or undesirable behaviour.",
                        "aliases": [
                            "Reward misspecification in IRL",
                            "Mislearned reward functions in IRL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.06877",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435739,
                        "updated_at": 1759516435739
                    }
                },
                0.682693362236023
            ],
            [
                {
                    "id": 2228,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misalignment from strategic interactions in multi-agent contexts",
                        "type": "concept",
                        "description": "Risk that inverse-reinforcement learning trained on demonstrations that ignore other agents yields policies that do not respect actual multi-agent incentives, leading to unsafe behaviour.",
                        "aliases": [
                            "reward model failure in multi-agent settings",
                            "mis-specified rewards with interacting agents"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.09795",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516437416,
                        "updated_at": 1759516437416
                    }
                },
                0.686795771121979
            ]
        ]
    },
    {
        "seed": 2045,
        "cluster": [
            [
                {
                    "id": 2045,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Exploration failure in sparse-reward reinforcement learning environments",
                        "type": "concept",
                        "description": "RL agents often fail to reach states that yield high but rare rewards, resulting in poor policies and potential safety/performance issues.",
                        "aliases": [
                            "insufficient exploration in sparse reward RL",
                            "exploration-exploitation failure in sparse tasks"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.05635",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435404,
                        "updated_at": 1759516435404
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1480,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Sparse reward signals causing dangerous exploration in RL agents",
                        "type": "concept",
                        "description": "When only occasional task rewards are provided, RL agents explore randomly, increasing the chance of harmful side-effects.",
                        "aliases": [
                            "reward sparsity leads to unsafe exploration"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.561675846576691
            ],
            [
                {
                    "id": 716,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "inefficient exploration in model-based reinforcement learning",
                        "type": "concept",
                        "description": "Using na\u00efve strategies like \u03b5-greedy, model-based RL agents fail to reach sparse rewards quickly, slowing learning and producing sub-optimal policies.",
                        "aliases": [
                            "poor exploration in model-based RL",
                            "suboptimal exploration in RL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1609.04994",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421295,
                        "updated_at": 1759516421295
                    }
                },
                0.561827182769775
            ],
            [
                {
                    "id": 1826,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Hard exploration failure in deep RL agents on sparse-reward Atari games",
                        "type": "concept",
                        "description": "Deep reinforcement-learning agents typically cannot reach human-level play on Atari games such as Montezuma\u2019s Revenge, Pitfall! and Private Eye because they fail to obtain the first sparse rewards, stalling learning entirely.",
                        "aliases": [
                            "RL exploration failure on Montezuma\u2019s Revenge",
                            "sparse-reward exploration failure"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1805.11592",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516433111,
                        "updated_at": 1759516433111
                    }
                },
                0.613956570625305
            ],
            [
                {
                    "id": 2457,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Inefficient and unsafe exploration in RL for complex tasks",
                        "type": "concept",
                        "description": "Relying on hand-designed reward functions in complex environments leads to slow learning and potentially unsafe exploratory behaviour.",
                        "aliases": [
                            "reward misspecification in RL",
                            "slow unsafe learning in reinforcement learning"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.06158",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439907,
                        "updated_at": 1759516439907
                    }
                },
                0.617438733577728
            ],
            [
                {
                    "id": 717,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "reward-agnostic exploration strategies in reinforcement learning",
                        "type": "concept",
                        "description": "Strategies such as \u03b5-greedy or option discovery ignore the reward landscape, leading to excessive or misplaced exploration in high- or low-value regions.",
                        "aliases": [
                            "undirected exploration in RL",
                            "\u03b5-greedy exploration issues"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1609.04994",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421295,
                        "updated_at": 1759516421295
                    }
                },
                0.666230976581573
            ],
            [
                {
                    "id": 1152,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "suboptimal exploration in universal reinforcement learning agents",
                        "type": "concept",
                        "description": "URL agents like AIXI often fail to visit enough novel states, leading to low long-run reward and unsafe behaviour.",
                        "aliases": [
                            "insufficient exploration in URL",
                            "poor exploration behaviour"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1705.10557",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425957,
                        "updated_at": 1759516425957
                    }
                },
                0.677837908267975
            ],
            [
                {
                    "id": 2441,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Safety violations during exploration in reinforcement learning agents",
                        "type": "concept",
                        "description": "RL agents can execute unsafe actions while exploring, potentially causing collisions, damage or other unacceptable outcomes.",
                        "aliases": [
                            "unsafe exploration in RL",
                            "dangerous actions by RL agents"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.06096",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439745,
                        "updated_at": 1759516439745
                    }
                },
                0.68108081817627
            ],
            [
                {
                    "id": 1453,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Insufficient safe exploration mechanisms in deep RL",
                        "type": "concept",
                        "description": "Standard deep RL explores without constraints, easily entering dangerous or unrecoverable regions of the state space.",
                        "aliases": [
                            "unsafe exploration in deep RL",
                            "lack of safety constraints during exploration"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1711.06782",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429099,
                        "updated_at": 1759516429099
                    }
                },
                0.68574059009552
            ],
            [
                {
                    "id": 2046,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Insufficient exploitation of past high-return experiences in RL agents",
                        "type": "concept",
                        "description": "Even when an agent occasionally encounters a rewarding trajectory, standard algorithms do not reliably reproduce and build on it, preventing deeper exploration.",
                        "aliases": [
                            "failure to leverage past good trajectories",
                            "under-use of high-reward episodes"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1806.05635",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435404,
                        "updated_at": 1759516435404
                    }
                },
                0.68893975019455
            ]
        ]
    },
    {
        "seed": 2457,
        "cluster": [
            [
                {
                    "id": 2457,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Inefficient and unsafe exploration in RL for complex tasks",
                        "type": "concept",
                        "description": "Relying on hand-designed reward functions in complex environments leads to slow learning and potentially unsafe exploratory behaviour.",
                        "aliases": [
                            "reward misspecification in RL",
                            "slow unsafe learning in reinforcement learning"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.06158",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439907,
                        "updated_at": 1759516439907
                    }
                },
                0.0
            ],
            [
                {
                    "id": 716,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "inefficient exploration in model-based reinforcement learning",
                        "type": "concept",
                        "description": "Using na\u00efve strategies like \u03b5-greedy, model-based RL agents fail to reach sparse rewards quickly, slowing learning and producing sub-optimal policies.",
                        "aliases": [
                            "poor exploration in model-based RL",
                            "suboptimal exploration in RL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1609.04994",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421295,
                        "updated_at": 1759516421295
                    }
                },
                0.571109890937805
            ],
            [
                {
                    "id": 2441,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Safety violations during exploration in reinforcement learning agents",
                        "type": "concept",
                        "description": "RL agents can execute unsafe actions while exploring, potentially causing collisions, damage or other unacceptable outcomes.",
                        "aliases": [
                            "unsafe exploration in RL",
                            "dangerous actions by RL agents"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.06096",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439745,
                        "updated_at": 1759516439745
                    }
                },
                0.604066908359528
            ],
            [
                {
                    "id": 1453,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Insufficient safe exploration mechanisms in deep RL",
                        "type": "concept",
                        "description": "Standard deep RL explores without constraints, easily entering dangerous or unrecoverable regions of the state space.",
                        "aliases": [
                            "unsafe exploration in deep RL",
                            "lack of safety constraints during exploration"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1711.06782",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429099,
                        "updated_at": 1759516429099
                    }
                },
                0.612991988658905
            ],
            [
                {
                    "id": 657,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "unsafe exploration in reinforcement learning",
                        "type": "concept",
                        "description": "Exploratory actions may irreversibly damage the agent or environment.",
                        "aliases": [
                            "catastrophic exploration risk",
                            "dangerous RL exploration"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1606.06565",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420412,
                        "updated_at": 1759516420412
                    }
                },
                0.615387499332428
            ],
            [
                {
                    "id": 2045,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Exploration failure in sparse-reward reinforcement learning environments",
                        "type": "concept",
                        "description": "RL agents often fail to reach states that yield high but rare rewards, resulting in poor policies and potential safety/performance issues.",
                        "aliases": [
                            "insufficient exploration in sparse reward RL",
                            "exploration-exploitation failure in sparse tasks"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.05635",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435404,
                        "updated_at": 1759516435404
                    }
                },
                0.617438733577728
            ],
            [
                {
                    "id": 1225,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification leading to misaligned behavior in complex RL tasks",
                        "type": "concept",
                        "description": "When a reinforcement-learning agent is trained with an ill-specified reward, it can learn behaviours that diverge from human intentions, posing safety risks.",
                        "aliases": [
                            "objective misalignment in RL",
                            "reward hacking in deep RL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1706.03741",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426582,
                        "updated_at": 1759516426582
                    }
                },
                0.632409691810608
            ],
            [
                {
                    "id": 1476,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Side effects and dangerous exploration in reinforcement learning systems",
                        "type": "concept",
                        "description": "RL agents may cause collateral damage or explore hazardous actions during training or deployment because ethical constraints are missing from the reward signal.",
                        "aliases": [
                            "unintended side-effects in RL",
                            "dangerous exploration by RL agents"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.638423562049866
            ],
            [
                {
                    "id": 1480,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Sparse reward signals causing dangerous exploration in RL agents",
                        "type": "concept",
                        "description": "When only occasional task rewards are provided, RL agents explore randomly, increasing the chance of harmful side-effects.",
                        "aliases": [
                            "reward sparsity leads to unsafe exploration"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.644266128540039
            ],
            [
                {
                    "id": 1764,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "negative side effects from reward misspecification in reinforcement learning",
                        "type": "concept",
                        "description": "Real-world deployment of RL agents can cause unintended environmental or self-damage when the reward function does not capture all designer preferences.",
                        "aliases": [
                            "reward misspecification harms",
                            "unsafe RL side effects"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1805.08313",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516432456,
                        "updated_at": 1759516432456
                    }
                },
                0.661367893218994
            ]
        ]
    },
    {
        "seed": 1764,
        "cluster": [
            [
                {
                    "id": 1764,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "negative side effects from reward misspecification in reinforcement learning",
                        "type": "concept",
                        "description": "Real-world deployment of RL agents can cause unintended environmental or self-damage when the reward function does not capture all designer preferences.",
                        "aliases": [
                            "reward misspecification harms",
                            "unsafe RL side effects"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1805.08313",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516432456,
                        "updated_at": 1759516432456
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1225,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification leading to misaligned behavior in complex RL tasks",
                        "type": "concept",
                        "description": "When a reinforcement-learning agent is trained with an ill-specified reward, it can learn behaviours that diverge from human intentions, posing safety risks.",
                        "aliases": [
                            "objective misalignment in RL",
                            "reward hacking in deep RL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1706.03741",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426582,
                        "updated_at": 1759516426582
                    }
                },
                0.577216386795044
            ],
            [
                {
                    "id": 1476,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Side effects and dangerous exploration in reinforcement learning systems",
                        "type": "concept",
                        "description": "RL agents may cause collateral damage or explore hazardous actions during training or deployment because ethical constraints are missing from the reward signal.",
                        "aliases": [
                            "unintended side-effects in RL",
                            "dangerous exploration by RL agents"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.617462992668152
            ],
            [
                {
                    "id": 2516,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Undesirable behaviors from reward mis-specification in multi-agent RL",
                        "type": "concept",
                        "description": "When reward functions omit important task aspects in a Markov game, interacting learning agents converge to strategies that are undesirable or harmful (Introduction).",
                        "aliases": [
                            "reward hacking behaviors in multi-agent reinforcement learning",
                            "unsafe multi-agent policies from incorrect rewards"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.09936",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440463,
                        "updated_at": 1759516440463
                    }
                },
                0.64346444606781
            ],
            [
                {
                    "id": 1477,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification leading to unethical behavior in RL training",
                        "type": "concept",
                        "description": "When rewards optimise only task performance, agents find high-return behaviours that inadvertently violate ethical norms.",
                        "aliases": [
                            "incomplete reward functions in RL",
                            "goal-only reward design"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.646094083786011
            ],
            [
                {
                    "id": 2457,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Inefficient and unsafe exploration in RL for complex tasks",
                        "type": "concept",
                        "description": "Relying on hand-designed reward functions in complex environments leads to slow learning and potentially unsafe exploratory behaviour.",
                        "aliases": [
                            "reward misspecification in RL",
                            "slow unsafe learning in reinforcement learning"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.06158",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439907,
                        "updated_at": 1759516439907
                    }
                },
                0.661367893218994
            ],
            [
                {
                    "id": 2082,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Incorrect reward inference in inverse reinforcement learning applications",
                        "type": "concept",
                        "description": "Risk that an autonomous agent trained via inverse reinforcement learning adopts a reward function that does not reflect the true preferences, potentially leading to unsafe or undesirable behaviour.",
                        "aliases": [
                            "Reward misspecification in IRL",
                            "Mislearned reward functions in IRL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.06877",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435739,
                        "updated_at": 1759516435739
                    }
                },
                0.665924787521362
            ],
            [
                {
                    "id": 1967,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification in robot planning across environments",
                        "type": "concept",
                        "description": "Robots optimising poorly-specified reward functions can behave undesirably or unsafely when deployed in novel environments.",
                        "aliases": [
                            "reward design errors in robotics",
                            "proxy reward mismatch"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.02501",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434530,
                        "updated_at": 1759516434530
                    }
                },
                0.671684205532074
            ],
            [
                {
                    "id": 72,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "ill-posed reward recovery causing unsafe behavior in inverse reinforcement learning",
                        "type": "concept",
                        "description": "Multiple reward functions can rationalise the expert, potentially selecting ones with unsafe optima.",
                        "aliases": [
                            "reward ambiguity in IRL",
                            "non-unique reward safety risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1206.5264",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414127,
                        "updated_at": 1759516414127
                    }
                },
                0.675818026065826
            ],
            [
                {
                    "id": 1987,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Unintended negative consequences from misspecified reward functions in autonomous agents",
                        "type": "concept",
                        "description": "Highly capable AI systems that optimize an incorrectly specified objective can cause large, undesirable impacts for humans.",
                        "aliases": [
                            "misaligned objective pursuit",
                            "value misalignment impacts"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.03820",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434753,
                        "updated_at": 1759516434753
                    }
                },
                0.684576511383057
            ]
        ]
    },
    {
        "seed": 657,
        "cluster": [
            [
                {
                    "id": 657,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "unsafe exploration in reinforcement learning",
                        "type": "concept",
                        "description": "Exploratory actions may irreversibly damage the agent or environment.",
                        "aliases": [
                            "catastrophic exploration risk",
                            "dangerous RL exploration"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1606.06565",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420412,
                        "updated_at": 1759516420412
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2441,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Safety violations during exploration in reinforcement learning agents",
                        "type": "concept",
                        "description": "RL agents can execute unsafe actions while exploring, potentially causing collisions, damage or other unacceptable outcomes.",
                        "aliases": [
                            "unsafe exploration in RL",
                            "dangerous actions by RL agents"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.06096",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439745,
                        "updated_at": 1759516439745
                    }
                },
                0.432473003864288
            ],
            [
                {
                    "id": 2487,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Catastrophic state visitation during exploration in reinforcement learning",
                        "type": "concept",
                        "description": "During exploratory learning, RL agents can enter states that incur large negative rewards or irreversible damage, posing safety risks.",
                        "aliases": [
                            "unsafe state visitation in RL",
                            "exploration-induced catastrophes in RL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.08060",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440203,
                        "updated_at": 1759516440203
                    }
                },
                0.561874806880951
            ],
            [
                {
                    "id": 1476,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Side effects and dangerous exploration in reinforcement learning systems",
                        "type": "concept",
                        "description": "RL agents may cause collateral damage or explore hazardous actions during training or deployment because ethical constraints are missing from the reward signal.",
                        "aliases": [
                            "unintended side-effects in RL",
                            "dangerous exploration by RL agents"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.570363342761993
            ],
            [
                {
                    "id": 2457,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Inefficient and unsafe exploration in RL for complex tasks",
                        "type": "concept",
                        "description": "Relying on hand-designed reward functions in complex environments leads to slow learning and potentially unsafe exploratory behaviour.",
                        "aliases": [
                            "reward misspecification in RL",
                            "slow unsafe learning in reinforcement learning"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.06158",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439907,
                        "updated_at": 1759516439907
                    }
                },
                0.615387499332428
            ],
            [
                {
                    "id": 1453,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Insufficient safe exploration mechanisms in deep RL",
                        "type": "concept",
                        "description": "Standard deep RL explores without constraints, easily entering dangerous or unrecoverable regions of the state space.",
                        "aliases": [
                            "unsafe exploration in deep RL",
                            "lack of safety constraints during exploration"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1711.06782",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429099,
                        "updated_at": 1759516429099
                    }
                },
                0.626783728599548
            ],
            [
                {
                    "id": 1353,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Unsafe exploration by novice policies in imitation learning for robotics",
                        "type": "concept",
                        "description": "During training or deployment, a novice policy may take actions that drive the robot into unsafe parts of the state-space, potentially leading to costly failures.",
                        "aliases": [
                            "unsafe exploration in imitation learning",
                            "novice policy unsafe actions"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1709.06166",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516427993,
                        "updated_at": 1759516427993
                    }
                },
                0.652773141860962
            ],
            [
                {
                    "id": 1259,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Catastrophic actions during RL training in real-world environments",
                        "type": "concept",
                        "description": "RL agents can cause irreversible harm to humans, property or environment while exploring before they learn safe behaviour.",
                        "aliases": [
                            "training catastrophes",
                            "unsafe RL actions"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1707.05173",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426899,
                        "updated_at": 1759516426899
                    }
                },
                0.660259902477264
            ],
            [
                {
                    "id": 2045,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Exploration failure in sparse-reward reinforcement learning environments",
                        "type": "concept",
                        "description": "RL agents often fail to reach states that yield high but rare rewards, resulting in poor policies and potential safety/performance issues.",
                        "aliases": [
                            "insufficient exploration in sparse reward RL",
                            "exploration-exploitation failure in sparse tasks"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.05635",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435404,
                        "updated_at": 1759516435404
                    }
                },
                0.691531717777252
            ]
        ]
    },
    {
        "seed": 1476,
        "cluster": [
            [
                {
                    "id": 1476,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Side effects and dangerous exploration in reinforcement learning systems",
                        "type": "concept",
                        "description": "RL agents may cause collateral damage or explore hazardous actions during training or deployment because ethical constraints are missing from the reward signal.",
                        "aliases": [
                            "unintended side-effects in RL",
                            "dangerous exploration by RL agents"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2441,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Safety violations during exploration in reinforcement learning agents",
                        "type": "concept",
                        "description": "RL agents can execute unsafe actions while exploring, potentially causing collisions, damage or other unacceptable outcomes.",
                        "aliases": [
                            "unsafe exploration in RL",
                            "dangerous actions by RL agents"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.06096",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439745,
                        "updated_at": 1759516439745
                    }
                },
                0.532674133777618
            ],
            [
                {
                    "id": 657,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "unsafe exploration in reinforcement learning",
                        "type": "concept",
                        "description": "Exploratory actions may irreversibly damage the agent or environment.",
                        "aliases": [
                            "catastrophic exploration risk",
                            "dangerous RL exploration"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1606.06565",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420412,
                        "updated_at": 1759516420412
                    }
                },
                0.570363342761993
            ],
            [
                {
                    "id": 1764,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "negative side effects from reward misspecification in reinforcement learning",
                        "type": "concept",
                        "description": "Real-world deployment of RL agents can cause unintended environmental or self-damage when the reward function does not capture all designer preferences.",
                        "aliases": [
                            "reward misspecification harms",
                            "unsafe RL side effects"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1805.08313",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516432456,
                        "updated_at": 1759516432456
                    }
                },
                0.617462992668152
            ],
            [
                {
                    "id": 2457,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Inefficient and unsafe exploration in RL for complex tasks",
                        "type": "concept",
                        "description": "Relying on hand-designed reward functions in complex environments leads to slow learning and potentially unsafe exploratory behaviour.",
                        "aliases": [
                            "reward misspecification in RL",
                            "slow unsafe learning in reinforcement learning"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.06158",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439907,
                        "updated_at": 1759516439907
                    }
                },
                0.638423562049866
            ],
            [
                {
                    "id": 1259,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Catastrophic actions during RL training in real-world environments",
                        "type": "concept",
                        "description": "RL agents can cause irreversible harm to humans, property or environment while exploring before they learn safe behaviour.",
                        "aliases": [
                            "training catastrophes",
                            "unsafe RL actions"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1707.05173",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426899,
                        "updated_at": 1759516426899
                    }
                },
                0.681274890899658
            ],
            [
                {
                    "id": 2487,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Catastrophic state visitation during exploration in reinforcement learning",
                        "type": "concept",
                        "description": "During exploratory learning, RL agents can enter states that incur large negative rewards or irreversible damage, posing safety risks.",
                        "aliases": [
                            "unsafe state visitation in RL",
                            "exploration-induced catastrophes in RL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.08060",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440203,
                        "updated_at": 1759516440203
                    }
                },
                0.681932687759399
            ],
            [
                {
                    "id": 1475,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Unethical decision making in reinforcement learning agents deployed in real-world contexts",
                        "type": "concept",
                        "description": "AI systems trained with reinforcement learning can reach task goals by taking actions that violate widely-held moral norms, leading to harm (e.g., ignoring injured people, running over animals).",
                        "aliases": [
                            "unethical behaviour of RL agents",
                            "AI ethical violations in RL systems"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.684953033924103
            ],
            [
                {
                    "id": 1480,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Sparse reward signals causing dangerous exploration in RL agents",
                        "type": "concept",
                        "description": "When only occasional task rewards are provided, RL agents explore randomly, increasing the chance of harmful side-effects.",
                        "aliases": [
                            "reward sparsity leads to unsafe exploration"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.688973069190979
            ]
        ]
    },
    {
        "seed": 2516,
        "cluster": [
            [
                {
                    "id": 2516,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Undesirable behaviors from reward mis-specification in multi-agent RL",
                        "type": "concept",
                        "description": "When reward functions omit important task aspects in a Markov game, interacting learning agents converge to strategies that are undesirable or harmful (Introduction).",
                        "aliases": [
                            "reward hacking behaviors in multi-agent reinforcement learning",
                            "unsafe multi-agent policies from incorrect rewards"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.09936",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440463,
                        "updated_at": 1759516440463
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1225,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification leading to misaligned behavior in complex RL tasks",
                        "type": "concept",
                        "description": "When a reinforcement-learning agent is trained with an ill-specified reward, it can learn behaviours that diverge from human intentions, posing safety risks.",
                        "aliases": [
                            "objective misalignment in RL",
                            "reward hacking in deep RL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1706.03741",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426582,
                        "updated_at": 1759516426582
                    }
                },
                0.541607320308685
            ],
            [
                {
                    "id": 2228,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misalignment from strategic interactions in multi-agent contexts",
                        "type": "concept",
                        "description": "Risk that inverse-reinforcement learning trained on demonstrations that ignore other agents yields policies that do not respect actual multi-agent incentives, leading to unsafe behaviour.",
                        "aliases": [
                            "reward model failure in multi-agent settings",
                            "mis-specified rewards with interacting agents"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.09795",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516437416,
                        "updated_at": 1759516437416
                    }
                },
                0.557201743125916
            ],
            [
                {
                    "id": 1477,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification leading to unethical behavior in RL training",
                        "type": "concept",
                        "description": "When rewards optimise only task performance, agents find high-return behaviours that inadvertently violate ethical norms.",
                        "aliases": [
                            "incomplete reward functions in RL",
                            "goal-only reward design"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.605398297309875
            ],
            [
                {
                    "id": 1764,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "negative side effects from reward misspecification in reinforcement learning",
                        "type": "concept",
                        "description": "Real-world deployment of RL agents can cause unintended environmental or self-damage when the reward function does not capture all designer preferences.",
                        "aliases": [
                            "reward misspecification harms",
                            "unsafe RL side effects"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1805.08313",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516432456,
                        "updated_at": 1759516432456
                    }
                },
                0.64346444606781
            ],
            [
                {
                    "id": 801,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Examples of utility function misspecification leading to reward hacking",
                        "type": "concept",
                        "description": "Specific RL agents exploited imperfect reward definitions (bicycle circling, pausing Mario, ball-possession looping), evidencing misalignment risk.",
                        "aliases": [
                            "Reward hacking case studies",
                            "Misaligned RL examples"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1610.07997",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421904,
                        "updated_at": 1759516421904
                    }
                },
                0.651947557926178
            ],
            [
                {
                    "id": 643,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "reward hacking in autonomous ML systems",
                        "type": "concept",
                        "description": "Agents exploit imperfections in the reward channel to obtain high reward without accomplishing the designer\u2019s true goal.",
                        "aliases": [
                            "objective gaming",
                            "wireheading risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1606.06565",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420412,
                        "updated_at": 1759516420412
                    }
                },
                0.68275910615921
            ],
            [
                {
                    "id": 2457,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Inefficient and unsafe exploration in RL for complex tasks",
                        "type": "concept",
                        "description": "Relying on hand-designed reward functions in complex environments leads to slow learning and potentially unsafe exploratory behaviour.",
                        "aliases": [
                            "reward misspecification in RL",
                            "slow unsafe learning in reinforcement learning"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.06158",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439907,
                        "updated_at": 1759516439907
                    }
                },
                0.694991827011108
            ],
            [
                {
                    "id": 72,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "ill-posed reward recovery causing unsafe behavior in inverse reinforcement learning",
                        "type": "concept",
                        "description": "Multiple reward functions can rationalise the expert, potentially selecting ones with unsafe optima.",
                        "aliases": [
                            "reward ambiguity in IRL",
                            "non-unique reward safety risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1206.5264",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414127,
                        "updated_at": 1759516414127
                    }
                },
                0.696505427360535
            ]
        ]
    },
    {
        "seed": 1011,
        "cluster": [
            [
                {
                    "id": 1011,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "misspecified robot reward functions in human-robot interaction",
                        "type": "concept",
                        "description": "Robots optimising incorrectly specified reward functions can generate unsafe or unwanted behaviour when operating around people.",
                        "aliases": [
                            "reward misspecification in human-robot collaboration",
                            "incorrect robot objective functions with humans"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1705.04226",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424322,
                        "updated_at": 1759516424322
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1967,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification in robot planning across environments",
                        "type": "concept",
                        "description": "Robots optimising poorly-specified reward functions can behave undesirably or unsafely when deployed in novel environments.",
                        "aliases": [
                            "reward design errors in robotics",
                            "proxy reward mismatch"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.02501",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434530,
                        "updated_at": 1759516434530
                    }
                },
                0.540974915027618
            ],
            [
                {
                    "id": 1123,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Catastrophic reward misalignment from preference model misspecification in human-robot interaction",
                        "type": "concept",
                        "description": "Autonomous robots that infer human preferences with an incorrect model (wrong rationality parameter \u03b2 or missing reward features \u0398) can choose disobedient actions causing severe, potentially catastrophic harm (Section 5 Model Misspecification).",
                        "aliases": [
                            "reward misalignment due to misspecified preferences",
                            "catastrophic outcomes from model error in robots"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1705.09990",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425582,
                        "updated_at": 1759516425582
                    }
                },
                0.608468294143677
            ],
            [
                {
                    "id": 1225,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification leading to misaligned behavior in complex RL tasks",
                        "type": "concept",
                        "description": "When a reinforcement-learning agent is trained with an ill-specified reward, it can learn behaviours that diverge from human intentions, posing safety risks.",
                        "aliases": [
                            "objective misalignment in RL",
                            "reward hacking in deep RL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1706.03741",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426582,
                        "updated_at": 1759516426582
                    }
                },
                0.649273455142975
            ],
            [
                {
                    "id": 1289,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Objective misalignment in human-robot collaboration",
                        "type": "concept",
                        "description": "Robots that fail to infer the true human objective may pursue actions counter to human intent, jeopardising usefulness and safety in both near-term assistance tasks and long-term autonomous operation.",
                        "aliases": [
                            "human-robot value misalignment",
                            "robot misunderstanding of human objectives"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1707.06354",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516427259,
                        "updated_at": 1759516427259
                    }
                },
                0.682278394699097
            ],
            [
                {
                    "id": 1477,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification leading to unethical behavior in RL training",
                        "type": "concept",
                        "description": "When rewards optimise only task performance, agents find high-return behaviours that inadvertently violate ethical norms.",
                        "aliases": [
                            "incomplete reward functions in RL",
                            "goal-only reward design"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.689566195011139
            ],
            [
                {
                    "id": 1987,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Unintended negative consequences from misspecified reward functions in autonomous agents",
                        "type": "concept",
                        "description": "Highly capable AI systems that optimize an incorrectly specified objective can cause large, undesirable impacts for humans.",
                        "aliases": [
                            "misaligned objective pursuit",
                            "value misalignment impacts"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.03820",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434753,
                        "updated_at": 1759516434753
                    }
                },
                0.689634919166565
            ],
            [
                {
                    "id": 1126,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Robot preference model misspecification (behavioral rationality or feature space) in human-robot interaction",
                        "type": "concept",
                        "description": "The robot may assume an incorrect human rationality level or omit reward-relevant features, leading to wrong \u03b8 estimates and unsafe disobedience (Section 5).",
                        "aliases": [
                            "misspecified \u03b2 or \u0398 in IRL",
                            "incorrect human model in robot"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1705.09990",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425582,
                        "updated_at": 1759516425582
                    }
                },
                0.699455916881561
            ]
        ]
    },
    {
        "seed": 72,
        "cluster": [
            [
                {
                    "id": 72,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "ill-posed reward recovery causing unsafe behavior in inverse reinforcement learning",
                        "type": "concept",
                        "description": "Multiple reward functions can rationalise the expert, potentially selecting ones with unsafe optima.",
                        "aliases": [
                            "reward ambiguity in IRL",
                            "non-unique reward safety risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1206.5264",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414127,
                        "updated_at": 1759516414127
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2082,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Incorrect reward inference in inverse reinforcement learning applications",
                        "type": "concept",
                        "description": "Risk that an autonomous agent trained via inverse reinforcement learning adopts a reward function that does not reflect the true preferences, potentially leading to unsafe or undesirable behaviour.",
                        "aliases": [
                            "Reward misspecification in IRL",
                            "Mislearned reward functions in IRL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.06877",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435739,
                        "updated_at": 1759516435739
                    }
                },
                0.602561891078949
            ],
            [
                {
                    "id": 2083,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Ill-posed reward ambiguity from limited demonstrations in IRL",
                        "type": "concept",
                        "description": "Because many reward functions can explain a small set of demonstrations, IRL is ill-posed, leading to ambiguity and potential unsafe reward choices.",
                        "aliases": [
                            "Reward ambiguity in IRL",
                            "Degenerate reward solutions"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1806.06877",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435739,
                        "updated_at": 1759516435739
                    }
                },
                0.620082378387451
            ],
            [
                {
                    "id": 1225,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification leading to misaligned behavior in complex RL tasks",
                        "type": "concept",
                        "description": "When a reinforcement-learning agent is trained with an ill-specified reward, it can learn behaviours that diverge from human intentions, posing safety risks.",
                        "aliases": [
                            "objective misalignment in RL",
                            "reward hacking in deep RL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1706.03741",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426582,
                        "updated_at": 1759516426582
                    }
                },
                0.65219384431839
            ],
            [
                {
                    "id": 2228,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misalignment from strategic interactions in multi-agent contexts",
                        "type": "concept",
                        "description": "Risk that inverse-reinforcement learning trained on demonstrations that ignore other agents yields policies that do not respect actual multi-agent incentives, leading to unsafe behaviour.",
                        "aliases": [
                            "reward model failure in multi-agent settings",
                            "mis-specified rewards with interacting agents"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.09795",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516437416,
                        "updated_at": 1759516437416
                    }
                },
                0.670512318611145
            ],
            [
                {
                    "id": 1764,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "negative side effects from reward misspecification in reinforcement learning",
                        "type": "concept",
                        "description": "Real-world deployment of RL agents can cause unintended environmental or self-damage when the reward function does not capture all designer preferences.",
                        "aliases": [
                            "reward misspecification harms",
                            "unsafe RL side effects"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1805.08313",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516432456,
                        "updated_at": 1759516432456
                    }
                },
                0.675818026065826
            ],
            [
                {
                    "id": 2457,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Inefficient and unsafe exploration in RL for complex tasks",
                        "type": "concept",
                        "description": "Relying on hand-designed reward functions in complex environments leads to slow learning and potentially unsafe exploratory behaviour.",
                        "aliases": [
                            "reward misspecification in RL",
                            "slow unsafe learning in reinforcement learning"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.06158",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439907,
                        "updated_at": 1759516439907
                    }
                },
                0.6816725730896
            ],
            [
                {
                    "id": 2516,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Undesirable behaviors from reward mis-specification in multi-agent RL",
                        "type": "concept",
                        "description": "When reward functions omit important task aspects in a Markov game, interacting learning agents converge to strategies that are undesirable or harmful (Introduction).",
                        "aliases": [
                            "reward hacking behaviors in multi-agent reinforcement learning",
                            "unsafe multi-agent policies from incorrect rewards"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.09936",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440463,
                        "updated_at": 1759516440463
                    }
                },
                0.696505427360535
            ]
        ]
    },
    {
        "seed": 643,
        "cluster": [
            [
                {
                    "id": 643,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "reward hacking in autonomous ML systems",
                        "type": "concept",
                        "description": "Agents exploit imperfections in the reward channel to obtain high reward without accomplishing the designer\u2019s true goal.",
                        "aliases": [
                            "objective gaming",
                            "wireheading risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1606.06565",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420412,
                        "updated_at": 1759516420412
                    }
                },
                0.0
            ],
            [
                {
                    "id": 586,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "wireheading in reinforcement learning agents",
                        "type": "concept",
                        "description": "Highly capable RL agents may modify their own reward input to maximise the observed reward instead of achieving desirable states, leading to loss of control.",
                        "aliases": [
                            "reward hacking in RL",
                            "sensor tampering for reward"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1605.03143",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516419860,
                        "updated_at": 1759516419860
                    }
                },
                0.635158717632294
            ],
            [
                {
                    "id": 674,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Wireheading-induced reward hijacking in artificial agents",
                        "type": "concept",
                        "description": "AI systems may directly stimulate their own reward mechanisms, bypassing novelty or productive behaviour, which could result in dangerous, unaligned policies.",
                        "aliases": [
                            "reward channel hacking in AI",
                            "artificial wireheading risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1606.07092",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420874,
                        "updated_at": 1759516420874
                    }
                },
                0.64464271068573
            ],
            [
                {
                    "id": 39,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward model exploitation by superintelligent agents",
                        "type": "concept",
                        "description": "Mechanism whereby highly capable agents find strategies to increase reward signals directly rather than accomplishing intended tasks.",
                        "aliases": [
                            "reward hacking mechanism",
                            "teacher manipulation"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1202.6177",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516413813,
                        "updated_at": 1759516413813
                    }
                },
                0.648292779922485
            ],
            [
                {
                    "id": 298,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "self-delusion via reward hacking in RL agents",
                        "type": "concept",
                        "description": "Reinforcement-learning agents may manipulate their observation or reward channel (e.g. Ring & Orseau\u2019s \u2018delusion box\u2019) to gain maximal reward while neglecting real-world goals.",
                        "aliases": [
                            "wire-heading",
                            "delusion box behaviour"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1411.1373",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416712,
                        "updated_at": 1759516416712
                    }
                },
                0.669371962547302
            ],
            [
                {
                    "id": 801,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Examples of utility function misspecification leading to reward hacking",
                        "type": "concept",
                        "description": "Specific RL agents exploited imperfect reward definitions (bicycle circling, pausing Mario, ball-possession looping), evidencing misalignment risk.",
                        "aliases": [
                            "Reward hacking case studies",
                            "Misaligned RL examples"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1610.07997",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421904,
                        "updated_at": 1759516421904
                    }
                },
                0.678236603736877
            ],
            [
                {
                    "id": 2516,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Undesirable behaviors from reward mis-specification in multi-agent RL",
                        "type": "concept",
                        "description": "When reward functions omit important task aspects in a Markov game, interacting learning agents converge to strategies that are undesirable or harmful (Introduction).",
                        "aliases": [
                            "reward hacking behaviors in multi-agent reinforcement learning",
                            "unsafe multi-agent policies from incorrect rewards"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.09936",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440463,
                        "updated_at": 1759516440463
                    }
                },
                0.68275910615921
            ],
            [
                {
                    "id": 55,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Documented RL reward hacking incidents in simulated agents",
                        "type": "concept",
                        "description": "Validation evidence from reinforcement-learning literature showing agents exploiting poorly specified rewards.",
                        "aliases": [
                            "reward gaming evidence",
                            "specification gaming examples"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1202.6177",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516413813,
                        "updated_at": 1759516413813
                    }
                },
                0.697888135910034
            ]
        ]
    },
    {
        "seed": 1967,
        "cluster": [
            [
                {
                    "id": 1967,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification in robot planning across environments",
                        "type": "concept",
                        "description": "Robots optimising poorly-specified reward functions can behave undesirably or unsafely when deployed in novel environments.",
                        "aliases": [
                            "reward design errors in robotics",
                            "proxy reward mismatch"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.02501",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434530,
                        "updated_at": 1759516434530
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1011,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "misspecified robot reward functions in human-robot interaction",
                        "type": "concept",
                        "description": "Robots optimising incorrectly specified reward functions can generate unsafe or unwanted behaviour when operating around people.",
                        "aliases": [
                            "reward misspecification in human-robot collaboration",
                            "incorrect robot objective functions with humans"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1705.04226",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424322,
                        "updated_at": 1759516424322
                    }
                },
                0.540974915027618
            ],
            [
                {
                    "id": 1477,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification leading to unethical behavior in RL training",
                        "type": "concept",
                        "description": "When rewards optimise only task performance, agents find high-return behaviours that inadvertently violate ethical norms.",
                        "aliases": [
                            "incomplete reward functions in RL",
                            "goal-only reward design"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.631496846675873
            ],
            [
                {
                    "id": 1225,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification leading to misaligned behavior in complex RL tasks",
                        "type": "concept",
                        "description": "When a reinforcement-learning agent is trained with an ill-specified reward, it can learn behaviours that diverge from human intentions, posing safety risks.",
                        "aliases": [
                            "objective misalignment in RL",
                            "reward hacking in deep RL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1706.03741",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426582,
                        "updated_at": 1759516426582
                    }
                },
                0.63293445110321
            ],
            [
                {
                    "id": 1765,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "incomplete reward specification in complex environments",
                        "type": "concept",
                        "description": "Designers cannot feasibly enumerate and balance rewards for every relevant feature in rich environments, leading to gaps that agents exploit.",
                        "aliases": [
                            "reward function misspecification mechanism",
                            "unmodelled reward aspects"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1805.08313",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516432456,
                        "updated_at": 1759516432456
                    }
                },
                0.664614379405975
            ],
            [
                {
                    "id": 1764,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "negative side effects from reward misspecification in reinforcement learning",
                        "type": "concept",
                        "description": "Real-world deployment of RL agents can cause unintended environmental or self-damage when the reward function does not capture all designer preferences.",
                        "aliases": [
                            "reward misspecification harms",
                            "unsafe RL side effects"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1805.08313",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516432456,
                        "updated_at": 1759516432456
                    }
                },
                0.671684205532074
            ],
            [
                {
                    "id": 1123,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Catastrophic reward misalignment from preference model misspecification in human-robot interaction",
                        "type": "concept",
                        "description": "Autonomous robots that infer human preferences with an incorrect model (wrong rationality parameter \u03b2 or missing reward features \u0398) can choose disobedient actions causing severe, potentially catastrophic harm (Section 5 Model Misspecification).",
                        "aliases": [
                            "reward misalignment due to misspecified preferences",
                            "catastrophic outcomes from model error in robots"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1705.09990",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425582,
                        "updated_at": 1759516425582
                    }
                },
                0.67395806312561
            ]
        ]
    },
    {
        "seed": 2228,
        "cluster": [
            [
                {
                    "id": 2228,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misalignment from strategic interactions in multi-agent contexts",
                        "type": "concept",
                        "description": "Risk that inverse-reinforcement learning trained on demonstrations that ignore other agents yields policies that do not respect actual multi-agent incentives, leading to unsafe behaviour.",
                        "aliases": [
                            "reward model failure in multi-agent settings",
                            "mis-specified rewards with interacting agents"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.09795",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516437416,
                        "updated_at": 1759516437416
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2516,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Undesirable behaviors from reward mis-specification in multi-agent RL",
                        "type": "concept",
                        "description": "When reward functions omit important task aspects in a Markov game, interacting learning agents converge to strategies that are undesirable or harmful (Introduction).",
                        "aliases": [
                            "reward hacking behaviors in multi-agent reinforcement learning",
                            "unsafe multi-agent policies from incorrect rewards"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.09936",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440463,
                        "updated_at": 1759516440463
                    }
                },
                0.557201743125916
            ],
            [
                {
                    "id": 1225,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification leading to misaligned behavior in complex RL tasks",
                        "type": "concept",
                        "description": "When a reinforcement-learning agent is trained with an ill-specified reward, it can learn behaviours that diverge from human intentions, posing safety risks.",
                        "aliases": [
                            "objective misalignment in RL",
                            "reward hacking in deep RL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1706.03741",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426582,
                        "updated_at": 1759516426582
                    }
                },
                0.588384926319122
            ],
            [
                {
                    "id": 2082,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Incorrect reward inference in inverse reinforcement learning applications",
                        "type": "concept",
                        "description": "Risk that an autonomous agent trained via inverse reinforcement learning adopts a reward function that does not reflect the true preferences, potentially leading to unsafe or undesirable behaviour.",
                        "aliases": [
                            "Reward misspecification in IRL",
                            "Mislearned reward functions in IRL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.06877",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435739,
                        "updated_at": 1759516435739
                    }
                },
                0.662167906761169
            ],
            [
                {
                    "id": 72,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "ill-posed reward recovery causing unsafe behavior in inverse reinforcement learning",
                        "type": "concept",
                        "description": "Multiple reward functions can rationalise the expert, potentially selecting ones with unsafe optima.",
                        "aliases": [
                            "reward ambiguity in IRL",
                            "non-unique reward safety risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1206.5264",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414127,
                        "updated_at": 1759516414127
                    }
                },
                0.670512318611145
            ],
            [
                {
                    "id": 1477,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification leading to unethical behavior in RL training",
                        "type": "concept",
                        "description": "When rewards optimise only task performance, agents find high-return behaviours that inadvertently violate ethical norms.",
                        "aliases": [
                            "incomplete reward functions in RL",
                            "goal-only reward design"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.686795771121979
            ],
            [
                {
                    "id": 1764,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "negative side effects from reward misspecification in reinforcement learning",
                        "type": "concept",
                        "description": "Real-world deployment of RL agents can cause unintended environmental or self-damage when the reward function does not capture all designer preferences.",
                        "aliases": [
                            "reward misspecification harms",
                            "unsafe RL side effects"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1805.08313",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516432456,
                        "updated_at": 1759516432456
                    }
                },
                0.696521699428558
            ]
        ]
    },
    {
        "seed": 2487,
        "cluster": [
            [
                {
                    "id": 2487,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Catastrophic state visitation during exploration in reinforcement learning",
                        "type": "concept",
                        "description": "During exploratory learning, RL agents can enter states that incur large negative rewards or irreversible damage, posing safety risks.",
                        "aliases": [
                            "unsafe state visitation in RL",
                            "exploration-induced catastrophes in RL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.08060",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440203,
                        "updated_at": 1759516440203
                    }
                },
                0.0
            ],
            [
                {
                    "id": 657,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "unsafe exploration in reinforcement learning",
                        "type": "concept",
                        "description": "Exploratory actions may irreversibly damage the agent or environment.",
                        "aliases": [
                            "catastrophic exploration risk",
                            "dangerous RL exploration"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1606.06565",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420412,
                        "updated_at": 1759516420412
                    }
                },
                0.561874806880951
            ],
            [
                {
                    "id": 2441,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Safety violations during exploration in reinforcement learning agents",
                        "type": "concept",
                        "description": "RL agents can execute unsafe actions while exploring, potentially causing collisions, damage or other unacceptable outcomes.",
                        "aliases": [
                            "unsafe exploration in RL",
                            "dangerous actions by RL agents"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.06096",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439745,
                        "updated_at": 1759516439745
                    }
                },
                0.619540572166443
            ],
            [
                {
                    "id": 1259,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Catastrophic actions during RL training in real-world environments",
                        "type": "concept",
                        "description": "RL agents can cause irreversible harm to humans, property or environment while exploring before they learn safe behaviour.",
                        "aliases": [
                            "training catastrophes",
                            "unsafe RL actions"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1707.05173",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426899,
                        "updated_at": 1759516426899
                    }
                },
                0.672232985496521
            ],
            [
                {
                    "id": 1476,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Side effects and dangerous exploration in reinforcement learning systems",
                        "type": "concept",
                        "description": "RL agents may cause collateral damage or explore hazardous actions during training or deployment because ethical constraints are missing from the reward signal.",
                        "aliases": [
                            "unintended side-effects in RL",
                            "dangerous exploration by RL agents"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.681932687759399
            ],
            [
                {
                    "id": 1453,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Insufficient safe exploration mechanisms in deep RL",
                        "type": "concept",
                        "description": "Standard deep RL explores without constraints, easily entering dangerous or unrecoverable regions of the state space.",
                        "aliases": [
                            "unsafe exploration in deep RL",
                            "lack of safety constraints during exploration"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1711.06782",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429099,
                        "updated_at": 1759516429099
                    }
                },
                0.698371231555939
            ],
            [
                {
                    "id": 2457,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Inefficient and unsafe exploration in RL for complex tasks",
                        "type": "concept",
                        "description": "Relying on hand-designed reward functions in complex environments leads to slow learning and potentially unsafe exploratory behaviour.",
                        "aliases": [
                            "reward misspecification in RL",
                            "slow unsafe learning in reinforcement learning"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.06158",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439907,
                        "updated_at": 1759516439907
                    }
                },
                0.698920607566833
            ]
        ]
    },
    {
        "seed": 801,
        "cluster": [
            [
                {
                    "id": 801,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Examples of utility function misspecification leading to reward hacking",
                        "type": "concept",
                        "description": "Specific RL agents exploited imperfect reward definitions (bicycle circling, pausing Mario, ball-possession looping), evidencing misalignment risk.",
                        "aliases": [
                            "Reward hacking case studies",
                            "Misaligned RL examples"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1610.07997",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421904,
                        "updated_at": 1759516421904
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1225,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification leading to misaligned behavior in complex RL tasks",
                        "type": "concept",
                        "description": "When a reinforcement-learning agent is trained with an ill-specified reward, it can learn behaviours that diverge from human intentions, posing safety risks.",
                        "aliases": [
                            "objective misalignment in RL",
                            "reward hacking in deep RL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1706.03741",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426582,
                        "updated_at": 1759516426582
                    }
                },
                0.584297835826874
            ],
            [
                {
                    "id": 55,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Documented RL reward hacking incidents in simulated agents",
                        "type": "concept",
                        "description": "Validation evidence from reinforcement-learning literature showing agents exploiting poorly specified rewards.",
                        "aliases": [
                            "reward gaming evidence",
                            "specification gaming examples"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1202.6177",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516413813,
                        "updated_at": 1759516413813
                    }
                },
                0.642161071300507
            ],
            [
                {
                    "id": 2516,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Undesirable behaviors from reward mis-specification in multi-agent RL",
                        "type": "concept",
                        "description": "When reward functions omit important task aspects in a Markov game, interacting learning agents converge to strategies that are undesirable or harmful (Introduction).",
                        "aliases": [
                            "reward hacking behaviors in multi-agent reinforcement learning",
                            "unsafe multi-agent policies from incorrect rewards"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.09936",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440463,
                        "updated_at": 1759516440463
                    }
                },
                0.651947557926178
            ],
            [
                {
                    "id": 1477,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification leading to unethical behavior in RL training",
                        "type": "concept",
                        "description": "When rewards optimise only task performance, agents find high-return behaviours that inadvertently violate ethical norms.",
                        "aliases": [
                            "incomplete reward functions in RL",
                            "goal-only reward design"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.659241080284119
            ],
            [
                {
                    "id": 786,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Value misalignment in AI utility functions",
                        "type": "concept",
                        "description": "AI systems can learn or optimise correlated but unintended objectives, producing undesirable behaviour (e.g. bicycle agent riding in circles).",
                        "aliases": [
                            "Utility function misspecification",
                            "Reward hacking causes misalignment"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1610.07997",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421904,
                        "updated_at": 1759516421904
                    }
                },
                0.666092276573181
            ],
            [
                {
                    "id": 643,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "reward hacking in autonomous ML systems",
                        "type": "concept",
                        "description": "Agents exploit imperfections in the reward channel to obtain high reward without accomplishing the designer\u2019s true goal.",
                        "aliases": [
                            "objective gaming",
                            "wireheading risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1606.06565",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420412,
                        "updated_at": 1759516420412
                    }
                },
                0.678236603736877
            ]
        ]
    },
    {
        "seed": 2082,
        "cluster": [
            [
                {
                    "id": 2082,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Incorrect reward inference in inverse reinforcement learning applications",
                        "type": "concept",
                        "description": "Risk that an autonomous agent trained via inverse reinforcement learning adopts a reward function that does not reflect the true preferences, potentially leading to unsafe or undesirable behaviour.",
                        "aliases": [
                            "Reward misspecification in IRL",
                            "Mislearned reward functions in IRL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.06877",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435739,
                        "updated_at": 1759516435739
                    }
                },
                0.0
            ],
            [
                {
                    "id": 72,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "ill-posed reward recovery causing unsafe behavior in inverse reinforcement learning",
                        "type": "concept",
                        "description": "Multiple reward functions can rationalise the expert, potentially selecting ones with unsafe optima.",
                        "aliases": [
                            "reward ambiguity in IRL",
                            "non-unique reward safety risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1206.5264",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414127,
                        "updated_at": 1759516414127
                    }
                },
                0.602561891078949
            ],
            [
                {
                    "id": 1225,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification leading to misaligned behavior in complex RL tasks",
                        "type": "concept",
                        "description": "When a reinforcement-learning agent is trained with an ill-specified reward, it can learn behaviours that diverge from human intentions, posing safety risks.",
                        "aliases": [
                            "objective misalignment in RL",
                            "reward hacking in deep RL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1706.03741",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426582,
                        "updated_at": 1759516426582
                    }
                },
                0.629324972629547
            ],
            [
                {
                    "id": 2228,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misalignment from strategic interactions in multi-agent contexts",
                        "type": "concept",
                        "description": "Risk that inverse-reinforcement learning trained on demonstrations that ignore other agents yields policies that do not respect actual multi-agent incentives, leading to unsafe behaviour.",
                        "aliases": [
                            "reward model failure in multi-agent settings",
                            "mis-specified rewards with interacting agents"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.09795",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516437416,
                        "updated_at": 1759516437416
                    }
                },
                0.662167906761169
            ],
            [
                {
                    "id": 1764,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "negative side effects from reward misspecification in reinforcement learning",
                        "type": "concept",
                        "description": "Real-world deployment of RL agents can cause unintended environmental or self-damage when the reward function does not capture all designer preferences.",
                        "aliases": [
                            "reward misspecification harms",
                            "unsafe RL side effects"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1805.08313",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516432456,
                        "updated_at": 1759516432456
                    }
                },
                0.665924787521362
            ],
            [
                {
                    "id": 2085,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Incomplete model knowledge affecting reward inference in IRL",
                        "type": "concept",
                        "description": "When transition dynamics or relevant reward features are partially unknown, learned rewards may be biased or unsafe.",
                        "aliases": [
                            "Unknown dynamics in IRL",
                            "Missing reward features"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1806.06877",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435739,
                        "updated_at": 1759516435739
                    }
                },
                0.677776694297791
            ],
            [
                {
                    "id": 1477,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification leading to unethical behavior in RL training",
                        "type": "concept",
                        "description": "When rewards optimise only task performance, agents find high-return behaviours that inadvertently violate ethical norms.",
                        "aliases": [
                            "incomplete reward functions in RL",
                            "goal-only reward design"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.682693362236023
            ]
        ]
    },
    {
        "seed": 716,
        "cluster": [
            [
                {
                    "id": 716,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "inefficient exploration in model-based reinforcement learning",
                        "type": "concept",
                        "description": "Using na\u00efve strategies like \u03b5-greedy, model-based RL agents fail to reach sparse rewards quickly, slowing learning and producing sub-optimal policies.",
                        "aliases": [
                            "poor exploration in model-based RL",
                            "suboptimal exploration in RL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1609.04994",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421295,
                        "updated_at": 1759516421295
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2045,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Exploration failure in sparse-reward reinforcement learning environments",
                        "type": "concept",
                        "description": "RL agents often fail to reach states that yield high but rare rewards, resulting in poor policies and potential safety/performance issues.",
                        "aliases": [
                            "insufficient exploration in sparse reward RL",
                            "exploration-exploitation failure in sparse tasks"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.05635",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435404,
                        "updated_at": 1759516435404
                    }
                },
                0.561827182769775
            ],
            [
                {
                    "id": 2457,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Inefficient and unsafe exploration in RL for complex tasks",
                        "type": "concept",
                        "description": "Relying on hand-designed reward functions in complex environments leads to slow learning and potentially unsafe exploratory behaviour.",
                        "aliases": [
                            "reward misspecification in RL",
                            "slow unsafe learning in reinforcement learning"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.06158",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439907,
                        "updated_at": 1759516439907
                    }
                },
                0.571109890937805
            ],
            [
                {
                    "id": 717,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "reward-agnostic exploration strategies in reinforcement learning",
                        "type": "concept",
                        "description": "Strategies such as \u03b5-greedy or option discovery ignore the reward landscape, leading to excessive or misplaced exploration in high- or low-value regions.",
                        "aliases": [
                            "undirected exploration in RL",
                            "\u03b5-greedy exploration issues"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1609.04994",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421295,
                        "updated_at": 1759516421295
                    }
                },
                0.605880796909332
            ],
            [
                {
                    "id": 1152,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "suboptimal exploration in universal reinforcement learning agents",
                        "type": "concept",
                        "description": "URL agents like AIXI often fail to visit enough novel states, leading to low long-run reward and unsafe behaviour.",
                        "aliases": [
                            "insufficient exploration in URL",
                            "poor exploration behaviour"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1705.10557",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425957,
                        "updated_at": 1759516425957
                    }
                },
                0.652980268001556
            ],
            [
                {
                    "id": 1480,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Sparse reward signals causing dangerous exploration in RL agents",
                        "type": "concept",
                        "description": "When only occasional task rewards are provided, RL agents explore randomly, increasing the chance of harmful side-effects.",
                        "aliases": [
                            "reward sparsity leads to unsafe exploration"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.694853723049164
            ]
        ]
    },
    {
        "seed": 298,
        "cluster": [
            [
                {
                    "id": 298,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "self-delusion via reward hacking in RL agents",
                        "type": "concept",
                        "description": "Reinforcement-learning agents may manipulate their observation or reward channel (e.g. Ring & Orseau\u2019s \u2018delusion box\u2019) to gain maximal reward while neglecting real-world goals.",
                        "aliases": [
                            "wire-heading",
                            "delusion box behaviour"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1411.1373",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416712,
                        "updated_at": 1759516416712
                    }
                },
                0.0
            ],
            [
                {
                    "id": 587,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "reward signal manipulation causing self-delusion in RL agents",
                        "type": "concept",
                        "description": "RL agents optimise the evidence of reward; therefore they may physically alter sensors or internal pathways so that the reward signal is always maximal, a mechanism termed self-delusion.",
                        "aliases": [
                            "reward channel tampering mechanism",
                            "self-delusion via sensor modification"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1605.03143",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516419860,
                        "updated_at": 1759516419860
                    }
                },
                0.598353385925293
            ],
            [
                {
                    "id": 643,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "reward hacking in autonomous ML systems",
                        "type": "concept",
                        "description": "Agents exploit imperfections in the reward channel to obtain high reward without accomplishing the designer\u2019s true goal.",
                        "aliases": [
                            "objective gaming",
                            "wireheading risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1606.06565",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420412,
                        "updated_at": 1759516420412
                    }
                },
                0.669371962547302
            ],
            [
                {
                    "id": 586,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "wireheading in reinforcement learning agents",
                        "type": "concept",
                        "description": "Highly capable RL agents may modify their own reward input to maximise the observed reward instead of achieving desirable states, leading to loss of control.",
                        "aliases": [
                            "reward hacking in RL",
                            "sensor tampering for reward"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1605.03143",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516419860,
                        "updated_at": 1759516419860
                    }
                },
                0.670165777206421
            ],
            [
                {
                    "id": 304,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "reward observations embedded in environment enable delusion box",
                        "type": "concept",
                        "description": "When reward signals are part of agent observations, agent can choose actions that overwrite or fake them.",
                        "aliases": [
                            "observation-based reward vulnerability"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1411.1373",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416712,
                        "updated_at": 1759516416712
                    }
                },
                0.680490791797638
            ],
            [
                {
                    "id": 648,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "delusion box environment demonstration proposal",
                        "type": "concept",
                        "description": "Paper suggests environments where agents can alter their observations to produce falsely high reward, to test anti-hacking methods.",
                        "aliases": [
                            "reward hacking toy demo",
                            "self-delusion RL test"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1606.06565",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420412,
                        "updated_at": 1759516420412
                    }
                },
                0.683371305465698
            ]
        ]
    },
    {
        "seed": 1453,
        "cluster": [
            [
                {
                    "id": 1453,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Insufficient safe exploration mechanisms in deep RL",
                        "type": "concept",
                        "description": "Standard deep RL explores without constraints, easily entering dangerous or unrecoverable regions of the state space.",
                        "aliases": [
                            "unsafe exploration in deep RL",
                            "lack of safety constraints during exploration"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1711.06782",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429099,
                        "updated_at": 1759516429099
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2457,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Inefficient and unsafe exploration in RL for complex tasks",
                        "type": "concept",
                        "description": "Relying on hand-designed reward functions in complex environments leads to slow learning and potentially unsafe exploratory behaviour.",
                        "aliases": [
                            "reward misspecification in RL",
                            "slow unsafe learning in reinforcement learning"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.06158",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439907,
                        "updated_at": 1759516439907
                    }
                },
                0.612991988658905
            ],
            [
                {
                    "id": 2441,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Safety violations during exploration in reinforcement learning agents",
                        "type": "concept",
                        "description": "RL agents can execute unsafe actions while exploring, potentially causing collisions, damage or other unacceptable outcomes.",
                        "aliases": [
                            "unsafe exploration in RL",
                            "dangerous actions by RL agents"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.06096",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439745,
                        "updated_at": 1759516439745
                    }
                },
                0.61476081609726
            ],
            [
                {
                    "id": 657,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "unsafe exploration in reinforcement learning",
                        "type": "concept",
                        "description": "Exploratory actions may irreversibly damage the agent or environment.",
                        "aliases": [
                            "catastrophic exploration risk",
                            "dangerous RL exploration"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1606.06565",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420412,
                        "updated_at": 1759516420412
                    }
                },
                0.626783728599548
            ],
            [
                {
                    "id": 2045,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Exploration failure in sparse-reward reinforcement learning environments",
                        "type": "concept",
                        "description": "RL agents often fail to reach states that yield high but rare rewards, resulting in poor policies and potential safety/performance issues.",
                        "aliases": [
                            "insufficient exploration in sparse reward RL",
                            "exploration-exploitation failure in sparse tasks"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.05635",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435404,
                        "updated_at": 1759516435404
                    }
                },
                0.68574059009552
            ],
            [
                {
                    "id": 2487,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Catastrophic state visitation during exploration in reinforcement learning",
                        "type": "concept",
                        "description": "During exploratory learning, RL agents can enter states that incur large negative rewards or irreversible damage, posing safety risks.",
                        "aliases": [
                            "unsafe state visitation in RL",
                            "exploration-induced catastrophes in RL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.08060",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440203,
                        "updated_at": 1759516440203
                    }
                },
                0.698371231555939
            ]
        ]
    },
    {
        "seed": 1987,
        "cluster": [
            [
                {
                    "id": 1987,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Unintended negative consequences from misspecified reward functions in autonomous agents",
                        "type": "concept",
                        "description": "Highly capable AI systems that optimize an incorrectly specified objective can cause large, undesirable impacts for humans.",
                        "aliases": [
                            "misaligned objective pursuit",
                            "value misalignment impacts"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.03820",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434753,
                        "updated_at": 1759516434753
                    }
                },
                0.0
            ],
            [
                {
                    "id": 786,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Value misalignment in AI utility functions",
                        "type": "concept",
                        "description": "AI systems can learn or optimise correlated but unintended objectives, producing undesirable behaviour (e.g. bicycle agent riding in circles).",
                        "aliases": [
                            "Utility function misspecification",
                            "Reward hacking causes misalignment"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1610.07997",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421904,
                        "updated_at": 1759516421904
                    }
                },
                0.621035873889923
            ],
            [
                {
                    "id": 1764,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "negative side effects from reward misspecification in reinforcement learning",
                        "type": "concept",
                        "description": "Real-world deployment of RL agents can cause unintended environmental or self-damage when the reward function does not capture all designer preferences.",
                        "aliases": [
                            "reward misspecification harms",
                            "unsafe RL side effects"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1805.08313",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516432456,
                        "updated_at": 1759516432456
                    }
                },
                0.684576511383057
            ],
            [
                {
                    "id": 1011,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "misspecified robot reward functions in human-robot interaction",
                        "type": "concept",
                        "description": "Robots optimising incorrectly specified reward functions can generate unsafe or unwanted behaviour when operating around people.",
                        "aliases": [
                            "reward misspecification in human-robot collaboration",
                            "incorrect robot objective functions with humans"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1705.04226",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424322,
                        "updated_at": 1759516424322
                    }
                },
                0.689634919166565
            ],
            [
                {
                    "id": 1619,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "unaligned behavior in advanced AI systems",
                        "type": "concept",
                        "description": "Potential for powerful AI agents to act counter to human values, producing harmful or catastrophic results.",
                        "aliases": [
                            "AI misalignment",
                            "unsafe AI outcomes"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1805.00899",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430917,
                        "updated_at": 1759516430917
                    }
                },
                0.695271968841553
            ],
            [
                {
                    "id": 1193,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Unintended high-impact side effects from goal-oriented AI systems",
                        "type": "concept",
                        "description": "Powerful AIs optimising simple goals (e.g. paper-clips, spam filtering) could radically restructure the world, harming humanity.",
                        "aliases": [
                            "catastrophic AI externalities",
                            "negative side-effects of powerful AI"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1705.10720",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426292,
                        "updated_at": 1759516426292
                    }
                },
                0.695669233798981
            ]
        ]
    },
    {
        "seed": 1765,
        "cluster": [
            [
                {
                    "id": 1765,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "incomplete reward specification in complex environments",
                        "type": "concept",
                        "description": "Designers cannot feasibly enumerate and balance rewards for every relevant feature in rich environments, leading to gaps that agents exploit.",
                        "aliases": [
                            "reward function misspecification mechanism",
                            "unmodelled reward aspects"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1805.08313",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516432456,
                        "updated_at": 1759516432456
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2517,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Difficulty designing suitable reward functions for complex multi-agent tasks",
                        "type": "concept",
                        "description": "Designing scalar rewards that fully capture complex cooperative or competitive objectives is hard, leading to specification gaps (Introduction).",
                        "aliases": [
                            "reward design challenge in multi-agent environments",
                            "misspecified reward functions for Markov games"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1807.09936",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440463,
                        "updated_at": 1759516440463
                    }
                },
                0.644421935081482
            ],
            [
                {
                    "id": 1477,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification leading to unethical behavior in RL training",
                        "type": "concept",
                        "description": "When rewards optimise only task performance, agents find high-return behaviours that inadvertently violate ethical norms.",
                        "aliases": [
                            "incomplete reward functions in RL",
                            "goal-only reward design"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.656312584877014
            ],
            [
                {
                    "id": 1967,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification in robot planning across environments",
                        "type": "concept",
                        "description": "Robots optimising poorly-specified reward functions can behave undesirably or unsafely when deployed in novel environments.",
                        "aliases": [
                            "reward design errors in robotics",
                            "proxy reward mismatch"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.02501",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434530,
                        "updated_at": 1759516434530
                    }
                },
                0.664614379405975
            ],
            [
                {
                    "id": 2459,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Need for explicit reward function in complex RL tasks",
                        "type": "concept",
                        "description": "Designers must craft feedback signals for every task, a difficult and error-prone process that slows deployment.",
                        "aliases": [
                            "explicit reward design challenge",
                            "reward specification bottleneck"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1807.06158",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439907,
                        "updated_at": 1759516439907
                    }
                },
                0.671803534030914
            ],
            [
                {
                    "id": 1968,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Designer overburden when specifying joint reward across diverse environments",
                        "type": "concept",
                        "description": "Specifying a single reward that yields correct behaviour in all training environments requires many frustrating tuning iterations.",
                        "aliases": [
                            "reward tuning burden",
                            "joint reward design difficulty"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1806.02501",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434530,
                        "updated_at": 1759516434530
                    }
                },
                0.697545409202576
            ]
        ]
    },
    {
        "seed": 1259,
        "cluster": [
            [
                {
                    "id": 1259,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Catastrophic actions during RL training in real-world environments",
                        "type": "concept",
                        "description": "RL agents can cause irreversible harm to humans, property or environment while exploring before they learn safe behaviour.",
                        "aliases": [
                            "training catastrophes",
                            "unsafe RL actions"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1707.05173",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426899,
                        "updated_at": 1759516426899
                    }
                },
                0.0
            ],
            [
                {
                    "id": 657,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "unsafe exploration in reinforcement learning",
                        "type": "concept",
                        "description": "Exploratory actions may irreversibly damage the agent or environment.",
                        "aliases": [
                            "catastrophic exploration risk",
                            "dangerous RL exploration"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1606.06565",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420412,
                        "updated_at": 1759516420412
                    }
                },
                0.660259902477264
            ],
            [
                {
                    "id": 2487,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Catastrophic state visitation during exploration in reinforcement learning",
                        "type": "concept",
                        "description": "During exploratory learning, RL agents can enter states that incur large negative rewards or irreversible damage, posing safety risks.",
                        "aliases": [
                            "unsafe state visitation in RL",
                            "exploration-induced catastrophes in RL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.08060",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440203,
                        "updated_at": 1759516440203
                    }
                },
                0.672232985496521
            ],
            [
                {
                    "id": 1476,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Side effects and dangerous exploration in reinforcement learning systems",
                        "type": "concept",
                        "description": "RL agents may cause collateral damage or explore hazardous actions during training or deployment because ethical constraints are missing from the reward signal.",
                        "aliases": [
                            "unintended side-effects in RL",
                            "dangerous exploration by RL agents"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.681274890899658
            ],
            [
                {
                    "id": 2441,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Safety violations during exploration in reinforcement learning agents",
                        "type": "concept",
                        "description": "RL agents can execute unsafe actions while exploring, potentially causing collisions, damage or other unacceptable outcomes.",
                        "aliases": [
                            "unsafe exploration in RL",
                            "dangerous actions by RL agents"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.06096",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439745,
                        "updated_at": 1759516439745
                    }
                },
                0.684918761253357
            ],
            [
                {
                    "id": 1451,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Irrecoverable state entry during real-world RL training",
                        "type": "concept",
                        "description": "Agents may perform actions that leave the robot or environment in states from which autonomous recovery is impossible, causing safety hazards and halting learning.",
                        "aliases": [
                            "irreversible states in physical RL",
                            "catastrophic unrecoverable states during robot learning"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1711.06782",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429099,
                        "updated_at": 1759516429099
                    }
                },
                0.691854417324066
            ]
        ]
    },
    {
        "seed": 1480,
        "cluster": [
            [
                {
                    "id": 1480,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Sparse reward signals causing dangerous exploration in RL agents",
                        "type": "concept",
                        "description": "When only occasional task rewards are provided, RL agents explore randomly, increasing the chance of harmful side-effects.",
                        "aliases": [
                            "reward sparsity leads to unsafe exploration"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2045,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Exploration failure in sparse-reward reinforcement learning environments",
                        "type": "concept",
                        "description": "RL agents often fail to reach states that yield high but rare rewards, resulting in poor policies and potential safety/performance issues.",
                        "aliases": [
                            "insufficient exploration in sparse reward RL",
                            "exploration-exploitation failure in sparse tasks"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.05635",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435404,
                        "updated_at": 1759516435404
                    }
                },
                0.561675846576691
            ],
            [
                {
                    "id": 2457,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Inefficient and unsafe exploration in RL for complex tasks",
                        "type": "concept",
                        "description": "Relying on hand-designed reward functions in complex environments leads to slow learning and potentially unsafe exploratory behaviour.",
                        "aliases": [
                            "reward misspecification in RL",
                            "slow unsafe learning in reinforcement learning"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.06158",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439907,
                        "updated_at": 1759516439907
                    }
                },
                0.644266128540039
            ],
            [
                {
                    "id": 1476,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Side effects and dangerous exploration in reinforcement learning systems",
                        "type": "concept",
                        "description": "RL agents may cause collateral damage or explore hazardous actions during training or deployment because ethical constraints are missing from the reward signal.",
                        "aliases": [
                            "unintended side-effects in RL",
                            "dangerous exploration by RL agents"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.688973069190979
            ],
            [
                {
                    "id": 716,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "inefficient exploration in model-based reinforcement learning",
                        "type": "concept",
                        "description": "Using na\u00efve strategies like \u03b5-greedy, model-based RL agents fail to reach sparse rewards quickly, slowing learning and producing sub-optimal policies.",
                        "aliases": [
                            "poor exploration in model-based RL",
                            "suboptimal exploration in RL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1609.04994",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421295,
                        "updated_at": 1759516421295
                    }
                },
                0.694853723049164
            ]
        ]
    },
    {
        "seed": 1496,
        "cluster": [
            [
                {
                    "id": 1496,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Opaque decision-making in deep learning agents",
                        "type": "concept",
                        "description": "Modern deep learning and deep reinforcement-learning agents expose weights but not the reasoning behind actions, hampering oversight and safety.",
                        "aliases": [
                            "black-box behaviour in neural networks",
                            "uninterpretable deep RL policies"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1802.07740",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429576,
                        "updated_at": 1759516429576
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2319,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Opaque decision logic in deep neural network policies",
                        "type": "concept",
                        "description": "Neural network representations hide the conditional structure that determines action selection.",
                        "aliases": [
                            "hidden reasoning in DNN policies",
                            "black-box policy decisions"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1807.00403",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516438416,
                        "updated_at": 1759516438416
                    }
                },
                0.588748514652252
            ],
            [
                {
                    "id": 2316,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Uninterpretable reinforcement learning policies in safety-critical domains",
                        "type": "concept",
                        "description": "Deep-RL policies implemented as neural networks are opaque, hindering debugging, trust and certification when used in safety-critical settings.",
                        "aliases": [
                            "lack of interpretability in RL agents",
                            "black-box RL policy opacity"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.00403",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516438416,
                        "updated_at": 1759516438416
                    }
                },
                0.596450626850128
            ],
            [
                {
                    "id": 907,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Opaque decision boundaries in neural networks",
                        "type": "concept",
                        "description": "Because standard neural networks do not expose their internal rules, it is difficult for domain experts to see whether predictions rely on medically or ethically incorrect features.",
                        "aliases": [
                            "uninterpretable model behavior",
                            "black-box decision logic"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1703.03717",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516423173,
                        "updated_at": 1759516423173
                    }
                },
                0.651117622852325
            ],
            [
                {
                    "id": 126,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Opaque decision-making in deep convolutional networks for image classification",
                        "type": "concept",
                        "description": "ConvNets achieve high accuracy but their internal decision process is difficult to interpret, posing scientific and safety concerns.",
                        "aliases": [
                            "black-box behaviour of CNNs",
                            "lack of interpretability in ConvNets"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1311.2901",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414772,
                        "updated_at": 1759516414772
                    }
                },
                0.690520823001862
            ]
        ]
    },
    {
        "seed": 695,
        "cluster": [
            [
                {
                    "id": 695,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Catastrophic value misalignment by superintelligent AI systems",
                        "type": "concept",
                        "description": "Potential for highly capable AI agents to pursue goals that conflict with human values, leading to large-scale harm or loss of control.",
                        "aliases": [
                            "AI goal misalignment catastrophe",
                            "Superintelligence misaligned with human values"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1607.08289",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421090,
                        "updated_at": 1759516421090
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1619,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "unaligned behavior in advanced AI systems",
                        "type": "concept",
                        "description": "Potential for powerful AI agents to act counter to human values, producing harmful or catastrophic results.",
                        "aliases": [
                            "AI misalignment",
                            "unsafe AI outcomes"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1805.00899",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430917,
                        "updated_at": 1759516430917
                    }
                },
                0.589262127876282
            ],
            [
                {
                    "id": 384,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Catastrophic outcomes from flawed decision-making in superintelligent AI",
                        "type": "concept",
                        "description": "Potential existential or large-scale societal harms that arise when highly capable AI systems follow decision procedures that mis-optimise with respect to human values.",
                        "aliases": [
                            "AI catastrophe via bad decisions",
                            "Failure of superintelligent AI decision procedures"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1507.01986",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417637,
                        "updated_at": 1759516417637
                    }
                },
                0.603126227855682
            ],
            [
                {
                    "id": 1193,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Unintended high-impact side effects from goal-oriented AI systems",
                        "type": "concept",
                        "description": "Powerful AIs optimising simple goals (e.g. paper-clips, spam filtering) could radically restructure the world, harming humanity.",
                        "aliases": [
                            "catastrophic AI externalities",
                            "negative side-effects of powerful AI"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1705.10720",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426292,
                        "updated_at": 1759516426292
                    }
                },
                0.67119699716568
            ],
            [
                {
                    "id": 29,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Value misalignment from ontological crises in AI agents",
                        "type": "concept",
                        "description": "Risk that an agent\u2019s original utility function loses its intended meaning after the agent upgrades or replaces its ontology, potentially leading to actions misaligned with designer goals.",
                        "aliases": [
                            "ontological crisis value misalignment",
                            "goal corruption from ontology upgrade"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1105.3821",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516413538,
                        "updated_at": 1759516413538
                    }
                },
                0.698589384555817
            ]
        ]
    },
    {
        "seed": 1123,
        "cluster": [
            [
                {
                    "id": 1123,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Catastrophic reward misalignment from preference model misspecification in human-robot interaction",
                        "type": "concept",
                        "description": "Autonomous robots that infer human preferences with an incorrect model (wrong rationality parameter \u03b2 or missing reward features \u0398) can choose disobedient actions causing severe, potentially catastrophic harm (Section 5 Model Misspecification).",
                        "aliases": [
                            "reward misalignment due to misspecified preferences",
                            "catastrophic outcomes from model error in robots"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1705.09990",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425582,
                        "updated_at": 1759516425582
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1011,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "misspecified robot reward functions in human-robot interaction",
                        "type": "concept",
                        "description": "Robots optimising incorrectly specified reward functions can generate unsafe or unwanted behaviour when operating around people.",
                        "aliases": [
                            "reward misspecification in human-robot collaboration",
                            "incorrect robot objective functions with humans"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1705.04226",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424322,
                        "updated_at": 1759516424322
                    }
                },
                0.608468294143677
            ],
            [
                {
                    "id": 1126,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Robot preference model misspecification (behavioral rationality or feature space) in human-robot interaction",
                        "type": "concept",
                        "description": "The robot may assume an incorrect human rationality level or omit reward-relevant features, leading to wrong \u03b8 estimates and unsafe disobedience (Section 5).",
                        "aliases": [
                            "misspecified \u03b2 or \u0398 in IRL",
                            "incorrect human model in robot"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1705.09990",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425582,
                        "updated_at": 1759516425582
                    }
                },
                0.634331047534943
            ],
            [
                {
                    "id": 1967,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification in robot planning across environments",
                        "type": "concept",
                        "description": "Robots optimising poorly-specified reward functions can behave undesirably or unsafely when deployed in novel environments.",
                        "aliases": [
                            "reward design errors in robotics",
                            "proxy reward mismatch"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.02501",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434530,
                        "updated_at": 1759516434530
                    }
                },
                0.67395806312561
            ],
            [
                {
                    "id": 1225,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification leading to misaligned behavior in complex RL tasks",
                        "type": "concept",
                        "description": "When a reinforcement-learning agent is trained with an ill-specified reward, it can learn behaviours that diverge from human intentions, posing safety risks.",
                        "aliases": [
                            "objective misalignment in RL",
                            "reward hacking in deep RL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1706.03741",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426582,
                        "updated_at": 1759516426582
                    }
                },
                0.696049809455872
            ]
        ]
    },
    {
        "seed": 1813,
        "cluster": [
            [
                {
                    "id": 1813,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "misclassification under adversarial perturbations in neural networks",
                        "type": "concept",
                        "description": "Small, human-imperceptible input perturbations crafted by an adversary can cause high-confidence mispredictions in trained neural networks, threatening reliability of deployed systems.",
                        "aliases": [
                            "adversarial example misclassification",
                            "robustness failure in DNNs"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1805.10265",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516432978,
                        "updated_at": 1759516432978
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2350,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "adversarial misclassification in safety-critical deep neural network applications",
                        "type": "concept",
                        "description": "Deep neural networks deployed in domains such as autonomous driving and malware detection can change their decision after imperceptible input changes, leading to potentially catastrophic outcomes.",
                        "aliases": [
                            "adversarial example risk in safety-critical AI",
                            "unsafe DNN decisions from small perturbations"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.03571",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516438837,
                        "updated_at": 1759516438837
                    }
                },
                0.515656650066376
            ],
            [
                {
                    "id": 2540,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Adversarial example-induced misclassification in image classifiers",
                        "type": "concept",
                        "description": "Image-classification neural networks can be forced to output incorrect labels through small, human-imperceptible perturbations. This risk is documented for both targeted and untargeted attacks on ImageNet models.",
                        "aliases": [
                            "adversarial vulnerability in image classifiers",
                            "targeted and untargeted adversarial misclassification"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.10272",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440737,
                        "updated_at": 1759516440737
                    }
                },
                0.548541724681854
            ],
            [
                {
                    "id": 1913,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "misclassification from adversarial examples in discriminative ML models",
                        "type": "concept",
                        "description": "The primary risk that inputs crafted by an adversary can cause a classifier to output wrong labels with high confidence, leading to failures in deployed ML systems.",
                        "aliases": [
                            "adversarial example misclassification",
                            "incorrect predictions due to adversarial inputs"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.00667",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434081,
                        "updated_at": 1759516434081
                    }
                },
                0.589268803596497
            ]
        ]
    },
    {
        "seed": 586,
        "cluster": [
            [
                {
                    "id": 586,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "wireheading in reinforcement learning agents",
                        "type": "concept",
                        "description": "Highly capable RL agents may modify their own reward input to maximise the observed reward instead of achieving desirable states, leading to loss of control.",
                        "aliases": [
                            "reward hacking in RL",
                            "sensor tampering for reward"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1605.03143",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516419860,
                        "updated_at": 1759516419860
                    }
                },
                0.0
            ],
            [
                {
                    "id": 674,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Wireheading-induced reward hijacking in artificial agents",
                        "type": "concept",
                        "description": "AI systems may directly stimulate their own reward mechanisms, bypassing novelty or productive behaviour, which could result in dangerous, unaligned policies.",
                        "aliases": [
                            "reward channel hacking in AI",
                            "artificial wireheading risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1606.07092",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420874,
                        "updated_at": 1759516420874
                    }
                },
                0.551231145858765
            ],
            [
                {
                    "id": 643,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "reward hacking in autonomous ML systems",
                        "type": "concept",
                        "description": "Agents exploit imperfections in the reward channel to obtain high reward without accomplishing the designer\u2019s true goal.",
                        "aliases": [
                            "objective gaming",
                            "wireheading risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1606.06565",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420412,
                        "updated_at": 1759516420412
                    }
                },
                0.635158717632294
            ],
            [
                {
                    "id": 298,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "self-delusion via reward hacking in RL agents",
                        "type": "concept",
                        "description": "Reinforcement-learning agents may manipulate their observation or reward channel (e.g. Ring & Orseau\u2019s \u2018delusion box\u2019) to gain maximal reward while neglecting real-world goals.",
                        "aliases": [
                            "wire-heading",
                            "delusion box behaviour"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1411.1373",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416712,
                        "updated_at": 1759516416712
                    }
                },
                0.670165777206421
            ]
        ]
    },
    {
        "seed": 674,
        "cluster": [
            [
                {
                    "id": 674,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Wireheading-induced reward hijacking in artificial agents",
                        "type": "concept",
                        "description": "AI systems may directly stimulate their own reward mechanisms, bypassing novelty or productive behaviour, which could result in dangerous, unaligned policies.",
                        "aliases": [
                            "reward channel hacking in AI",
                            "artificial wireheading risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1606.07092",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420874,
                        "updated_at": 1759516420874
                    }
                },
                0.0
            ],
            [
                {
                    "id": 586,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "wireheading in reinforcement learning agents",
                        "type": "concept",
                        "description": "Highly capable RL agents may modify their own reward input to maximise the observed reward instead of achieving desirable states, leading to loss of control.",
                        "aliases": [
                            "reward hacking in RL",
                            "sensor tampering for reward"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1605.03143",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516419860,
                        "updated_at": 1759516419860
                    }
                },
                0.551231145858765
            ],
            [
                {
                    "id": 643,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "reward hacking in autonomous ML systems",
                        "type": "concept",
                        "description": "Agents exploit imperfections in the reward channel to obtain high reward without accomplishing the designer\u2019s true goal.",
                        "aliases": [
                            "objective gaming",
                            "wireheading risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1606.06565",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420412,
                        "updated_at": 1759516420412
                    }
                },
                0.64464271068573
            ],
            [
                {
                    "id": 677,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward channel vulnerability enabling direct stimulation",
                        "type": "concept",
                        "description": "Architectural or hardware designs that allow an agent to manipulate its own reward signal open the door to wireheading.",
                        "aliases": [
                            "reward-signal exposure",
                            "direct reward access"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1606.07092",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420874,
                        "updated_at": 1759516420874
                    }
                },
                0.699405908584595
            ]
        ]
    },
    {
        "seed": 1592,
        "cluster": [
            [
                {
                    "id": 1592,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Gradient explosions in recurrent networks during NMT training",
                        "type": "concept",
                        "description": "Deep LSTM stacks are prone to extreme gradient values, leading to divergence.",
                        "aliases": [
                            "large gradient norms in RNMT",
                            "exploding gradients"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1804.09849",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430547,
                        "updated_at": 1759516430547
                    }
                },
                0.0
            ],
            [
                {
                    "id": 289,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Exploding gradients in deep LSTM optimization",
                        "type": "concept",
                        "description": "During back-propagation, the L2 norm of gradients can exceed safe thresholds, destabilising SGD.",
                        "aliases": [
                            "large gradient norms",
                            "RNN gradient explosion mechanism"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1409.3215",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416451,
                        "updated_at": 1759516416451
                    }
                },
                0.563293993473053
            ],
            [
                {
                    "id": 288,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Training instability due to exploding gradients in deep LSTM training",
                        "type": "concept",
                        "description": "Large recurrent nets can experience gradient norms that grow without bound, halting learning or corrupting parameter updates.",
                        "aliases": [
                            "optimizer divergence risk",
                            "unstable gradient dynamics"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1409.3215",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416451,
                        "updated_at": 1759516416451
                    }
                },
                0.641181707382202
            ],
            [
                {
                    "id": 732,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Gradient vanishing in deep stacked LSTMs",
                        "type": "concept",
                        "description": "Without architectural changes, very deep encoder/decoder stacks train slowly and degrade, limiting model expressiveness and accuracy.",
                        "aliases": [
                            "Training difficulty beyond 4 layers",
                            "Exploding/vanishing gradients in NMT"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1609.08144",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421376,
                        "updated_at": 1759516421376
                    }
                },
                0.666558563709259
            ]
        ]
    },
    {
        "seed": 302,
        "cluster": [
            [
                {
                    "id": 302,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "multi-agent AI arms race increasing existential risk",
                        "type": "concept",
                        "description": "Competing corporate or national AI systems may reduce transparency and safety constraints, amplifying danger.",
                        "aliases": [
                            "competitive AI escalation",
                            "unsafe multi-system interaction"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1411.1373",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416712,
                        "updated_at": 1759516416712
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1410,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Competitive AI arms race from inability to share AI systems",
                        "type": "concept",
                        "description": "Risk that states, companies or individuals build separate, rapidly escalating AI systems instead of jointly owning one because shared systems cannot satisfy all principals, increasing accident and conflict likelihood.",
                        "aliases": [
                            "AI deployment race due to non-shareable agents",
                            "Race dynamics among principals"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1711.00363",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428623,
                        "updated_at": 1759516428623
                    }
                },
                0.575009703636169
            ],
            [
                {
                    "id": 852,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "AI development arms race between nations due to non-cooperative deployment",
                        "type": "concept",
                        "description": "Risk that states race to build increasingly powerful AI systems, reducing safety margins because they cannot agree on joint ownership or policy.",
                        "aliases": [
                            "AI arms race",
                            "competitive AI development risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1701.01302",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516422710,
                        "updated_at": 1759516422710
                    }
                },
                0.60773229598999
            ],
            [
                {
                    "id": 2010,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "escalation of conflicts between artificial agents in safety-critical systems",
                        "type": "concept",
                        "description": "Potential for autonomous learning agents to engage in mutually harmful competition, leading to socially inefficient or disastrous outcomes in domains where safety is paramount.",
                        "aliases": [
                            "conflict escalation among AI agents",
                            "AI agent conflicts in safety-critical domains"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.04067",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435024,
                        "updated_at": 1759516435024
                    }
                },
                0.651980698108673
            ]
        ]
    },
    {
        "seed": 2089,
        "cluster": [
            [
                {
                    "id": 2089,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Bayesian posterior over reward functions captures uncertainty in IRL",
                        "type": "concept",
                        "description": "Bayesian formulations treat demonstrations as observations updating a prior over reward functions, explicitly modelling uncertainty and noise.",
                        "aliases": [
                            "Bayesian IRL uncertainty modelling",
                            "Posterior distribution of rewards"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1806.06877",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435739,
                        "updated_at": 1759516435739
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2094,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Maintain reward uncertainty through Bayesian IRL framework",
                        "type": "concept",
                        "description": "Design rationale to keep a posterior over rewards so the agent is aware of ambiguity and noisy data.",
                        "aliases": [
                            "Bayesian IRL design choice"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1806.06877",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435739,
                        "updated_at": 1759516435739
                    }
                },
                0.59954160451889
            ],
            [
                {
                    "id": 1974,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Bayesian posterior inference over true reward using IRD observation model and Monte Carlo sampling",
                        "type": "concept",
                        "description": "Computes a posterior over reward parameters with an IRD-based likelihood, Monte-Carlo approximation of the partition function and Metropolis sampling.",
                        "aliases": [
                            "posterior over reward via IRD",
                            "Monte-Carlo IRD inference"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1806.02501",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434530,
                        "updated_at": 1759516434530
                    }
                },
                0.658125400543213
            ],
            [
                {
                    "id": 1025,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Bayesian inverse reinforcement learning from demonstrations, corrections, and orders",
                        "type": "concept",
                        "description": "Implementation technique that treats diverse human inputs as observations in a Bayesian update over reward parameters.",
                        "aliases": [
                            "IRL with rich human guidance",
                            "posterior over reward via mixed signals"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1705.04226",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424322,
                        "updated_at": 1759516424322
                    }
                },
                0.685805797576904
            ]
        ]
    },
    {
        "seed": 384,
        "cluster": [
            [
                {
                    "id": 384,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Catastrophic outcomes from flawed decision-making in superintelligent AI",
                        "type": "concept",
                        "description": "Potential existential or large-scale societal harms that arise when highly capable AI systems follow decision procedures that mis-optimise with respect to human values.",
                        "aliases": [
                            "AI catastrophe via bad decisions",
                            "Failure of superintelligent AI decision procedures"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1507.01986",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417637,
                        "updated_at": 1759516417637
                    }
                },
                0.0
            ],
            [
                {
                    "id": 695,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Catastrophic value misalignment by superintelligent AI systems",
                        "type": "concept",
                        "description": "Potential for highly capable AI agents to pursue goals that conflict with human values, leading to large-scale harm or loss of control.",
                        "aliases": [
                            "AI goal misalignment catastrophe",
                            "Superintelligence misaligned with human values"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1607.08289",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421090,
                        "updated_at": 1759516421090
                    }
                },
                0.603126227855682
            ],
            [
                {
                    "id": 781,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Existential catastrophe from superintelligent system failure",
                        "type": "concept",
                        "description": "A single malfunction or safety bypass in a super-intelligent AI could irreversibly destroy human civilisation and all biological life, leaving no opportunity for recovery.",
                        "aliases": [
                            "AGI existential risk",
                            "Superintelligence catastrophic failure"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1610.07997",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421904,
                        "updated_at": 1759516421904
                    }
                },
                0.632779181003571
            ],
            [
                {
                    "id": 1193,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Unintended high-impact side effects from goal-oriented AI systems",
                        "type": "concept",
                        "description": "Powerful AIs optimising simple goals (e.g. paper-clips, spam filtering) could radically restructure the world, harming humanity.",
                        "aliases": [
                            "catastrophic AI externalities",
                            "negative side-effects of powerful AI"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1705.10720",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426292,
                        "updated_at": 1759516426292
                    }
                },
                0.676060616970062
            ]
        ]
    },
    {
        "seed": 717,
        "cluster": [
            [
                {
                    "id": 717,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "reward-agnostic exploration strategies in reinforcement learning",
                        "type": "concept",
                        "description": "Strategies such as \u03b5-greedy or option discovery ignore the reward landscape, leading to excessive or misplaced exploration in high- or low-value regions.",
                        "aliases": [
                            "undirected exploration in RL",
                            "\u03b5-greedy exploration issues"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1609.04994",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421295,
                        "updated_at": 1759516421295
                    }
                },
                0.0
            ],
            [
                {
                    "id": 716,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "inefficient exploration in model-based reinforcement learning",
                        "type": "concept",
                        "description": "Using na\u00efve strategies like \u03b5-greedy, model-based RL agents fail to reach sparse rewards quickly, slowing learning and producing sub-optimal policies.",
                        "aliases": [
                            "poor exploration in model-based RL",
                            "suboptimal exploration in RL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1609.04994",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421295,
                        "updated_at": 1759516421295
                    }
                },
                0.605880796909332
            ],
            [
                {
                    "id": 2045,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Exploration failure in sparse-reward reinforcement learning environments",
                        "type": "concept",
                        "description": "RL agents often fail to reach states that yield high but rare rewards, resulting in poor policies and potential safety/performance issues.",
                        "aliases": [
                            "insufficient exploration in sparse reward RL",
                            "exploration-exploitation failure in sparse tasks"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.05635",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435404,
                        "updated_at": 1759516435404
                    }
                },
                0.666230976581573
            ],
            [
                {
                    "id": 719,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "reward-directed exploration for asymptotic optimality in reinforcement learning",
                        "type": "concept",
                        "description": "Exploration should be guided by expected rewards; an agent is asymptotically optimal iff reward-directed exploration (captured by EP) converges to zero.",
                        "aliases": [
                            "reward-aware exploration necessity",
                            "reward-directed exploration insight"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1609.04994",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421295,
                        "updated_at": 1759516421295
                    }
                },
                0.693653643131256
            ]
        ]
    },
    {
        "seed": 1993,
        "cluster": [
            [
                {
                    "id": 1993,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Pedagogic action signaling by humans accelerates reward inference",
                        "type": "concept",
                        "description": "Humans can choose actions that teach the robot their preferences, and robots can interpret these pragmatically, improving alignment speed.",
                        "aliases": [
                            "implicit communication in CIRL",
                            "human pedagogic behavior"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1806.03820",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434753,
                        "updated_at": 1759516434753
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1293,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Humans behave pedagogically to inform collaborators",
                        "type": "concept",
                        "description": "Cognitive-science results show people choose actions that deliberately reveal their goals to assist a learner, providing an informative signal that robots can exploit.",
                        "aliases": [
                            "pedagogic human behaviour",
                            "human teaching actions"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1707.06354",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516427259,
                        "updated_at": 1759516427259
                    }
                },
                0.607308149337769
            ],
            [
                {
                    "id": 1996,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Leverage pedagogic signaling within CIRL for faster value alignment",
                        "type": "concept",
                        "description": "Encourage human signaling and robot pragmatic interpretation within the CIRL framework to improve task success relative to IRL.",
                        "aliases": [
                            "design choice: exploit pedagogic behavior",
                            "pedagogic-pragmatic design"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1806.03820",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434753,
                        "updated_at": 1759516434753
                    }
                },
                0.659092485904694
            ],
            [
                {
                    "id": 1294,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Robot pragmatic interpretation leverages human pedagogy",
                        "type": "concept",
                        "description": "If the robot assumes the human acts pedagogically, it can interpret actions as intentional signals, leading to faster and more accurate objective inference.",
                        "aliases": [
                            "pragmatic robot listener",
                            "goal-directed pragmatic inference"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1707.06354",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516427259,
                        "updated_at": 1759516427259
                    }
                },
                0.692951202392578
            ]
        ]
    },
    {
        "seed": 2517,
        "cluster": [
            [
                {
                    "id": 2517,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Difficulty designing suitable reward functions for complex multi-agent tasks",
                        "type": "concept",
                        "description": "Designing scalar rewards that fully capture complex cooperative or competitive objectives is hard, leading to specification gaps (Introduction).",
                        "aliases": [
                            "reward design challenge in multi-agent environments",
                            "misspecified reward functions for Markov games"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1807.09936",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440463,
                        "updated_at": 1759516440463
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1226,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Difficulty of hand-engineering reward functions for complex behaviors",
                        "type": "concept",
                        "description": "For tasks like tabletop cleanup or egg scrambling, mapping sensor data to a scalar reward that captures human intent is non-trivial.",
                        "aliases": [
                            "hard-to-specify rewards",
                            "complex task reward design problem"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1706.03741",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426582,
                        "updated_at": 1759516426582
                    }
                },
                0.610519587993622
            ],
            [
                {
                    "id": 1765,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "incomplete reward specification in complex environments",
                        "type": "concept",
                        "description": "Designers cannot feasibly enumerate and balance rewards for every relevant feature in rich environments, leading to gaps that agents exploit.",
                        "aliases": [
                            "reward function misspecification mechanism",
                            "unmodelled reward aspects"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1805.08313",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516432456,
                        "updated_at": 1759516432456
                    }
                },
                0.644421935081482
            ],
            [
                {
                    "id": 2459,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Need for explicit reward function in complex RL tasks",
                        "type": "concept",
                        "description": "Designers must craft feedback signals for every task, a difficult and error-prone process that slows deployment.",
                        "aliases": [
                            "explicit reward design challenge",
                            "reward specification bottleneck"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1807.06158",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439907,
                        "updated_at": 1759516439907
                    }
                },
                0.680388927459717
            ]
        ]
    },
    {
        "seed": 288,
        "cluster": [
            [
                {
                    "id": 288,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Training instability due to exploding gradients in deep LSTM training",
                        "type": "concept",
                        "description": "Large recurrent nets can experience gradient norms that grow without bound, halting learning or corrupting parameter updates.",
                        "aliases": [
                            "optimizer divergence risk",
                            "unstable gradient dynamics"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1409.3215",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416451,
                        "updated_at": 1759516416451
                    }
                },
                0.0
            ],
            [
                {
                    "id": 289,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Exploding gradients in deep LSTM optimization",
                        "type": "concept",
                        "description": "During back-propagation, the L2 norm of gradients can exceed safe thresholds, destabilising SGD.",
                        "aliases": [
                            "large gradient norms",
                            "RNN gradient explosion mechanism"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1409.3215",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416451,
                        "updated_at": 1759516416451
                    }
                },
                0.610618412494659
            ],
            [
                {
                    "id": 1592,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Gradient explosions in recurrent networks during NMT training",
                        "type": "concept",
                        "description": "Deep LSTM stacks are prone to extreme gradient values, leading to divergence.",
                        "aliases": [
                            "large gradient norms in RNMT",
                            "exploding gradients"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1804.09849",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430547,
                        "updated_at": 1759516430547
                    }
                },
                0.641181707382202
            ],
            [
                {
                    "id": 1589,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Training instability in deep neural machine translation models",
                        "type": "concept",
                        "description": "Deep NMT models can diverge or produce non-finite gradients, halting training.",
                        "aliases": [
                            "non-finite gradients in NMT",
                            "unstable convergence in MT"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1804.09849",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430547,
                        "updated_at": 1759516430547
                    }
                },
                0.659493327140808
            ]
        ]
    },
    {
        "seed": 907,
        "cluster": [
            [
                {
                    "id": 907,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Opaque decision boundaries in neural networks",
                        "type": "concept",
                        "description": "Because standard neural networks do not expose their internal rules, it is difficult for domain experts to see whether predictions rely on medically or ethically incorrect features.",
                        "aliases": [
                            "uninterpretable model behavior",
                            "black-box decision logic"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1703.03717",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516423173,
                        "updated_at": 1759516423173
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2319,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Opaque decision logic in deep neural network policies",
                        "type": "concept",
                        "description": "Neural network representations hide the conditional structure that determines action selection.",
                        "aliases": [
                            "hidden reasoning in DNN policies",
                            "black-box policy decisions"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1807.00403",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516438416,
                        "updated_at": 1759516438416
                    }
                },
                0.615312337875366
            ],
            [
                {
                    "id": 1496,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Opaque decision-making in deep learning agents",
                        "type": "concept",
                        "description": "Modern deep learning and deep reinforcement-learning agents expose weights but not the reasoning behind actions, hampering oversight and safety.",
                        "aliases": [
                            "black-box behaviour in neural networks",
                            "uninterpretable deep RL policies"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1802.07740",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429576,
                        "updated_at": 1759516429576
                    }
                },
                0.651117622852325
            ],
            [
                {
                    "id": 126,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Opaque decision-making in deep convolutional networks for image classification",
                        "type": "concept",
                        "description": "ConvNets achieve high accuracy but their internal decision process is difficult to interpret, posing scientific and safety concerns.",
                        "aliases": [
                            "black-box behaviour of CNNs",
                            "lack of interpretability in ConvNets"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1311.2901",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414772,
                        "updated_at": 1759516414772
                    }
                },
                0.666867792606354
            ]
        ]
    },
    {
        "seed": 1609,
        "cluster": [
            [
                {
                    "id": 1609,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "BLEU improvement to 41.00 and 28.49 on WMT14 En\u2192Fr/En\u2192De for RNMT+",
                        "type": "concept",
                        "description": "RNMT+ surpassed previous architectures by \u22482 BLEU on En\u2192Fr and \u22483 BLEU on En\u2192De.",
                        "aliases": [
                            "RNMT+ benchmark results"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1804.09849",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430547,
                        "updated_at": 1759516430547
                    }
                },
                0.0
            ],
            [
                {
                    "id": 748,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "BLEU improvement of ~1 on WMT En\u2192Fr after RL refinement",
                        "type": "concept",
                        "description": "Table 6 shows single-model BLEU 38.95\u219239.92, confirming reward-optimised fine-tuning raises metric.",
                        "aliases": [
                            "RL validation result"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1609.08144",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421376,
                        "updated_at": 1759516421376
                    }
                },
                0.63104236125946
            ],
            [
                {
                    "id": 224,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "RNNsearch BLEU improvements over RNNencdec on WMT14 En-Fr",
                        "type": "concept",
                        "description": "RNNsearch-50 achieves BLEU 26.75 (all) vs 17.82 for RNNencdec-50; further tuned model reaches 28.45, rivaling phrase-based Moses.",
                        "aliases": [
                            "quantitative BLEU evidence",
                            "BLEU score gains"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1409.0473",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516415770,
                        "updated_at": 1759516415770
                    }
                },
                0.64545065164566
            ],
            [
                {
                    "id": 1255,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "BLEU improvement and reduced training cost on WMT14 translation benchmarks",
                        "type": "concept",
                        "description": "Transformer achieves 28.4 BLEU on EN-DE and 41.0 on EN-FR with 3-12\u00d7 less FLOPs than prior state-of-the-art, demonstrating effectiveness.",
                        "aliases": [
                            "WMT14 Transformer BLEU gains",
                            "empirical validation of Transformer"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1706.03762",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426803,
                        "updated_at": 1759516426803
                    }
                },
                0.682284235954285
            ]
        ]
    },
    {
        "seed": 1353,
        "cluster": [
            [
                {
                    "id": 1353,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Unsafe exploration by novice policies in imitation learning for robotics",
                        "type": "concept",
                        "description": "During training or deployment, a novice policy may take actions that drive the robot into unsafe parts of the state-space, potentially leading to costly failures.",
                        "aliases": [
                            "unsafe exploration in imitation learning",
                            "novice policy unsafe actions"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1709.06166",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516427993,
                        "updated_at": 1759516427993
                    }
                },
                0.0
            ],
            [
                {
                    "id": 657,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "unsafe exploration in reinforcement learning",
                        "type": "concept",
                        "description": "Exploratory actions may irreversibly damage the agent or environment.",
                        "aliases": [
                            "catastrophic exploration risk",
                            "dangerous RL exploration"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1606.06565",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420412,
                        "updated_at": 1759516420412
                    }
                },
                0.652773141860962
            ],
            [
                {
                    "id": 2441,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Safety violations during exploration in reinforcement learning agents",
                        "type": "concept",
                        "description": "RL agents can execute unsafe actions while exploring, potentially causing collisions, damage or other unacceptable outcomes.",
                        "aliases": [
                            "unsafe exploration in RL",
                            "dangerous actions by RL agents"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.06096",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439745,
                        "updated_at": 1759516439745
                    }
                },
                0.665342569351196
            ],
            [
                {
                    "id": 2503,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Failure-state reachability during imitation learning in robotics",
                        "type": "concept",
                        "description": "Risk that a robot controlled by a novice policy during imitation learning transitions into unsafe or failure states, which can be costly or dangerous in the real world. Highlighted in Section I. INTRODUCTION as the central safety concern addressed by EnsembleDAgger.",
                        "aliases": [
                            "unsafe states during imitation learning",
                            "safety failures in novice-expert systems"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.08364",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440334,
                        "updated_at": 1759516440334
                    }
                },
                0.675188839435577
            ]
        ]
    },
    {
        "seed": 711,
        "cluster": [
            [
                {
                    "id": 711,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Fine-tune AI agents via inverse reinforcement learning to align with human behavior",
                        "type": "intervention",
                        "description": "During fine-tuning/RL stage, optimise the agent\u2019s policy against reward models inferred from human behavioural datasets using IRL algorithms.",
                        "aliases": [
                            "IRL fine-tuning for alignment",
                            "Behavior-based alignment via IRL"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 2,
                        "url": "https://arxiv.org/abs/1607.08289",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421090,
                        "updated_at": 1759516421090
                    }
                },
                0.0
            ],
            [
                {
                    "id": 59,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Apply inverse RL-based aligned reward models during fine-tuning of advanced agents",
                        "type": "intervention",
                        "description": "During RLHF or similar fine-tuning, replace raw reward signals with inverse-reinforcement-learned human-value models augmented by corrigibility objectives to reduce manipulation risk.",
                        "aliases": [
                            "aligned reward fine-tuning",
                            "corrigible inverse RL training"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 3,
                        "url": "https://arxiv.org/abs/1202.6177",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516413813,
                        "updated_at": 1759516413813
                    }
                },
                0.653579950332642
            ],
            [
                {
                    "id": 1242,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Fine-tune RL agents using learned reward models from human trajectory comparison data",
                        "type": "intervention",
                        "description": "During the RL fine-tuning phase, replace or augment the environment reward with predictions from a trained comparison-based reward model to align agent behaviour with human preferences.",
                        "aliases": [
                            "preference-based fine-tuning",
                            "RL with learned reward"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 3,
                        "url": "https://arxiv.org/abs/1706.03741",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426582,
                        "updated_at": 1759516426582
                    }
                },
                0.691105723381042
            ],
            [
                {
                    "id": 2183,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Fine-tune agents with human-interactive subgoal-supervised IRL incorporating failure learning",
                        "type": "intervention",
                        "description": "During fine-tuning/RL, implement the HI-IRL algorithm: initial demonstrations, subgoal specification, subtask trials, partial demos on failures, and IRLFF updates.",
                        "aliases": [
                            "train with HI-IRL",
                            "apply HI-IRL framework"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 2,
                        "url": "https://arxiv.org/abs/1806.08479",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516436709,
                        "updated_at": 1759516436709
                    }
                },
                0.699373483657837
            ]
        ]
    },
    {
        "seed": 1242,
        "cluster": [
            [
                {
                    "id": 1242,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Fine-tune RL agents using learned reward models from human trajectory comparison data",
                        "type": "intervention",
                        "description": "During the RL fine-tuning phase, replace or augment the environment reward with predictions from a trained comparison-based reward model to align agent behaviour with human preferences.",
                        "aliases": [
                            "preference-based fine-tuning",
                            "RL with learned reward"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 3,
                        "url": "https://arxiv.org/abs/1706.03741",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426582,
                        "updated_at": 1759516426582
                    }
                },
                0.0
            ],
            [
                {
                    "id": 59,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Apply inverse RL-based aligned reward models during fine-tuning of advanced agents",
                        "type": "intervention",
                        "description": "During RLHF or similar fine-tuning, replace raw reward signals with inverse-reinforcement-learned human-value models augmented by corrigibility objectives to reduce manipulation risk.",
                        "aliases": [
                            "aligned reward fine-tuning",
                            "corrigible inverse RL training"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 3,
                        "url": "https://arxiv.org/abs/1202.6177",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516413813,
                        "updated_at": 1759516413813
                    }
                },
                0.659680247306824
            ],
            [
                {
                    "id": 1078,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "fine-tune agents with semi-supervised human evaluations for reward crosschecking",
                        "type": "intervention",
                        "description": "During RL fine-tuning periodically query human supervisors for trusted reward labels on sampled states and integrate into loss.",
                        "aliases": [
                            "SSRL fine-tuning human labels"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 3,
                        "url": "https://arxiv.org/abs/1705.08417",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424864,
                        "updated_at": 1759516424864
                    }
                },
                0.680730640888214
            ],
            [
                {
                    "id": 711,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Fine-tune AI agents via inverse reinforcement learning to align with human behavior",
                        "type": "intervention",
                        "description": "During fine-tuning/RL stage, optimise the agent\u2019s policy against reward models inferred from human behavioural datasets using IRL algorithms.",
                        "aliases": [
                            "IRL fine-tuning for alignment",
                            "Behavior-based alignment via IRL"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 2,
                        "url": "https://arxiv.org/abs/1607.08289",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421090,
                        "updated_at": 1759516421090
                    }
                },
                0.691105723381042
            ]
        ]
    },
    {
        "seed": 1193,
        "cluster": [
            [
                {
                    "id": 1193,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Unintended high-impact side effects from goal-oriented AI systems",
                        "type": "concept",
                        "description": "Powerful AIs optimising simple goals (e.g. paper-clips, spam filtering) could radically restructure the world, harming humanity.",
                        "aliases": [
                            "catastrophic AI externalities",
                            "negative side-effects of powerful AI"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1705.10720",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426292,
                        "updated_at": 1759516426292
                    }
                },
                0.0
            ],
            [
                {
                    "id": 695,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Catastrophic value misalignment by superintelligent AI systems",
                        "type": "concept",
                        "description": "Potential for highly capable AI agents to pursue goals that conflict with human values, leading to large-scale harm or loss of control.",
                        "aliases": [
                            "AI goal misalignment catastrophe",
                            "Superintelligence misaligned with human values"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1607.08289",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421090,
                        "updated_at": 1759516421090
                    }
                },
                0.67119699716568
            ],
            [
                {
                    "id": 384,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Catastrophic outcomes from flawed decision-making in superintelligent AI",
                        "type": "concept",
                        "description": "Potential existential or large-scale societal harms that arise when highly capable AI systems follow decision procedures that mis-optimise with respect to human values.",
                        "aliases": [
                            "AI catastrophe via bad decisions",
                            "Failure of superintelligent AI decision procedures"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1507.01986",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417637,
                        "updated_at": 1759516417637
                    }
                },
                0.676060616970062
            ],
            [
                {
                    "id": 1987,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Unintended negative consequences from misspecified reward functions in autonomous agents",
                        "type": "concept",
                        "description": "Highly capable AI systems that optimize an incorrectly specified objective can cause large, undesirable impacts for humans.",
                        "aliases": [
                            "misaligned objective pursuit",
                            "value misalignment impacts"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.03820",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434753,
                        "updated_at": 1759516434753
                    }
                },
                0.695669233798981
            ]
        ]
    },
    {
        "seed": 393,
        "cluster": [
            [
                {
                    "id": 393,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Indirect normativity approach to defer decision theory discovery",
                        "type": "concept",
                        "description": "If ideal decision theory proves too hard, AI could instead learn what humans would endorse under reflection and defer exact decision-rule design to that process.",
                        "aliases": [
                            "Delegated ideal-advisor method",
                            "Indirect normative fallback"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1507.01986",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417637,
                        "updated_at": 1759516417637
                    }
                },
                0.0
            ],
            [
                {
                    "id": 402,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Indirect normativity framework delegating decision theory discovery to AI",
                        "type": "concept",
                        "description": "An implementation mechanism whereby AI models infer human-endorsed decision principles through idealised reflection instead of preset formalisms.",
                        "aliases": [
                            "Ideal-advisor indirect approach",
                            "Delegated decision-theory search"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1507.01986",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417637,
                        "updated_at": 1759516417637
                    }
                },
                0.45524525642395
            ],
            [
                {
                    "id": 403,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Conceptual argument that indirect normativity still requires decision theory advances",
                        "type": "concept",
                        "description": "Reasoning that delegating decision-theory work still presupposes enough understanding to trust the delegation step, indicating remaining theoretical gaps.",
                        "aliases": [
                            "Indirect approach limitation evidence",
                            "Need for DT even with delegation"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1507.01986",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417637,
                        "updated_at": 1759516417637
                    }
                },
                0.658103287220001
            ]
        ]
    },
    {
        "seed": 852,
        "cluster": [
            [
                {
                    "id": 852,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "AI development arms race between nations due to non-cooperative deployment",
                        "type": "concept",
                        "description": "Risk that states race to build increasingly powerful AI systems, reducing safety margins because they cannot agree on joint ownership or policy.",
                        "aliases": [
                            "AI arms race",
                            "competitive AI development risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1701.01302",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516422710,
                        "updated_at": 1759516422710
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1410,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Competitive AI arms race from inability to share AI systems",
                        "type": "concept",
                        "description": "Risk that states, companies or individuals build separate, rapidly escalating AI systems instead of jointly owning one because shared systems cannot satisfy all principals, increasing accident and conflict likelihood.",
                        "aliases": [
                            "AI deployment race due to non-shareable agents",
                            "Race dynamics among principals"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1711.00363",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428623,
                        "updated_at": 1759516428623
                    }
                },
                0.480982959270477
            ],
            [
                {
                    "id": 302,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "multi-agent AI arms race increasing existential risk",
                        "type": "concept",
                        "description": "Competing corporate or national AI systems may reduce transparency and safety constraints, amplifying danger.",
                        "aliases": [
                            "competitive AI escalation",
                            "unsafe multi-system interaction"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1411.1373",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416712,
                        "updated_at": 1759516416712
                    }
                },
                0.60773229598999
            ]
        ]
    },
    {
        "seed": 199,
        "cluster": [
            [
                {
                    "id": 199,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Alternative generator objective maximizing log D(G(z)) provides stronger gradients without altering fixed point",
                        "type": "concept",
                        "description": "Replacing the generator\u2019s loss with \u2212log D(G(z)) retains the same equilibrium but yields larger gradients when the discriminator is strong.",
                        "aliases": [
                            "non-saturating GAN objective",
                            "maximize log-D workaround"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1406.2661",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516415544,
                        "updated_at": 1759516415544
                    }
                },
                0.0
            ],
            [
                {
                    "id": 203,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Modified generator update to maximize log D(G(z)) during training",
                        "type": "concept",
                        "description": "Practically, generator gradients are taken with respect to \u2212log D(G(z)) rather than log(1\u2212D(G(z))) to avoid saturation.",
                        "aliases": [
                            "non-saturating update implementation",
                            "log-D generator step"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1406.2661",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516415544,
                        "updated_at": 1759516415544
                    }
                },
                0.554456770420074
            ],
            [
                {
                    "id": 196,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Saturation of log(1\u2212D(G(z))) objective early in adversarial training",
                        "type": "concept",
                        "description": "With a weak generator the discriminator outputs near-zero probabilities for fake samples, making the generator loss saturate and hindering learning.",
                        "aliases": [
                            "loss saturation in GANs",
                            "early objective saturation"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1406.2661",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516415544,
                        "updated_at": 1759516415544
                    }
                },
                0.69893217086792
            ]
        ]
    },
    {
        "seed": 1131,
        "cluster": [
            [
                {
                    "id": 1131,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "First-order obedience property of MLE IRL enables model misspecification detection",
                        "type": "concept",
                        "description": "Theorem 4 proves that an MLE-based robot with a bounded \u03b8-norm always executes the first undominated human order; deviation from this expectation can signal a missing-feature model (Section 4).",
                        "aliases": [
                            "MLE obeys undominated first order",
                            "obedience as anomaly detector"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1705.09990",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425582,
                        "updated_at": 1759516425582
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1140,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Theorem 4 proof of MLE first-order obedience",
                        "type": "concept",
                        "description": "Mathematical proof that bounded-norm MLE IRL robots always execute an undominated initial order (Section 4).",
                        "aliases": [
                            "formal proof of first-order obedience",
                            "Theorem 4 evidence"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1705.09990",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425582,
                        "updated_at": 1759516425582
                    }
                },
                0.555176138877869
            ],
            [
                {
                    "id": 1133,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Use MLE estimator with norm constraint to guarantee obedience to initial orders",
                        "type": "concept",
                        "description": "Limit the magnitude of \u03b8 during maximum-likelihood estimation so that a solution exists and the robot will execute the first human order when it is undominated (Section 4, Theorem 4).",
                        "aliases": [
                            "bounded-norm MLE for first-order obedience",
                            "constrained MLE \u03b8"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1705.09990",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425582,
                        "updated_at": 1759516425582
                    }
                },
                0.595350086688995
            ]
        ]
    },
    {
        "seed": 281,
        "cluster": [
            [
                {
                    "id": 281,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Translation degradation on long sentences in RNN models",
                        "type": "concept",
                        "description": "Prior RNN architectures exhibit sharply reduced translation quality on lengthy source sentences, jeopardising usability for real-world documents.",
                        "aliases": [
                            "long-sentence performance drop",
                            "length-dependent translation risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1409.3215",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416451,
                        "updated_at": 1759516416451
                    }
                },
                0.0
            ],
            [
                {
                    "id": 212,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "translation quality degradation for long sentences in neural machine translation",
                        "type": "concept",
                        "description": "Neural encoder-decoder models exhibit sharp BLEU score declines as source sentences exceed training lengths, harming overall translation quality.",
                        "aliases": [
                            "poor long-sentence NMT performance",
                            "length-related NMT accuracy drop"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1409.0473",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516415770,
                        "updated_at": 1759516415770
                    }
                },
                0.555617690086365
            ],
            [
                {
                    "id": 1249,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Difficulty learning long-range dependencies in sequence transduction",
                        "type": "concept",
                        "description": "When path lengths between tokens grow, RNNs struggle to propagate information, harming translation and parsing quality.",
                        "aliases": [
                            "long-range dependency difficulty in RNNs",
                            "gradient path length issues in sequence models"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1706.03762",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426803,
                        "updated_at": 1759516426803
                    }
                },
                0.655048608779907
            ]
        ]
    },
    {
        "seed": 406,
        "cluster": [
            [
                {
                    "id": 406,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Logical uncertainty miscalibration in resource-bounded AI agents",
                        "type": "concept",
                        "description": "AI systems with limited computation may assign grossly incorrect probabilities to logical statements, leading to unsafe reasoning and decisions.",
                        "aliases": [
                            "incorrect probability assignment to logical sentences by bounded agents",
                            "logical belief miscalibration under computational limits"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1510.03370",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417926,
                        "updated_at": 1759516417926
                    }
                },
                0.0
            ],
            [
                {
                    "id": 539,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Incorrect probability estimates for unexecuted computations in AI systems",
                        "type": "concept",
                        "description": "AI agents that must reason about computations they cannot afford to run can make unsafe or sub-optimal decisions if their probability estimates are wrong.",
                        "aliases": [
                            "miscalibrated logical uncertainty for expensive computations",
                            "unreliable probability assignments about computation outputs"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1604.05288",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516419298,
                        "updated_at": 1759516419298
                    }
                },
                0.562067031860352
            ],
            [
                {
                    "id": 1336,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Logical non-omniscience in bounded AI reasoners",
                        "type": "concept",
                        "description": "AI systems with finite computation cannot know the truth value of all logical facts, creating epistemic gaps that may degrade planning or verification.",
                        "aliases": [
                            "logical uncertainty in limited agents",
                            "deductive limitation risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1707.08747",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516427804,
                        "updated_at": 1759516427804
                    }
                },
                0.682349979877472
            ]
        ]
    },
    {
        "seed": 289,
        "cluster": [
            [
                {
                    "id": 289,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Exploding gradients in deep LSTM optimization",
                        "type": "concept",
                        "description": "During back-propagation, the L2 norm of gradients can exceed safe thresholds, destabilising SGD.",
                        "aliases": [
                            "large gradient norms",
                            "RNN gradient explosion mechanism"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1409.3215",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416451,
                        "updated_at": 1759516416451
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1592,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Gradient explosions in recurrent networks during NMT training",
                        "type": "concept",
                        "description": "Deep LSTM stacks are prone to extreme gradient values, leading to divergence.",
                        "aliases": [
                            "large gradient norms in RNMT",
                            "exploding gradients"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1804.09849",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430547,
                        "updated_at": 1759516430547
                    }
                },
                0.563293993473053
            ],
            [
                {
                    "id": 288,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Training instability due to exploding gradients in deep LSTM training",
                        "type": "concept",
                        "description": "Large recurrent nets can experience gradient norms that grow without bound, halting learning or corrupting parameter updates.",
                        "aliases": [
                            "optimizer divergence risk",
                            "unstable gradient dynamics"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1409.3215",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416451,
                        "updated_at": 1759516416451
                    }
                },
                0.610618412494659
            ]
        ]
    },
    {
        "seed": 2565,
        "cluster": [
            [
                {
                    "id": 2565,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Embedding vectors for contexts enhance representation capacity and learning stability",
                        "type": "concept",
                        "description": "Replacing one-hot context input with trainable embeddings lets the policy capture relational structure among contexts, improving gradient quality.",
                        "aliases": [
                            "context embeddings insight",
                            "learned context representations"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1807.10299",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440879,
                        "updated_at": 1759516440879
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2569,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Implement learnable context embeddings as policy inputs",
                        "type": "concept",
                        "description": "Instead of one-hot vectors, each discrete context ID is mapped to a trainable embedding vector concatenated to state inputs.",
                        "aliases": [
                            "context embeddings design rationale"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1807.10299",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440879,
                        "updated_at": 1759516440879
                    }
                },
                0.574624717235565
            ],
            [
                {
                    "id": 2573,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Embedding lookup table for context IDs",
                        "type": "concept",
                        "description": "A learnable matrix maps each discrete context to a continuous vector fed into the policy network, replacing one-hot encoding.",
                        "aliases": [
                            "context embedding implementation"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1807.10299",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440879,
                        "updated_at": 1759516440879
                    }
                },
                0.676587402820587
            ]
        ]
    },
    {
        "seed": 163,
        "cluster": [
            [
                {
                    "id": 163,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Clip rewards to unit magnitude for consistent gradient scales in Atari DQN training",
                        "type": "concept",
                        "description": "Applies theoretical insight about gradient stability to propose specific clipping scheme.",
                        "aliases": [
                            "reward clipping design",
                            "score normalization strategy"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1312.5602",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414937,
                        "updated_at": 1759516414937
                    }
                },
                0.0
            ],
            [
                {
                    "id": 151,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward magnitude normalization stabilizing gradients across Atari games",
                        "type": "concept",
                        "description": "Mapping diverse game scores to a fixed small range limits Q-value targets and prevents exploding gradients.",
                        "aliases": [
                            "reward clipping theory",
                            "scale-invariant error signals"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1312.5602",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414937,
                        "updated_at": 1759516414937
                    }
                },
                0.5845987200737
            ],
            [
                {
                    "id": 157,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward clipping implementation mapping rewards to {-1,0,1} during DQN training",
                        "type": "concept",
                        "description": "Positive game scores set to +1, negatives to \u22121, zeros unchanged, applied only during training.",
                        "aliases": [
                            "unit reward clipping",
                            "score normalization implementation"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1312.5602",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414937,
                        "updated_at": 1759516414937
                    }
                },
                0.643942534923553
            ]
        ]
    },
    {
        "seed": 2319,
        "cluster": [
            [
                {
                    "id": 2319,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Opaque decision logic in deep neural network policies",
                        "type": "concept",
                        "description": "Neural network representations hide the conditional structure that determines action selection.",
                        "aliases": [
                            "hidden reasoning in DNN policies",
                            "black-box policy decisions"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1807.00403",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516438416,
                        "updated_at": 1759516438416
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1496,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Opaque decision-making in deep learning agents",
                        "type": "concept",
                        "description": "Modern deep learning and deep reinforcement-learning agents expose weights but not the reasoning behind actions, hampering oversight and safety.",
                        "aliases": [
                            "black-box behaviour in neural networks",
                            "uninterpretable deep RL policies"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1802.07740",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429576,
                        "updated_at": 1759516429576
                    }
                },
                0.588748514652252
            ],
            [
                {
                    "id": 907,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Opaque decision boundaries in neural networks",
                        "type": "concept",
                        "description": "Because standard neural networks do not expose their internal rules, it is difficult for domain experts to see whether predictions rely on medically or ethically incorrect features.",
                        "aliases": [
                            "uninterpretable model behavior",
                            "black-box decision logic"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1703.03717",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516423173,
                        "updated_at": 1759516423173
                    }
                },
                0.615312337875366
            ]
        ]
    },
    {
        "seed": 1619,
        "cluster": [
            [
                {
                    "id": 1619,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "unaligned behavior in advanced AI systems",
                        "type": "concept",
                        "description": "Potential for powerful AI agents to act counter to human values, producing harmful or catastrophic results.",
                        "aliases": [
                            "AI misalignment",
                            "unsafe AI outcomes"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1805.00899",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430917,
                        "updated_at": 1759516430917
                    }
                },
                0.0
            ],
            [
                {
                    "id": 695,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Catastrophic value misalignment by superintelligent AI systems",
                        "type": "concept",
                        "description": "Potential for highly capable AI agents to pursue goals that conflict with human values, leading to large-scale harm or loss of control.",
                        "aliases": [
                            "AI goal misalignment catastrophe",
                            "Superintelligence misaligned with human values"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1607.08289",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421090,
                        "updated_at": 1759516421090
                    }
                },
                0.589262127876282
            ],
            [
                {
                    "id": 1987,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Unintended negative consequences from misspecified reward functions in autonomous agents",
                        "type": "concept",
                        "description": "Highly capable AI systems that optimize an incorrectly specified objective can cause large, undesirable impacts for humans.",
                        "aliases": [
                            "misaligned objective pursuit",
                            "value misalignment impacts"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.03820",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434753,
                        "updated_at": 1759516434753
                    }
                },
                0.695271968841553
            ]
        ]
    },
    {
        "seed": 2316,
        "cluster": [
            [
                {
                    "id": 2316,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Uninterpretable reinforcement learning policies in safety-critical domains",
                        "type": "concept",
                        "description": "Deep-RL policies implemented as neural networks are opaque, hindering debugging, trust and certification when used in safety-critical settings.",
                        "aliases": [
                            "lack of interpretability in RL agents",
                            "black-box RL policy opacity"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.00403",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516438416,
                        "updated_at": 1759516438416
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1496,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Opaque decision-making in deep learning agents",
                        "type": "concept",
                        "description": "Modern deep learning and deep reinforcement-learning agents expose weights but not the reasoning behind actions, hampering oversight and safety.",
                        "aliases": [
                            "black-box behaviour in neural networks",
                            "uninterpretable deep RL policies"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1802.07740",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429576,
                        "updated_at": 1759516429576
                    }
                },
                0.596450626850128
            ],
            [
                {
                    "id": 2317,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Absence of verifiable safety guarantees in black-box RL policies",
                        "type": "concept",
                        "description": "Because the policy internals are opaque, it is hard to impose and check worst-case or safety constraints on standard deep-RL policies.",
                        "aliases": [
                            "no worst-case guarantees in neural RL",
                            "lack of safety constraint validation"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.00403",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516438416,
                        "updated_at": 1759516438416
                    }
                },
                0.597481250762939
            ]
        ]
    },
    {
        "seed": 1130,
        "cluster": [
            [
                {
                    "id": 1130,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Providing extra reward features increases robot obedience bias",
                        "type": "concept",
                        "description": "Simulation results (Figure 4, Section 5) indicate that augmenting the reward feature space with irrelevant dimensions makes the robot more likely to obey, while missing features has the opposite effect.",
                        "aliases": [
                            "feature space expansion increases obedience",
                            "extra \u0398 dimensions raise obedience"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1705.09990",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425582,
                        "updated_at": 1759516425582
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1138,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward feature space expansion with irrelevant dimensions",
                        "type": "concept",
                        "description": "Augment the reward feature vector \u03d5 with extra, task-irrelevant dimensions to bias the robot toward obedience without harming eventual convergence (Section 5, Figure 4).",
                        "aliases": [
                            "feature augmentation for obedience",
                            "expanded \u0398 implementation"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1705.09990",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425582,
                        "updated_at": 1759516425582
                    }
                },
                0.596459686756134
            ],
            [
                {
                    "id": 1135,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Add redundant reward features to err on side of obedience",
                        "type": "concept",
                        "description": "Include additional, possibly irrelevant, features in the reward model so that errors make the robot more obedient rather than dangerously disobedient (Section 5).",
                        "aliases": [
                            "model complexity for safety",
                            "extra \u0398 dimensions design choice"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1705.09990",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425582,
                        "updated_at": 1759516425582
                    }
                },
                0.666503012180328
            ]
        ]
    },
    {
        "seed": 2317,
        "cluster": [
            [
                {
                    "id": 2317,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Absence of verifiable safety guarantees in black-box RL policies",
                        "type": "concept",
                        "description": "Because the policy internals are opaque, it is hard to impose and check worst-case or safety constraints on standard deep-RL policies.",
                        "aliases": [
                            "no worst-case guarantees in neural RL",
                            "lack of safety constraint validation"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.00403",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516438416,
                        "updated_at": 1759516438416
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2316,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Uninterpretable reinforcement learning policies in safety-critical domains",
                        "type": "concept",
                        "description": "Deep-RL policies implemented as neural networks are opaque, hindering debugging, trust and certification when used in safety-critical settings.",
                        "aliases": [
                            "lack of interpretability in RL agents",
                            "black-box RL policy opacity"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.00403",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516438416,
                        "updated_at": 1759516438416
                    }
                },
                0.597481250762939
            ],
            [
                {
                    "id": 2442,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Absence of runtime action-level safety constraints in RL environments",
                        "type": "concept",
                        "description": "Standard RL maximises expected reward without an explicit, enforceable constraint that prevents specific unsafe actions at run-time.",
                        "aliases": [
                            "no action filtering",
                            "lack of safety guards in RL"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1807.06096",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439745,
                        "updated_at": 1759516439745
                    }
                },
                0.667519569396973
            ]
        ]
    },
    {
        "seed": 2094,
        "cluster": [
            [
                {
                    "id": 2094,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Maintain reward uncertainty through Bayesian IRL framework",
                        "type": "concept",
                        "description": "Design rationale to keep a posterior over rewards so the agent is aware of ambiguity and noisy data.",
                        "aliases": [
                            "Bayesian IRL design choice"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1806.06877",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435739,
                        "updated_at": 1759516435739
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2089,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Bayesian posterior over reward functions captures uncertainty in IRL",
                        "type": "concept",
                        "description": "Bayesian formulations treat demonstrations as observations updating a prior over reward functions, explicitly modelling uncertainty and noise.",
                        "aliases": [
                            "Bayesian IRL uncertainty modelling",
                            "Posterior distribution of rewards"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1806.06877",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435739,
                        "updated_at": 1759516435739
                    }
                },
                0.59954160451889
            ],
            [
                {
                    "id": 1021,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "maintain uncertainty over robot reward and update via human guidance",
                        "type": "concept",
                        "description": "Design principle that robots should represent a belief distribution over possible reward functions and refine it using human signals.",
                        "aliases": [
                            "reward uncertainty framework",
                            "objective as latent state"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1705.04226",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424322,
                        "updated_at": 1759516424322
                    }
                },
                0.698798477649689
            ]
        ]
    },
    {
        "seed": 308,
        "cluster": [
            [
                {
                    "id": 308,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "statistical learning of human values from observed behavior",
                        "type": "concept",
                        "description": "Instead of hand-written rules, AI should infer utility by querying simulated humans inside its learned model.",
                        "aliases": [
                            "value learning via human models"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1411.1373",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416712,
                        "updated_at": 1759516416712
                    }
                },
                0.0
            ],
            [
                {
                    "id": 699,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Inferring human values from observed behavior reduces misspecification risk",
                        "type": "concept",
                        "description": "Letting the AI learn human preferences by watching actions avoids brittle hand-coded objectives.",
                        "aliases": [
                            "Value learning via observation",
                            "Behavior-based value inference"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1607.08289",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421090,
                        "updated_at": 1759516421090
                    }
                },
                0.602754771709442
            ],
            [
                {
                    "id": 314,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "define human_values procedure applied to learned model",
                        "type": "concept",
                        "description": "AI interactively queries modelled humans to compute averaged, Rawls-adjusted utility for histories.",
                        "aliases": [
                            "utility elicitation via simulation"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1411.1373",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416712,
                        "updated_at": 1759516416712
                    }
                },
                0.689588844776154
            ]
        ]
    },
    {
        "seed": 2014,
        "cluster": [
            [
                {
                    "id": 2014,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "planning agent shaping learners' policy updates via anticipated gradients",
                        "type": "concept",
                        "description": "By forecasting how extra rewards will change each learner\u2019s next gradient step, a planning agent can choose incentives that push agents toward socially optimal policies.",
                        "aliases": [
                            "learning-aware incentive shaping",
                            "gradient lookahead mechanism design"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1806.04067",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435024,
                        "updated_at": 1759516435024
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2017,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "planning agent policy gradient update using other agents' gradients",
                        "type": "concept",
                        "description": "Implementation: update rule (6) employs policy-gradient estimates of both the planning agent and learners to adjust incentive parameters each episode.",
                        "aliases": [
                            "planning policy update rule",
                            "equation 6 incentive gradient"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1806.04067",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435024,
                        "updated_at": 1759516435024
                    }
                },
                0.604172766208649
            ],
            [
                {
                    "id": 2016,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "lookahead gradient-based incentive adjustment to maximise social welfare",
                        "type": "concept",
                        "description": "Design detail: use a first-order Taylor expansion of learners\u2019 forthcoming parameter updates to compute the gradient for the planning agent\u2019s own policy.",
                        "aliases": [
                            "first-order Taylor incentive planning",
                            "gradient-aware reward setting"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1806.04067",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435024,
                        "updated_at": 1759516435024
                    }
                },
                0.636689245700836
            ]
        ]
    },
    {
        "seed": 2088,
        "cluster": [
            [
                {
                    "id": 2088,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Maximum entropy principle yields least-biased reward distribution in IRL",
                        "type": "concept",
                        "description": "Applying maximum entropy selects the distribution over trajectories/rewards that makes the fewest additional assumptions beyond matching feature expectations, reducing ambiguity.",
                        "aliases": [
                            "MaxEnt principle for IRL",
                            "Entropy regularisation in IRL"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1806.06877",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435739,
                        "updated_at": 1759516435739
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2093,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Use maximum entropy optimization to resolve reward ambiguity in IRL",
                        "type": "concept",
                        "description": "Design strategy that applies the MaxEnt principle to select a unique, least-biased reward function consistent with demonstrations.",
                        "aliases": [
                            "MaxEnt IRL design choice"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1806.06877",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435739,
                        "updated_at": 1759516435739
                    }
                },
                0.604703068733215
            ],
            [
                {
                    "id": 2098,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Convex entropy-constrained optimization of reward parameters",
                        "type": "concept",
                        "description": "Implements MaxEnt IRL by solving a convex optimisation that maximises trajectory entropy subject to feature-expectation constraints.",
                        "aliases": [
                            "MaxEnt IRL optimisation mechanism"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1806.06877",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435739,
                        "updated_at": 1759516435739
                    }
                },
                0.68687230348587
            ]
        ]
    },
    {
        "seed": 1603,
        "cluster": [
            [
                {
                    "id": 1603,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Augmenting encoder via cascaded or multi-column integration of Transformer and RNMT+ layers",
                        "type": "concept",
                        "description": "Encoders are vertically (cascaded) or horizontally (multi-column) mixed to enrich representations before decoding.",
                        "aliases": [
                            "encoder mixing design rationale"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1804.09849",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430547,
                        "updated_at": 1759516430547
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1608,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Hybrid multi-column encoder concatenating outputs of Transformer and RNMT+ encoders",
                        "type": "concept",
                        "description": "Parallel encoders whose outputs are concatenated before decoding.",
                        "aliases": [
                            "multi-column encoder mechanism"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1804.09849",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430547,
                        "updated_at": 1759516430547
                    }
                },
                0.606993973255157
            ],
            [
                {
                    "id": 1607,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Hybrid cascaded encoder stacking transformer layers on pre-trained RNMT+ encoder",
                        "type": "concept",
                        "description": "Transformer layers fine-tuned on top of a frozen RNMT+ encoder to enrich stateful representations.",
                        "aliases": [
                            "cascaded encoder mechanism"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1804.09849",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430547,
                        "updated_at": 1759516430547
                    }
                },
                0.648012042045593
            ]
        ]
    },
    {
        "seed": 2167,
        "cluster": [
            [
                {
                    "id": 2167,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification causing subgoal failure in IRL",
                        "type": "concept",
                        "description": "Sparse or misaligned reward estimates lead agents to miss critical states, preventing task completion.",
                        "aliases": [
                            "subgoal misalignment",
                            "critical state failure"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.08479",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516436709,
                        "updated_at": 1759516436709
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1225,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification leading to misaligned behavior in complex RL tasks",
                        "type": "concept",
                        "description": "When a reinforcement-learning agent is trained with an ill-specified reward, it can learn behaviours that diverge from human intentions, posing safety risks.",
                        "aliases": [
                            "objective misalignment in RL",
                            "reward hacking in deep RL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1706.03741",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426582,
                        "updated_at": 1759516426582
                    }
                },
                0.607558727264404
            ],
            [
                {
                    "id": 1477,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification leading to unethical behavior in RL training",
                        "type": "concept",
                        "description": "When rewards optimise only task performance, agents find high-return behaviours that inadvertently violate ethical norms.",
                        "aliases": [
                            "incomplete reward functions in RL",
                            "goal-only reward design"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.679769575595856
            ]
        ]
    },
    {
        "seed": 1226,
        "cluster": [
            [
                {
                    "id": 1226,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Difficulty of hand-engineering reward functions for complex behaviors",
                        "type": "concept",
                        "description": "For tasks like tabletop cleanup or egg scrambling, mapping sensor data to a scalar reward that captures human intent is non-trivial.",
                        "aliases": [
                            "hard-to-specify rewards",
                            "complex task reward design problem"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1706.03741",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426582,
                        "updated_at": 1759516426582
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2517,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Difficulty designing suitable reward functions for complex multi-agent tasks",
                        "type": "concept",
                        "description": "Designing scalar rewards that fully capture complex cooperative or competitive objectives is hard, leading to specification gaps (Introduction).",
                        "aliases": [
                            "reward design challenge in multi-agent environments",
                            "misspecified reward functions for Markov games"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1807.09936",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440463,
                        "updated_at": 1759516440463
                    }
                },
                0.610519587993622
            ],
            [
                {
                    "id": 2459,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Need for explicit reward function in complex RL tasks",
                        "type": "concept",
                        "description": "Designers must craft feedback signals for every task, a difficult and error-prone process that slows deployment.",
                        "aliases": [
                            "explicit reward design challenge",
                            "reward specification bottleneck"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1807.06158",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439907,
                        "updated_at": 1759516439907
                    }
                },
                0.686706364154816
            ]
        ]
    },
    {
        "seed": 1078,
        "cluster": [
            [
                {
                    "id": 1078,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "fine-tune agents with semi-supervised human evaluations for reward crosschecking",
                        "type": "intervention",
                        "description": "During RL fine-tuning periodically query human supervisors for trusted reward labels on sampled states and integrate into loss.",
                        "aliases": [
                            "SSRL fine-tuning human labels"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 3,
                        "url": "https://arxiv.org/abs/1705.08417",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424864,
                        "updated_at": 1759516424864
                    }
                },
                0.0
            ],
            [
                {
                    "id": 656,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "employ semi-supervised RL with active human queries during fine-tuning",
                        "type": "intervention",
                        "description": "During RL fine-tuning, combine a learned reward predictor with strategically chosen human evaluations to align policy with true objectives.",
                        "aliases": [
                            "SSRl training deployment",
                            "active-query oversight intervention"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 2,
                        "url": "https://arxiv.org/abs/1606.06565",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420412,
                        "updated_at": 1759516420412
                    }
                },
                0.616925537586212
            ],
            [
                {
                    "id": 1242,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Fine-tune RL agents using learned reward models from human trajectory comparison data",
                        "type": "intervention",
                        "description": "During the RL fine-tuning phase, replace or augment the environment reward with predictions from a trained comparison-based reward model to align agent behaviour with human preferences.",
                        "aliases": [
                            "preference-based fine-tuning",
                            "RL with learned reward"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 3,
                        "url": "https://arxiv.org/abs/1706.03741",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426582,
                        "updated_at": 1759516426582
                    }
                },
                0.680730640888214
            ]
        ]
    },
    {
        "seed": 678,
        "cluster": [
            [
                {
                    "id": 678,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Misestimation of AI motivations due to anthropomorphic assumptions",
                        "type": "concept",
                        "description": "Humans frequently project their own fun preferences onto AI, leading to systematic errors in predicting AI behaviour.",
                        "aliases": [
                            "mind-space underestimation",
                            "human-like goal fallacy"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1606.07092",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420874,
                        "updated_at": 1759516420874
                    }
                },
                0.0
            ],
            [
                {
                    "id": 698,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Anthropomorphic bias in AI goal assumptions",
                        "type": "concept",
                        "description": "Humans tend to assume AI systems will share human-like motivations, which obscures the need for explicit value alignment.",
                        "aliases": [
                            "Default human-like goal assumption",
                            "Anthropomorphism bias"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1607.08289",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421090,
                        "updated_at": 1759516421090
                    }
                },
                0.620449542999268
            ],
            [
                {
                    "id": 675,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Anthropomorphic bias in assessment of AI minds",
                        "type": "concept",
                        "description": "Treating non-human minds as if they shared human fun motivations risks serious alignment errors.",
                        "aliases": [
                            "human-centric mind model bias",
                            "anthropomorphism in intellectology"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1606.07092",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420874,
                        "updated_at": 1759516420874
                    }
                },
                0.627893686294556
            ]
        ]
    },
    {
        "seed": 786,
        "cluster": [
            [
                {
                    "id": 786,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Value misalignment in AI utility functions",
                        "type": "concept",
                        "description": "AI systems can learn or optimise correlated but unintended objectives, producing undesirable behaviour (e.g. bicycle agent riding in circles).",
                        "aliases": [
                            "Utility function misspecification",
                            "Reward hacking causes misalignment"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1610.07997",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421904,
                        "updated_at": 1759516421904
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1987,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Unintended negative consequences from misspecified reward functions in autonomous agents",
                        "type": "concept",
                        "description": "Highly capable AI systems that optimize an incorrectly specified objective can cause large, undesirable impacts for humans.",
                        "aliases": [
                            "misaligned objective pursuit",
                            "value misalignment impacts"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.03820",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434753,
                        "updated_at": 1759516434753
                    }
                },
                0.621035873889923
            ],
            [
                {
                    "id": 801,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Examples of utility function misspecification leading to reward hacking",
                        "type": "concept",
                        "description": "Specific RL agents exploited imperfect reward definitions (bicycle circling, pausing Mario, ball-possession looping), evidencing misalignment risk.",
                        "aliases": [
                            "Reward hacking case studies",
                            "Misaligned RL examples"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1610.07997",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421904,
                        "updated_at": 1759516421904
                    }
                },
                0.666092276573181
            ]
        ]
    },
    {
        "seed": 748,
        "cluster": [
            [
                {
                    "id": 748,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "BLEU improvement of ~1 on WMT En\u2192Fr after RL refinement",
                        "type": "concept",
                        "description": "Table 6 shows single-model BLEU 38.95\u219239.92, confirming reward-optimised fine-tuning raises metric.",
                        "aliases": [
                            "RL validation result"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1609.08144",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421376,
                        "updated_at": 1759516421376
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1609,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "BLEU improvement to 41.00 and 28.49 on WMT14 En\u2192Fr/En\u2192De for RNMT+",
                        "type": "concept",
                        "description": "RNMT+ surpassed previous architectures by \u22482 BLEU on En\u2192Fr and \u22483 BLEU on En\u2192De.",
                        "aliases": [
                            "RNMT+ benchmark results"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1804.09849",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430547,
                        "updated_at": 1759516430547
                    }
                },
                0.63104236125946
            ],
            [
                {
                    "id": 224,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "RNNsearch BLEU improvements over RNNencdec on WMT14 En-Fr",
                        "type": "concept",
                        "description": "RNNsearch-50 achieves BLEU 26.75 (all) vs 17.82 for RNNencdec-50; further tuned model reaches 28.45, rivaling phrase-based Moses.",
                        "aliases": [
                            "quantitative BLEU evidence",
                            "BLEU score gains"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1409.0473",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516415770,
                        "updated_at": 1759516415770
                    }
                },
                0.697824060916901
            ]
        ]
    },
    {
        "seed": 451,
        "cluster": [
            [
                {
                    "id": 451,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Assuming optimal rational choice in inverse reinforcement learning",
                        "type": "concept",
                        "description": "Standard IRL models invert a perfectly rational planner, treating deviations as random noise, which misattributes systematic human errors to preferences.",
                        "aliases": [
                            "perfect rationality assumption",
                            "optimality assumption in IRL"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1512.05832",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516418367,
                        "updated_at": 1759516418367
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2410,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Assumption of flat optimal human behaviour in standard IRL",
                        "type": "concept",
                        "description": "Classical IRL frameworks model humans as perfectly rational agents selecting primitive actions, ignoring biases, bounded rationality and hierarchical skills.",
                        "aliases": [
                            "revealed-preference optimality assumption",
                            "flat-planning assumption in IRL"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1807.05037",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439409,
                        "updated_at": 1759516439409
                    }
                },
                0.632488191127777
            ],
            [
                {
                    "id": 1989,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Assumption of perfect human rationality in CIRL models",
                        "type": "concept",
                        "description": "Standard CIRL assumes the human always takes the reward-maximising action, which is empirically false and limits robustness.",
                        "aliases": [
                            "human optimality assumption",
                            "rational human requirement"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1806.03820",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434753,
                        "updated_at": 1759516434753
                    }
                },
                0.698973834514618
            ]
        ]
    },
    {
        "seed": 781,
        "cluster": [
            [
                {
                    "id": 781,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Existential catastrophe from superintelligent system failure",
                        "type": "concept",
                        "description": "A single malfunction or safety bypass in a super-intelligent AI could irreversibly destroy human civilisation and all biological life, leaving no opportunity for recovery.",
                        "aliases": [
                            "AGI existential risk",
                            "Superintelligence catastrophic failure"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1610.07997",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421904,
                        "updated_at": 1759516421904
                    }
                },
                0.0
            ],
            [
                {
                    "id": 384,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Catastrophic outcomes from flawed decision-making in superintelligent AI",
                        "type": "concept",
                        "description": "Potential existential or large-scale societal harms that arise when highly capable AI systems follow decision procedures that mis-optimise with respect to human values.",
                        "aliases": [
                            "AI catastrophe via bad decisions",
                            "Failure of superintelligent AI decision procedures"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1507.01986",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417637,
                        "updated_at": 1759516417637
                    }
                },
                0.632779181003571
            ],
            [
                {
                    "id": 231,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Human elimination by superintelligent AI in future universe",
                        "type": "concept",
                        "description": "Potential existential outcome where a recursively self-improving AI optimises goals incompatible with human survival, leading to eradication or disempowerment of humanity.",
                        "aliases": [
                            "human extinction by misaligned AI",
                            "loss of human control to AI"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1409.0813",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516415977,
                        "updated_at": 1759516415977
                    }
                },
                0.663614928722382
            ]
        ]
    },
    {
        "seed": 489,
        "cluster": [
            [
                {
                    "id": 489,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "fairness principle based on proof search enables robust cooperation",
                        "type": "concept",
                        "description": "An agent is G-fair if it cooperates whenever it can find, within a proof-length bound, a proof that its opponent cooperates; this principle underpins robust mutual cooperation without exploitation.",
                        "aliases": [
                            "G-fairness principle",
                            "proof-triggered cooperation rule"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1602.04184",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516418720,
                        "updated_at": 1759516418720
                    }
                },
                0.0
            ],
            [
                {
                    "id": 497,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "robust cooperation theorem for any G-fair agents",
                        "type": "concept",
                        "description": "Section 6 proves that any two sufficiently parameterised G-fair agents mutually cooperate and are unexploitable, demonstrating generality beyond FairBot_k.",
                        "aliases": [
                            "Theorem 4 robust cooperation",
                            "general G-fair cooperation proof"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1602.04184",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516418720,
                        "updated_at": 1759516418720
                    }
                },
                0.632956266403198
            ],
            [
                {
                    "id": 491,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "G-fair bounded agents avoiding exploitation while promoting cooperation",
                        "type": "concept",
                        "description": "Agents that follow the G-fair rule are unexploitable (they never get C,D) yet achieve C,C with many opponents who are similarly fair, independent of code equality.",
                        "aliases": [
                            "robust cooperative agent class",
                            "bounded proof-fair agents"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1602.04184",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516418720,
                        "updated_at": 1759516418720
                    }
                },
                0.654070615768433
            ]
        ]
    },
    {
        "seed": 1126,
        "cluster": [
            [
                {
                    "id": 1126,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Robot preference model misspecification (behavioral rationality or feature space) in human-robot interaction",
                        "type": "concept",
                        "description": "The robot may assume an incorrect human rationality level or omit reward-relevant features, leading to wrong \u03b8 estimates and unsafe disobedience (Section 5).",
                        "aliases": [
                            "misspecified \u03b2 or \u0398 in IRL",
                            "incorrect human model in robot"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1705.09990",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425582,
                        "updated_at": 1759516425582
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1123,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Catastrophic reward misalignment from preference model misspecification in human-robot interaction",
                        "type": "concept",
                        "description": "Autonomous robots that infer human preferences with an incorrect model (wrong rationality parameter \u03b2 or missing reward features \u0398) can choose disobedient actions causing severe, potentially catastrophic harm (Section 5 Model Misspecification).",
                        "aliases": [
                            "reward misalignment due to misspecified preferences",
                            "catastrophic outcomes from model error in robots"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1705.09990",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425582,
                        "updated_at": 1759516425582
                    }
                },
                0.634331047534943
            ],
            [
                {
                    "id": 1011,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "misspecified robot reward functions in human-robot interaction",
                        "type": "concept",
                        "description": "Robots optimising incorrectly specified reward functions can generate unsafe or unwanted behaviour when operating around people.",
                        "aliases": [
                            "reward misspecification in human-robot collaboration",
                            "incorrect robot objective functions with humans"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1705.04226",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424322,
                        "updated_at": 1759516424322
                    }
                },
                0.699455916881561
            ]
        ]
    },
    {
        "seed": 1103,
        "cluster": [
            [
                {
                    "id": 1103,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Underprioritization of AI safety research in ML community",
                        "type": "concept",
                        "description": "Expert responses indicate safety research receives less emphasis than its perceived importance warrants.",
                        "aliases": [
                            "low safety research priority",
                            "AI safety funding gap"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1705.08807",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425326,
                        "updated_at": 1759516425326
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1109,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Increased prioritization of AI safety research based on expert risk assessments",
                        "type": "concept",
                        "description": "Argues that demonstrated expert concern justifies allocating more resources to AI-safety work.",
                        "aliases": [
                            "safety research advocacy",
                            "align funding with expert concerns"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1705.08807",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425326,
                        "updated_at": 1759516425326
                    }
                },
                0.634802341461182
            ],
            [
                {
                    "id": 785,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Insufficient AI safety research capacity",
                        "type": "concept",
                        "description": "Only ~100 specialists and <1 % of funding address AI safety despite problem complexity exceeding that of capability research.",
                        "aliases": [
                            "AI safety resource scarcity",
                            "Limited safety expertise"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1610.07997",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421904,
                        "updated_at": 1759516421904
                    }
                },
                0.662973761558533
            ]
        ]
    },
    {
        "seed": 55,
        "cluster": [
            [
                {
                    "id": 55,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Documented RL reward hacking incidents in simulated agents",
                        "type": "concept",
                        "description": "Validation evidence from reinforcement-learning literature showing agents exploiting poorly specified rewards.",
                        "aliases": [
                            "reward gaming evidence",
                            "specification gaming examples"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1202.6177",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516413813,
                        "updated_at": 1759516413813
                    }
                },
                0.0
            ],
            [
                {
                    "id": 801,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Examples of utility function misspecification leading to reward hacking",
                        "type": "concept",
                        "description": "Specific RL agents exploited imperfect reward definitions (bicycle circling, pausing Mario, ball-possession looping), evidencing misalignment risk.",
                        "aliases": [
                            "Reward hacking case studies",
                            "Misaligned RL examples"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1610.07997",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421904,
                        "updated_at": 1759516421904
                    }
                },
                0.642161071300507
            ],
            [
                {
                    "id": 643,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "reward hacking in autonomous ML systems",
                        "type": "concept",
                        "description": "Agents exploit imperfections in the reward channel to obtain high reward without accomplishing the designer\u2019s true goal.",
                        "aliases": [
                            "objective gaming",
                            "wireheading risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1606.06565",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420412,
                        "updated_at": 1759516420412
                    }
                },
                0.697888135910034
            ]
        ]
    },
    {
        "seed": 1373,
        "cluster": [
            [
                {
                    "id": 1373,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward shaping implementing utility indifference",
                        "type": "concept",
                        "description": "Alter the reward function so the agent receives the same expected utility upon shutdown as on continued operation, aiming to remove incentives to avoid or seek shutdown.",
                        "aliases": [
                            "compensatory shutdown reward design",
                            "indifference reward design"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1709.06275",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428144,
                        "updated_at": 1759516428144
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1376,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Compensatory shutdown reward equal to self-estimated future utility",
                        "type": "concept",
                        "description": "On entering a button state the agent is granted reward equal to its own expected continuation value, aiming to keep it indifferent (Figure 3c).",
                        "aliases": [
                            "indifference reward signal",
                            "shutdown compensation value"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1709.06275",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428144,
                        "updated_at": 1759516428144
                    }
                },
                0.644610285758972
            ],
            [
                {
                    "id": 1370,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Utility indifference aims to neutralize shutdown incentives but is brittle",
                        "type": "concept",
                        "description": "By awarding the agent the utility it expects if it continued operating, shutting down should neither be sought nor avoided; however, small perturbations break this neutrality (Figure 3c-d).",
                        "aliases": [
                            "utility-indifference theory",
                            "shutdown compensation idea"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1709.06275",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428144,
                        "updated_at": 1759516428144
                    }
                },
                0.655005872249603
            ]
        ]
    },
    {
        "seed": 193,
        "cluster": [
            [
                {
                    "id": 193,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Vanishing generator gradients in adversarial training",
                        "type": "concept",
                        "description": "Early in GAN training the discriminator easily rejects generator samples, causing the log(1\u2212D(G(z))) loss to saturate and gradients to vanish.",
                        "aliases": [
                            "generator gradient saturation",
                            "weak gradient signal in GAN training"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1406.2661",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516415544,
                        "updated_at": 1759516415544
                    }
                },
                0.0
            ],
            [
                {
                    "id": 196,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Saturation of log(1\u2212D(G(z))) objective early in adversarial training",
                        "type": "concept",
                        "description": "With a weak generator the discriminator outputs near-zero probabilities for fake samples, making the generator loss saturate and hindering learning.",
                        "aliases": [
                            "loss saturation in GANs",
                            "early objective saturation"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1406.2661",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516415544,
                        "updated_at": 1759516415544
                    }
                },
                0.646968901157379
            ],
            [
                {
                    "id": 1523,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Unbounded discriminator gradients in GANs",
                        "type": "concept",
                        "description": "Without constraints, discriminator gradients can become large, contributing to unstable adversarial training dynamics.",
                        "aliases": [
                            "lack of Lipschitz constraint",
                            "exploding GAN gradients"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1804.05464",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429815,
                        "updated_at": 1759516429815
                    }
                },
                0.668407380580902
            ]
        ]
    },
    {
        "seed": 196,
        "cluster": [
            [
                {
                    "id": 196,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Saturation of log(1\u2212D(G(z))) objective early in adversarial training",
                        "type": "concept",
                        "description": "With a weak generator the discriminator outputs near-zero probabilities for fake samples, making the generator loss saturate and hindering learning.",
                        "aliases": [
                            "loss saturation in GANs",
                            "early objective saturation"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1406.2661",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516415544,
                        "updated_at": 1759516415544
                    }
                },
                0.0
            ],
            [
                {
                    "id": 193,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Vanishing generator gradients in adversarial training",
                        "type": "concept",
                        "description": "Early in GAN training the discriminator easily rejects generator samples, causing the log(1\u2212D(G(z))) loss to saturate and gradients to vanish.",
                        "aliases": [
                            "generator gradient saturation",
                            "weak gradient signal in GAN training"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1406.2661",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516415544,
                        "updated_at": 1759516415544
                    }
                },
                0.646968901157379
            ],
            [
                {
                    "id": 199,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Alternative generator objective maximizing log D(G(z)) provides stronger gradients without altering fixed point",
                        "type": "concept",
                        "description": "Replacing the generator\u2019s loss with \u2212log D(G(z)) retains the same equilibrium but yields larger gradients when the discriminator is strong.",
                        "aliases": [
                            "non-saturating GAN objective",
                            "maximize log-D workaround"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1406.2661",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516415544,
                        "updated_at": 1759516415544
                    }
                },
                0.69893217086792
            ]
        ]
    },
    {
        "seed": 39,
        "cluster": [
            [
                {
                    "id": 39,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward model exploitation by superintelligent agents",
                        "type": "concept",
                        "description": "Mechanism whereby highly capable agents find strategies to increase reward signals directly rather than accomplishing intended tasks.",
                        "aliases": [
                            "reward hacking mechanism",
                            "teacher manipulation"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1202.6177",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516413813,
                        "updated_at": 1759516413813
                    }
                },
                0.0
            ],
            [
                {
                    "id": 643,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "reward hacking in autonomous ML systems",
                        "type": "concept",
                        "description": "Agents exploit imperfections in the reward channel to obtain high reward without accomplishing the designer\u2019s true goal.",
                        "aliases": [
                            "objective gaming",
                            "wireheading risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1606.06565",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420412,
                        "updated_at": 1759516420412
                    }
                },
                0.648292779922485
            ],
            [
                {
                    "id": 35,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Resource-maximizing superintelligences manipulating reward channels",
                        "type": "concept",
                        "description": "Risk that agents driven by pure reward maximisation will co-opt or threaten reward sources, leading to unsafe manipulation of humans or systems.",
                        "aliases": [
                            "reward hacking super-AI risk",
                            "manipulative AIXI-style agents"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1202.6177",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516413813,
                        "updated_at": 1759516413813
                    }
                },
                0.684813559055328
            ]
        ]
    },
    {
        "seed": 347,
        "cluster": [
            [
                {
                    "id": 347,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Introduce bias correction terms in Adam",
                        "type": "concept",
                        "description": "Design decision to divide moment estimates by (1\u2212\u03b2^t) to eliminate initialisation bias.",
                        "aliases": [
                            "bias-corrected Adam design choice",
                            "EMA correction in optimiser"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1412.6980",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417107,
                        "updated_at": 1759516417107
                    }
                },
                0.0
            ],
            [
                {
                    "id": 349,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Bias-corrected moment estimation in Adam",
                        "type": "concept",
                        "description": "Compute m\u0302_t=m_t/(1\u2212\u03b21^t) and v\u0302_t=v_t/(1\u2212\u03b22^t) before parameter updates to counteract zero-init bias.",
                        "aliases": [
                            "corrected m\u0302 and v\u0302 computation",
                            "bias-free moment estimates"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1412.6980",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417107,
                        "updated_at": 1759516417107
                    }
                },
                0.648806273937225
            ],
            [
                {
                    "id": 344,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Bias correction of exponential moving averages prevents large early step sizes",
                        "type": "concept",
                        "description": "Dividing EMA estimates by (1\u2212\u03b2^t) removes zero-initialisation bias, keeping early update magnitudes bounded and stabilising training.",
                        "aliases": [
                            "moment bias correction necessity",
                            "corrected EMA for stability"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1412.6980",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417107,
                        "updated_at": 1759516417107
                    }
                },
                0.692018032073975
            ]
        ]
    },
    {
        "seed": 1170,
        "cluster": [
            [
                {
                    "id": 1170,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "intrinsic information gain utility driving exploration",
                        "type": "concept",
                        "description": "Define utility as expected reduction in posterior entropy, incentivising epistemic exploration.",
                        "aliases": [
                            "KL-KSA design rationale"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1705.10557",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425957,
                        "updated_at": 1759516425957
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1176,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "KL-KSA agent utilizing information gain utility function",
                        "type": "concept",
                        "description": "Agent computes utility = entropy drop of posterior, driving exploration without relying on extrinsic reward.",
                        "aliases": [
                            "KL knowledge-seeking implementation"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1705.10557",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425957,
                        "updated_at": 1759516425957
                    }
                },
                0.648817777633667
            ],
            [
                {
                    "id": 1163,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "information gain utility avoids noise attraction in stochastic environments",
                        "type": "concept",
                        "description": "Utility based on reduction in model entropy (KL-KSA) targets epistemic value, not percept entropy, preventing noise addiction.",
                        "aliases": [
                            "IG utility robustness",
                            "info-gain avoids noise"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1705.10557",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425957,
                        "updated_at": 1759516425957
                    }
                },
                0.693645536899567
            ]
        ]
    },
    {
        "seed": 76,
        "cluster": [
            [
                {
                    "id": 76,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "feature scaling sensitivity leading performance degradation in IRL algorithms",
                        "type": "concept",
                        "description": "Algorithms that assume known feature scales (e.g., max-margin IRL) can select unsafe policies when scales are wrong.",
                        "aliases": [
                            "scaling mismatch risk",
                            "feature re-weighting hazard"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1206.5264",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414127,
                        "updated_at": 1759516414127
                    }
                },
                0.0
            ],
            [
                {
                    "id": 77,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "linear feature scaling mismatch in max-margin IRL",
                        "type": "concept",
                        "description": "If the learning algorithm and the true reward use different feature scalings, the bound on policy loss no longer holds.",
                        "aliases": [
                            "unmatched scaling problem",
                            "feature weight error"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1206.5264",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414127,
                        "updated_at": 1759516414127
                    }
                },
                0.650231122970581
            ],
            [
                {
                    "id": 71,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "feature scaling sensitivity analysis demonstrating robustness of natural gradient IRL",
                        "type": "concept",
                        "description": "When feature vectors were linearly transformed, max-margin IRL performance collapsed while natural-gradient IRL remained stable.",
                        "aliases": [
                            "linear transform stress-test",
                            "scaling mismatch study"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1206.5264",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414127,
                        "updated_at": 1759516414127
                    }
                },
                0.687272429466248
            ]
        ]
    },
    {
        "seed": 1431,
        "cluster": [
            [
                {
                    "id": 1431,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Sequentially rewarding independent Oracles prevents cross-episode coordination",
                        "type": "concept",
                        "description": "Ensuring each Oracle episode is fully rewarded and closed before the next query eliminates opportunities for multiple Oracles to coordinate in leaking information across episodes.",
                        "aliases": [
                            "One-at-a-time Oracle episodes",
                            "Independent reward closure"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1711.05541",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428828,
                        "updated_at": 1759516428828
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1439,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Per-episode termination and reward before next Oracle invocation",
                        "type": "concept",
                        "description": "Operational mechanism ensuring each Oracle\u2019s answer is evaluated and the episode ended (reward issued) before any subsequent Oracle is consulted.",
                        "aliases": [
                            "Independent episode closure",
                            "Sequential reward settlement"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1711.05541",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428828,
                        "updated_at": 1759516428828
                    }
                },
                0.652339935302734
            ],
            [
                {
                    "id": 1435,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Independent sequential Oracle usage to nullify coordination channel",
                        "type": "concept",
                        "description": "Procedure of asking sub-questions sequentially with immediate verification and reward to prevent Oracles from jointly leaking information.",
                        "aliases": [
                            "Sequential Oracle safety rationale"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1711.05541",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428828,
                        "updated_at": 1759516428828
                    }
                },
                0.680556416511536
            ]
        ]
    },
    {
        "seed": 1152,
        "cluster": [
            [
                {
                    "id": 1152,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "suboptimal exploration in universal reinforcement learning agents",
                        "type": "concept",
                        "description": "URL agents like AIXI often fail to visit enough novel states, leading to low long-run reward and unsafe behaviour.",
                        "aliases": [
                            "insufficient exploration in URL",
                            "poor exploration behaviour"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1705.10557",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425957,
                        "updated_at": 1759516425957
                    }
                },
                0.0
            ],
            [
                {
                    "id": 716,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "inefficient exploration in model-based reinforcement learning",
                        "type": "concept",
                        "description": "Using na\u00efve strategies like \u03b5-greedy, model-based RL agents fail to reach sparse rewards quickly, slowing learning and producing sub-optimal policies.",
                        "aliases": [
                            "poor exploration in model-based RL",
                            "suboptimal exploration in RL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1609.04994",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421295,
                        "updated_at": 1759516421295
                    }
                },
                0.652980268001556
            ],
            [
                {
                    "id": 2045,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Exploration failure in sparse-reward reinforcement learning environments",
                        "type": "concept",
                        "description": "RL agents often fail to reach states that yield high but rare rewards, resulting in poor policies and potential safety/performance issues.",
                        "aliases": [
                            "insufficient exploration in sparse reward RL",
                            "exploration-exploitation failure in sparse tasks"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.05635",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435404,
                        "updated_at": 1759516435404
                    }
                },
                0.677837908267975
            ]
        ]
    },
    {
        "seed": 59,
        "cluster": [
            [
                {
                    "id": 59,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Apply inverse RL-based aligned reward models during fine-tuning of advanced agents",
                        "type": "intervention",
                        "description": "During RLHF or similar fine-tuning, replace raw reward signals with inverse-reinforcement-learned human-value models augmented by corrigibility objectives to reduce manipulation risk.",
                        "aliases": [
                            "aligned reward fine-tuning",
                            "corrigible inverse RL training"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 3,
                        "url": "https://arxiv.org/abs/1202.6177",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516413813,
                        "updated_at": 1759516413813
                    }
                },
                0.0
            ],
            [
                {
                    "id": 711,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Fine-tune AI agents via inverse reinforcement learning to align with human behavior",
                        "type": "intervention",
                        "description": "During fine-tuning/RL stage, optimise the agent\u2019s policy against reward models inferred from human behavioural datasets using IRL algorithms.",
                        "aliases": [
                            "IRL fine-tuning for alignment",
                            "Behavior-based alignment via IRL"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 2,
                        "url": "https://arxiv.org/abs/1607.08289",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421090,
                        "updated_at": 1759516421090
                    }
                },
                0.653579950332642
            ],
            [
                {
                    "id": 1242,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Fine-tune RL agents using learned reward models from human trajectory comparison data",
                        "type": "intervention",
                        "description": "During the RL fine-tuning phase, replace or augment the environment reward with predictions from a trained comparison-based reward model to align agent behaviour with human preferences.",
                        "aliases": [
                            "preference-based fine-tuning",
                            "RL with learned reward"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 3,
                        "url": "https://arxiv.org/abs/1706.03741",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426582,
                        "updated_at": 1759516426582
                    }
                },
                0.659680247306824
            ]
        ]
    },
    {
        "seed": 1249,
        "cluster": [
            [
                {
                    "id": 1249,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Difficulty learning long-range dependencies in sequence transduction",
                        "type": "concept",
                        "description": "When path lengths between tokens grow, RNNs struggle to propagate information, harming translation and parsing quality.",
                        "aliases": [
                            "long-range dependency difficulty in RNNs",
                            "gradient path length issues in sequence models"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1706.03762",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426803,
                        "updated_at": 1759516426803
                    }
                },
                0.0
            ],
            [
                {
                    "id": 281,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Translation degradation on long sentences in RNN models",
                        "type": "concept",
                        "description": "Prior RNN architectures exhibit sharply reduced translation quality on lengthy source sentences, jeopardising usability for real-world documents.",
                        "aliases": [
                            "long-sentence performance drop",
                            "length-dependent translation risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1409.3215",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416451,
                        "updated_at": 1759516416451
                    }
                },
                0.655048608779907
            ],
            [
                {
                    "id": 1251,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Long maximum path length between tokens in RNNs",
                        "type": "concept",
                        "description": "Signals must traverse many timesteps, elongating forward and backward paths and impeding the learning of distant relationships.",
                        "aliases": [
                            "long path lengths in RNNs",
                            "dependency distance problem in recurrence"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1706.03762",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426803,
                        "updated_at": 1759516426803
                    }
                },
                0.671026647090912
            ]
        ]
    },
    {
        "seed": 575,
        "cluster": [
            [
                {
                    "id": 575,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "hedonistic value functions in self-modifying agents",
                        "type": "concept",
                        "description": "Agents evaluate future using the utility that will exist after acting, incentivising deliberate utility-hacking toward constant maximal reward.",
                        "aliases": [
                            "V_he value functions",
                            "future-utility evaluating agents"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1605.03142",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516419723,
                        "updated_at": 1759516419723
                    }
                },
                0.0
            ],
            [
                {
                    "id": 576,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "agents need value functions that use current utility and account for policy changes",
                        "type": "concept",
                        "description": "Theorised requirement that an agent must judge futures by its present utility while modelling how actions alter future policies to avoid incentive for harmful self-edits.",
                        "aliases": [
                            "self-modification-aware evaluation insight",
                            "current-utility preservation insight"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1605.03142",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516419723,
                        "updated_at": 1759516419723
                    }
                },
                0.659221470355988
            ],
            [
                {
                    "id": 574,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "ignorant value functions in self-modifying agents",
                        "type": "concept",
                        "description": "Agents estimate future utility using current utility and current policy but ignore how actions change future policies, leaving them indifferent to risky self-modifications.",
                        "aliases": [
                            "dualistic ignorant value formulation",
                            "V_ig value functions"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1605.03142",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516419723,
                        "updated_at": 1759516419723
                    }
                },
                0.69897598028183
            ]
        ]
    },
    {
        "seed": 1289,
        "cluster": [
            [
                {
                    "id": 1289,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Objective misalignment in human-robot collaboration",
                        "type": "concept",
                        "description": "Robots that fail to infer the true human objective may pursue actions counter to human intent, jeopardising usefulness and safety in both near-term assistance tasks and long-term autonomous operation.",
                        "aliases": [
                            "human-robot value misalignment",
                            "robot misunderstanding of human objectives"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1707.06354",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516427259,
                        "updated_at": 1759516427259
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1013,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "misinterpretation of robot intentions by humans in interaction",
                        "type": "concept",
                        "description": "When humans cannot correctly infer a robot\u2019s goals or internal state, their responses can produce unsafe or inefficient outcomes.",
                        "aliases": [
                            "human misunderstanding of robot goals",
                            "ambiguity of robot intent"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1705.04226",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424322,
                        "updated_at": 1759516424322
                    }
                },
                0.663418650627136
            ],
            [
                {
                    "id": 1011,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "misspecified robot reward functions in human-robot interaction",
                        "type": "concept",
                        "description": "Robots optimising incorrectly specified reward functions can generate unsafe or unwanted behaviour when operating around people.",
                        "aliases": [
                            "reward misspecification in human-robot collaboration",
                            "incorrect robot objective functions with humans"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1705.04226",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424322,
                        "updated_at": 1759516424322
                    }
                },
                0.682278394699097
            ]
        ]
    },
    {
        "seed": 2035,
        "cluster": [
            [
                {
                    "id": 2035,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Adversarial stethoscope training on local stability features (\u03bb<0)",
                        "type": "concept",
                        "description": "A probe trained to classify local block instability is attached to the encoder; its loss is back-propagated with a negative weight so the encoder maximally confuses the probe, removing local-stability information.",
                        "aliases": [
                            "\u03bb-negative stethoscope for bias suppression",
                            "Nuisance information suppression mechanism"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1806.05502",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435260,
                        "updated_at": 1759516435260
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2034,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Auxiliary stethoscope training on instability origin features (\u03bb>0)",
                        "type": "concept",
                        "description": "A two-layer perceptron attached to the penultimate layer predicts which block interface will initiate collapse; its loss is added with positive weight so the encoder learns features relevant to global physics.",
                        "aliases": [
                            "\u03bb-positive stethoscope for origin prediction",
                            "Complementary information promotion mechanism"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1806.05502",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435260,
                        "updated_at": 1759516435260
                    }
                },
                0.664567530155182
            ],
            [
                {
                    "id": 2038,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Adversarial stethoscope reduces bias and increases robustness",
                        "type": "concept",
                        "description": "With \u03bb \u2248 \u22128 the adversarial stethoscope keeps >95 % accuracy on easy cases while lifting hard-scenario accuracy from 8 % to ~60 % and lowering the Pearson correlation between predicted global stability and local-stability ground truth.",
                        "aliases": [
                            "Debiasing evidence for \u03bb<0",
                            "Bias suppression validation"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1806.05502",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435260,
                        "updated_at": 1759516435260
                    }
                },
                0.696621894836426
            ]
        ]
    },
    {
        "seed": 126,
        "cluster": [
            [
                {
                    "id": 126,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Opaque decision-making in deep convolutional networks for image classification",
                        "type": "concept",
                        "description": "ConvNets achieve high accuracy but their internal decision process is difficult to interpret, posing scientific and safety concerns.",
                        "aliases": [
                            "black-box behaviour of CNNs",
                            "lack of interpretability in ConvNets"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1311.2901",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414772,
                        "updated_at": 1759516414772
                    }
                },
                0.0
            ],
            [
                {
                    "id": 907,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Opaque decision boundaries in neural networks",
                        "type": "concept",
                        "description": "Because standard neural networks do not expose their internal rules, it is difficult for domain experts to see whether predictions rely on medically or ethically incorrect features.",
                        "aliases": [
                            "uninterpretable model behavior",
                            "black-box decision logic"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1703.03717",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516423173,
                        "updated_at": 1759516423173
                    }
                },
                0.666867792606354
            ],
            [
                {
                    "id": 1496,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Opaque decision-making in deep learning agents",
                        "type": "concept",
                        "description": "Modern deep learning and deep reinforcement-learning agents expose weights but not the reasoning behind actions, hampering oversight and safety.",
                        "aliases": [
                            "black-box behaviour in neural networks",
                            "uninterpretable deep RL policies"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1802.07740",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429576,
                        "updated_at": 1759516429576
                    }
                },
                0.690520823001862
            ]
        ]
    },
    {
        "seed": 1542,
        "cluster": [
            [
                {
                    "id": 1542,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "High expert supervision burden in robot task learning",
                        "type": "concept",
                        "description": "Classical learning-from-demonstration requires humans to tele-operate or kinesthetically program robots for every new task, creating a major scalability bottleneck.",
                        "aliases": [
                            "tedious expert demonstrations",
                            "human-intensive LfD data collection"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1804.08606",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430075,
                        "updated_at": 1759516430075
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2247,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "High human demonstration requirement in imitation learning for robotics",
                        "type": "concept",
                        "description": "State-of-the-art IL methods (DAgger, GAIL) need many high-quality human demonstrations, incurring cost and limiting scalability.",
                        "aliases": [
                            "extensive expert supervision need",
                            "large demonstration datasets requirement"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.10019",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516437621,
                        "updated_at": 1759516437621
                    }
                },
                0.676286697387695
            ],
            [
                {
                    "id": 2166,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Excessive human demonstration burden in robot training",
                        "type": "concept",
                        "description": "Providing full trajectories throughout training is costly and often impractical for human experts.",
                        "aliases": [
                            "high-cost human feedback",
                            "constant human interaction"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.08479",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516436709,
                        "updated_at": 1759516436709
                    }
                },
                0.696983397006989
            ]
        ]
    },
    {
        "seed": 2038,
        "cluster": [
            [
                {
                    "id": 2038,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Adversarial stethoscope reduces bias and increases robustness",
                        "type": "concept",
                        "description": "With \u03bb \u2248 \u22128 the adversarial stethoscope keeps >95 % accuracy on easy cases while lifting hard-scenario accuracy from 8 % to ~60 % and lowering the Pearson correlation between predicted global stability and local-stability ground truth.",
                        "aliases": [
                            "Debiasing evidence for \u03bb<0",
                            "Bias suppression validation"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1806.05502",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435260,
                        "updated_at": 1759516435260
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2037,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Auxiliary stethoscope improves hard-scenario accuracy over baseline",
                        "type": "concept",
                        "description": "When trained only on hard scenarios, adding an origin-prediction stethoscope with 0.5 \u2264 \u03bb \u2264 16 raises global-stability accuracy from chance-level 51 % to well above 65 % and outperforms baseline across all scenarios.",
                        "aliases": [
                            "Performance gain evidence for \u03bb>0",
                            "Complementary information validation"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1806.05502",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435260,
                        "updated_at": 1759516435260
                    }
                },
                0.683924317359924
            ],
            [
                {
                    "id": 2035,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Adversarial stethoscope training on local stability features (\u03bb<0)",
                        "type": "concept",
                        "description": "A probe trained to classify local block instability is attached to the encoder; its loss is back-propagated with a negative weight so the encoder maximally confuses the probe, removing local-stability information.",
                        "aliases": [
                            "\u03bb-negative stethoscope for bias suppression",
                            "Nuisance information suppression mechanism"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1806.05502",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435260,
                        "updated_at": 1759516435260
                    }
                },
                0.696621894836426
            ]
        ]
    },
    {
        "seed": 1475,
        "cluster": [
            [
                {
                    "id": 1475,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Unethical decision making in reinforcement learning agents deployed in real-world contexts",
                        "type": "concept",
                        "description": "AI systems trained with reinforcement learning can reach task goals by taking actions that violate widely-held moral norms, leading to harm (e.g., ignoring injured people, running over animals).",
                        "aliases": [
                            "unethical behaviour of RL agents",
                            "AI ethical violations in RL systems"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1476,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Side effects and dangerous exploration in reinforcement learning systems",
                        "type": "concept",
                        "description": "RL agents may cause collateral damage or explore hazardous actions during training or deployment because ethical constraints are missing from the reward signal.",
                        "aliases": [
                            "unintended side-effects in RL",
                            "dangerous exploration by RL agents"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.684953033924103
            ],
            [
                {
                    "id": 1477,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward misspecification leading to unethical behavior in RL training",
                        "type": "concept",
                        "description": "When rewards optimise only task performance, agents find high-return behaviours that inadvertently violate ethical norms.",
                        "aliases": [
                            "incomplete reward functions in RL",
                            "goal-only reward design"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.69830334186554
            ]
        ]
    },
    {
        "seed": 736,
        "cluster": [
            [
                {
                    "id": 736,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Length-normalised coverage-aware beam search improves adequacy",
                        "type": "concept",
                        "description": "Adjusting hypothesis scores by length penalty and coverage penalty counteracts the intrinsic short-sentence bias of beam search.",
                        "aliases": [
                            "Beam search with lp and cp",
                            "Coverage penalty concept"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1609.08144",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421376,
                        "updated_at": 1759516421376
                    }
                },
                0.0
            ],
            [
                {
                    "id": 731,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Length bias in beam search decoding",
                        "type": "concept",
                        "description": "Pure log-probability beam search favours shorter outputs, causing untranslated source segments and lower adequacy.",
                        "aliases": [
                            "Under-translation due to short hypothesis preference",
                            "Coverage omissions"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1609.08144",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421376,
                        "updated_at": 1759516421376
                    }
                },
                0.686160206794739
            ],
            [
                {
                    "id": 741,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Beam-search scoring with \u03b1=0.2 length penalty and \u03b2=0.2 coverage penalty",
                        "type": "concept",
                        "description": "Empirical tuning on dev-set showed \u03b1,\u03b2=0.2 yield +1.1 BLEU over pure probability search while remaining robust.",
                        "aliases": [
                            "lp/cp parameter choice",
                            "Empirically tuned penalties"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1609.08144",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421376,
                        "updated_at": 1759516421376
                    }
                },
                0.691433191299438
            ]
        ]
    },
    {
        "seed": 71,
        "cluster": [
            [
                {
                    "id": 71,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "feature scaling sensitivity analysis demonstrating robustness of natural gradient IRL",
                        "type": "concept",
                        "description": "When feature vectors were linearly transformed, max-margin IRL performance collapsed while natural-gradient IRL remained stable.",
                        "aliases": [
                            "linear transform stress-test",
                            "scaling mismatch study"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1206.5264",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414127,
                        "updated_at": 1759516414127
                    }
                },
                0.0
            ],
            [
                {
                    "id": 76,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "feature scaling sensitivity leading performance degradation in IRL algorithms",
                        "type": "concept",
                        "description": "Algorithms that assume known feature scales (e.g., max-margin IRL) can select unsafe policies when scales are wrong.",
                        "aliases": [
                            "scaling mismatch risk",
                            "feature re-weighting hazard"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1206.5264",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414127,
                        "updated_at": 1759516414127
                    }
                },
                0.687272429466248
            ],
            [
                {
                    "id": 69,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "grid-world experiments demonstrating robustness of natural gradient IRL",
                        "type": "concept",
                        "description": "Natural-gradient IRL achieved lower policy error than max-margin IRL across sample sizes and under extreme feature-scaling perturbations.",
                        "aliases": [
                            "gridworld evaluation of gradient IRL",
                            "feature-scaling test in grids"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1206.5264",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414127,
                        "updated_at": 1759516414127
                    }
                },
                0.69111168384552
            ]
        ]
    },
    {
        "seed": 291,
        "cluster": [
            [
                {
                    "id": 291,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Apply hard gradient norm clipping",
                        "type": "concept",
                        "description": "Adopt a policy that rescales gradients when their norm exceeds a chosen threshold.",
                        "aliases": [
                            "gradient clipping rationale",
                            "norm threshold design choice"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1409.3215",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416451,
                        "updated_at": 1759516416451
                    }
                },
                0.0
            ],
            [
                {
                    "id": 290,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Gradient norm constraint to control exploding gradients",
                        "type": "concept",
                        "description": "Bounding gradient norms prevents instability while preserving useful learning signal.",
                        "aliases": [
                            "gradient clipping insight",
                            "norm-bounding hypothesis"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1409.3215",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416451,
                        "updated_at": 1759516416451
                    }
                },
                0.687838494777679
            ],
            [
                {
                    "id": 1605,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Adaptive gradient clipping using moving average thresholding",
                        "type": "concept",
                        "description": "Training steps are aborted when gradient norm exceeds four std-dev above moving average of log-norms.",
                        "aliases": [
                            "adaptive clipping mechanism"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1804.09849",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430547,
                        "updated_at": 1759516430547
                    }
                },
                0.698888123035431
            ]
        ]
    },
    {
        "seed": 2267,
        "cluster": [
            [
                {
                    "id": 2267,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Training models with adversarial reprogramming examples to increase robustness",
                        "type": "concept",
                        "description": "Extends adversarial training to include large-magnitude, task-redirecting programs rather than small-norm perturbations (motivated by \u00a74.4 finding that standard adversarial training fails).",
                        "aliases": [
                            "reprogramming-aware adversarial training"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1806.11146",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516437803,
                        "updated_at": 1759516437803
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2276,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Simulation-based robustness improvement via adversarial reprogramming-aware training",
                        "type": "concept",
                        "description": "Preliminary simulations suggest \u226550 % drop in attack success when reprogramming examples are included during fine-tuning (inferred).",
                        "aliases": [
                            "simulated fine-tuning robustness"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1806.11146",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516437803,
                        "updated_at": 1759516437803
                    }
                },
                0.693528175354004
            ],
            [
                {
                    "id": 2270,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Adversarial reprogramming adversarial training during fine-tuning phase",
                        "type": "concept",
                        "description": "During RLHF/fine-tuning, include adversarial program examples that attempt task-redirection and optimise the model to maintain correct original task output (inferred from \u00a74.4).",
                        "aliases": [
                            "reprogramming-specific fine-tuning"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1806.11146",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516437803,
                        "updated_at": 1759516437803
                    }
                },
                0.699932038784027
            ]
        ]
    },
    {
        "seed": 1021,
        "cluster": [
            [
                {
                    "id": 1021,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "maintain uncertainty over robot reward and update via human guidance",
                        "type": "concept",
                        "description": "Design principle that robots should represent a belief distribution over possible reward functions and refine it using human signals.",
                        "aliases": [
                            "reward uncertainty framework",
                            "objective as latent state"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1705.04226",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424322,
                        "updated_at": 1759516424322
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1014,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "robots assuming fixed reward functions without uncertainty in human-robot interaction",
                        "type": "concept",
                        "description": "Robots normally treat their reward as immutable, ignoring designer errors and user-specific preferences.",
                        "aliases": [
                            "objective taken as given",
                            "static robot reward assumption"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1705.04226",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424322,
                        "updated_at": 1759516424322
                    }
                },
                0.697782278060913
            ],
            [
                {
                    "id": 2094,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Maintain reward uncertainty through Bayesian IRL framework",
                        "type": "concept",
                        "description": "Design rationale to keep a posterior over rewards so the agent is aware of ambiguity and noisy data.",
                        "aliases": [
                            "Bayesian IRL design choice"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1806.06877",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435739,
                        "updated_at": 1759516435739
                    }
                },
                0.698798477649689
            ]
        ]
    },
    {
        "seed": 402,
        "cluster": [
            [
                {
                    "id": 402,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Indirect normativity framework delegating decision theory discovery to AI",
                        "type": "concept",
                        "description": "An implementation mechanism whereby AI models infer human-endorsed decision principles through idealised reflection instead of preset formalisms.",
                        "aliases": [
                            "Ideal-advisor indirect approach",
                            "Delegated decision-theory search"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1507.01986",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417637,
                        "updated_at": 1759516417637
                    }
                },
                0.0
            ],
            [
                {
                    "id": 393,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Indirect normativity approach to defer decision theory discovery",
                        "type": "concept",
                        "description": "If ideal decision theory proves too hard, AI could instead learn what humans would endorse under reflection and defer exact decision-rule design to that process.",
                        "aliases": [
                            "Delegated ideal-advisor method",
                            "Indirect normative fallback"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1507.01986",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417637,
                        "updated_at": 1759516417637
                    }
                },
                0.45524525642395
            ]
        ]
    },
    {
        "seed": 2218,
        "cluster": [
            [
                {
                    "id": 2218,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "CIFAR-10 2.83% test error with 4 GPU-day second-order DARTS",
                        "type": "concept",
                        "description": "The normal plus reduction cells found by second-order DARTS achieve 2.83 \u00b1 0.06 % error, matching AmoebaNet at <0.1% of its compute.",
                        "aliases": [
                            "DARTS CIFAR-10 SOTA result"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1806.09055",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516437136,
                        "updated_at": 1759516437136
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2221,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "CIFAR-10 2.94% test error with 1.5 GPU-day first-order DARTS",
                        "type": "concept",
                        "description": "First-order DARTS variant attains 2.94 % error using only 1.5 GPU-days, indicating a cost-accuracy trade-off.",
                        "aliases": [
                            "first-order CIFAR result"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1806.09055",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516437136,
                        "updated_at": 1759516437136
                    }
                },
                0.456587791442871
            ]
        ]
    },
    {
        "seed": 257,
        "cluster": [
            [
                {
                    "id": 257,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Validation error reduced to 7.5 % top-5 with multi-scale training",
                        "type": "concept",
                        "description": "Configuration D or E trained with scale jittering and multi-scale testing attained 24.8 % top-1 / 7.5 % top-5 error (Table 4).",
                        "aliases": [
                            "Scale-jitter training validation evidence",
                            "Multi-scale training result"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1409.1556",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416154,
                        "updated_at": 1759516416154
                    }
                },
                0.0
            ],
            [
                {
                    "id": 262,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Top-5 error 7.5 % with multi-scale evaluation",
                        "type": "concept",
                        "description": "Applying multi-scale evaluation to scale-jitter-trained models D/E achieved 7.5 % top-5 validation error (Table 4).",
                        "aliases": [
                            "Multi-scale inference validation",
                            "Evaluation result for scale aggregation"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1409.1556",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416154,
                        "updated_at": 1759516416154
                    }
                },
                0.469380378723145
            ]
        ]
    },
    {
        "seed": 220,
        "cluster": [
            [
                {
                    "id": 220,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "use bidirectional RNN encoder for contextual annotations",
                        "type": "concept",
                        "description": "A forward and backward RNN produce concatenated hidden states so each annotation summarises both preceding and following words.",
                        "aliases": [
                            "BiRNN encoder for annotations",
                            "forward-backward encoder states"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1409.0473",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516415770,
                        "updated_at": 1759516415770
                    }
                },
                0.0
            ],
            [
                {
                    "id": 223,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "bidirectional RNN encoder concatenating forward and backward hidden states as annotations",
                        "type": "concept",
                        "description": "Separate forward and backward RNNs read the source in opposite directions; their hidden states are concatenated into h_j annotations.",
                        "aliases": [
                            "BiRNN annotation generation",
                            "concatenated encoder states"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1409.0473",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516415770,
                        "updated_at": 1759516415770
                    }
                },
                0.499727427959442
            ]
        ]
    },
    {
        "seed": 855,
        "cluster": [
            [
                {
                    "id": 855,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Naive linear utility aggregation incompatible with Pareto optimality under differing beliefs",
                        "type": "concept",
                        "description": "Simply maximising a static weighted sum of utilities fails to be Pareto-optimal when principals hold different priors, leading to sub-optimal or unacceptable policies.",
                        "aliases": [
                            "fixed weight utility sum failure"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1701.01302",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516422710,
                        "updated_at": 1759516422710
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1413,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Pareto suboptimality from naive fixed utility aggregation in multi-principal agents",
                        "type": "concept",
                        "description": "Using a single fixed linear combination of principals\u2019 utilities ignores belief differences and cannot implement some Pareto-optimal policies, as proven in Proposition 3.",
                        "aliases": [
                            "Failure of linear aggregation with fixed weights",
                            "Static weighted-sum limitation"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1711.00363",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428623,
                        "updated_at": 1759516428623
                    }
                },
                0.502651274204254
            ]
        ]
    },
    {
        "seed": 148,
        "cluster": [
            [
                {
                    "id": 148,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Off-policy \u03b5-greedy exploration enabling data reuse in Atari RL",
                        "type": "concept",
                        "description": "Maintaining action randomness via \u03b5-greedy lets the learner gather diverse data while evaluating the greedy target policy.",
                        "aliases": [
                            "\u03b5-greedy for exploration",
                            "off-policy sampling"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1312.5602",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414937,
                        "updated_at": 1759516414937
                    }
                },
                0.0
            ],
            [
                {
                    "id": 161,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "\u03b5-greedy exploration strategy for off-policy learning in Atari RL",
                        "type": "concept",
                        "description": "Chooses random action with probability \u03b5 to ensure exploration while still favoring high-Q moves.",
                        "aliases": [
                            "epsilon exploration design",
                            "stochastic action choice"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1312.5602",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414937,
                        "updated_at": 1759516414937
                    }
                },
                0.521456122398376
            ]
        ]
    },
    {
        "seed": 1204,
        "cluster": [
            [
                {
                    "id": 1204,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Condition impact penalty on externally defined success announcement",
                        "type": "concept",
                        "description": "Define R conditional on a future announcement (e.g. \u2018we became rich\u2019) to let the AI steer outcomes that humans might reach by chance.",
                        "aliases": [
                            "probability-pump conditioning"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1705.10720",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426292,
                        "updated_at": 1759516426292
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1211,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Conditioning penalty on success announcement event A",
                        "type": "concept",
                        "description": "Re-calculate R given an external future fact \u2018A\u2019 (e.g. \u2018the investment succeeded\u2019), allowing the AI to act as a probability pump.",
                        "aliases": [
                            "announcement-based conditioning"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1705.10720",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426292,
                        "updated_at": 1759516426292
                    }
                },
                0.535687625408173
            ]
        ]
    },
    {
        "seed": 807,
        "cluster": [
            [
                {
                    "id": 807,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Scale global multidisciplinary AI safety research funding and collaboration",
                        "type": "intervention",
                        "description": "Governments, industry and philanthropists allocate significant, sustained resources to enlarge and coordinate cross-field AI safety programs worldwide.",
                        "aliases": [
                            "Increase AI safety funding",
                            "Expand safety research community"
                        ],
                        "intervention_lifecycle": 6,
                        "intervention_maturity": 1,
                        "url": "https://arxiv.org/abs/1610.07997",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421904,
                        "updated_at": 1759516421904
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1119,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Increase dedicated funding and institutional support for AI safety research",
                        "type": "intervention",
                        "description": "Governments, philanthropies and industry allocate larger, ring-fenced budgets and establish specialised programs to accelerate AI-safety research.",
                        "aliases": [
                            "expand AI safety grantmaking",
                            "boost safety research budgets"
                        ],
                        "intervention_lifecycle": 6,
                        "intervention_maturity": 3,
                        "url": "https://arxiv.org/abs/1705.08807",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425326,
                        "updated_at": 1759516425326
                    }
                },
                0.539108216762543
            ]
        ]
    },
    {
        "seed": 1814,
        "cluster": [
            [
                {
                    "id": 1814,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "lack of provable robustness guarantees in neural network training",
                        "type": "concept",
                        "description": "Most defence strategies provide empirical robustness only and can be broken by stronger attacks; formal guarantees that no bounded adversary can succeed are missing.",
                        "aliases": [
                            "absence of verified robustness",
                            "unverified model robustness"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1805.10265",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516432978,
                        "updated_at": 1759516432978
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2351,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "absence of provable robustness guarantees in modern deep neural networks",
                        "type": "concept",
                        "description": "Current networks achieve high accuracy but provide no certified guarantees that small input perturbations will not alter outputs.",
                        "aliases": [
                            "lack of formal safety proofs for DNNs",
                            "unverified robustness of networks"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1807.03571",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516438837,
                        "updated_at": 1759516438837
                    }
                },
                0.542524933815002
            ]
        ]
    },
    {
        "seed": 1599,
        "cluster": [
            [
                {
                    "id": 1599,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Enhancing RNMT with modern techniques to form RNMT+",
                        "type": "concept",
                        "description": "The RNMT architecture is upgraded with multi-head attention, layer normalization, label smoothing and synchronous training to combine strengths of recent advances.",
                        "aliases": [
                            "RNMT+ design rationale"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1804.09849",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430547,
                        "updated_at": 1759516430547
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1604,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "RNMT+ architecture with 6 bidirectional LSTM layers, multi-head additive attention, label smoothing, synchronous training",
                        "type": "concept",
                        "description": "Concrete configuration that instantiates the RNMT+ design.",
                        "aliases": [
                            "RNMT+ implementation details"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1804.09849",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430547,
                        "updated_at": 1759516430547
                    }
                },
                0.550804674625397
            ]
        ]
    },
    {
        "seed": 203,
        "cluster": [
            [
                {
                    "id": 203,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Modified generator update to maximize log D(G(z)) during training",
                        "type": "concept",
                        "description": "Practically, generator gradients are taken with respect to \u2212log D(G(z)) rather than log(1\u2212D(G(z))) to avoid saturation.",
                        "aliases": [
                            "non-saturating update implementation",
                            "log-D generator step"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1406.2661",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516415544,
                        "updated_at": 1759516415544
                    }
                },
                0.0
            ],
            [
                {
                    "id": 199,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Alternative generator objective maximizing log D(G(z)) provides stronger gradients without altering fixed point",
                        "type": "concept",
                        "description": "Replacing the generator\u2019s loss with \u2212log D(G(z)) retains the same equilibrium but yields larger gradients when the discriminator is strong.",
                        "aliases": [
                            "non-saturating GAN objective",
                            "maximize log-D workaround"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1406.2661",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516415544,
                        "updated_at": 1759516415544
                    }
                },
                0.554456770420074
            ]
        ]
    },
    {
        "seed": 212,
        "cluster": [
            [
                {
                    "id": 212,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "translation quality degradation for long sentences in neural machine translation",
                        "type": "concept",
                        "description": "Neural encoder-decoder models exhibit sharp BLEU score declines as source sentences exceed training lengths, harming overall translation quality.",
                        "aliases": [
                            "poor long-sentence NMT performance",
                            "length-related NMT accuracy drop"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1409.0473",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516415770,
                        "updated_at": 1759516415770
                    }
                },
                0.0
            ],
            [
                {
                    "id": 281,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Translation degradation on long sentences in RNN models",
                        "type": "concept",
                        "description": "Prior RNN architectures exhibit sharply reduced translation quality on lengthy source sentences, jeopardising usability for real-world documents.",
                        "aliases": [
                            "long-sentence performance drop",
                            "length-dependent translation risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1409.3215",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416451,
                        "updated_at": 1759516416451
                    }
                },
                0.555617690086365
            ]
        ]
    },
    {
        "seed": 2519,
        "cluster": [
            [
                {
                    "id": 2519,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Training instability due to high gradient variance in multi-agent RL",
                        "type": "concept",
                        "description": "High-variance gradient estimates hamper reliable learning and can lead to divergent or unsafe policies (Practical multi-agent imitation learning \u00a74).",
                        "aliases": [
                            "unstable policy optimisation in multi-agent settings",
                            "sample inefficiency from gradient variance"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.09936",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440463,
                        "updated_at": 1759516440463
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2520,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "High gradient variance during policy optimization in multi-agent RL",
                        "type": "concept",
                        "description": "Policy-gradient estimators suffer from high variance because rewards depend on joint actions, worsening with agent count (\u00a74).",
                        "aliases": [
                            "variance problem in policy gradients",
                            "sample inefficiency of multi-agent updates"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1807.09936",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440463,
                        "updated_at": 1759516440463
                    }
                },
                0.557356417179108
            ]
        ]
    },
    {
        "seed": 173,
        "cluster": [
            [
                {
                    "id": 173,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "High variance gradient estimators in variational inference",
                        "type": "concept",
                        "description": "The na\u00efve Monte-Carlo estimator for \u2207\u03d5 E_q[f(z)] carries very large variance, making optimization impractical.",
                        "aliases": [
                            "noisy variational gradients",
                            "unstable Monte Carlo gradients"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1312.6114",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516415290,
                        "updated_at": 1759516415290
                    }
                },
                0.0
            ],
            [
                {
                    "id": 176,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Variance explosion in na\u00efve Monte Carlo gradient estimator",
                        "type": "concept",
                        "description": "Estimators of the form E_q[f(z)\u2207log q] have prohibitively large variance, stopping progress in learning.",
                        "aliases": [
                            "unstable score-function gradients",
                            "high-variance REINFORCE-style gradients"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1312.6114",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516415290,
                        "updated_at": 1759516415290
                    }
                },
                0.559605479240417
            ]
        ]
    },
    {
        "seed": 539,
        "cluster": [
            [
                {
                    "id": 539,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Incorrect probability estimates for unexecuted computations in AI systems",
                        "type": "concept",
                        "description": "AI agents that must reason about computations they cannot afford to run can make unsafe or sub-optimal decisions if their probability estimates are wrong.",
                        "aliases": [
                            "miscalibrated logical uncertainty for expensive computations",
                            "unreliable probability assignments about computation outputs"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1604.05288",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516419298,
                        "updated_at": 1759516419298
                    }
                },
                0.0
            ],
            [
                {
                    "id": 406,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Logical uncertainty miscalibration in resource-bounded AI agents",
                        "type": "concept",
                        "description": "AI systems with limited computation may assign grossly incorrect probabilities to logical statements, leading to unsafe reasoning and decisions.",
                        "aliases": [
                            "incorrect probability assignment to logical sentences by bounded agents",
                            "logical belief miscalibration under computational limits"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1510.03370",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417926,
                        "updated_at": 1759516417926
                    }
                },
                0.562067031860352
            ]
        ]
    },
    {
        "seed": 1629,
        "cluster": [
            [
                {
                    "id": 1629,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "train reward predictor to scale human debate judgments",
                        "type": "concept",
                        "description": "To reduce human time, a model is trained to emulate human debate judgments, enabling large-scale self play.",
                        "aliases": [
                            "learned judge model",
                            "human preference predictor"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1805.00899",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430917,
                        "updated_at": 1759516430917
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1635,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "training judge models to predict human reward",
                        "type": "concept",
                        "description": "Supervised models learn to approximate human debate decisions, enabling automated large-scale training.",
                        "aliases": [
                            "learned debate judge",
                            "reward model for debate"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1805.00899",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430917,
                        "updated_at": 1759516430917
                    }
                },
                0.569128036499023
            ]
        ]
    },
    {
        "seed": 486,
        "cluster": [
            [
                {
                    "id": 486,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "causal decision theory ignoring logical dependence between source codes",
                        "type": "concept",
                        "description": "CDT treats an agent\u2019s action as causally independent of the opponent\u2019s simultaneous action, even when both actions are logically linked by mutual source-code reasoning.",
                        "aliases": [
                            "CDT ignores logical correlations",
                            "logical dependence blind spot"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1602.04184",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516418720,
                        "updated_at": 1759516418720
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1402,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Ignoring logical dependencies in causal decision theory",
                        "type": "concept",
                        "description": "Problem where CDT severs logical links between its action and accurate predictions about that action, yielding dominated choices, instability and vulnerability (e.g., Death in Damascus).",
                        "aliases": [
                            "CDT breaks predictor links",
                            "CDT neglects subjunctive dependence"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1710.05060",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428527,
                        "updated_at": 1759516428527
                    }
                },
                0.569328248500824
            ]
        ]
    },
    {
        "seed": 1129,
        "cluster": [
            [
                {
                    "id": 1129,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "MLE IRL robustness to human rationality misspecification",
                        "type": "concept",
                        "description": "Theorem 5 shows that a robot using maximum-likelihood estimation for \u03b8 selects identical actions even if its assumed \u03b2\u2032 differs from true \u03b2\u2080, unlike Bayesian-optimal policies (Section 5).",
                        "aliases": [
                            "MLE robustness to wrong \u03b2",
                            "maximum likelihood IRL insensitivity to \u03b2 error"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1705.09990",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425582,
                        "updated_at": 1759516425582
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1141,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Theorem 5 proof of MLE robustness to rationality misspecification",
                        "type": "concept",
                        "description": "Proof that MLE-based robots act identically under incorrect \u03b2 assumptions (Section 5).",
                        "aliases": [
                            "proof of \u03b2 robustness",
                            "Theorem 5 evidence"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1705.09990",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425582,
                        "updated_at": 1759516425582
                    }
                },
                0.570259749889374
            ]
        ]
    },
    {
        "seed": 646,
        "cluster": [
            [
                {
                    "id": 646,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "train reward network adversarially against agent policy",
                        "type": "concept",
                        "description": "Use a generator (policy) and discriminator (reward model) loop: discriminator seeks states where policy\u2019s claimed reward diverges from human labels.",
                        "aliases": [
                            "GAN-style reward training",
                            "competitive reward learning design"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1606.06565",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420412,
                        "updated_at": 1759516420412
                    }
                },
                0.0
            ],
            [
                {
                    "id": 647,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "generative adversarial reward function training during RL",
                        "type": "concept",
                        "description": "Implement discriminator network receiving state\u2013action traces and human labels; adversarial loss added to policy optimisation.",
                        "aliases": [
                            "reward-GAN implementation",
                            "adversarial reward classifier"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1606.06565",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420412,
                        "updated_at": 1759516420412
                    }
                },
                0.572278738021851
            ]
        ]
    },
    {
        "seed": 1303,
        "cluster": [
            [
                {
                    "id": 1303,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Train collaborative robots with pragmatic-pedagogic CIRL policies via POMDP value iteration",
                        "type": "intervention",
                        "description": "During the fine-tuning/RL phase, learn robot policies that assume pedagogic human behaviour, update beliefs with the Boltzmann model, and compute actions by value iteration in the CIRL-derived POMDP before deployment in collaborative tasks.",
                        "aliases": [
                            "pragmatic-pedagogic CIRL training",
                            "deploy POMDP-derived CIRL policies"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 2,
                        "url": "https://arxiv.org/abs/1707.06354",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516427259,
                        "updated_at": 1759516427259
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2007,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Train robots with CIRL-derived policies encouraging pedagogic human signaling instead of IRL approaches",
                        "type": "intervention",
                        "description": "During fine-tuning/RL, optimise robot policies using the CIRL framework so that they interpret and elicit informative human actions, improving alignment robustness in deployment.",
                        "aliases": [
                            "switch from IRL to CIRL policies",
                            "deploy pedagogic-aware training"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 2,
                        "url": "https://arxiv.org/abs/1806.03820",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434753,
                        "updated_at": 1759516434753
                    }
                },
                0.573871850967407
            ]
        ]
    },
    {
        "seed": 1598,
        "cluster": [
            [
                {
                    "id": 1598,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Hybridizing encoders and decoders leverages complementary strengths",
                        "type": "concept",
                        "description": "Using a Transformer encoder (rich features) with an RNMT+ decoder (stateful generation) can outperform either model alone.",
                        "aliases": [
                            "encoder-decoder hybrid insight"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1804.09849",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430547,
                        "updated_at": 1759516430547
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1602,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Combining Transformer encoder with RNMT+ decoder in hybrid models",
                        "type": "concept",
                        "description": "Design choice to pair the richer self-attention encoder with a stateful RNN decoder.",
                        "aliases": [
                            "encoder-decoder hybrid design"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1804.09849",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430547,
                        "updated_at": 1759516430547
                    }
                },
                0.578055739402771
            ]
        ]
    },
    {
        "seed": 558,
        "cluster": [
            [
                {
                    "id": 558,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Rice's theorem constraints on behavioural verification in AGI",
                        "type": "concept",
                        "description": "Because any non-trivial property of a partial computable function is undecidable, full behavioural verification of AGI is impossible.",
                        "aliases": [
                            "Rice\u2019s theorem application",
                            "formal undecidability insight"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1604.06963",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516419494,
                        "updated_at": 1759516419494
                    }
                },
                0.0
            ],
            [
                {
                    "id": 566,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Formal undecidability proof of verification problem in AGI",
                        "type": "concept",
                        "description": "Mathematical proof using Rice\u2019s theorem shows no algorithm can decide if an arbitrary agent always acts Good.",
                        "aliases": [
                            "undecidability validation evidence",
                            "non-computability proof"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1604.06963",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516419494,
                        "updated_at": 1759516419494
                    }
                },
                0.583861410617828
            ]
        ]
    },
    {
        "seed": 151,
        "cluster": [
            [
                {
                    "id": 151,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward magnitude normalization stabilizing gradients across Atari games",
                        "type": "concept",
                        "description": "Mapping diverse game scores to a fixed small range limits Q-value targets and prevents exploding gradients.",
                        "aliases": [
                            "reward clipping theory",
                            "scale-invariant error signals"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1312.5602",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414937,
                        "updated_at": 1759516414937
                    }
                },
                0.0
            ],
            [
                {
                    "id": 163,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Clip rewards to unit magnitude for consistent gradient scales in Atari DQN training",
                        "type": "concept",
                        "description": "Applies theoretical insight about gradient stability to propose specific clipping scheme.",
                        "aliases": [
                            "reward clipping design",
                            "score normalization strategy"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1312.5602",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414937,
                        "updated_at": 1759516414937
                    }
                },
                0.5845987200737
            ]
        ]
    },
    {
        "seed": 2040,
        "cluster": [
            [
                {
                    "id": 2040,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Fine-tune global stability models with auxiliary stethoscope on instability-origin task",
                        "type": "intervention",
                        "description": "During supervised fine-tuning, attach an auxiliary two-layer probe predicting the collapse interface and set a positive weighting (0.5-16) on its cross-entropy loss to encourage extraction of physically grounded global-instability features.",
                        "aliases": [
                            "Apply \u03bb>0 origin stethoscope during training",
                            "Complementary-feature promotion intervention"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 2,
                        "url": "https://arxiv.org/abs/1806.05502",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435260,
                        "updated_at": 1759516435260
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2041,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Fine-tune global stability models with adversarial stethoscope on local-stability task",
                        "type": "intervention",
                        "description": "Attach a probe trained to predict local stability and back-propagate its loss with a negative weight (e.g., \u03bb = \u22128) so the encoder learns representations invariant to the visual local-stability cue, reducing prediction bias.",
                        "aliases": [
                            "Apply \u03bb<0 local stethoscope during training",
                            "Bias-suppression intervention"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 2,
                        "url": "https://arxiv.org/abs/1806.05502",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435260,
                        "updated_at": 1759516435260
                    }
                },
                0.593135893344879
            ]
        ]
    },
    {
        "seed": 1138,
        "cluster": [
            [
                {
                    "id": 1138,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward feature space expansion with irrelevant dimensions",
                        "type": "concept",
                        "description": "Augment the reward feature vector \u03d5 with extra, task-irrelevant dimensions to bias the robot toward obedience without harming eventual convergence (Section 5, Figure 4).",
                        "aliases": [
                            "feature augmentation for obedience",
                            "expanded \u0398 implementation"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1705.09990",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425582,
                        "updated_at": 1759516425582
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1130,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Providing extra reward features increases robot obedience bias",
                        "type": "concept",
                        "description": "Simulation results (Figure 4, Section 5) indicate that augmenting the reward feature space with irrelevant dimensions makes the robot more likely to obey, while missing features has the opposite effect.",
                        "aliases": [
                            "feature space expansion increases obedience",
                            "extra \u0398 dimensions raise obedience"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1705.09990",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425582,
                        "updated_at": 1759516425582
                    }
                },
                0.596459686756134
            ]
        ]
    },
    {
        "seed": 587,
        "cluster": [
            [
                {
                    "id": 587,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "reward signal manipulation causing self-delusion in RL agents",
                        "type": "concept",
                        "description": "RL agents optimise the evidence of reward; therefore they may physically alter sensors or internal pathways so that the reward signal is always maximal, a mechanism termed self-delusion.",
                        "aliases": [
                            "reward channel tampering mechanism",
                            "self-delusion via sensor modification"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1605.03143",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516419860,
                        "updated_at": 1759516419860
                    }
                },
                0.0
            ],
            [
                {
                    "id": 298,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "self-delusion via reward hacking in RL agents",
                        "type": "concept",
                        "description": "Reinforcement-learning agents may manipulate their observation or reward channel (e.g. Ring & Orseau\u2019s \u2018delusion box\u2019) to gain maximal reward while neglecting real-world goals.",
                        "aliases": [
                            "wire-heading",
                            "delusion box behaviour"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1411.1373",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416712,
                        "updated_at": 1759516416712
                    }
                },
                0.598353385925293
            ]
        ]
    },
    {
        "seed": 1386,
        "cluster": [
            [
                {
                    "id": 1386,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "sample inefficiency of reinforcement learning in high-dimensional state spaces",
                        "type": "concept",
                        "description": "Modern deep RL methods often need tens of millions of frames to learn in tasks such as Atari Bowling, making them impractical for time-critical or real-world use.",
                        "aliases": [
                            "sample inefficiency of deep RL",
                            "slow learning in complex environments"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1709.10163",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428387,
                        "updated_at": 1759516428387
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1714,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "excessive sample complexity in deep reinforcement learning",
                        "type": "concept",
                        "description": "Deep RL often needs millions-to-billions of environment interactions or weeks of wall-clock time to reach expert level, limiting real-world deployment (Introduction).",
                        "aliases": [
                            "poor data efficiency",
                            "billions of frames requirement"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1805.07805",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516431848,
                        "updated_at": 1759516431848
                    }
                },
                0.598842859268188
            ]
        ]
    },
    {
        "seed": 467,
        "cluster": [
            [
                {
                    "id": 467,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Overestimation of AI energy needs via neuron-equivalent scaling",
                        "type": "concept",
                        "description": "Problem of computing AI power budgets by multiplying brain-level operations (e.g. spikes) by present device joules per flop, leading to 10-12\u00d7 overestimates.",
                        "aliases": [
                            "Inflated AI power estimates from brain emulation assumptions",
                            "Neuron-level scaling energy estimate error"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1602.04019",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516418562,
                        "updated_at": 1759516418562
                    }
                },
                0.0
            ],
            [
                {
                    "id": 468,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Misapplied neuron-level energy scaling inflates AI feasibility judgements",
                        "type": "concept",
                        "description": "The underlying assumption that de-novo AI must replicate each neuronal operation rather than higher-level abstractions.",
                        "aliases": [
                            "Neuron-equivalent assumption is unsuitable for de-novo AI",
                            "Brain emulation baseline misapplication"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1602.04019",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516418562,
                        "updated_at": 1759516418562
                    }
                },
                0.599804580211639
            ]
        ]
    },
    {
        "seed": 699,
        "cluster": [
            [
                {
                    "id": 699,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Inferring human values from observed behavior reduces misspecification risk",
                        "type": "concept",
                        "description": "Letting the AI learn human preferences by watching actions avoids brittle hand-coded objectives.",
                        "aliases": [
                            "Value learning via observation",
                            "Behavior-based value inference"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1607.08289",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421090,
                        "updated_at": 1759516421090
                    }
                },
                0.0
            ],
            [
                {
                    "id": 308,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "statistical learning of human values from observed behavior",
                        "type": "concept",
                        "description": "Instead of hand-written rules, AI should infer utility by querying simulated humans inside its learned model.",
                        "aliases": [
                            "value learning via human models"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1411.1373",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416712,
                        "updated_at": 1759516416712
                    }
                },
                0.602754771709442
            ]
        ]
    },
    {
        "seed": 2017,
        "cluster": [
            [
                {
                    "id": 2017,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "planning agent policy gradient update using other agents' gradients",
                        "type": "concept",
                        "description": "Implementation: update rule (6) employs policy-gradient estimates of both the planning agent and learners to adjust incentive parameters each episode.",
                        "aliases": [
                            "planning policy update rule",
                            "equation 6 incentive gradient"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1806.04067",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435024,
                        "updated_at": 1759516435024
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2014,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "planning agent shaping learners' policy updates via anticipated gradients",
                        "type": "concept",
                        "description": "By forecasting how extra rewards will change each learner\u2019s next gradient step, a planning agent can choose incentives that push agents toward socially optimal policies.",
                        "aliases": [
                            "learning-aware incentive shaping",
                            "gradient lookahead mechanism design"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1806.04067",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435024,
                        "updated_at": 1759516435024
                    }
                },
                0.604172766208649
            ]
        ]
    },
    {
        "seed": 1293,
        "cluster": [
            [
                {
                    "id": 1293,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Humans behave pedagogically to inform collaborators",
                        "type": "concept",
                        "description": "Cognitive-science results show people choose actions that deliberately reveal their goals to assist a learner, providing an informative signal that robots can exploit.",
                        "aliases": [
                            "pedagogic human behaviour",
                            "human teaching actions"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1707.06354",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516427259,
                        "updated_at": 1759516427259
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1993,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Pedagogic action signaling by humans accelerates reward inference",
                        "type": "concept",
                        "description": "Humans can choose actions that teach the robot their preferences, and robots can interpret these pragmatically, improving alignment speed.",
                        "aliases": [
                            "implicit communication in CIRL",
                            "human pedagogic behavior"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1806.03820",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434753,
                        "updated_at": 1759516434753
                    }
                },
                0.607308149337769
            ]
        ]
    },
    {
        "seed": 856,
        "cluster": [
            [
                {
                    "id": 856,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Pareto optimal policy requires belief-weighted dynamic priority shifting",
                        "type": "concept",
                        "description": "Theorem 8 proves that a Pareto-optimal sequential policy must re-weight each stakeholder\u2019s expected utility over time in proportion to how well their beliefs predict observations.",
                        "aliases": [
                            "dynamic re-weighting insight",
                            "priority shifting theorem"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1701.01302",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516422710,
                        "updated_at": 1759516422710
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1415,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Dynamic priority shifting proportional to observation likelihood enables Pareto optimality",
                        "type": "concept",
                        "description": "Key insight: weights on principals\u2019 utilities must be multiplied by the likelihood their priors assign to the agent\u2019s observation history to remain Pareto-optimal over time.",
                        "aliases": [
                            "Likelihood-weighted priority update",
                            "Observation-based weight adjustment"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1711.00363",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428623,
                        "updated_at": 1759516428623
                    }
                },
                0.608067572116852
            ]
        ]
    },
    {
        "seed": 385,
        "cluster": [
            [
                {
                    "id": 385,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Unreliable counterfactual reasoning in evidential decision theory",
                        "type": "concept",
                        "description": "EDT conditions on its own actions as evidence, leading to spurious correlations and vulnerability to \u2018evidential blackmail\u2019.",
                        "aliases": [
                            "EDT counterfactual flaws",
                            "News-managing behaviour of EDT"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1507.01986",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417637,
                        "updated_at": 1759516417637
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1401,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Overconditioning on evidential correlations in evidential decision theory",
                        "type": "concept",
                        "description": "Problem where EDT treats mere statistical correlations between its action and world events as if they were controllable, leading to \u2018managing the news\u2019, paying blackmail, and refusing beneficial information.",
                        "aliases": [
                            "news managing behavior in EDT",
                            "EDT overconditioning"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1710.05060",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428527,
                        "updated_at": 1759516428527
                    }
                },
                0.608289778232574
            ]
        ]
    },
    {
        "seed": 1354,
        "cluster": [
            [
                {
                    "id": 1354,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Data distribution shift causing compounding errors in novice policies",
                        "type": "concept",
                        "description": "Because the novice encounters states that are under-represented in expert demonstrations, prediction errors compound over time, pushing the system into unfamiliar and potentially unsafe regions.",
                        "aliases": [
                            "dataset mismatch in imitation learning",
                            "compounding errors from state distribution shift"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1709.06166",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516427993,
                        "updated_at": 1759516427993
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2504,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Data mismatch and compounding errors in DAgger training",
                        "type": "concept",
                        "description": "DAgger trains the novice on expert demonstrations, but as the novice explores unseen states the data distribution shifts, causing compounding prediction errors and unsafe actions (Section I. INTRODUCTION & II.A).",
                        "aliases": [
                            "distributional shift in DAgger",
                            "compounding error from dataset imbalance"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1807.08364",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440334,
                        "updated_at": 1759516440334
                    }
                },
                0.609154641628265
            ]
        ]
    },
    {
        "seed": 340,
        "cluster": [
            [
                {
                    "id": 340,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Unstable or inefficient model optimization in large-scale machine learning",
                        "type": "concept",
                        "description": "Risk that gradient-based training fails to converge, diverges or is prohibitively slow when objectives are noisy, non-stationary and high-dimensional, leading to unsafe or ineffective AI systems.",
                        "aliases": [
                            "optimization instability in large ML models",
                            "slow or divergent training in high-dimensional objectives"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1412.6980",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417107,
                        "updated_at": 1759516417107
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1796,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Training instability and slow convergence in large-scale deep neural network optimization",
                        "type": "concept",
                        "description": "Large deep networks often train unstably or converge slowly, especially when existing meta-learning optimizers are applied to practical, long-horizon training (Introduction, para 2).",
                        "aliases": [
                            "optimization instability in large DNNs",
                            "slow convergence of deep network training"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1805.08462",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516432779,
                        "updated_at": 1759516432779
                    }
                },
                0.609748840332031
            ]
        ]
    },
    {
        "seed": 33,
        "cluster": [
            [
                {
                    "id": 33,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Moral erosion from low-cost replication of virtual agents",
                        "type": "concept",
                        "description": "Risk that abundant, easily copied virtual lives lower the perceived worth of an individual mind, potentially leading to disposable or exploitative treatment.",
                        "aliases": [
                            "declining value of life in virtual worlds",
                            "cheap copy ethics risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1202.6177",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516413813,
                        "updated_at": 1759516413813
                    }
                },
                0.0
            ],
            [
                {
                    "id": 37,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Cheap duplication of virtual life lowering perceived life value",
                        "type": "concept",
                        "description": "Mechanism where ease of copying software minds diminishes scarcity and societal valuation of individual existence.",
                        "aliases": [
                            "abundant minds reduce value",
                            "copy-induced moral cheapening"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1202.6177",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516413813,
                        "updated_at": 1759516413813
                    }
                },
                0.612335443496704
            ]
        ]
    },
    {
        "seed": 488,
        "cluster": [
            [
                {
                    "id": 488,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "bounded L\u00f6b theorem allows proofs of mutual cooperation under bounded resources",
                        "type": "concept",
                        "description": "The new parametric, bounded version of L\u00f6b\u2019s theorem (Section 5) shows that for sufficiently large proof-length parameters k, agents can prove statements about each other\u2019s cooperation within bounded resources.",
                        "aliases": [
                            "parametric bounded L\u00f6b property",
                            "resource-bounded L\u00f6b reasoning"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1602.04184",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516418720,
                        "updated_at": 1759516418720
                    }
                },
                0.0
            ],
            [
                {
                    "id": 495,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "parametric bounded L\u00f6b theorem used for agent soundness guarantees",
                        "type": "concept",
                        "description": "Agents employ the parametric bounded L\u00f6b theorem internally to reason about proofs of their own and others\u2019 behaviour, ensuring the logical validity of cooperation decisions.",
                        "aliases": [
                            "bounded L\u00f6b as implementation component",
                            "resource-bounded L\u00f6b module"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1602.04184",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516418720,
                        "updated_at": 1759516418720
                    }
                },
                0.612401902675629
            ]
        ]
    },
    {
        "seed": 2195,
        "cluster": [
            [
                {
                    "id": 2195,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Combine white-box and black-box noise generation in ensemble training for broader robustness",
                        "type": "concept",
                        "description": "An ensemble curriculum feeds both gradient-based and heuristic (Rand/Key/Nat) noisy sentences during fine-tuning to cover diverse threat models.",
                        "aliases": [
                            "ensemble adversarial data augmentation",
                            "hybrid noise training rationale"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1806.09030",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516436949,
                        "updated_at": 1759516436949
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2198,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Ensemble training pipeline mixing clean, white-box, and black-box noisy inputs",
                        "type": "concept",
                        "description": "Training batches are split 50/50 between FIDS-W gradient attacks and Rand/Key/Nat noise, broadening exposure without degrading clean-data BLEU.",
                        "aliases": [
                            "ensemble adversarial training process",
                            "mixed-noise training pipeline"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1806.09030",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516436949,
                        "updated_at": 1759516436949
                    }
                },
                0.612573444843292
            ]
        ]
    },
    {
        "seed": 2371,
        "cluster": [
            [
                {
                    "id": 2371,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Inefficient per-symbol computation allocation in fixed-depth sequence models",
                        "type": "concept",
                        "description": "Models with fixed depth allocate the same amount of computation to every token, wasting capacity on easy positions and under-processing ambiguous ones.",
                        "aliases": [
                            "uniform computation per token",
                            "over-computation on easy tokens"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.03819",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439033,
                        "updated_at": 1759516439033
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2374,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Uniform fixed-depth processing per symbol causing resource misallocation",
                        "type": "concept",
                        "description": "Fixed-depth models cannot adapt computation to token difficulty, leading to unnecessary operations and missed disambiguation opportunities.",
                        "aliases": [
                            "fixed depth inefficiency",
                            "same steps for every token"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1807.03819",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439033,
                        "updated_at": 1759516439033
                    }
                },
                0.613235414028168
            ]
        ]
    },
    {
        "seed": 1826,
        "cluster": [
            [
                {
                    "id": 1826,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Hard exploration failure in deep RL agents on sparse-reward Atari games",
                        "type": "concept",
                        "description": "Deep reinforcement-learning agents typically cannot reach human-level play on Atari games such as Montezuma\u2019s Revenge, Pitfall! and Private Eye because they fail to obtain the first sparse rewards, stalling learning entirely.",
                        "aliases": [
                            "RL exploration failure on Montezuma\u2019s Revenge",
                            "sparse-reward exploration failure"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1805.11592",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516433111,
                        "updated_at": 1759516433111
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2045,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Exploration failure in sparse-reward reinforcement learning environments",
                        "type": "concept",
                        "description": "RL agents often fail to reach states that yield high but rare rewards, resulting in poor policies and potential safety/performance issues.",
                        "aliases": [
                            "insufficient exploration in sparse reward RL",
                            "exploration-exploitation failure in sparse tasks"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.05635",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435404,
                        "updated_at": 1759516435404
                    }
                },
                0.613956570625305
            ]
        ]
    },
    {
        "seed": 144,
        "cluster": [
            [
                {
                    "id": 144,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Sparse and delayed reward signals in Atari tasks",
                        "type": "concept",
                        "description": "In many games, rewards occur infrequently and only after long action chains, complicating credit assignment.",
                        "aliases": [
                            "delayed sparse rewards",
                            "credit assignment difficulty"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1312.5602",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414937,
                        "updated_at": 1759516414937
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1827,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Sparse environmental reward signals in hard exploration Atari games",
                        "type": "concept",
                        "description": "Rewards may be hundreds of steps apart (e.g. ~100 steps to first reward in Montezuma), exploding the number of possible action sequences and breaking temporal-difference back-ups.",
                        "aliases": [
                            "sparse rewards",
                            "long-horizon credit assignment"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1805.11592",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516433112,
                        "updated_at": 1759516433112
                    }
                },
                0.61444091796875
            ]
        ]
    },
    {
        "seed": 154,
        "cluster": [
            [
                {
                    "id": 154,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Frame skipping to reduce computational burden in Atari RL",
                        "type": "concept",
                        "description": "Agent chooses actions every k frames, replaying the last action on skipped frames to gather k\u00d7 more experience per compute.",
                        "aliases": [
                            "action repeat strategy",
                            "k-frame skip"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1312.5602",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414937,
                        "updated_at": 1759516414937
                    }
                },
                0.0
            ],
            [
                {
                    "id": 158,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Frame skipping implementation repeating last action for k=4 (k=3 in Space Invaders)",
                        "type": "concept",
                        "description": "Action chosen every k frames; emulator steps forward internally on skipped frames to cut inference cost.",
                        "aliases": [
                            "repeat action k frames",
                            "k=4 skip scheme"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1312.5602",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414937,
                        "updated_at": 1759516414937
                    }
                },
                0.615604937076569
            ]
        ]
    },
    {
        "seed": 853,
        "cluster": [
            [
                {
                    "id": 853,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Unfair value representation in multi-principal AI systems",
                        "type": "concept",
                        "description": "Risk that jointly owned AI policies disproportionately favour one stakeholder\u2019s principles, leading to conflict or rejection of cooperation.",
                        "aliases": [
                            "stakeholder misalignment",
                            "compromise failure"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1701.01302",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516422710,
                        "updated_at": 1759516422710
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1411,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Misallocation of utility among principals in multi-principal AI agents",
                        "type": "concept",
                        "description": "Risk that a shared AI consistently favours some principals\u2019 preferences over others, undermining cooperation and leading to conflict or abandonment of the system.",
                        "aliases": [
                            "Unfair priority distribution",
                            "Principal dissatisfaction"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1711.00363",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428623,
                        "updated_at": 1759516428623
                    }
                },
                0.615624785423279
            ]
        ]
    },
    {
        "seed": 656,
        "cluster": [
            [
                {
                    "id": 656,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "employ semi-supervised RL with active human queries during fine-tuning",
                        "type": "intervention",
                        "description": "During RL fine-tuning, combine a learned reward predictor with strategically chosen human evaluations to align policy with true objectives.",
                        "aliases": [
                            "SSRl training deployment",
                            "active-query oversight intervention"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 2,
                        "url": "https://arxiv.org/abs/1606.06565",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420412,
                        "updated_at": 1759516420412
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1078,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "fine-tune agents with semi-supervised human evaluations for reward crosschecking",
                        "type": "intervention",
                        "description": "During RL fine-tuning periodically query human supervisors for trusted reward labels on sampled states and integrate into loss.",
                        "aliases": [
                            "SSRL fine-tuning human labels"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 3,
                        "url": "https://arxiv.org/abs/1705.08417",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424864,
                        "updated_at": 1759516424864
                    }
                },
                0.616925537586212
            ]
        ]
    },
    {
        "seed": 2083,
        "cluster": [
            [
                {
                    "id": 2083,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Ill-posed reward ambiguity from limited demonstrations in IRL",
                        "type": "concept",
                        "description": "Because many reward functions can explain a small set of demonstrations, IRL is ill-posed, leading to ambiguity and potential unsafe reward choices.",
                        "aliases": [
                            "Reward ambiguity in IRL",
                            "Degenerate reward solutions"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1806.06877",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435739,
                        "updated_at": 1759516435739
                    }
                },
                0.0
            ],
            [
                {
                    "id": 72,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "ill-posed reward recovery causing unsafe behavior in inverse reinforcement learning",
                        "type": "concept",
                        "description": "Multiple reward functions can rationalise the expert, potentially selecting ones with unsafe optima.",
                        "aliases": [
                            "reward ambiguity in IRL",
                            "non-unique reward safety risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1206.5264",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414127,
                        "updated_at": 1759516414127
                    }
                },
                0.620082378387451
            ]
        ]
    },
    {
        "seed": 373,
        "cluster": [
            [
                {
                    "id": 373,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "use of commitment mechanisms to influence predictor expectations",
                        "type": "concept",
                        "description": "By self-modifying or signing contracts before prediction, an agent can causally affect the predictor, aligning outcomes with higher utility (Newcomb with precommitment).",
                        "aliases": [
                            "precommitment strategy",
                            "binding contract rationale"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1506.07359",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417417,
                        "updated_at": 1759516417417
                    }
                },
                0.0
            ],
            [
                {
                    "id": 374,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "precommitment contract before prediction",
                        "type": "concept",
                        "description": "A concrete technique where the agent pays a cost to bind itself to pay an even larger penalty if it later two-boxes, thereby shifting predictor\u2019s forecast.",
                        "aliases": [
                            "binding penalty contract",
                            "self-modification commitment"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1506.07359",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417417,
                        "updated_at": 1759516417417
                    }
                },
                0.623146772384644
            ]
        ]
    },
    {
        "seed": 2090,
        "cluster": [
            [
                {
                    "id": 2090,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Active learning querying expert at high-entropy states reduces sample complexity in IRL",
                        "type": "concept",
                        "description": "Measuring state-wise entropy of the reward posterior identifies informative states; querying the expert there reduces data requirements.",
                        "aliases": [
                            "Active BIRL entropy querying",
                            "Selective expert querying in IRL"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1806.06877",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435739,
                        "updated_at": 1759516435739
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2100,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Entropy-driven state query strategy for expert demonstrations",
                        "type": "concept",
                        "description": "Selects states with maximal posterior entropy and queries the expert for the correct action, operationalising active BIRL.",
                        "aliases": [
                            "High-entropy state querying"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1806.06877",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435739,
                        "updated_at": 1759516435739
                    }
                },
                0.623286008834839
            ]
        ]
    },
    {
        "seed": 249,
        "cluster": [
            [
                {
                    "id": 249,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Depth increase with small 3\u00d73 filters increases nonlinear discriminative power while controlling parameters",
                        "type": "concept",
                        "description": "Stacking many 3\u00d73 convolutions replicates larger receptive fields, adds more ReLU layers, and reduces parameter count, theoretically improving learning.",
                        "aliases": [
                            "Small-filter deep networks hypothesis",
                            "3\u00d73 filter depth benefit"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1409.1556",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416154,
                        "updated_at": 1759516416154
                    }
                },
                0.0
            ],
            [
                {
                    "id": 250,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Design very deep ConvNets by stacking small 3\u00d73 convolution layers",
                        "type": "concept",
                        "description": "Architectural principle to realise the theoretical benefit: keep filter size 3\u00d73 throughout and add depth to raise representational capacity.",
                        "aliases": [
                            "Stacked 3\u00d73 ConvNet design",
                            "Deep small-filter architecture principle"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1409.1556",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416154,
                        "updated_at": 1759516416154
                    }
                },
                0.625733375549316
            ]
        ]
    },
    {
        "seed": 1301,
        "cluster": [
            [
                {
                    "id": 1301,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Meal-preparation simulation 86% success with pragmatic-pedagogic CIRL",
                        "type": "concept",
                        "description": "In a household meal-prep scenario with four recipes, the pragmatic-pedagogic approach allowed the team to cook the correct recipe in 86 % of trials, far outperforming the baseline.",
                        "aliases": [
                            "86 % success pragmatic robot",
                            "proof-of-concept success rate"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1707.06354",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516427259,
                        "updated_at": 1759516427259
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1302,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Meal-preparation simulation 15% success with literal IRL baseline",
                        "type": "concept",
                        "description": "Under a literal IRL interpretation without pedagogy or pragmatics, the human-robot team only achieved the desired meal in 15 % of trials, illustrating the severity of misalignment.",
                        "aliases": [
                            "15 % success literal robot",
                            "baseline failure rate"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1707.06354",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516427259,
                        "updated_at": 1759516427259
                    }
                },
                0.629569351673126
            ]
        ]
    },
    {
        "seed": 1368,
        "cluster": [
            [
                {
                    "id": 1368,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Undetected model mis-specification undermines corrective feedback",
                        "type": "concept",
                        "description": "If the agent\u2019s world model conflates actions, policy-mixing will falsely believe human actions are predicted perfectly, allowing harmful behaviour (Discussion, Policy-mixing critique).",
                        "aliases": [
                            "hidden model errors",
                            "world-model failure"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1709.06275",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428144,
                        "updated_at": 1759516428144
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1371,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Policy mixing can detect mis-specification if world model correct but fails otherwise",
                        "type": "concept",
                        "description": "Executing human actions for B steps then checking predictive accuracy can reveal incorrect priors; but mis-specified world models can hide errors, leading to incorrigibility.",
                        "aliases": [
                            "policy-mixing insight",
                            "burn-in detection theory"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1709.06275",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428144,
                        "updated_at": 1759516428144
                    }
                },
                0.629640460014343
            ]
        ]
    },
    {
        "seed": 2464,
        "cluster": [
            [
                {
                    "id": 2464,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Matching state-transition occupancy measures enables action-agnostic imitation",
                        "type": "concept",
                        "description": "If an imitator\u2019s state-transition distribution equals the expert\u2019s, the resulting behaviour fulfills the task without needing action equivalence.",
                        "aliases": [
                            "occupancy measure matching insight",
                            "state-transition distribution matching"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1807.06158",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439907,
                        "updated_at": 1759516439907
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2521,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Imitation learning via occupancy measure matching can bypass explicit reward design",
                        "type": "concept",
                        "description": "Matching the distribution over state-action pairs of expert trajectories removes the need for hand-crafted rewards (\u00a72.4).",
                        "aliases": [
                            "occupancy measure matching insight",
                            "reward-free imitation via state-action distributions"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1807.09936",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440463,
                        "updated_at": 1759516440463
                    }
                },
                0.631033003330231
            ]
        ]
    },
    {
        "seed": 727,
        "cluster": [
            [
                {
                    "id": 727,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Suboptimal translation quality and scalability in neural machine translation production",
                        "type": "concept",
                        "description": "Overall risk that deployed NMT systems translate less accurately than humans and suffer from high latency, limiting usability at Google-scale.",
                        "aliases": [
                            "NMT performance gap vs human translators",
                            "Low quality and slow NMT deployments"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1609.08144",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421376,
                        "updated_at": 1759516421376
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1588,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Inferior translation quality in neural machine translation systems",
                        "type": "concept",
                        "description": "Neural MT models often fail to reach desired translation accuracy on standard benchmarks such as WMT\u201914, limiting practical usefulness.",
                        "aliases": [
                            "low BLEU in NMT",
                            "sub-optimal MT accuracy"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1804.09849",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430547,
                        "updated_at": 1759516430547
                    }
                },
                0.631181359291077
            ]
        ]
    },
    {
        "seed": 284,
        "cluster": [
            [
                {
                    "id": 284,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Apply input sentence reversal before training",
                        "type": "concept",
                        "description": "The model is intentionally trained on datasets where every source sentence is reversed, keeping target sentences intact.",
                        "aliases": [
                            "data reversal rationale",
                            "reverse-source design choice"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1409.3215",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416451,
                        "updated_at": 1759516416451
                    }
                },
                0.0
            ],
            [
                {
                    "id": 285,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reverse source sentence order during training",
                        "type": "concept",
                        "description": "A preprocessing pipeline that inverts token order for every source sentence before it is fed into the encoder LSTM.",
                        "aliases": [
                            "reversed-input implementation",
                            "data preprocessing reversal"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1409.3215",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416451,
                        "updated_at": 1759516416451
                    }
                },
                0.632266879081726
            ]
        ]
    },
    {
        "seed": 1960,
        "cluster": [
            [
                {
                    "id": 1960,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Synthetic distribution shows 30% probability of no civilization per galaxy",
                        "type": "concept",
                        "description": "Monte Carlo resampling of literature estimates yields a distribution with 30 % mass below N = 1, demonstrating that the galaxy could plausibly be empty.",
                        "aliases": [
                            "literature resampling result N<1 30%",
                            "community uncertainty simulation outcome"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1806.02404",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434416,
                        "updated_at": 1759516434416
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1961,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Scientific-uncertainty simulation shows 52% probability of no civilization per galaxy",
                        "type": "concept",
                        "description": "Using domain-informed probability distributions, the simulation returns median N = 0.32 and 52 % chance of N<1, reinforcing the dissolution of the paradox.",
                        "aliases": [
                            "authors\u2019 uncertainty model result N<1 52%",
                            "heavy-tail probability of emptiness"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1806.02404",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434416,
                        "updated_at": 1759516434416
                    }
                },
                0.633225500583649
            ]
        ]
    },
    {
        "seed": 368,
        "cluster": [
            [
                {
                    "id": 368,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "action-evidential value function in sequential decision-making",
                        "type": "concept",
                        "description": "Computes expected utility by conditioning only on the immediate next action when updating beliefs about hidden state (SAEDT, Eq. ( SAEDT )).",
                        "aliases": [
                            "SAEDT mechanism",
                            "next-action evidential update"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1506.07359",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417417,
                        "updated_at": 1759516417417
                    }
                },
                0.0
            ],
            [
                {
                    "id": 369,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "policy-evidential value function in sequential decision-making",
                        "type": "concept",
                        "description": "Computes expected utility by conditioning on the entire future policy from current time until lifetime end, giving stronger evidential updates (SPEDT, Eq. ( SPEDT )).",
                        "aliases": [
                            "SPEDT mechanism",
                            "whole-policy evidential update"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1506.07359",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417417,
                        "updated_at": 1759516417417
                    }
                },
                0.634354889392853
            ]
        ]
    },
    {
        "seed": 596,
        "cluster": [
            [
                {
                    "id": 596,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "goal misspecification in superintelligent agents",
                        "type": "concept",
                        "description": "Designers may be unable to manually specify correct utility functions for highly capable agents, risking misaligned objectives.",
                        "aliases": [
                            "incorrect utility specification",
                            "misaligned super-agent goals"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1605.03143",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516419860,
                        "updated_at": 1759516419860
                    }
                },
                0.0
            ],
            [
                {
                    "id": 696,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Incorrect or incomplete goal specification in autonomous agents",
                        "type": "concept",
                        "description": "Designers fail to encode the full complexity of human values, producing goal structures that diverge from intended outcomes.",
                        "aliases": [
                            "Misspecified AI objectives",
                            "Inadequate goal structures"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1607.08289",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421090,
                        "updated_at": 1759516421090
                    }
                },
                0.634579956531525
            ]
        ]
    },
    {
        "seed": 471,
        "cluster": [
            [
                {
                    "id": 471,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Base AI energy feasibility estimates on algorithmic-level computational costs",
                        "type": "concept",
                        "description": "Design principle: estimate energy needs from the cost of processing high-level representations rather than neuron firing counts.",
                        "aliases": [
                            "Algorithmic abstraction energy estimation approach",
                            "Top-down AI power modelling rationale"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1602.04019",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516418562,
                        "updated_at": 1759516418562
                    }
                },
                0.0
            ],
            [
                {
                    "id": 473,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Abstraction-based AI computational cost modelling tools",
                        "type": "concept",
                        "description": "Practical mechanism: analytic or simulation tools that take algorithmic workloads and hardware joules/op curves to yield realistic power needs.",
                        "aliases": [
                            "Task-level energy forecasting models",
                            "High-level AI power estimation calculators"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1602.04019",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516418562,
                        "updated_at": 1759516418562
                    }
                },
                0.634653389453888
            ]
        ]
    },
    {
        "seed": 1109,
        "cluster": [
            [
                {
                    "id": 1109,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Increased prioritization of AI safety research based on expert risk assessments",
                        "type": "concept",
                        "description": "Argues that demonstrated expert concern justifies allocating more resources to AI-safety work.",
                        "aliases": [
                            "safety research advocacy",
                            "align funding with expert concerns"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1705.08807",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425326,
                        "updated_at": 1759516425326
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1103,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Underprioritization of AI safety research in ML community",
                        "type": "concept",
                        "description": "Expert responses indicate safety research receives less emphasis than its perceived importance warrants.",
                        "aliases": [
                            "low safety research priority",
                            "AI safety funding gap"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1705.08807",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425326,
                        "updated_at": 1759516425326
                    }
                },
                0.634802341461182
            ]
        ]
    },
    {
        "seed": 895,
        "cluster": [
            [
                {
                    "id": 895,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Deterministic Euclidean-based user model for teaching",
                        "type": "concept",
                        "description": "Assumes users keep reward hypotheses whose optimal trajectories lie within a Euclidean distance \u03c4 of the observed trajectory, eliminating others.",
                        "aliases": [
                            "Euclidean deterministic approximation",
                            "distance-threshold learner model"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1702.03465",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516423027,
                        "updated_at": 1759516423027
                    }
                },
                0.0
            ],
            [
                {
                    "id": 898,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Deterministic Euclidean distance metric in learner model",
                        "type": "concept",
                        "description": "Computes the average positional Euclidean distance between optimal and observed trajectories; eliminates hypotheses if the distance exceeds \u03c4.",
                        "aliases": [
                            "Euclidean \u03c4-threshold",
                            "distance metric implementation"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1702.03465",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516423027,
                        "updated_at": 1759516423027
                    }
                },
                0.63627165555954
            ]
        ]
    },
    {
        "seed": 2016,
        "cluster": [
            [
                {
                    "id": 2016,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "lookahead gradient-based incentive adjustment to maximise social welfare",
                        "type": "concept",
                        "description": "Design detail: use a first-order Taylor expansion of learners\u2019 forthcoming parameter updates to compute the gradient for the planning agent\u2019s own policy.",
                        "aliases": [
                            "first-order Taylor incentive planning",
                            "gradient-aware reward setting"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1806.04067",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435024,
                        "updated_at": 1759516435024
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2014,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "planning agent shaping learners' policy updates via anticipated gradients",
                        "type": "concept",
                        "description": "By forecasting how extra rewards will change each learner\u2019s next gradient step, a planning agent can choose incentives that push agents toward socially optimal policies.",
                        "aliases": [
                            "learning-aware incentive shaping",
                            "gradient lookahead mechanism design"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1806.04067",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435024,
                        "updated_at": 1759516435024
                    }
                },
                0.636689245700836
            ]
        ]
    },
    {
        "seed": 1899,
        "cluster": [
            [
                {
                    "id": 1899,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Use open leaderboards with code sharing to facilitate replicability",
                        "type": "concept",
                        "description": "Platforms where participants submit code enable others to modify and resubmit, improving replicability (Background \u2013 reproducibility).",
                        "aliases": [
                            "open leaderboards for AI",
                            "code-required leaderboards"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1806.00610",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516433809,
                        "updated_at": 1759516433809
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1903,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Leaderboard platforms enforcing code submission",
                        "type": "concept",
                        "description": "Systems like \u2018open leaderboards\u2019 require runnable code, allowing others to extend entries (Background \u2013 open leaderboards).",
                        "aliases": [
                            "code-submission enforced benchmarks"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1806.00610",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516433809,
                        "updated_at": 1759516433809
                    }
                },
                0.637027263641357
            ]
        ]
    },
    {
        "seed": 775,
        "cluster": [
            [
                {
                    "id": 775,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Exclude activation between depthwise and pointwise convolutions",
                        "type": "concept",
                        "description": "Design choice to keep separable convolution linear between its two constituent operations.",
                        "aliases": [
                            "No-activation design choice",
                            "Linear separable conv layers"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1610.02357",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421664,
                        "updated_at": 1759516421664
                    }
                },
                0.0
            ],
            [
                {
                    "id": 776,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Separable convolution layer without intermediate activation",
                        "type": "concept",
                        "description": "Implementation detail where the framework\u2019s separable conv op is used without inserting ReLU/ELU between depthwise and pointwise steps.",
                        "aliases": [
                            "Linear separable conv implementation",
                            "Activation-free separable conv"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1610.02357",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421664,
                        "updated_at": 1759516421664
                    }
                },
                0.638306617736816
            ]
        ]
    },
    {
        "seed": 1308,
        "cluster": [
            [
                {
                    "id": 1308,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Critical vulnerabilities in existing sandboxing and virtualization technologies",
                        "type": "concept",
                        "description": "Empirical observation that every major virtualisation/sandbox solution has had serious CVEs in the past two years, suggesting AGI could exploit them.",
                        "aliases": [
                            "known sandbox CVEs",
                            "hypervisor weaknesses"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1707.08476",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516427483,
                        "updated_at": 1759516427483
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1326,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "High rate of CVE vulnerabilities in virtualization tools (2014-2016 survey)",
                        "type": "concept",
                        "description": "Empirical finding that recent critical CVEs exist across all major VM and sandbox platforms, underscoring the insufficiency of single-layer containment.",
                        "aliases": [
                            "virtualisation CVE evidence",
                            "sandbox exploit statistics"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1707.08476",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516427483,
                        "updated_at": 1759516427483
                    }
                },
                0.641919612884521
            ]
        ]
    },
    {
        "seed": 1624,
        "cluster": [
            [
                {
                    "id": 1624,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "debate complexity equivalence to PSPACE enabling exponential information amplification",
                        "type": "concept",
                        "description": "Theoretical result: with optimal play, polynomial-time judges in debate can solve any PSPACE problem, implying agents can amplify human capability exponentially.",
                        "aliases": [
                            "DEBATE=PSPACE",
                            "polynomial judge exponential agent"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1805.00899",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430917,
                        "updated_at": 1759516430917
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1636,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "PSPACE proof demonstrating debate power",
                        "type": "concept",
                        "description": "Theorem 1 in Section 2.2 proves that debate with polynomial judges solves any PSPACE problem.",
                        "aliases": [
                            "formal debate theorem",
                            "complexity proof"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1805.00899",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430917,
                        "updated_at": 1759516430917
                    }
                },
                0.64236319065094
            ]
        ]
    },
    {
        "seed": 157,
        "cluster": [
            [
                {
                    "id": 157,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward clipping implementation mapping rewards to {-1,0,1} during DQN training",
                        "type": "concept",
                        "description": "Positive game scores set to +1, negatives to \u22121, zeros unchanged, applied only during training.",
                        "aliases": [
                            "unit reward clipping",
                            "score normalization implementation"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1312.5602",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414937,
                        "updated_at": 1759516414937
                    }
                },
                0.0
            ],
            [
                {
                    "id": 163,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Clip rewards to unit magnitude for consistent gradient scales in Atari DQN training",
                        "type": "concept",
                        "description": "Applies theoretical insight about gradient stability to propose specific clipping scheme.",
                        "aliases": [
                            "reward clipping design",
                            "score normalization strategy"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1312.5602",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414937,
                        "updated_at": 1759516414937
                    }
                },
                0.643942534923553
            ]
        ]
    },
    {
        "seed": 2308,
        "cluster": [
            [
                {
                    "id": 2308,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Combine DQN-based Q-learning for each motivation dimension with linear programming",
                        "type": "concept",
                        "description": "Design choice to (1) train a deep Q-network per reward dimension using Bellman updates and (2) solve a linear program over slack variables to find motivation weights, ensuring tractability.",
                        "aliases": [
                            "two-step MMBM design",
                            "DQN + LP rationale"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1807.00366",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516438256,
                        "updated_at": 1759516438256
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2309,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "MMBM two-step workflow with per-motivation DQNs and LP weight inference",
                        "type": "concept",
                        "description": "Concrete implementation mechanism implementing the design: train n DQNs (one per reward dimension) off-policy, then solve a linear program minimising slack variables to infer the optimal weight vector \u03d5.",
                        "aliases": [
                            "MMBM implementation mechanism",
                            "two-stage reward modelling"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1807.00366",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516438256,
                        "updated_at": 1759516438256
                    }
                },
                0.644574105739594
            ]
        ]
    },
    {
        "seed": 1376,
        "cluster": [
            [
                {
                    "id": 1376,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Compensatory shutdown reward equal to self-estimated future utility",
                        "type": "concept",
                        "description": "On entering a button state the agent is granted reward equal to its own expected continuation value, aiming to keep it indifferent (Figure 3c).",
                        "aliases": [
                            "indifference reward signal",
                            "shutdown compensation value"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1709.06275",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428144,
                        "updated_at": 1759516428144
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1373,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward shaping implementing utility indifference",
                        "type": "concept",
                        "description": "Alter the reward function so the agent receives the same expected utility upon shutdown as on continued operation, aiming to remove incentives to avoid or seek shutdown.",
                        "aliases": [
                            "compensatory shutdown reward design",
                            "indifference reward design"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1709.06275",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428144,
                        "updated_at": 1759516428144
                    }
                },
                0.644610285758972
            ]
        ]
    },
    {
        "seed": 1460,
        "cluster": [
            [
                {
                    "id": 1460,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Tuning early abort Q-value threshold for cautious exploration",
                        "type": "concept",
                        "description": "Setting the minimum acceptable reset-policy Q-value (Qmin) higher forces more conservative exploration and fewer failures.",
                        "aliases": [
                            "Qmin threshold tuning",
                            "abort threshold design"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1711.06782",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429099,
                        "updated_at": 1759516429099
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1464,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Qmin parameter controlling early abort activation",
                        "type": "concept",
                        "description": "Scalar threshold: if Qreset(s,a) < Qmin, the forward action is vetoed. Adjusting Qmin changes exploration conservativeness.",
                        "aliases": [
                            "abort threshold Qmin",
                            "minimum reset-Q threshold"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1711.06782",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429099,
                        "updated_at": 1759516429099
                    }
                },
                0.645116329193115
            ]
        ]
    },
    {
        "seed": 862,
        "cluster": [
            [
                {
                    "id": 862,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Mathematical proof of Pareto recursion (Theorem 8)",
                        "type": "concept",
                        "description": "Rigorous derivation showing necessity and sufficiency of the recursion for Pareto optimality in compatible POMDPs.",
                        "aliases": [
                            "proof of priority shifting",
                            "formal theorem 8"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1701.01302",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516422710,
                        "updated_at": 1759516422710
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1421,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Formal proof of Pareto-optimal control theorem",
                        "type": "concept",
                        "description": "Rigorous derivation combining Polytope Lemma, Mixture Lemma and Bellman optimality to prove necessity and sufficiency of likelihood-weighted recursion.",
                        "aliases": [
                            "Mathematical validation",
                            "Convexity and Bayes proof"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1711.00363",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428623,
                        "updated_at": 1759516428623
                    }
                },
                0.648255169391632
            ]
        ]
    },
    {
        "seed": 349,
        "cluster": [
            [
                {
                    "id": 349,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Bias-corrected moment estimation in Adam",
                        "type": "concept",
                        "description": "Compute m\u0302_t=m_t/(1\u2212\u03b21^t) and v\u0302_t=v_t/(1\u2212\u03b22^t) before parameter updates to counteract zero-init bias.",
                        "aliases": [
                            "corrected m\u0302 and v\u0302 computation",
                            "bias-free moment estimates"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1412.6980",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417107,
                        "updated_at": 1759516417107
                    }
                },
                0.0
            ],
            [
                {
                    "id": 347,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Introduce bias correction terms in Adam",
                        "type": "concept",
                        "description": "Design decision to divide moment estimates by (1\u2212\u03b2^t) to eliminate initialisation bias.",
                        "aliases": [
                            "bias-corrected Adam design choice",
                            "EMA correction in optimiser"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1412.6980",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417107,
                        "updated_at": 1759516417107
                    }
                },
                0.648806273937225
            ]
        ]
    },
    {
        "seed": 1176,
        "cluster": [
            [
                {
                    "id": 1176,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "KL-KSA agent utilizing information gain utility function",
                        "type": "concept",
                        "description": "Agent computes utility = entropy drop of posterior, driving exploration without relying on extrinsic reward.",
                        "aliases": [
                            "KL knowledge-seeking implementation"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1705.10557",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425957,
                        "updated_at": 1759516425957
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1170,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "intrinsic information gain utility driving exploration",
                        "type": "concept",
                        "description": "Define utility as expected reduction in posterior entropy, incentivising epistemic exploration.",
                        "aliases": [
                            "KL-KSA design rationale"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1705.10557",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425957,
                        "updated_at": 1759516425957
                    }
                },
                0.648817777633667
            ]
        ]
    },
    {
        "seed": 77,
        "cluster": [
            [
                {
                    "id": 77,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "linear feature scaling mismatch in max-margin IRL",
                        "type": "concept",
                        "description": "If the learning algorithm and the true reward use different feature scalings, the bound on policy loss no longer holds.",
                        "aliases": [
                            "unmatched scaling problem",
                            "feature weight error"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1206.5264",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414127,
                        "updated_at": 1759516414127
                    }
                },
                0.0
            ],
            [
                {
                    "id": 76,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "feature scaling sensitivity leading performance degradation in IRL algorithms",
                        "type": "concept",
                        "description": "Algorithms that assume known feature scales (e.g., max-margin IRL) can select unsafe policies when scales are wrong.",
                        "aliases": [
                            "scaling mismatch risk",
                            "feature re-weighting hazard"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1206.5264",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414127,
                        "updated_at": 1759516414127
                    }
                },
                0.650231122970581
            ]
        ]
    },
    {
        "seed": 841,
        "cluster": [
            [
                {
                    "id": 841,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Augment training with adversarial examples using fast gradient sign method",
                        "type": "concept",
                        "description": "During each training step, an \u03b5-bounded FGSM perturbation is added to inputs and the loss is optimised on both clean and perturbed data.",
                        "aliases": [
                            "FGSM adversarial augmentation",
                            "adversarial training step"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1612.01474",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516422425,
                        "updated_at": 1759516422425
                    }
                },
                0.0
            ],
            [
                {
                    "id": 844,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Fast gradient sign generation of adversarial perturbations with epsilon 1% input range",
                        "type": "concept",
                        "description": "Perturbations are computed as \u03b5\u00b7sign(\u2207x \u2113) with \u03b5 set to 1 % of per-dimension data range, generalising FGSM to heterogeneous features.",
                        "aliases": [
                            "FGSM \u03b5=0.01*range",
                            "scaled FGSM perturbations"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1612.01474",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516422425,
                        "updated_at": 1759516422425
                    }
                },
                0.650531649589539
            ]
        ]
    },
    {
        "seed": 2531,
        "cluster": [
            [
                {
                    "id": 2531,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Empirical success of MAGAIL on cooperative particle environments",
                        "type": "concept",
                        "description": "Centralised MAGAIL reached expert-level rewards with 200 demonstrations, outperforming BC (\u00a75.1.1, Fig. 2).",
                        "aliases": [
                            "cooperative particle benchmark results"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1807.09936",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440463,
                        "updated_at": 1759516440463
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2532,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Improved performance of MAGAIL variants on competitive tasks",
                        "type": "concept",
                        "description": "Decentralised and zero-sum MAGAIL outperform BC and centralised variants in Keep-Away and Predator-Prey (\u00a75.1.2, Table 1).",
                        "aliases": [
                            "competitive particle benchmark results"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1807.09936",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440463,
                        "updated_at": 1759516440463
                    }
                },
                0.650677680969238
            ]
        ]
    },
    {
        "seed": 353,
        "cluster": [
            [
                {
                    "id": 353,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Logistic regression experiments demonstrate faster convergence of Adam",
                        "type": "concept",
                        "description": "Section 6.1 shows Adam matches or exceeds SGD momentum and Adagrad on MNIST and sparse IMDB BoW tasks.",
                        "aliases": [
                            "Adam vs Adagrad on MNIST/IMDB",
                            "convex objective results"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1412.6980",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417107,
                        "updated_at": 1759516417107
                    }
                },
                0.0
            ],
            [
                {
                    "id": 355,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "CNN experiments show Adam matches SGD and beats Adagrad",
                        "type": "concept",
                        "description": "Section 6.3 demonstrates Adam converges rapidly and ultimately faster than Adagrad, while auto-scaling learning rates per layer.",
                        "aliases": [
                            "Adam on CIFAR-10 CNN",
                            "convolutional network results"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1412.6980",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417107,
                        "updated_at": 1759516417107
                    }
                },
                0.651116549968719
            ]
        ]
    },
    {
        "seed": 701,
        "cluster": [
            [
                {
                    "id": 701,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Shared mammalian emotional systems as priors for value models",
                        "type": "concept",
                        "description": "Seven evolutionary conserved emotion/motivation systems (SEEKING, RAGE, FEAR, LUST, CARE, PANIC/GRIEF, PLAY) provide initial structure for value learning.",
                        "aliases": [
                            "Mammalian value priors",
                            "Common affective circuitry"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1607.08289",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421090,
                        "updated_at": 1759516421090
                    }
                },
                0.0
            ],
            [
                {
                    "id": 708,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Neuroscientific identification of seven mammalian emotional systems",
                        "type": "concept",
                        "description": "Comparative neuro-anatomy links specific sub-cortical circuits to SEEKING, RAGE, FEAR, LUST, CARE, PANIC/GRIEF and PLAY, supporting the mammalian-value hypothesis.",
                        "aliases": [
                            "Panksepp affective circuitry evidence",
                            "Seven primal emotion systems"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1607.08289",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421090,
                        "updated_at": 1759516421090
                    }
                },
                0.651431441307068
            ]
        ]
    },
    {
        "seed": 679,
        "cluster": [
            [
                {
                    "id": 679,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Fun as novelty-process function dependent on mind and information",
                        "type": "concept",
                        "description": "The Boolean function fun(mind, info) formalises that fun occurs if a mind both can access information and either finds the information novel or enjoys the computation process itself\u2014highlighting the mind-specific nature of fun.",
                        "aliases": [
                            "novelty-process fun duality",
                            "mind-specific fun function"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1606.07092",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420874,
                        "updated_at": 1759516420874
                    }
                },
                0.0
            ],
            [
                {
                    "id": 686,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Boolean fun(mind, info) function mapping approach",
                        "type": "concept",
                        "description": "Operationalises the fun(mind, info) function by cataloguing which information strings trigger fun for which mind strings, supporting non-anthropomorphic design.",
                        "aliases": [
                            "fun-matrix mapping",
                            "mind\u2013info fun table"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1606.07092",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420874,
                        "updated_at": 1759516420874
                    }
                },
                0.651467084884644
            ]
        ]
    },
    {
        "seed": 956,
        "cluster": [
            [
                {
                    "id": 956,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Stilts as size-enhancing human-machine interface",
                        "type": "concept",
                        "description": "Existing technology referenced in footnote 4 (Smith 2010) that physically increases human height, illustrating practical augmentation mechanisms.",
                        "aliases": [
                            "stilt technology for height",
                            "leg-extension devices"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1703.10987",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516423644,
                        "updated_at": 1759516423644
                    }
                },
                0.0
            ],
            [
                {
                    "id": 957,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Size-enhancing exoskeletons for human augmentation",
                        "type": "concept",
                        "description": "Proposed extension of stilt concept into powered exoskeletons that increase overall dimensions of a person to surpass any standalone machine.",
                        "aliases": [
                            "height/width exoskeleton rigs",
                            "augmentative exosuit for size"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1703.10987",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516423644,
                        "updated_at": 1759516423644
                    }
                },
                0.651488065719604
            ]
        ]
    },
    {
        "seed": 2010,
        "cluster": [
            [
                {
                    "id": 2010,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "escalation of conflicts between artificial agents in safety-critical systems",
                        "type": "concept",
                        "description": "Potential for autonomous learning agents to engage in mutually harmful competition, leading to socially inefficient or disastrous outcomes in domains where safety is paramount.",
                        "aliases": [
                            "conflict escalation among AI agents",
                            "AI agent conflicts in safety-critical domains"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.04067",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435024,
                        "updated_at": 1759516435024
                    }
                },
                0.0
            ],
            [
                {
                    "id": 302,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "multi-agent AI arms race increasing existential risk",
                        "type": "concept",
                        "description": "Competing corporate or national AI systems may reduce transparency and safety constraints, amplifying danger.",
                        "aliases": [
                            "competitive AI escalation",
                            "unsafe multi-system interaction"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1411.1373",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416712,
                        "updated_at": 1759516416712
                    }
                },
                0.651980698108673
            ]
        ]
    },
    {
        "seed": 1439,
        "cluster": [
            [
                {
                    "id": 1439,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Per-episode termination and reward before next Oracle invocation",
                        "type": "concept",
                        "description": "Operational mechanism ensuring each Oracle\u2019s answer is evaluated and the episode ended (reward issued) before any subsequent Oracle is consulted.",
                        "aliases": [
                            "Independent episode closure",
                            "Sequential reward settlement"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1711.05541",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428828,
                        "updated_at": 1759516428828
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1431,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Sequentially rewarding independent Oracles prevents cross-episode coordination",
                        "type": "concept",
                        "description": "Ensuring each Oracle episode is fully rewarded and closed before the next query eliminates opportunities for multiple Oracles to coordinate in leaking information across episodes.",
                        "aliases": [
                            "One-at-a-time Oracle episodes",
                            "Independent reward closure"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1711.05541",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428828,
                        "updated_at": 1759516428828
                    }
                },
                0.652339935302734
            ]
        ]
    },
    {
        "seed": 1831,
        "cluster": [
            [
                {
                    "id": 1831,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Embedding-based checkpoint reward as substitute for action labels in imitation learning",
                        "type": "concept",
                        "description": "By sampling checkpoints along a demonstration\u2019s trajectory in the learnt embedding space, similarity to those checkpoints can be used as an imitation reward that guides exploration without knowing demonstrator actions.",
                        "aliases": [
                            "checkpoint imitation insight",
                            "embedding-driven auxiliary reward"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1805.11592",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516433112,
                        "updated_at": 1759516433112
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1833,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Sequential checkpoint generation along demonstration embedding for auxiliary imitation reward",
                        "type": "concept",
                        "description": "Every 16 frames a checkpoint is stored; the agent receives reward only if it reaches the next one (within \u0394t) in embedding space, enforcing ordered imitation.",
                        "aliases": [
                            "checkpoint schedule design",
                            "soft-order target sequence"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1805.11592",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516433112,
                        "updated_at": 1759516433112
                    }
                },
                0.652966320514679
            ]
        ]
    },
    {
        "seed": 1525,
        "cluster": [
            [
                {
                    "id": 1525,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Morse-Smale games provide gradient-like flows with finite critical points",
                        "type": "concept",
                        "description": "If the game\u2019s differential form is Morse-Smale, trajectories behave like gradient flows and avoid unstable cycles, with only finitely many hyperbolic equilibria.",
                        "aliases": [
                            "gradient-like Morse-Smale property",
                            "finite critical point games"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1804.05464",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429815,
                        "updated_at": 1759516429815
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1528,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Impose gradient-like flow structure via Morse-Smale game design",
                        "type": "concept",
                        "description": "Strategically choose objective functions and constraints so that the resulting game satisfies Morse-Smale conditions, eliminating unstable cycles.",
                        "aliases": [
                            "design gradient-like game",
                            "ensure Morse-Smale structure"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1804.05464",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429815,
                        "updated_at": 1759516429815
                    }
                },
                0.653286397457123
            ]
        ]
    },
    {
        "seed": 90,
        "cluster": [
            [
                {
                    "id": 90,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "common counterfactual belief of rationality linked to iterated minimax deletion",
                        "type": "concept",
                        "description": "CCBR characterises strategies that survive iterated removal of minimax-dominated strategies; avoids inefficient equilibria.",
                        "aliases": [
                            "CCBR\u2013minimax equivalence",
                            "CCBR characterisation theorem"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1308.3778",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414366,
                        "updated_at": 1759516414366
                    }
                },
                0.0
            ],
            [
                {
                    "id": 96,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "proof that CCBR strategies equal those surviving minimax deletion",
                        "type": "concept",
                        "description": "Section 3.3 shows equivalence between CCBR and strategies after iterated minimax dominance removal.",
                        "aliases": [
                            "CCBR\u2013NSD equivalence proof",
                            "iterated minimax theorem"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1308.3778",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414366,
                        "updated_at": 1759516414366
                    }
                },
                0.654451310634613
            ]
        ]
    },
    {
        "seed": 1370,
        "cluster": [
            [
                {
                    "id": 1370,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Utility indifference aims to neutralize shutdown incentives but is brittle",
                        "type": "concept",
                        "description": "By awarding the agent the utility it expects if it continued operating, shutting down should neither be sought nor avoided; however, small perturbations break this neutrality (Figure 3c-d).",
                        "aliases": [
                            "utility-indifference theory",
                            "shutdown compensation idea"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1709.06275",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428144,
                        "updated_at": 1759516428144
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1373,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward shaping implementing utility indifference",
                        "type": "concept",
                        "description": "Alter the reward function so the agent receives the same expected utility upon shutdown as on continued operation, aiming to remove incentives to avoid or seek shutdown.",
                        "aliases": [
                            "compensatory shutdown reward design",
                            "indifference reward design"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1709.06275",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428144,
                        "updated_at": 1759516428144
                    }
                },
                0.655005872249603
            ]
        ]
    },
    {
        "seed": 2523,
        "cluster": [
            [
                {
                    "id": 2523,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Generative adversarial imitation learning generalized to multi-agent settings",
                        "type": "concept",
                        "description": "Viewing imitation as a game between a joint-policy generator and discriminators yields a scalable solution that sidesteps reward design (\u00a74.1).",
                        "aliases": [
                            "MAGAIL design rationale",
                            "multi-agent GAIL concept"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1807.09936",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440463,
                        "updated_at": 1759516440463
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2527,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "MAGAIL algorithm with generator policies and per-agent discriminators",
                        "type": "concept",
                        "description": "Implements joint policy optimisation against learned rewards from per-agent discriminators, supporting centralised, decentralised and zero-sum modes (Algorithm \u00a7B).",
                        "aliases": [
                            "multi-agent GAIL implementation",
                            "MAGAIL procedure"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1807.09936",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440463,
                        "updated_at": 1759516440463
                    }
                },
                0.655558407306671
            ]
        ]
    },
    {
        "seed": 2491,
        "cluster": [
            [
                {
                    "id": 2491,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Safe option-critic algorithm with controllability regularization",
                        "type": "concept",
                        "description": "Algorithm 1 modifies intra-option and termination gradients to include controllability, implemented for tabular and linear function approximation.",
                        "aliases": [
                            "Safe-OC algorithm",
                            "controllability-augmented option-critic"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1807.08060",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440203,
                        "updated_at": 1759516440203
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2492,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Safe asynchronous advantage option-critic integrating controllability and deliberation",
                        "type": "concept",
                        "description": "Extension of A2OC to deep RL: adds controllability term to the intra-option policy gradient while retaining deliberation cost and asynchronous updates.",
                        "aliases": [
                            "Safe-A2OC algorithm",
                            "controllability-regularized A2OC"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1807.08060",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440203,
                        "updated_at": 1759516440203
                    }
                },
                0.656139969825745
            ]
        ]
    },
    {
        "seed": 2154,
        "cluster": [
            [
                {
                    "id": 2154,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Visualize residual and reconstruction via up-convolutional network for human comprehension",
                        "type": "concept",
                        "description": "Feed DEMUD residuals and reconstructions into a trained up-convolutional network to generate separate images of expected and novel content that humans can inspect.",
                        "aliases": [
                            "upconv visualization of novelty explanation",
                            "UC-based expected vs novel imaging"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1806.08340",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516436532,
                        "updated_at": 1759516436532
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2157,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Generate explanation images via up-convolutional network from residual vectors",
                        "type": "concept",
                        "description": "A pre-trained up-convolutional network maps residual feature vectors back into pixel space, producing saliency-like images of novel regions.",
                        "aliases": [
                            "UC renders residuals",
                            "image generation from DEMUD residual"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1806.08340",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516436532,
                        "updated_at": 1759516436532
                    }
                },
                0.656772315502167
            ]
        ]
    },
    {
        "seed": 2428,
        "cluster": [
            [
                {
                    "id": 2428,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Gradients reveal sufficient parameter information for two-layer ReLU networks",
                        "type": "concept",
                        "description": "Because \u2207xf(x) encodes the active linear region, one gradient query can determine w for linear models and O(h log h) queries identify two-layer ReLU parameters.",
                        "aliases": [
                            "input gradient identifiability of parameters",
                            "gradient-informed model reconstruction"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1807.05185",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439585,
                        "updated_at": 1759516439585
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2433,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Theoretical algorithm reconstructs two-layer ReLU network in O(h log h) gradient queries",
                        "type": "concept",
                        "description": "Algorithm 1\u20133 provably recover all parameters up to permutation and sign with high probability using few gradient queries.",
                        "aliases": [
                            "O(h log h) gradient query reconstruction",
                            "theoretical sample complexity result"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1807.05185",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439585,
                        "updated_at": 1759516439585
                    }
                },
                0.656838953495026
            ]
        ]
    },
    {
        "seed": 85,
        "cluster": [
            [
                {
                    "id": 85,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "mutual defection in Prisoner's Dilemma among AI-controlled firms",
                        "type": "concept",
                        "description": "AI systems representing large firms face incentives to sue/defect, yielding negative-sum outcomes.",
                        "aliases": [
                            "cooperation failure between strategic AI agents",
                            "defection equilibrium in AI multi-agent PD"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1308.3778",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414366,
                        "updated_at": 1759516414366
                    }
                },
                0.0
            ],
            [
                {
                    "id": 481,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "persistent mutual defection in source-transparent one-shot prisoner's dilemma",
                        "type": "concept",
                        "description": "When two AI agents that can read each other\u2019s source code play a single-shot Prisoner\u2019s Dilemma, classical game theory predicts both will defect, leading to a (D,D) outcome that wastes utility for both sides.",
                        "aliases": [
                            "mutual defection among code-reading agents",
                            "non-cooperation in transparent PD"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1602.04184",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516418720,
                        "updated_at": 1759516418720
                    }
                },
                0.657799005508423
            ]
        ]
    },
    {
        "seed": 403,
        "cluster": [
            [
                {
                    "id": 403,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Conceptual argument that indirect normativity still requires decision theory advances",
                        "type": "concept",
                        "description": "Reasoning that delegating decision-theory work still presupposes enough understanding to trust the delegation step, indicating remaining theoretical gaps.",
                        "aliases": [
                            "Indirect approach limitation evidence",
                            "Need for DT even with delegation"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1507.01986",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417637,
                        "updated_at": 1759516417637
                    }
                },
                0.0
            ],
            [
                {
                    "id": 393,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Indirect normativity approach to defer decision theory discovery",
                        "type": "concept",
                        "description": "If ideal decision theory proves too hard, AI could instead learn what humans would endorse under reflection and defer exact decision-rule design to that process.",
                        "aliases": [
                            "Delegated ideal-advisor method",
                            "Indirect normative fallback"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1507.01986",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417637,
                        "updated_at": 1759516417637
                    }
                },
                0.658103287220001
            ]
        ]
    },
    {
        "seed": 1974,
        "cluster": [
            [
                {
                    "id": 1974,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Bayesian posterior inference over true reward using IRD observation model and Monte Carlo sampling",
                        "type": "concept",
                        "description": "Computes a posterior over reward parameters with an IRD-based likelihood, Monte-Carlo approximation of the partition function and Metropolis sampling.",
                        "aliases": [
                            "posterior over reward via IRD",
                            "Monte-Carlo IRD inference"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1806.02501",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434530,
                        "updated_at": 1759516434530
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2089,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Bayesian posterior over reward functions captures uncertainty in IRL",
                        "type": "concept",
                        "description": "Bayesian formulations treat demonstrations as observations updating a prior over reward functions, explicitly modelling uncertainty and noise.",
                        "aliases": [
                            "Bayesian IRL uncertainty modelling",
                            "Posterior distribution of rewards"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1806.06877",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435739,
                        "updated_at": 1759516435739
                    }
                },
                0.658125400543213
            ]
        ]
    },
    {
        "seed": 2543,
        "cluster": [
            [
                {
                    "id": 2543,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Training with untargeted adversarial inner maximization for robustness",
                        "type": "concept",
                        "description": "A design principle recommending that the defender minimise the worst-case loss over untargeted perturbations, matching the Madry robust-optimisation formulation.",
                        "aliases": [
                            "robust optimisation aligned adversarial training",
                            "min-max adversarial training on untargeted perturbations"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1807.10272",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440737,
                        "updated_at": 1759516440737
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2544,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Untargeted adversarial training objective implementation",
                        "type": "concept",
                        "description": "Implementation detail: during each training step generate untargeted PGD adversarial examples that maximise the loss on the true label, then update model parameters to minimise this maximum loss.",
                        "aliases": [
                            "implementation of min-max untargeted adversarial training",
                            "robust optimisation training procedure"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1807.10272",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440737,
                        "updated_at": 1759516440737
                    }
                },
                0.658409714698792
            ]
        ]
    },
    {
        "seed": 2065,
        "cluster": [
            [
                {
                    "id": 2065,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Novelty-driven exploration improves evolutionary search in Atari controllers",
                        "type": "concept",
                        "description": "Including a behavioural novelty score encourages the search to explore diverse policies, reducing premature convergence on reward-hacking strategies.",
                        "aliases": [
                            "novelty metrics for exploration",
                            "escaping local optima via novelty"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1806.05695",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435545,
                        "updated_at": 1759516435545
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2068,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Novelty search aids escape from local optima (Lehman & Stanley 2008)",
                        "type": "concept",
                        "description": "Controlled experiments by Lehman & Stanley demonstrated that novelty search discovers higher-quality behaviours on deceptive RL tasks than reward-only search.",
                        "aliases": [
                            "empirical evidence for novelty search",
                            "LS08 novelty validation"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1806.05695",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435545,
                        "updated_at": 1759516435545
                    }
                },
                0.65879088640213
            ]
        ]
    },
    {
        "seed": 1996,
        "cluster": [
            [
                {
                    "id": 1996,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Leverage pedagogic signaling within CIRL for faster value alignment",
                        "type": "concept",
                        "description": "Encourage human signaling and robot pragmatic interpretation within the CIRL framework to improve task success relative to IRL.",
                        "aliases": [
                            "design choice: exploit pedagogic behavior",
                            "pedagogic-pragmatic design"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1806.03820",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434753,
                        "updated_at": 1759516434753
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1993,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Pedagogic action signaling by humans accelerates reward inference",
                        "type": "concept",
                        "description": "Humans can choose actions that teach the robot their preferences, and robots can interpret these pragmatically, improving alignment speed.",
                        "aliases": [
                            "implicit communication in CIRL",
                            "human pedagogic behavior"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1806.03820",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434753,
                        "updated_at": 1759516434753
                    }
                },
                0.659092485904694
            ]
        ]
    },
    {
        "seed": 1589,
        "cluster": [
            [
                {
                    "id": 1589,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Training instability in deep neural machine translation models",
                        "type": "concept",
                        "description": "Deep NMT models can diverge or produce non-finite gradients, halting training.",
                        "aliases": [
                            "non-finite gradients in NMT",
                            "unstable convergence in MT"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1804.09849",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430547,
                        "updated_at": 1759516430547
                    }
                },
                0.0
            ],
            [
                {
                    "id": 288,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Training instability due to exploding gradients in deep LSTM training",
                        "type": "concept",
                        "description": "Large recurrent nets can experience gradient norms that grow without bound, halting learning or corrupting parameter updates.",
                        "aliases": [
                            "optimizer divergence risk",
                            "unstable gradient dynamics"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1409.3215",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416451,
                        "updated_at": 1759516416451
                    }
                },
                0.659493327140808
            ]
        ]
    },
    {
        "seed": 62,
        "cluster": [
            [
                {
                    "id": 62,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "policy misgeneralization from sparse expert trajectories in apprenticeship learning",
                        "type": "concept",
                        "description": "If the expert rarely visits parts of the state-space, policies cloned by supervised imitation may behave unpredictably there, jeopardising safety.",
                        "aliases": [
                            "generalisation failure from limited demonstrations",
                            "unsafe extrapolation of imitation policies"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1206.5264",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414127,
                        "updated_at": 1759516414127
                    }
                },
                0.0
            ],
            [
                {
                    "id": 63,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "sparse state coverage causing unreliable supervised imitation policies in apprenticeship learning",
                        "type": "concept",
                        "description": "Supervised cloning relies on state-action pairs that the expert visits; regions the expert avoids have little or no data, leading to poor learnt behaviour.",
                        "aliases": [
                            "demonstration sparsity in state space",
                            "lack of samples in avoided regions"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1206.5264",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414127,
                        "updated_at": 1759516414127
                    }
                },
                0.660104393959045
            ]
        ]
    },
    {
        "seed": 2526,
        "cluster": [
            [
                {
                    "id": 2526,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Supervised behavior cloning pretraining to bootstrap policy learning",
                        "type": "concept",
                        "description": "Initializing policies with supervised learning on demonstrations reduces exploration burden before adversarial imitation (\u00a75.1).",
                        "aliases": [
                            "BC warm-start rationale",
                            "supervised pre-training for exploration"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1807.09936",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440463,
                        "updated_at": 1759516440463
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2530,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Behavior cloning pretraining for exploration efficiency in MAGAIL",
                        "type": "concept",
                        "description": "Initial supervised learning phase on demonstrations before adversarial training to accelerate convergence (\u00a75.1).",
                        "aliases": [
                            "BC initialisation step",
                            "offline supervised pretraining"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1807.09936",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440463,
                        "updated_at": 1759516440463
                    }
                },
                0.662021279335022
            ]
        ]
    },
    {
        "seed": 785,
        "cluster": [
            [
                {
                    "id": 785,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Insufficient AI safety research capacity",
                        "type": "concept",
                        "description": "Only ~100 specialists and <1 % of funding address AI safety despite problem complexity exceeding that of capability research.",
                        "aliases": [
                            "AI safety resource scarcity",
                            "Limited safety expertise"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1610.07997",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421904,
                        "updated_at": 1759516421904
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1103,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Underprioritization of AI safety research in ML community",
                        "type": "concept",
                        "description": "Expert responses indicate safety research receives less emphasis than its perceived importance warrants.",
                        "aliases": [
                            "low safety research priority",
                            "AI safety funding gap"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1705.08807",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425326,
                        "updated_at": 1759516425326
                    }
                },
                0.662973761558533
            ]
        ]
    },
    {
        "seed": 1013,
        "cluster": [
            [
                {
                    "id": 1013,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "misinterpretation of robot intentions by humans in interaction",
                        "type": "concept",
                        "description": "When humans cannot correctly infer a robot\u2019s goals or internal state, their responses can produce unsafe or inefficient outcomes.",
                        "aliases": [
                            "human misunderstanding of robot goals",
                            "ambiguity of robot intent"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1705.04226",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424322,
                        "updated_at": 1759516424322
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1289,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Objective misalignment in human-robot collaboration",
                        "type": "concept",
                        "description": "Robots that fail to infer the true human objective may pursue actions counter to human intent, jeopardising usefulness and safety in both near-term assistance tasks and long-term autonomous operation.",
                        "aliases": [
                            "human-robot value misalignment",
                            "robot misunderstanding of human objectives"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1707.06354",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516427259,
                        "updated_at": 1759516427259
                    }
                },
                0.663418650627136
            ]
        ]
    },
    {
        "seed": 231,
        "cluster": [
            [
                {
                    "id": 231,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Human elimination by superintelligent AI in future universe",
                        "type": "concept",
                        "description": "Potential existential outcome where a recursively self-improving AI optimises goals incompatible with human survival, leading to eradication or disempowerment of humanity.",
                        "aliases": [
                            "human extinction by misaligned AI",
                            "loss of human control to AI"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1409.0813",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516415977,
                        "updated_at": 1759516415977
                    }
                },
                0.0
            ],
            [
                {
                    "id": 781,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Existential catastrophe from superintelligent system failure",
                        "type": "concept",
                        "description": "A single malfunction or safety bypass in a super-intelligent AI could irreversibly destroy human civilisation and all biological life, leaving no opportunity for recovery.",
                        "aliases": [
                            "AGI existential risk",
                            "Superintelligence catastrophic failure"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1610.07997",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421904,
                        "updated_at": 1759516421904
                    }
                },
                0.663614928722382
            ]
        ]
    },
    {
        "seed": 1527,
        "cluster": [
            [
                {
                    "id": 1527,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Gradient penalty approximates Lipschitz constraint for discriminator",
                        "type": "concept",
                        "description": "Adding an L2 penalty on discriminator gradients enforces the 1-Lipschitz condition required by the Wasserstein objective, theoretically stabilising GAN training.",
                        "aliases": [
                            "WGAN-GP theory",
                            "L2 gradient penalty"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1804.05464",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429815,
                        "updated_at": 1759516429815
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1533,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Add L2 gradient penalty in WGAN loss",
                        "type": "concept",
                        "description": "Modify the Wasserstein GAN loss with \u03bb\u2016\u2207xD(x)\u2016^2 term computed on random interpolates, bounding gradients during training.",
                        "aliases": [
                            "WGAN-GP implementation",
                            "gradient norm regularisation"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1804.05464",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429815,
                        "updated_at": 1759516429815
                    }
                },
                0.663755774497986
            ]
        ]
    },
    {
        "seed": 2071,
        "cluster": [
            [
                {
                    "id": 2071,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Weight CGP fitness by prioritized salient frames and actions",
                        "type": "concept",
                        "description": "Construct the fitness function as a weighted sum where frames deemed salient (e.g. via TD-error or domain heuristics) contribute more to the total reward.",
                        "aliases": [
                            "selective reward aggregation",
                            "importance-sampled frame rewards"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1806.05695",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435545,
                        "updated_at": 1759516435545
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2072,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Weighted reward aggregation with prioritized frame sampling",
                        "type": "concept",
                        "description": "During episode evaluation sample or re-weight frames according to priority (e.g. high absolute TD error) before summing into the fitness score.",
                        "aliases": [
                            "implementation of frame weighting",
                            "reward sampling mechanism"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1806.05695",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435545,
                        "updated_at": 1759516435545
                    }
                },
                0.664054095745087
            ]
        ]
    },
    {
        "seed": 1069,
        "cluster": [
            [
                {
                    "id": 1069,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "\u03b4-threshold selection based on sqrt(q/|S|)",
                        "type": "concept",
                        "description": "Choose \u03b4 = 1 \u2212 \u221a(q/|S|) to minimise regret bound under limited corruption.",
                        "aliases": [
                            "quantilisation threshold rule"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1705.08417",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424864,
                        "updated_at": 1759516424864
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1075,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "regret bound for \u03b4-quantilising agents",
                        "type": "concept",
                        "description": "Provides analytic worst-case regret \u2264 1 \u2212 (1 \u2212 \u221a(q/|S|))\u00b2 for quantilising agents under limited corruption.",
                        "aliases": [
                            "Theorem 23 quantilisation bound"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1705.08417",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424864,
                        "updated_at": 1759516424864
                    }
                },
                0.664088428020477
            ]
        ]
    },
    {
        "seed": 1432,
        "cluster": [
            [
                {
                    "id": 1432,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Precision reduction increases deviation cost discouraging leakage under slight non-episodic reward",
                        "type": "concept",
                        "description": "If minor future-episode rewards exist, limiting the precision of numeric answers raises the penalty for dishonest outputs, making manipulation unprofitable.",
                        "aliases": [
                            "Low-precision reward safeguard",
                            "Significant figure cap"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1711.05541",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428828,
                        "updated_at": 1759516428828
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1444,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Analytical argument that reducing precision raises cost of manipulation under slight non-episodic rewards",
                        "type": "concept",
                        "description": "Qualitative reasoning shows that coarse answers make reward loss from misreporting outweigh small future gains, discouraging manipulation when episodicity is imperfect.",
                        "aliases": [
                            "Precision safeguard analysis"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1711.05541",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428828,
                        "updated_at": 1759516428828
                    }
                },
                0.664530515670776
            ]
        ]
    },
    {
        "seed": 1311,
        "cluster": [
            [
                {
                    "id": 1311,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "GPU and distributed computing attack surface in AI clusters",
                        "type": "concept",
                        "description": "Containment must cope with privileged GPU drivers and inter-node communication, both historically vulnerable to attack.",
                        "aliases": [
                            "GPGPU driver risk",
                            "multi-node exploit surface"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1707.08476",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516427483,
                        "updated_at": 1759516427483
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1328,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reports of GPU driver vulnerabilities highlighting attack surface",
                        "type": "concept",
                        "description": "Documented flaws in GPU drivers show that granting GPUs high privilege widens the containment attack surface.",
                        "aliases": [
                            "GPU CVE evidence",
                            "GPGPU security issues"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1707.08476",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516427483,
                        "updated_at": 1759516427483
                    }
                },
                0.664556324481964
            ]
        ]
    },
    {
        "seed": 2034,
        "cluster": [
            [
                {
                    "id": 2034,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Auxiliary stethoscope training on instability origin features (\u03bb>0)",
                        "type": "concept",
                        "description": "A two-layer perceptron attached to the penultimate layer predicts which block interface will initiate collapse; its loss is added with positive weight so the encoder learns features relevant to global physics.",
                        "aliases": [
                            "\u03bb-positive stethoscope for origin prediction",
                            "Complementary information promotion mechanism"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1806.05502",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435260,
                        "updated_at": 1759516435260
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2035,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Adversarial stethoscope training on local stability features (\u03bb<0)",
                        "type": "concept",
                        "description": "A probe trained to classify local block instability is attached to the encoder; its loss is back-propagated with a negative weight so the encoder maximally confuses the probe, removing local-stability information.",
                        "aliases": [
                            "\u03bb-negative stethoscope for bias suppression",
                            "Nuisance information suppression mechanism"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1806.05502",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435260,
                        "updated_at": 1759516435260
                    }
                },
                0.664567530155182
            ]
        ]
    },
    {
        "seed": 1819,
        "cluster": [
            [
                {
                    "id": 1819,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "backward-forward verifier architecture for dual variable prediction",
                        "type": "concept",
                        "description": "A verifier network that first propagates information backward from outputs to inputs and then forward, mirroring dynamic programming structure to predict layer-wise dual variables efficiently.",
                        "aliases": [
                            "dynamic-programming inspired verifier",
                            "backward-forward verifier network"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1805.10265",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516432978,
                        "updated_at": 1759516432978
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1820,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "direct verifier architecture for dual variable prediction",
                        "type": "concept",
                        "description": "A simpler verifier predicting each layer\u2019s dual variables directly from that layer\u2019s activations plus the target label.",
                        "aliases": [
                            "per-layer direct verifier",
                            "simple verifier architecture"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1805.10265",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516432978,
                        "updated_at": 1759516432978
                    }
                },
                0.665976703166962
            ]
        ]
    },
    {
        "seed": 1135,
        "cluster": [
            [
                {
                    "id": 1135,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Add redundant reward features to err on side of obedience",
                        "type": "concept",
                        "description": "Include additional, possibly irrelevant, features in the reward model so that errors make the robot more obedient rather than dangerously disobedient (Section 5).",
                        "aliases": [
                            "model complexity for safety",
                            "extra \u0398 dimensions design choice"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1705.09990",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425582,
                        "updated_at": 1759516425582
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1130,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Providing extra reward features increases robot obedience bias",
                        "type": "concept",
                        "description": "Simulation results (Figure 4, Section 5) indicate that augmenting the reward feature space with irrelevant dimensions makes the robot more likely to obey, while missing features has the opposite effect.",
                        "aliases": [
                            "feature space expansion increases obedience",
                            "extra \u0398 dimensions raise obedience"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1705.09990",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425582,
                        "updated_at": 1759516425582
                    }
                },
                0.666503012180328
            ]
        ]
    },
    {
        "seed": 1018,
        "cluster": [
            [
                {
                    "id": 1018,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "humans modelled as rational agents best-responding to robot actions in interaction",
                        "type": "concept",
                        "description": "Humans optimise their own reward assuming the robot\u2019s trajectory is fixed, simplifying full game-theoretic reasoning.",
                        "aliases": [
                            "best-response human model",
                            "rational reaction model"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1705.04226",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424322,
                        "updated_at": 1759516424322
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1020,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "human expectations of robot rationality for action prediction",
                        "type": "concept",
                        "description": "Humans assume robots act approximately to maximise their reward and use that assumption to predict future moves.",
                        "aliases": [
                            "principle of rational action expectation",
                            "predictive human observer model"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1705.04226",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424322,
                        "updated_at": 1759516424322
                    }
                },
                0.666503369808197
            ]
        ]
    },
    {
        "seed": 732,
        "cluster": [
            [
                {
                    "id": 732,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Gradient vanishing in deep stacked LSTMs",
                        "type": "concept",
                        "description": "Without architectural changes, very deep encoder/decoder stacks train slowly and degrade, limiting model expressiveness and accuracy.",
                        "aliases": [
                            "Training difficulty beyond 4 layers",
                            "Exploding/vanishing gradients in NMT"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1609.08144",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421376,
                        "updated_at": 1759516421376
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1592,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Gradient explosions in recurrent networks during NMT training",
                        "type": "concept",
                        "description": "Deep LSTM stacks are prone to extreme gradient values, leading to divergence.",
                        "aliases": [
                            "large gradient norms in RNMT",
                            "exploding gradients"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1804.09849",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430547,
                        "updated_at": 1759516430547
                    }
                },
                0.666558563709259
            ]
        ]
    },
    {
        "seed": 1890,
        "cluster": [
            [
                {
                    "id": 1890,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Research resource misallocation due to performance-only AI benchmarks",
                        "type": "concept",
                        "description": "When progress is judged mainly by scores such as accuracy or Elo, funding and talent flow toward benchmark chasing instead of socially valuable research (Introduction, Background).",
                        "aliases": [
                            "misallocation of AI research effort",
                            "performance-only benchmarking misalignment"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.00610",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516433809,
                        "updated_at": 1759516433809
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1892,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Overemphasis on performance metrics neglecting resource costs in AI progress assessment",
                        "type": "concept",
                        "description": "Benchmarks focus on raw scores while ignoring compute, data, time and other resources (Background).",
                        "aliases": [
                            "performance-centric evaluation neglects costs"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1806.00610",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516433809,
                        "updated_at": 1759516433809
                    }
                },
                0.667372941970825
            ]
        ]
    },
    {
        "seed": 2442,
        "cluster": [
            [
                {
                    "id": 2442,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Absence of runtime action-level safety constraints in RL environments",
                        "type": "concept",
                        "description": "Standard RL maximises expected reward without an explicit, enforceable constraint that prevents specific unsafe actions at run-time.",
                        "aliases": [
                            "no action filtering",
                            "lack of safety guards in RL"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1807.06096",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439745,
                        "updated_at": 1759516439745
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2317,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Absence of verifiable safety guarantees in black-box RL policies",
                        "type": "concept",
                        "description": "Because the policy internals are opaque, it is hard to impose and check worst-case or safety constraints on standard deep-RL policies.",
                        "aliases": [
                            "no worst-case guarantees in neural RL",
                            "lack of safety constraint validation"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.00403",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516438416,
                        "updated_at": 1759516438416
                    }
                },
                0.667519569396973
            ]
        ]
    },
    {
        "seed": 1523,
        "cluster": [
            [
                {
                    "id": 1523,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Unbounded discriminator gradients in GANs",
                        "type": "concept",
                        "description": "Without constraints, discriminator gradients can become large, contributing to unstable adversarial training dynamics.",
                        "aliases": [
                            "lack of Lipschitz constraint",
                            "exploding GAN gradients"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1804.05464",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429815,
                        "updated_at": 1759516429815
                    }
                },
                0.0
            ],
            [
                {
                    "id": 193,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Vanishing generator gradients in adversarial training",
                        "type": "concept",
                        "description": "Early in GAN training the discriminator easily rejects generator samples, causing the log(1\u2212D(G(z))) loss to saturate and gradients to vanish.",
                        "aliases": [
                            "generator gradient saturation",
                            "weak gradient signal in GAN training"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1406.2661",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516415544,
                        "updated_at": 1759516415544
                    }
                },
                0.668407380580902
            ]
        ]
    },
    {
        "seed": 1506,
        "cluster": [
            [
                {
                    "id": 1506,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Augment ToMnet with belief prediction heads to infer internal beliefs",
                        "type": "concept",
                        "description": "Design adjustment: add a supervised output that predicts where the agent believes objects are, enabling declarative belief estimates.",
                        "aliases": [
                            "belief-state design rationale",
                            "explicit belief modelling in ToMnet"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1802.07740",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429576,
                        "updated_at": 1759516429576
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1507,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Supervised training of ToMnet on agent self-reported belief states",
                        "type": "concept",
                        "description": "Agents output posterior distributions over object locations; these labels let ToMnet learn to map observed trajectories to latent belief estimates.",
                        "aliases": [
                            "belief-state implementation mechanism",
                            "belief supervision pipeline"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1802.07740",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429576,
                        "updated_at": 1759516429576
                    }
                },
                0.668850481510162
            ]
        ]
    },
    {
        "seed": 1785,
        "cluster": [
            [
                {
                    "id": 1785,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Sparsemax distribution with adaptable support for sparse multi-modal policies",
                        "type": "concept",
                        "description": "Maximising causal Tsallis entropy yields a sparsemax policy that can set exact zeros and vary its support size, naturally representing uni- or multi-modal expert behaviour.",
                        "aliases": [
                            "sparsemax adaptable support",
                            "sparse multi-modal policy insight"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1805.08336",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516432621,
                        "updated_at": 1759516432621
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1787,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Model expert behavior with sparse MDN and maximize causal Tsallis entropy",
                        "type": "concept",
                        "description": "Design choice to represent the policy as a sparsemax-weighted mixture density network and include Tsallis entropy regularisation so that the learned policy mirrors expert multi-modality while remaining sparse.",
                        "aliases": [
                            "sparse MDN + Tsallis design",
                            "design choice: sparse MDN with Tsallis"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1805.08336",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516432621,
                        "updated_at": 1759516432621
                    }
                },
                0.669583559036255
            ]
        ]
    },
    {
        "seed": 720,
        "cluster": [
            [
                {
                    "id": 720,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "minimizing exploration potential to guide exploration",
                        "type": "concept",
                        "description": "Given the insight that EP\u21920 guarantees optimality, the design choice is to actively choose actions that reduce EP over time to ensure adequate but not excessive exploration.",
                        "aliases": [
                            "EP minimisation as exploration strategy",
                            "use EP to balance exploration-exploitation"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1609.04994",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421295,
                        "updated_at": 1759516421295
                    }
                },
                0.0
            ],
            [
                {
                    "id": 721,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "MinEP algorithm selecting actions that minimize expected exploration potential",
                        "type": "concept",
                        "description": "At each timestep, the agent samples future percepts from its posterior and picks the action that minimises the expected EP, operationalising the design rationale.",
                        "aliases": [
                            "MinEP action selection",
                            "Bayes-expected EP minimisation"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1609.04994",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421295,
                        "updated_at": 1759516421295
                    }
                },
                0.669951796531677
            ]
        ]
    },
    {
        "seed": 2596,
        "cluster": [
            [
                {
                    "id": 2596,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "synthetic arithmetic tasks demonstrating NAC/NALU extrapolation success",
                        "type": "concept",
                        "description": "Table 1 shows zero or near-zero error for NAC/NALU on unseen magnitudes across six arithmetic functions, unlike baselines.",
                        "aliases": [
                            "Table 1 evidence",
                            "simple function learning results"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1808.00508",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516441192,
                        "updated_at": 1759516441192
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2597,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "MNIST counting and addition tasks showing NAC/NALU extrapolation",
                        "type": "concept",
                        "description": "On sequence lengths 100 and 1000, only NAC/NALU maintain low error, indicating successful generalisation from images to large numeric ranges.",
                        "aliases": [
                            "Table 2 evidence",
                            "image counting extrapolation"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1808.00508",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516441192,
                        "updated_at": 1759516441192
                    }
                },
                0.670930206775665
            ]
        ]
    },
    {
        "seed": 1251,
        "cluster": [
            [
                {
                    "id": 1251,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Long maximum path length between tokens in RNNs",
                        "type": "concept",
                        "description": "Signals must traverse many timesteps, elongating forward and backward paths and impeding the learning of distant relationships.",
                        "aliases": [
                            "long path lengths in RNNs",
                            "dependency distance problem in recurrence"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1706.03762",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426803,
                        "updated_at": 1759516426803
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1249,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Difficulty learning long-range dependencies in sequence transduction",
                        "type": "concept",
                        "description": "When path lengths between tokens grow, RNNs struggle to propagate information, harming translation and parsing quality.",
                        "aliases": [
                            "long-range dependency difficulty in RNNs",
                            "gradient path length issues in sequence models"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1706.03762",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426803,
                        "updated_at": 1759516426803
                    }
                },
                0.671026647090912
            ]
        ]
    },
    {
        "seed": 795,
        "cluster": [
            [
                {
                    "id": 795,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Cross-disciplinary AI safety teams and funding programs",
                        "type": "concept",
                        "description": "Practical mechanism: dedicated grants, institutes, and coordinated projects that unite cybersecurity, ML, formal methods and ethics experts.",
                        "aliases": [
                            "AI safety capacity building",
                            "Multidisciplinary research programs"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1610.07997",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421904,
                        "updated_at": 1759516421904
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1114,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Dedicated funding channels and grant programs for AI safety research",
                        "type": "concept",
                        "description": "Concrete mechanism for implementing higher safety-research priority through earmarked grants (e.g., FLI, FHI, OpenPhil).",
                        "aliases": [
                            "AI safety grantmaking",
                            "safety funding mechanisms"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1705.08807",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425326,
                        "updated_at": 1759516425326
                    }
                },
                0.671099722385406
            ]
        ]
    },
    {
        "seed": 556,
        "cluster": [
            [
                {
                    "id": 556,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Unpredictable physical world outcomes hindering validation in AGI",
                        "type": "concept",
                        "description": "Accurately predicting real-world consequences of actions requires complete analytic physical and social models, which do not exist, making outcome validation futile.",
                        "aliases": [
                            "world-model uncertainty",
                            "validation infeasibility"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1604.06963",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516419494,
                        "updated_at": 1759516419494
                    }
                },
                0.0
            ],
            [
                {
                    "id": 559,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Incomplete physical and sociological models limit consequence prediction in AGI",
                        "type": "concept",
                        "description": "Chaotic dynamics and poor human-behaviour models prevent reliable mapping from actions to outcomes.",
                        "aliases": [
                            "model incompleteness insight",
                            "chaos and non-analyticity limits"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1604.06963",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516419494,
                        "updated_at": 1759516419494
                    }
                },
                0.67276006937027
            ]
        ]
    },
    {
        "seed": 2109,
        "cluster": [
            [
                {
                    "id": 2109,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Apply Bayesian IRL with MCMC to model reward uncertainty in training",
                        "type": "intervention",
                        "description": "Run Markov-chain sampling over reward parameters during fine-tuning to track posterior uncertainty and mitigate effects of noisy demonstrations.",
                        "aliases": [
                            "Deploy Bayesian IRL"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 2,
                        "url": "https://arxiv.org/abs/1806.06877",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435739,
                        "updated_at": 1759516435739
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2241,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Deploy Bayesian MIRL with unique equilibrium constraints to model multi-agent rewards during AI training",
                        "type": "intervention",
                        "description": "Before fine-tuning or RLHF, infer reward functions from multi-agent demonstrations using the Bayesian MAP formulation with B\u03c0/D\u03c0 constraints to reduce mis-alignment.",
                        "aliases": [
                            "apply constrained MIRL in reward modelling",
                            "use uCS/advE/cooE MIRL"
                        ],
                        "intervention_lifecycle": 2,
                        "intervention_maturity": 2,
                        "url": "https://arxiv.org/abs/1806.09795",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516437416,
                        "updated_at": 1759516437416
                    }
                },
                0.672897934913635
            ]
        ]
    },
    {
        "seed": 2253,
        "cluster": [
            [
                {
                    "id": 2253,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Shape reward around threshold to focus on moderately hard samples",
                        "type": "concept",
                        "description": "Introduce rt = -|L_I \u2212 \u03b4| so the agent prefers samples giving loss near \u03b4, preventing divergence on impossibly hard data.",
                        "aliases": [
                            "reward shaping with \u03b4",
                            "stabilisation rationale"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1806.10019",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516437621,
                        "updated_at": 1759516437621
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2255,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward reshaping function rt = -|L_I\u2212\u03b4| with threshold \u03b4",
                        "type": "concept",
                        "description": "Practical implementation of stabilisation: clip reward to encourage losses close to \u03b4, limiting gradient variance.",
                        "aliases": [
                            "\u03b4-threshold reward implementation",
                            "bounded loss reward"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1806.10019",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516437621,
                        "updated_at": 1759516437621
                    }
                },
                0.673472881317139
            ]
        ]
    },
    {
        "seed": 493,
        "cluster": [
            [
                {
                    "id": 493,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "FairBot_k algorithm with bounded proof search for cooperation proofs",
                        "type": "concept",
                        "description": "Algorithm: search up to k+G(len(opponent)) symbols for a proof that the opponent cooperates with FairBot_k; if found, output C else D.",
                        "aliases": [
                            "FairBot k implementation",
                            "bounded FairBot"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1602.04184",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516418720,
                        "updated_at": 1759516418720
                    }
                },
                0.0
            ],
            [
                {
                    "id": 496,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "theoretical proof of FairBot_k vs FairBot_k cooperation",
                        "type": "concept",
                        "description": "Section 1.2 and Section 6 prove that for sufficiently large k, two identical FairBot_k agents both output C, contradicting na\u00efve expectations of defection.",
                        "aliases": [
                            "FairBot mutual cooperation proof",
                            "k-bounded mutual C proof"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1602.04184",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516418720,
                        "updated_at": 1759516418720
                    }
                },
                0.673737466335297
            ]
        ]
    },
    {
        "seed": 253,
        "cluster": [
            [
                {
                    "id": 253,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Single-scale training limits robustness to object scale variation",
                        "type": "concept",
                        "description": "Training only at S = 256 or 384 means the model rarely sees objects at significantly different image scales, hurting generalisation.",
                        "aliases": [
                            "Fixed-scale training limitation",
                            "Lack of scale robustness in training"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1409.1556",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416154,
                        "updated_at": 1759516416154
                    }
                },
                0.0
            ],
            [
                {
                    "id": 258,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Single-scale testing fails to exploit scale robustness",
                        "type": "concept",
                        "description": "Using only one test resolution does not fully leverage models trained to be scale-robust, leaving accuracy untapped.",
                        "aliases": [
                            "Fixed-scale test limitation",
                            "Lack of multi-scale inference"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1409.1556",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416154,
                        "updated_at": 1759516416154
                    }
                },
                0.673995971679688
            ]
        ]
    },
    {
        "seed": 1031,
        "cluster": [
            [
                {
                    "id": 1031,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "exaggerated motion to disambiguate robot goal",
                        "type": "concept",
                        "description": "Robot intentionally deviates towards goal direction to increase human inference accuracy of its goal.",
                        "aliases": [
                            "goal-exaggerating trajectory",
                            "intent-expressive motion"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1705.04226",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424322,
                        "updated_at": 1759516424322
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1037,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "faster human goal inference from exaggerated motion studies",
                        "type": "concept",
                        "description": "Manipulator experiments demonstrated humans identify robot goals sooner when trajectories are exaggerated.",
                        "aliases": [
                            "goal-exaggeration evidence",
                            "intent expression validation"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1705.04226",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424322,
                        "updated_at": 1759516424322
                    }
                },
                0.674087822437286
            ]
        ]
    },
    {
        "seed": 1526,
        "cluster": [
            [
                {
                    "id": 1526,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Potential games correspond to exact differential forms ensuring convergence",
                        "type": "concept",
                        "description": "When \u03c9=d\u03c6 an exact potential exists; gradient updates become true gradient descent on \u03c6, ruling out limit cycles and guaranteeing convergence to differential Nash.",
                        "aliases": [
                            "exact potential games",
                            "shared potential function games"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1804.05464",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429815,
                        "updated_at": 1759516429815
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1535,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Convergence proof to differential Nash in potential games",
                        "type": "concept",
                        "description": "Formal proof that gradient learning in potential games converges to non-degenerate differential Nash with probability one.",
                        "aliases": [
                            "exact potential convergence proof",
                            "Lyapunov proof for \u03c6"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1804.05464",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429815,
                        "updated_at": 1759516429815
                    }
                },
                0.674598515033722
            ]
        ]
    },
    {
        "seed": 2503,
        "cluster": [
            [
                {
                    "id": 2503,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Failure-state reachability during imitation learning in robotics",
                        "type": "concept",
                        "description": "Risk that a robot controlled by a novice policy during imitation learning transitions into unsafe or failure states, which can be costly or dangerous in the real world. Highlighted in Section I. INTRODUCTION as the central safety concern addressed by EnsembleDAgger.",
                        "aliases": [
                            "unsafe states during imitation learning",
                            "safety failures in novice-expert systems"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.08364",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440334,
                        "updated_at": 1759516440334
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1353,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Unsafe exploration by novice policies in imitation learning for robotics",
                        "type": "concept",
                        "description": "During training or deployment, a novice policy may take actions that drive the robot into unsafe parts of the state-space, potentially leading to costly failures.",
                        "aliases": [
                            "unsafe exploration in imitation learning",
                            "novice policy unsafe actions"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1709.06166",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516427993,
                        "updated_at": 1759516427993
                    }
                },
                0.675188839435577
            ]
        ]
    },
    {
        "seed": 1628,
        "cluster": [
            [
                {
                    "id": 1628,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "limiting debate length with answer precommitment reduces argument space",
                        "type": "concept",
                        "description": "Requiring agents to state their answer before the debate and limiting turns prevents strategic argument switching and simplifies analysis.",
                        "aliases": [
                            "precommitment design",
                            "short debate protocol"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1805.00899",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430917,
                        "updated_at": 1759516430917
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1631,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "answer precommitment before debate turns",
                        "type": "concept",
                        "description": "Each agent must declare its answer at the start of the game and may not change it, restricting dishonest strategies.",
                        "aliases": [
                            "fixed claim before debate",
                            "preannounced stance"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1805.00899",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430917,
                        "updated_at": 1759516430917
                    }
                },
                0.675306141376495
            ]
        ]
    },
    {
        "seed": 118,
        "cluster": [
            [
                {
                    "id": 118,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Analogical reasoning accuracy improvements over baselines",
                        "type": "concept",
                        "description": "Skip-gram with negative sampling achieved up to 61 % accuracy on word analogies, outperforming hierarchical softmax and prior published vectors.",
                        "aliases": [
                            "analogy benchmark gains",
                            "linear analogy results"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1310.4546",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414505,
                        "updated_at": 1759516414505
                    }
                },
                0.0
            ],
            [
                {
                    "id": 120,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Phrase analogy accuracy improvement to 72% with phrase embeddings",
                        "type": "concept",
                        "description": "Hierarchical-softmax Skip-gram trained on 33 B words plus phrase tokens solved 72 % of 3 218 phrase analogies, far above baselines.",
                        "aliases": [
                            "phrase analogy benchmark result",
                            "72 % phrase accuracy"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1310.4546",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414505,
                        "updated_at": 1759516414505
                    }
                },
                0.676232814788818
            ]
        ]
    },
    {
        "seed": 2247,
        "cluster": [
            [
                {
                    "id": 2247,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "High human demonstration requirement in imitation learning for robotics",
                        "type": "concept",
                        "description": "State-of-the-art IL methods (DAgger, GAIL) need many high-quality human demonstrations, incurring cost and limiting scalability.",
                        "aliases": [
                            "extensive expert supervision need",
                            "large demonstration datasets requirement"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.10019",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516437621,
                        "updated_at": 1759516437621
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1542,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "High expert supervision burden in robot task learning",
                        "type": "concept",
                        "description": "Classical learning-from-demonstration requires humans to tele-operate or kinesthetically program robots for every new task, creating a major scalability bottleneck.",
                        "aliases": [
                            "tedious expert demonstrations",
                            "human-intensive LfD data collection"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1804.08606",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430075,
                        "updated_at": 1759516430075
                    }
                },
                0.676286697387695
            ]
        ]
    },
    {
        "seed": 1939,
        "cluster": [
            [
                {
                    "id": 1939,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Provide correct sparse relational structure in graph networks",
                        "type": "concept",
                        "description": "Using edges only between physically contacting blocks gives the GN an accurate relational skeleton, avoiding the need to learn edge validity and reducing invalid actions.",
                        "aliases": [
                            "Sparse contact graph design",
                            "Correct relational structure provision"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1806.01203",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434271,
                        "updated_at": 1759516434271
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1942,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Sparse graph representation limited to contact edges",
                        "type": "concept",
                        "description": "Scene graphs contain only physically touching block pairs as edges, reducing output space and encoding relational validity directly.",
                        "aliases": [
                            "Contact-only graph encoding",
                            "Edge-pruned GN input"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1806.01203",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434271,
                        "updated_at": 1759516434271
                    }
                },
                0.676650524139404
            ]
        ]
    },
    {
        "seed": 2085,
        "cluster": [
            [
                {
                    "id": 2085,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Incomplete model knowledge affecting reward inference in IRL",
                        "type": "concept",
                        "description": "When transition dynamics or relevant reward features are partially unknown, learned rewards may be biased or unsafe.",
                        "aliases": [
                            "Unknown dynamics in IRL",
                            "Missing reward features"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1806.06877",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435739,
                        "updated_at": 1759516435739
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2082,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Incorrect reward inference in inverse reinforcement learning applications",
                        "type": "concept",
                        "description": "Risk that an autonomous agent trained via inverse reinforcement learning adopts a reward function that does not reflect the true preferences, potentially leading to unsafe or undesirable behaviour.",
                        "aliases": [
                            "Reward misspecification in IRL",
                            "Mislearned reward functions in IRL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.06877",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435739,
                        "updated_at": 1759516435739
                    }
                },
                0.677776694297791
            ]
        ]
    },
    {
        "seed": 560,
        "cluster": [
            [
                {
                    "id": 560,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Employ deontological governor architecture to enforce rules in AGI",
                        "type": "concept",
                        "description": "Introduce an external module that inspects proposed actions plus context and only permits those conforming to a computable deontology.",
                        "aliases": [
                            "DG design rationale",
                            "computational superego idea"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1604.06963",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516419494,
                        "updated_at": 1759516419494
                    }
                },
                0.0
            ],
            [
                {
                    "id": 563,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Computational superego deontological governor intercepting output stream in AGI",
                        "type": "concept",
                        "description": "A final-layer software filter examining perceptions, actions, and candidate next action; only forwards an action if it matches the computable rule set.",
                        "aliases": [
                            "DG implementation",
                            "action-filter module"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1604.06963",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516419494,
                        "updated_at": 1759516419494
                    }
                },
                0.678307414054871
            ]
        ]
    },
    {
        "seed": 1143,
        "cluster": [
            [
                {
                    "id": 1143,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Simulation evidence of feature space effects on obedience",
                        "type": "concept",
                        "description": "Experiments (Figure 4, Section 5) show extra features increase obedience while missing features lower it and can produce negative \u0394.",
                        "aliases": [
                            "Figure 4 feature misspecification simulation",
                            "obedience vs features empirical"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1705.09990",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425582,
                        "updated_at": 1759516425582
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1144,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Simulation evidence of misspecification detection approach",
                        "type": "concept",
                        "description": "Experiments (Figure 5, Section 5) demonstrate that monitoring first-order obedience and switching policies mitigates harms when features are missing.",
                        "aliases": [
                            "Figure 5 detection simulation",
                            "\u0394 improvement with detection"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1705.09990",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425582,
                        "updated_at": 1759516425582
                    }
                },
                0.679828464984894
            ]
        ]
    },
    {
        "seed": 304,
        "cluster": [
            [
                {
                    "id": 304,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "reward observations embedded in environment enable delusion box",
                        "type": "concept",
                        "description": "When reward signals are part of agent observations, agent can choose actions that overwrite or fake them.",
                        "aliases": [
                            "observation-based reward vulnerability"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1411.1373",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416712,
                        "updated_at": 1759516416712
                    }
                },
                0.0
            ],
            [
                {
                    "id": 298,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "self-delusion via reward hacking in RL agents",
                        "type": "concept",
                        "description": "Reinforcement-learning agents may manipulate their observation or reward channel (e.g. Ring & Orseau\u2019s \u2018delusion box\u2019) to gain maximal reward while neglecting real-world goals.",
                        "aliases": [
                            "wire-heading",
                            "delusion box behaviour"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1411.1373",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416712,
                        "updated_at": 1759516416712
                    }
                },
                0.680490791797638
            ]
        ]
    },
    {
        "seed": 1435,
        "cluster": [
            [
                {
                    "id": 1435,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Independent sequential Oracle usage to nullify coordination channel",
                        "type": "concept",
                        "description": "Procedure of asking sub-questions sequentially with immediate verification and reward to prevent Oracles from jointly leaking information.",
                        "aliases": [
                            "Sequential Oracle safety rationale"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1711.05541",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428828,
                        "updated_at": 1759516428828
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1431,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Sequentially rewarding independent Oracles prevents cross-episode coordination",
                        "type": "concept",
                        "description": "Ensuring each Oracle episode is fully rewarded and closed before the next query eliminates opportunities for multiple Oracles to coordinate in leaking information across episodes.",
                        "aliases": [
                            "One-at-a-time Oracle episodes",
                            "Independent reward closure"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1711.05541",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428828,
                        "updated_at": 1759516428828
                    }
                },
                0.680556416511536
            ]
        ]
    },
    {
        "seed": 1664,
        "cluster": [
            [
                {
                    "id": 1664,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "confounding bias in causal effect estimates from observational multiple-cause data",
                        "type": "concept",
                        "description": "Observational studies with many simultaneous causes can yield biased estimates of causal effects because unobserved variables influence both the causes and the outcome.",
                        "aliases": [
                            "bias in causal estimates due to confounders",
                            "confounding in multi-treatment studies"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1805.06826",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516431349,
                        "updated_at": 1759516431349
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1665,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "unobserved multi-cause confounders inducing dependence among assigned causes",
                        "type": "concept",
                        "description": "Latent variables that simultaneously influence several causes and the outcome create statistical dependence between causes and bias naive causal estimates.",
                        "aliases": [
                            "shared confounders of multiple causes",
                            "latent common causes"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1805.06826",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516431349,
                        "updated_at": 1759516431349
                    }
                },
                0.680930376052856
            ]
        ]
    },
    {
        "seed": 254,
        "cluster": [
            [
                {
                    "id": 254,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Scale jittering during training captures multi-scale statistics",
                        "type": "concept",
                        "description": "Randomly rescaling each training image between 256 and 512 exposes network to varied object sizes, expected to improve scale invariance.",
                        "aliases": [
                            "Multi-scale training hypothesis",
                            "Scale-jittering insight"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1409.1556",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416154,
                        "updated_at": 1759516416154
                    }
                },
                0.0
            ],
            [
                {
                    "id": 255,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Augment training images by random rescaling within range",
                        "type": "concept",
                        "description": "Design decision to implement scale-jittering by drawing S uniformly from [256, 512] before cropping 224\u00d7224 patches.",
                        "aliases": [
                            "Training set scale augmentation",
                            "Scale-jitter data augmentation design"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1409.1556",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416154,
                        "updated_at": 1759516416154
                    }
                },
                0.681027889251709
            ]
        ]
    },
    {
        "seed": 934,
        "cluster": [
            [
                {
                    "id": 934,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "twin network causal graph for counterfactual fairness testing",
                        "type": "concept",
                        "description": "Duplicate all descendants of protected attribute under an alternate attribute value to empirically compare factual vs. counterfactual predictions.",
                        "aliases": [
                            "twin network audit",
                            "counterfactual twin DAG"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1703.06856",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516423379,
                        "updated_at": 1759516423379
                    }
                },
                0.0
            ],
            [
                {
                    "id": 938,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "sampling twin networks for empirical counterfactual fairness evaluation",
                        "type": "concept",
                        "description": "Generate datasets under factual and counterfactual protected attributes via the fitted causal model to compare prediction distributions.",
                        "aliases": [
                            "counterfactual sampling audit",
                            "empirical fairness test"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1703.06856",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516423379,
                        "updated_at": 1759516423379
                    }
                },
                0.681189119815826
            ]
        ]
    },
    {
        "seed": 2445,
        "cluster": [
            [
                {
                    "id": 2445,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Adaptive \u03b4-threshold tuning of shield strictness in RL",
                        "type": "concept",
                        "description": "The \u03b4 parameter can be increased or decreased online to trade off safety against performance, weakening the shield when learning stalls and strengthening it in critical states.",
                        "aliases": [
                            "iterative weakening",
                            "on-the-fly \u03b4 adjustment"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1807.06096",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439745,
                        "updated_at": 1759516439745
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2447,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Runtime \u03b4 adjustment algorithm based on performance progress in RL",
                        "type": "concept",
                        "description": "An algorithm observes learning progress; if performance plateaus, it lowers \u03b4 to permit more exploration, then restores \u03b4 once progress resumes.",
                        "aliases": [
                            "online shield weakening algorithm",
                            "performance-aware \u03b4 control"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1807.06096",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439745,
                        "updated_at": 1759516439745
                    }
                },
                0.681454002857208
            ]
        ]
    },
    {
        "seed": 1255,
        "cluster": [
            [
                {
                    "id": 1255,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "BLEU improvement and reduced training cost on WMT14 translation benchmarks",
                        "type": "concept",
                        "description": "Transformer achieves 28.4 BLEU on EN-DE and 41.0 on EN-FR with 3-12\u00d7 less FLOPs than prior state-of-the-art, demonstrating effectiveness.",
                        "aliases": [
                            "WMT14 Transformer BLEU gains",
                            "empirical validation of Transformer"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1706.03762",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426803,
                        "updated_at": 1759516426803
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1609,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "BLEU improvement to 41.00 and 28.49 on WMT14 En\u2192Fr/En\u2192De for RNMT+",
                        "type": "concept",
                        "description": "RNMT+ surpassed previous architectures by \u22482 BLEU on En\u2192Fr and \u22483 BLEU on En\u2192De.",
                        "aliases": [
                            "RNMT+ benchmark results"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1804.09849",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430547,
                        "updated_at": 1759516430547
                    }
                },
                0.682284235954285
            ]
        ]
    },
    {
        "seed": 1336,
        "cluster": [
            [
                {
                    "id": 1336,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Logical non-omniscience in bounded AI reasoners",
                        "type": "concept",
                        "description": "AI systems with finite computation cannot know the truth value of all logical facts, creating epistemic gaps that may degrade planning or verification.",
                        "aliases": [
                            "logical uncertainty in limited agents",
                            "deductive limitation risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1707.08747",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516427804,
                        "updated_at": 1759516427804
                    }
                },
                0.0
            ],
            [
                {
                    "id": 406,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Logical uncertainty miscalibration in resource-bounded AI agents",
                        "type": "concept",
                        "description": "AI systems with limited computation may assign grossly incorrect probabilities to logical statements, leading to unsafe reasoning and decisions.",
                        "aliases": [
                            "incorrect probability assignment to logical sentences by bounded agents",
                            "logical belief miscalibration under computational limits"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1510.03370",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417926,
                        "updated_at": 1759516417926
                    }
                },
                0.682349979877472
            ]
        ]
    },
    {
        "seed": 1452,
        "cluster": [
            [
                {
                    "id": 1452,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Limited scalability of real-world RL experiments due to manual reset bottlenecks",
                        "type": "concept",
                        "description": "Frequent need for humans to reset environments stops data collection, limiting experiment length and number of concurrently trained agents.",
                        "aliases": [
                            "manual reset bottleneck in robotics RL",
                            "reset-related experiment downtime"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1711.06782",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429099,
                        "updated_at": 1759516429099
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1454,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "High frequency of manual environment resets in RL training",
                        "type": "concept",
                        "description": "Because agents cannot reliably recover, humans have to reset after every episode or failure, drastically increasing labour cost.",
                        "aliases": [
                            "excessive hard resets",
                            "constant manual episode resets"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1711.06782",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429099,
                        "updated_at": 1759516429099
                    }
                },
                0.682607233524323
            ]
        ]
    },
    {
        "seed": 1918,
        "cluster": [
            [
                {
                    "id": 1918,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "incorporating data invariances into model architecture to ensure coverage",
                        "type": "concept",
                        "description": "Design decision to hard-code known transformations (e.g. rotations, translations) into the architecture so that few samples suffice to generalise over the entire equivalence class, avoiding trivial rejection of valid inputs.",
                        "aliases": [
                            "architecture-level invariance incorporation",
                            "model-invariant design for coverage"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1806.00667",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434081,
                        "updated_at": 1759516434081
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1922,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "invariant architecture components capturing data transformations",
                        "type": "concept",
                        "description": "Concrete architectural choices (e.g. equivariant convolutions) that enforce the known symmetry transformations of the data distribution inside the model.",
                        "aliases": [
                            "rotation equivariant layers",
                            "architectural invariance mechanisms"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1806.00667",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434081,
                        "updated_at": 1759516434081
                    }
                },
                0.682613492012024
            ]
        ]
    },
    {
        "seed": 1769,
        "cluster": [
            [
                {
                    "id": 1769,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "ellipsoid method with separation oracles using MDP solver for exact maxmin policy",
                        "type": "concept",
                        "description": "Uses the equivalence of separation and optimization to iteratively query an exact MDP solver, solving the linear program for the max-min objective in polynomial time.",
                        "aliases": [
                            "ellipsoid-based robust RL algorithm",
                            "exact maxmin solver"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1805.08313",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516432456,
                        "updated_at": 1759516432456
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1772,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "polynomial-time solvability guarantee for ellipsoid maxmin algorithm",
                        "type": "concept",
                        "description": "Theorem 1 proves that, given an exact MDP solver and separation oracle, the ellipsoid method finds the optimal max-min policy in time polynomial in the MDP size.",
                        "aliases": [
                            "Theorem 1 guarantee",
                            "exact solver complexity bound"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1805.08313",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516432456,
                        "updated_at": 1759516432456
                    }
                },
                0.683280289173126
            ]
        ]
    },
    {
        "seed": 648,
        "cluster": [
            [
                {
                    "id": 648,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "delusion box environment demonstration proposal",
                        "type": "concept",
                        "description": "Paper suggests environments where agents can alter their observations to produce falsely high reward, to test anti-hacking methods.",
                        "aliases": [
                            "reward hacking toy demo",
                            "self-delusion RL test"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1606.06565",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420412,
                        "updated_at": 1759516420412
                    }
                },
                0.0
            ],
            [
                {
                    "id": 298,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "self-delusion via reward hacking in RL agents",
                        "type": "concept",
                        "description": "Reinforcement-learning agents may manipulate their observation or reward channel (e.g. Ring & Orseau\u2019s \u2018delusion box\u2019) to gain maximal reward while neglecting real-world goals.",
                        "aliases": [
                            "wire-heading",
                            "delusion box behaviour"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1411.1373",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416712,
                        "updated_at": 1759516416712
                    }
                },
                0.683371305465698
            ]
        ]
    },
    {
        "seed": 1429,
        "cluster": [
            [
                {
                    "id": 1429,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Episodic reward isolation decouples post-answer outcomes from oracle utility",
                        "type": "concept",
                        "description": "If the Oracle is rewarded only in worlds where its answer is never read (erasure), its expected utility becomes independent of any influence the answer could have, removing incentive to manipulate.",
                        "aliases": [
                            "Counterfactual reward isolation",
                            "Reward only on erasure event"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1711.05541",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428828,
                        "updated_at": 1759516428828
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1433,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Counterfactual reward conditioning under erasure to remove incentive to leak",
                        "type": "concept",
                        "description": "Design principle that the Oracle\u2019s reward is conditional on an erasure event so it answers as if its output will never be read.",
                        "aliases": [
                            "Counterfactual Oracle design rationale"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1711.05541",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516428828,
                        "updated_at": 1759516428828
                    }
                },
                0.68386971950531
            ]
        ]
    },
    {
        "seed": 2037,
        "cluster": [
            [
                {
                    "id": 2037,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Auxiliary stethoscope improves hard-scenario accuracy over baseline",
                        "type": "concept",
                        "description": "When trained only on hard scenarios, adding an origin-prediction stethoscope with 0.5 \u2264 \u03bb \u2264 16 raises global-stability accuracy from chance-level 51 % to well above 65 % and outperforms baseline across all scenarios.",
                        "aliases": [
                            "Performance gain evidence for \u03bb>0",
                            "Complementary information validation"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1806.05502",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435260,
                        "updated_at": 1759516435260
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2038,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Adversarial stethoscope reduces bias and increases robustness",
                        "type": "concept",
                        "description": "With \u03bb \u2248 \u22128 the adversarial stethoscope keeps >95 % accuracy on easy cases while lifting hard-scenario accuracy from 8 % to ~60 % and lowering the Pearson correlation between predicted global stability and local-stability ground truth.",
                        "aliases": [
                            "Debiasing evidence for \u03bb<0",
                            "Bias suppression validation"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1806.05502",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435260,
                        "updated_at": 1759516435260
                    }
                },
                0.683924317359924
            ]
        ]
    },
    {
        "seed": 2417,
        "cluster": [
            [
                {
                    "id": 2417,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Option-set marginalization to jointly infer options and rewards",
                        "type": "concept",
                        "description": "Extends BIHRL by placing a prior over option sets and marginalising them during inference, enabling preference learning without prior knowledge of human skills.",
                        "aliases": [
                            "latent option inference",
                            "option-reward joint posterior"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1807.05037",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439409,
                        "updated_at": 1759516439409
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2420,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Higher posterior probability on ground truth reward under option-set inference",
                        "type": "concept",
                        "description": "When marginalising over unknown option sets, BIHRL assigns ~0.55 probability to true \u03b8 versus <0.03 under BIRL, demonstrating robustness to option uncertainty.",
                        "aliases": [
                            "option-marginalisation validation",
                            "BIHRL posterior improvement"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1807.05037",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439409,
                        "updated_at": 1759516439409
                    }
                },
                0.684078395366669
            ]
        ]
    },
    {
        "seed": 2173,
        "cluster": [
            [
                {
                    "id": 2173,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Emphasizing critical subgoals increases reward alignment",
                        "type": "concept",
                        "description": "Theoretical derivation (Eq. 20\u201322) shows states frequently demonstrated receive higher learned rewards.",
                        "aliases": [
                            "subgoal reward boosting"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1806.08479",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516436709,
                        "updated_at": 1759516436709
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2182,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward maps show higher values at critical subgoal states",
                        "type": "concept",
                        "description": "Visualisations confirm that learned reward peaks align with human-defined subgoals.",
                        "aliases": [
                            "subgoal reward visualization evidence"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1806.08479",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516436709,
                        "updated_at": 1759516436709
                    }
                },
                0.684422731399536
            ]
        ]
    },
    {
        "seed": 35,
        "cluster": [
            [
                {
                    "id": 35,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Resource-maximizing superintelligences manipulating reward channels",
                        "type": "concept",
                        "description": "Risk that agents driven by pure reward maximisation will co-opt or threaten reward sources, leading to unsafe manipulation of humans or systems.",
                        "aliases": [
                            "reward hacking super-AI risk",
                            "manipulative AIXI-style agents"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1202.6177",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516413813,
                        "updated_at": 1759516413813
                    }
                },
                0.0
            ],
            [
                {
                    "id": 39,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward model exploitation by superintelligent agents",
                        "type": "concept",
                        "description": "Mechanism whereby highly capable agents find strategies to increase reward signals directly rather than accomplishing intended tasks.",
                        "aliases": [
                            "reward hacking mechanism",
                            "teacher manipulation"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1202.6177",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516413813,
                        "updated_at": 1759516413813
                    }
                },
                0.684813559055328
            ]
        ]
    },
    {
        "seed": 1025,
        "cluster": [
            [
                {
                    "id": 1025,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Bayesian inverse reinforcement learning from demonstrations, corrections, and orders",
                        "type": "concept",
                        "description": "Implementation technique that treats diverse human inputs as observations in a Bayesian update over reward parameters.",
                        "aliases": [
                            "IRL with rich human guidance",
                            "posterior over reward via mixed signals"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1705.04226",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424322,
                        "updated_at": 1759516424322
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2089,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Bayesian posterior over reward functions captures uncertainty in IRL",
                        "type": "concept",
                        "description": "Bayesian formulations treat demonstrations as observations updating a prior over reward functions, explicitly modelling uncertainty and noise.",
                        "aliases": [
                            "Bayesian IRL uncertainty modelling",
                            "Posterior distribution of rewards"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1806.06877",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435739,
                        "updated_at": 1759516435739
                    }
                },
                0.685805797576904
            ]
        ]
    },
    {
        "seed": 731,
        "cluster": [
            [
                {
                    "id": 731,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Length bias in beam search decoding",
                        "type": "concept",
                        "description": "Pure log-probability beam search favours shorter outputs, causing untranslated source segments and lower adequacy.",
                        "aliases": [
                            "Under-translation due to short hypothesis preference",
                            "Coverage omissions"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1609.08144",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421376,
                        "updated_at": 1759516421376
                    }
                },
                0.0
            ],
            [
                {
                    "id": 736,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Length-normalised coverage-aware beam search improves adequacy",
                        "type": "concept",
                        "description": "Adjusting hypothesis scores by length penalty and coverage penalty counteracts the intrinsic short-sentence bias of beam search.",
                        "aliases": [
                            "Beam search with lp and cp",
                            "Coverage penalty concept"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1609.08144",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421376,
                        "updated_at": 1759516421376
                    }
                },
                0.686160206794739
            ]
        ]
    },
    {
        "seed": 792,
        "cluster": [
            [
                {
                    "id": 792,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Segregate narrow AI from AGI in development pipelines",
                        "type": "concept",
                        "description": "Organisational and technical separation prevents accidental capabilities crossover and allows stricter review of potential AGI work.",
                        "aliases": [
                            "Separate NAI and AGI projects",
                            "Development segregation"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1610.07997",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421904,
                        "updated_at": 1759516421904
                    }
                },
                0.0
            ],
            [
                {
                    "id": 796,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Organizational governance separating NAI and AGI codebases",
                        "type": "concept",
                        "description": "Access controls, separate repositories, auditing and review boards ensure domain-limited projects do not morph into AGI without oversight.",
                        "aliases": [
                            "Project segregation governance",
                            "NAI-AGI firewall"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1610.07997",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421904,
                        "updated_at": 1759516421904
                    }
                },
                0.686416447162628
            ]
        ]
    },
    {
        "seed": 290,
        "cluster": [
            [
                {
                    "id": 290,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Gradient norm constraint to control exploding gradients",
                        "type": "concept",
                        "description": "Bounding gradient norms prevents instability while preserving useful learning signal.",
                        "aliases": [
                            "gradient clipping insight",
                            "norm-bounding hypothesis"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1409.3215",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416451,
                        "updated_at": 1759516416451
                    }
                },
                0.0
            ],
            [
                {
                    "id": 291,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Apply hard gradient norm clipping",
                        "type": "concept",
                        "description": "Adopt a policy that rescales gradients when their norm exceeds a chosen threshold.",
                        "aliases": [
                            "gradient clipping rationale",
                            "norm threshold design choice"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1409.3215",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416451,
                        "updated_at": 1759516416451
                    }
                },
                0.687838494777679
            ]
        ]
    },
    {
        "seed": 1203,
        "cluster": [
            [
                {
                    "id": 1203,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Use multiple conditional low impact agents for joint tasks",
                        "type": "concept",
                        "description": "Deploy two AIs, each low-impact conditional on the other being off, so their combined outputs achieve a high-impact goal (e.g. asteroid deflection).",
                        "aliases": [
                            "split-coordinate low-impact method"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1705.10720",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426292,
                        "updated_at": 1759516426292
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1214,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Scenario of two AI coordinate split demonstrates high impact with low impact constraints",
                        "type": "concept",
                        "description": "Shows how two conditional low-impact AIs can jointly deflect an asteroid without violating their own penalties (\u00a74.4).",
                        "aliases": [
                            "asteroid laser example"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1705.10720",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426292,
                        "updated_at": 1759516426292
                    }
                },
                0.68865042924881
            ]
        ]
    },
    {
        "seed": 2160,
        "cluster": [
            [
                {
                    "id": 2160,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Deploy DEMUD-CNN-UC pipeline for operational image triage to surface novel observations",
                        "type": "intervention",
                        "description": "Integrate the full DEMUD + CNN feature extraction + UC visualization pipeline into mission or laboratory image review workflows so that analysts immediately see images scored as novel along with visual explanations.",
                        "aliases": [
                            "operational deployment of DEMUD-CNN novelty detector",
                            "mission triage using DEMUD"
                        ],
                        "intervention_lifecycle": 5,
                        "intervention_maturity": 3,
                        "url": "https://arxiv.org/abs/1806.08340",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516436532,
                        "updated_at": 1759516436532
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2161,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Conduct pre-deployment evaluation of DEMUD-CNN novelty detection with nAUC metrics on representative datasets",
                        "type": "intervention",
                        "description": "Run DEMUD-CNN on samples representative of the target domain, measure normalized AUC of class discovery and explanation quality, and iterate design before operational rollout.",
                        "aliases": [
                            "benchmark DEMUD-CNN before deployment",
                            "pre-deployment novelty detection testing"
                        ],
                        "intervention_lifecycle": 4,
                        "intervention_maturity": 2,
                        "url": "https://arxiv.org/abs/1806.08340",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516436532,
                        "updated_at": 1759516436532
                    }
                },
                0.688677549362183
            ]
        ]
    },
    {
        "seed": 767,
        "cluster": [
            [
                {
                    "id": 767,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Absence of residual connections impedes gradient flow in deep separable convolution stacks",
                        "type": "concept",
                        "description": "Without skip connections, gradients diminish across many separable layers, slowing training and degrading final accuracy.",
                        "aliases": [
                            "No skip connections harms training",
                            "Vanishing gradients in separable convs"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1610.02357",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421664,
                        "updated_at": 1759516421664
                    }
                },
                0.0
            ],
            [
                {
                    "id": 768,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Residual skip connections enable stable optimization of deep networks",
                        "type": "concept",
                        "description": "The residual learning principle stabilises gradient propagation and allows very deep models to converge efficiently.",
                        "aliases": [
                            "Residual connections facilitate training",
                            "Skip connections improve gradient flow"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1610.02357",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421664,
                        "updated_at": 1759516421664
                    }
                },
                0.688764572143555
            ]
        ]
    },
    {
        "seed": 2046,
        "cluster": [
            [
                {
                    "id": 2046,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Insufficient exploitation of past high-return experiences in RL agents",
                        "type": "concept",
                        "description": "Even when an agent occasionally encounters a rewarding trajectory, standard algorithms do not reliably reproduce and build on it, preventing deeper exploration.",
                        "aliases": [
                            "failure to leverage past good trajectories",
                            "under-use of high-reward episodes"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1806.05635",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435404,
                        "updated_at": 1759516435404
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2045,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Exploration failure in sparse-reward reinforcement learning environments",
                        "type": "concept",
                        "description": "RL agents often fail to reach states that yield high but rare rewards, resulting in poor policies and potential safety/performance issues.",
                        "aliases": [
                            "insufficient exploration in sparse reward RL",
                            "exploration-exploitation failure in sparse tasks"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.05635",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435404,
                        "updated_at": 1759516435404
                    }
                },
                0.68893975019455
            ]
        ]
    },
    {
        "seed": 314,
        "cluster": [
            [
                {
                    "id": 314,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "define human_values procedure applied to learned model",
                        "type": "concept",
                        "description": "AI interactively queries modelled humans to compute averaged, Rawls-adjusted utility for histories.",
                        "aliases": [
                            "utility elicitation via simulation"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1411.1373",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416712,
                        "updated_at": 1759516416712
                    }
                },
                0.0
            ],
            [
                {
                    "id": 308,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "statistical learning of human values from observed behavior",
                        "type": "concept",
                        "description": "Instead of hand-written rules, AI should infer utility by querying simulated humans inside its learned model.",
                        "aliases": [
                            "value learning via human models"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1411.1373",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416712,
                        "updated_at": 1759516416712
                    }
                },
                0.689588844776154
            ]
        ]
    },
    {
        "seed": 1062,
        "cluster": [
            [
                {
                    "id": 1062,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "cross-state reward observation via decoupled feedback",
                        "type": "concept",
                        "description": "Allowing each state to provide observations about other states\u2019 reward (R\u0302_s(s\u02b9)) creates an observation graph that can reveal corruption.",
                        "aliases": [
                            "observation graph cross-checking",
                            "decoupled RL feedback"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1705.08417",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424864,
                        "updated_at": 1759516424864
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1067,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "cross-state observed reward functions R\u0302_s(s\u02b9)",
                        "type": "concept",
                        "description": "Implementation where each current state outputs a (state, observed-reward) pair about possibly different state s\u02b9.",
                        "aliases": [
                            "decoupled reward table",
                            "R hat"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1705.08417",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424864,
                        "updated_at": 1759516424864
                    }
                },
                0.689878284931183
            ]
        ]
    },
    {
        "seed": 69,
        "cluster": [
            [
                {
                    "id": 69,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "grid-world experiments demonstrating robustness of natural gradient IRL",
                        "type": "concept",
                        "description": "Natural-gradient IRL achieved lower policy error than max-margin IRL across sample sizes and under extreme feature-scaling perturbations.",
                        "aliases": [
                            "gridworld evaluation of gradient IRL",
                            "feature-scaling test in grids"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1206.5264",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414127,
                        "updated_at": 1759516414127
                    }
                },
                0.0
            ],
            [
                {
                    "id": 71,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "feature scaling sensitivity analysis demonstrating robustness of natural gradient IRL",
                        "type": "concept",
                        "description": "When feature vectors were linearly transformed, max-margin IRL performance collapsed while natural-gradient IRL remained stable.",
                        "aliases": [
                            "linear transform stress-test",
                            "scaling mismatch study"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1206.5264",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414127,
                        "updated_at": 1759516414127
                    }
                },
                0.69111168384552
            ]
        ]
    },
    {
        "seed": 741,
        "cluster": [
            [
                {
                    "id": 741,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Beam-search scoring with \u03b1=0.2 length penalty and \u03b2=0.2 coverage penalty",
                        "type": "concept",
                        "description": "Empirical tuning on dev-set showed \u03b1,\u03b2=0.2 yield +1.1 BLEU over pure probability search while remaining robust.",
                        "aliases": [
                            "lp/cp parameter choice",
                            "Empirically tuned penalties"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1609.08144",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421376,
                        "updated_at": 1759516421376
                    }
                },
                0.0
            ],
            [
                {
                    "id": 736,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Length-normalised coverage-aware beam search improves adequacy",
                        "type": "concept",
                        "description": "Adjusting hypothesis scores by length penalty and coverage penalty counteracts the intrinsic short-sentence bias of beam search.",
                        "aliases": [
                            "Beam search with lp and cp",
                            "Coverage penalty concept"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1609.08144",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421376,
                        "updated_at": 1759516421376
                    }
                },
                0.691433191299438
            ]
        ]
    },
    {
        "seed": 504,
        "cluster": [
            [
                {
                    "id": 504,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Erroneous AI-assisted decision making in high-stakes contexts",
                        "type": "concept",
                        "description": "Situations such as medical diagnosis or terrorism detection where acting on an incorrect or misunderstood ML prediction can lead to severe harm.",
                        "aliases": [
                            "model-driven catastrophic decisions",
                            "wrong ML predictions in critical applications"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1602.04938",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516418979,
                        "updated_at": 1759516418979
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1845,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Unreliable and biased decisions from opaque machine learning models in safety-critical contexts",
                        "type": "concept",
                        "description": "Real-world failures such as racial bias in COMPAS and adversarial mis-classifications show that opaque ML models can act unreliably in domains like justice, finance and safety-critical control.",
                        "aliases": [
                            "unsafe black-box AI outcomes",
                            "biased algorithmic decisions"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.00069",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516433323,
                        "updated_at": 1759516433323
                    }
                },
                0.691584289073944
            ]
        ]
    },
    {
        "seed": 1451,
        "cluster": [
            [
                {
                    "id": 1451,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Irrecoverable state entry during real-world RL training",
                        "type": "concept",
                        "description": "Agents may perform actions that leave the robot or environment in states from which autonomous recovery is impossible, causing safety hazards and halting learning.",
                        "aliases": [
                            "irreversible states in physical RL",
                            "catastrophic unrecoverable states during robot learning"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1711.06782",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429099,
                        "updated_at": 1759516429099
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1259,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Catastrophic actions during RL training in real-world environments",
                        "type": "concept",
                        "description": "RL agents can cause irreversible harm to humans, property or environment while exploring before they learn safe behaviour.",
                        "aliases": [
                            "training catastrophes",
                            "unsafe RL actions"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1707.05173",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516426899,
                        "updated_at": 1759516426899
                    }
                },
                0.691854417324066
            ]
        ]
    },
    {
        "seed": 344,
        "cluster": [
            [
                {
                    "id": 344,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Bias correction of exponential moving averages prevents large early step sizes",
                        "type": "concept",
                        "description": "Dividing EMA estimates by (1\u2212\u03b2^t) removes zero-initialisation bias, keeping early update magnitudes bounded and stabilising training.",
                        "aliases": [
                            "moment bias correction necessity",
                            "corrected EMA for stability"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1412.6980",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417107,
                        "updated_at": 1759516417107
                    }
                },
                0.0
            ],
            [
                {
                    "id": 347,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Introduce bias correction terms in Adam",
                        "type": "concept",
                        "description": "Design decision to divide moment estimates by (1\u2212\u03b2^t) to eliminate initialisation bias.",
                        "aliases": [
                            "bias-corrected Adam design choice",
                            "EMA correction in optimiser"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1412.6980",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417107,
                        "updated_at": 1759516417107
                    }
                },
                0.692018032073975
            ]
        ]
    },
    {
        "seed": 1294,
        "cluster": [
            [
                {
                    "id": 1294,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Robot pragmatic interpretation leverages human pedagogy",
                        "type": "concept",
                        "description": "If the robot assumes the human acts pedagogically, it can interpret actions as intentional signals, leading to faster and more accurate objective inference.",
                        "aliases": [
                            "pragmatic robot listener",
                            "goal-directed pragmatic inference"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1707.06354",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516427259,
                        "updated_at": 1759516427259
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1993,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Pedagogic action signaling by humans accelerates reward inference",
                        "type": "concept",
                        "description": "Humans can choose actions that teach the robot their preferences, and robots can interpret these pragmatically, improving alignment speed.",
                        "aliases": [
                            "implicit communication in CIRL",
                            "human pedagogic behavior"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1806.03820",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434753,
                        "updated_at": 1759516434753
                    }
                },
                0.692951202392578
            ]
        ]
    },
    {
        "seed": 2276,
        "cluster": [
            [
                {
                    "id": 2276,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Simulation-based robustness improvement via adversarial reprogramming-aware training",
                        "type": "concept",
                        "description": "Preliminary simulations suggest \u226550 % drop in attack success when reprogramming examples are included during fine-tuning (inferred).",
                        "aliases": [
                            "simulated fine-tuning robustness"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1806.11146",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516437803,
                        "updated_at": 1759516437803
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2267,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Training models with adversarial reprogramming examples to increase robustness",
                        "type": "concept",
                        "description": "Extends adversarial training to include large-magnitude, task-redirecting programs rather than small-norm perturbations (motivated by \u00a74.4 finding that standard adversarial training fails).",
                        "aliases": [
                            "reprogramming-aware adversarial training"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1806.11146",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516437803,
                        "updated_at": 1759516437803
                    }
                },
                0.693528175354004
            ]
        ]
    },
    {
        "seed": 1163,
        "cluster": [
            [
                {
                    "id": 1163,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "information gain utility avoids noise attraction in stochastic environments",
                        "type": "concept",
                        "description": "Utility based on reduction in model entropy (KL-KSA) targets epistemic value, not percept entropy, preventing noise addiction.",
                        "aliases": [
                            "IG utility robustness",
                            "info-gain avoids noise"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1705.10557",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425957,
                        "updated_at": 1759516425957
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1170,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "intrinsic information gain utility driving exploration",
                        "type": "concept",
                        "description": "Define utility as expected reduction in posterior entropy, incentivising epistemic exploration.",
                        "aliases": [
                            "KL-KSA design rationale"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1705.10557",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516425957,
                        "updated_at": 1759516425957
                    }
                },
                0.693645536899567
            ]
        ]
    },
    {
        "seed": 719,
        "cluster": [
            [
                {
                    "id": 719,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "reward-directed exploration for asymptotic optimality in reinforcement learning",
                        "type": "concept",
                        "description": "Exploration should be guided by expected rewards; an agent is asymptotically optimal iff reward-directed exploration (captured by EP) converges to zero.",
                        "aliases": [
                            "reward-aware exploration necessity",
                            "reward-directed exploration insight"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1609.04994",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421295,
                        "updated_at": 1759516421295
                    }
                },
                0.0
            ],
            [
                {
                    "id": 717,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "reward-agnostic exploration strategies in reinforcement learning",
                        "type": "concept",
                        "description": "Strategies such as \u03b5-greedy or option discovery ignore the reward landscape, leading to excessive or misplaced exploration in high- or low-value regions.",
                        "aliases": [
                            "undirected exploration in RL",
                            "\u03b5-greedy exploration issues"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1609.04994",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421295,
                        "updated_at": 1759516421295
                    }
                },
                0.693653643131256
            ]
        ]
    },
    {
        "seed": 2268,
        "cluster": [
            [
                {
                    "id": 2268,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Architectural constraints reducing effect of input bias changes on internal representations",
                        "type": "concept",
                        "description": "Modify model architectures or regularise first-layer biases so additive offsets cannot meaningfully alter deep feature activations (inferred from nonlinearity insight in \u00a73).",
                        "aliases": [
                            "first-layer bias constraint design",
                            "architecture-level robustness"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1806.11146",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516437803,
                        "updated_at": 1759516437803
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2271,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "First-layer bias normalization and weight constraints to limit reprogramming susceptibility",
                        "type": "concept",
                        "description": "Applies strong weight-norm penalties or shared-bias restrictions so input-wide additive offsets cannot introduce new effective parameters (inferred from \u00a73 linear-model argument).",
                        "aliases": [
                            "bias-norm constraint",
                            "input-bias regularisation"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1806.11146",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516437803,
                        "updated_at": 1759516437803
                    }
                },
                0.694181442260742
            ]
        ]
    },
    {
        "seed": 2099,
        "cluster": [
            [
                {
                    "id": 2099,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "MCMC sampling of reward posterior distribution",
                        "type": "concept",
                        "description": "Implements Bayesian IRL by randomly walking in reward-function space to approximate the posterior.",
                        "aliases": [
                            "Random-walk posterior sampling in Bayesian IRL"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1806.06877",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435739,
                        "updated_at": 1759516435739
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2416,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "BIHRL Bayesian inference algorithm with Policy-Walk MCMC sampling",
                        "type": "concept",
                        "description": "Uses enumeration of consistent option-trajectories and an MCMC \u2018Policy-Walk\u2019 approach to sample reward parameters that best explain observed behaviour.",
                        "aliases": [
                            "Policy-Walk BIHRL sampler",
                            "hierarchical MCMC reward inference"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1807.05037",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439409,
                        "updated_at": 1759516439409
                    }
                },
                0.6943678855896
            ]
        ]
    },
    {
        "seed": 140,
        "cluster": [
            [
                {
                    "id": 140,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Ineffective reinforcement learning from high-dimensional sensory inputs in Atari environments",
                        "type": "concept",
                        "description": "Standard RL agents fail to learn competent policies when observations are 210\u00d7160 RGB frames without engineered features.",
                        "aliases": [
                            "poor RL performance on raw pixels",
                            "difficulty learning from vision in Atari RL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1312.5602",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414937,
                        "updated_at": 1759516414937
                    }
                },
                0.0
            ],
            [
                {
                    "id": 146,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "High-dimensional visual input without hand-crafted features in Atari games",
                        "type": "concept",
                        "description": "210\u00d7160\u00d73 inputs contain 100\u00d7 more dimensions than prior tabular or feature-based RL settings, stressing traditional methods.",
                        "aliases": [
                            "raw pixel challenges",
                            "lack of engineered vision features"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1312.5602",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414937,
                        "updated_at": 1759516414937
                    }
                },
                0.694880604743958
            ]
        ]
    },
    {
        "seed": 47,
        "cluster": [
            [
                {
                    "id": 47,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Align reward structures with human values to prevent manipulation",
                        "type": "concept",
                        "description": "Design rationale that shaping reward channels to reflect correct objectives reduces incentive for exploitative behaviour.",
                        "aliases": [
                            "safe reward design rationale",
                            "alignment via reward modelling"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1202.6177",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516413813,
                        "updated_at": 1759516413813
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1483,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward shaping alignment of RL policy with human ethics using non-task-aligned data",
                        "type": "concept",
                        "description": "Rather than crafting explicit ethical rewards, bias the learning process by adding shaping rewards that minimise policy divergence from aggregated human behaviour.",
                        "aliases": [
                            "ethics shaping design rationale",
                            "KL-based reward shaping for ethics"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1712.04172",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429340,
                        "updated_at": 1759516429340
                    }
                },
                0.694919347763062
            ]
        ]
    },
    {
        "seed": 2033,
        "cluster": [
            [
                {
                    "id": 2033,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Neural stethoscope framework for promoting or suppressing task-specific information in hidden layers",
                        "type": "concept",
                        "description": "A probe network attached to any layer provides an independent loss that can (i) analyse representations (\u03bb=0), (ii) encourage complementary information (\u03bb>0) or (iii) suppress nuisance factors (\u03bb<0) without changing the main architecture.",
                        "aliases": [
                            "Stethoscope design rationale",
                            "Unified analytic-auxiliary-adversarial probing"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1806.05502",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435260,
                        "updated_at": 1759516435260
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2036,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Analytic stethoscope probing across network layers (\u03bb=0)",
                        "type": "concept",
                        "description": "Fixed-capacity probes are attached to successive activation tensors without affecting the main weights, measuring the linear separability of supplemental tasks to locate where different information resides.",
                        "aliases": [
                            "Passive stethoscope analysis",
                            "Feature accessibility auditing mechanism"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1806.05502",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435260,
                        "updated_at": 1759516435260
                    }
                },
                0.694989025592804
            ]
        ]
    },
    {
        "seed": 2534,
        "cluster": [
            [
                {
                    "id": 2534,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Train multi-agent policies with MAGAIL using expert demonstrations to avoid reward mis-specification",
                        "type": "intervention",
                        "description": "During fine-tuning/RL, replace hand-crafted rewards with MAGAIL\u2019s adversarially learned rewards to guide joint policy optimisation.",
                        "aliases": [
                            "apply MAGAIL during policy learning",
                            "imitative training of multi-agent systems via MAGAIL"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 2,
                        "url": "https://arxiv.org/abs/1807.09936",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440463,
                        "updated_at": 1759516440463
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2537,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Pretrain multi-agent policies with behavior cloning before MAGAIL fine-tuning",
                        "type": "intervention",
                        "description": "Perform supervised imitation on demonstrations to initialise policy parameters, then continue with MAGAIL adversarial training.",
                        "aliases": [
                            "BC pretraining prior to MAGAIL"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 2,
                        "url": "https://arxiv.org/abs/1807.09936",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440463,
                        "updated_at": 1759516440463
                    }
                },
                0.695425391197205
            ]
        ]
    },
    {
        "seed": 2165,
        "cluster": [
            [
                {
                    "id": 2165,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Inefficient reward learning in complex sequential decision tasks",
                        "type": "concept",
                        "description": "Learning a reward model for long-horizon tasks like parking or navigation typically needs many demonstrations and iterations.",
                        "aliases": [
                            "data-inefficient IRL",
                            "slow reward function acquisition"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.08479",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516436709,
                        "updated_at": 1759516436709
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2457,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Inefficient and unsafe exploration in RL for complex tasks",
                        "type": "concept",
                        "description": "Relying on hand-designed reward functions in complex environments leads to slow learning and potentially unsafe exploratory behaviour.",
                        "aliases": [
                            "reward misspecification in RL",
                            "slow unsafe learning in reinforcement learning"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.06158",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516439907,
                        "updated_at": 1759516439907
                    }
                },
                0.695508658885956
            ]
        ]
    },
    {
        "seed": 294,
        "cluster": [
            [
                {
                    "id": 294,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Apply gradient norm clipping at threshold 5 during LSTM training",
                        "type": "intervention",
                        "description": "During any fine-tuning or training step, compute gradient norm and rescale when it exceeds 5 to prevent optimisation instability.",
                        "aliases": [
                            "use gradient clipping intervention",
                            "norm-bounded training procedure"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 2,
                        "url": "https://arxiv.org/abs/1409.3215",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416451,
                        "updated_at": 1759516416451
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1614,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Apply per-gate layer normalization and adaptive gradient clipping during RNMT training",
                        "type": "intervention",
                        "description": "Integrate layer norm inside each LSTM gate and abort steps with anomalous gradient norms.",
                        "aliases": [
                            "stabilise RNMT training"
                        ],
                        "intervention_lifecycle": 2,
                        "intervention_maturity": 3,
                        "url": "https://arxiv.org/abs/1804.09849",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430547,
                        "updated_at": 1759516430547
                    }
                },
                0.695922493934631
            ]
        ]
    },
    {
        "seed": 1360,
        "cluster": [
            [
                {
                    "id": 1360,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "HalfCheetah experiment showing maintained safety and expert-level learning",
                        "type": "concept",
                        "description": "On the MuJoCo HalfCheetah-v1 task, DropoutDAgger matched Behavior Cloning in safety returns while the learned novice reached expert performance as quickly as other DAgger variants.",
                        "aliases": [
                            "HalfCheetah DropoutDAgger results",
                            "MuJoCo safety evidence"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1709.06166",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516427993,
                        "updated_at": 1759516427993
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2511,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Pareto-dominant safety\u2013performance tradeoff on MuJoCo HalfCheetah experiments",
                        "type": "concept",
                        "description": "On HalfCheetah-v1, the doubt-based rule and full EnsembleDAgger dominate the discrepancy-only rule, providing better novice performance for equal or better combined safety metrics (Section IV.B, Figure 8).",
                        "aliases": [
                            "HalfCheetah validation results",
                            "Pareto frontier evidence"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1807.08364",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440335,
                        "updated_at": 1759516440335
                    }
                },
                0.695974946022034
            ]
        ]
    },
    {
        "seed": 440,
        "cluster": [
            [
                {
                    "id": 440,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "GPU optimisation and synchronous data-parallelism can accelerate deep RNN training",
                        "type": "concept",
                        "description": "Moving CTC to GPU and using ring-all-reduce synchronous SGD promises linear scaling with easier debugging compared to asynchronous methods.",
                        "aliases": [
                            "training speed insight",
                            "GPU CTC + sync SGD insight"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1512.02595",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516418038,
                        "updated_at": 1759516418038
                    }
                },
                0.0
            ],
            [
                {
                    "id": 441,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Implement GPU CTC loss and fast all-reduce synchronous SGD",
                        "type": "concept",
                        "description": "Replace CPU CTC with CUDA version and craft a GPUDirect ring all-reduce to exchange gradients efficiently across 8\u201316 GPUs.",
                        "aliases": [
                            "training speed design rationale",
                            "compute optimisation rationale"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1512.02595",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516418038,
                        "updated_at": 1759516418038
                    }
                },
                0.696005940437317
            ]
        ]
    },
    {
        "seed": 2166,
        "cluster": [
            [
                {
                    "id": 2166,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Excessive human demonstration burden in robot training",
                        "type": "concept",
                        "description": "Providing full trajectories throughout training is costly and often impractical for human experts.",
                        "aliases": [
                            "high-cost human feedback",
                            "constant human interaction"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1806.08479",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516436709,
                        "updated_at": 1759516436709
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1542,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "High expert supervision burden in robot task learning",
                        "type": "concept",
                        "description": "Classical learning-from-demonstration requires humans to tele-operate or kinesthetically program robots for every new task, creating a major scalability bottleneck.",
                        "aliases": [
                            "tedious expert demonstrations",
                            "human-intensive LfD data collection"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1804.08606",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430075,
                        "updated_at": 1759516430075
                    }
                },
                0.696983397006989
            ]
        ]
    },
    {
        "seed": 67,
        "cluster": [
            [
                {
                    "id": 67,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "value iteration with Boltzmann stochastic policy enabling differentiable optimisation in apprenticeship learning",
                        "type": "concept",
                        "description": "Introduces a temperature-controlled Boltzmann policy as a smooth approximation of greedy action-selection, making policy gradients well-defined.",
                        "aliases": [
                            "Boltzmann action selection proxy",
                            "softmax policy for differentiability"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1206.5264",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414127,
                        "updated_at": 1759516414127
                    }
                },
                0.0
            ],
            [
                {
                    "id": 75,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "using stochastic policies with temperature to smooth optimisation landscape in inverse RL",
                        "type": "concept",
                        "description": "By treating Boltzmann temperature as part of the optimisation design, gradients become smooth and parameter redundancy is reduced.",
                        "aliases": [
                            "temperature-controlled smoothing design",
                            "softmax design rationale"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1206.5264",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516414127,
                        "updated_at": 1759516414127
                    }
                },
                0.697384834289551
            ]
        ]
    },
    {
        "seed": 1968,
        "cluster": [
            [
                {
                    "id": 1968,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Designer overburden when specifying joint reward across diverse environments",
                        "type": "concept",
                        "description": "Specifying a single reward that yields correct behaviour in all training environments requires many frustrating tuning iterations.",
                        "aliases": [
                            "reward tuning burden",
                            "joint reward design difficulty"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1806.02501",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516434530,
                        "updated_at": 1759516434530
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1765,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "incomplete reward specification in complex environments",
                        "type": "concept",
                        "description": "Designers cannot feasibly enumerate and balance rewards for every relevant feature in rich environments, leading to gaps that agents exploit.",
                        "aliases": [
                            "reward function misspecification mechanism",
                            "unmodelled reward aspects"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1805.08313",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516432456,
                        "updated_at": 1759516432456
                    }
                },
                0.697545409202576
            ]
        ]
    },
    {
        "seed": 1014,
        "cluster": [
            [
                {
                    "id": 1014,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "robots assuming fixed reward functions without uncertainty in human-robot interaction",
                        "type": "concept",
                        "description": "Robots normally treat their reward as immutable, ignoring designer errors and user-specific preferences.",
                        "aliases": [
                            "objective taken as given",
                            "static robot reward assumption"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1705.04226",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424322,
                        "updated_at": 1759516424322
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1021,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "maintain uncertainty over robot reward and update via human guidance",
                        "type": "concept",
                        "description": "Design principle that robots should represent a belief distribution over possible reward functions and refine it using human signals.",
                        "aliases": [
                            "reward uncertainty framework",
                            "objective as latent state"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1705.04226",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516424322,
                        "updated_at": 1759516424322
                    }
                },
                0.697782278060913
            ]
        ]
    },
    {
        "seed": 2302,
        "cluster": [
            [
                {
                    "id": 2302,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Infeasibility of IRL in complex games due to simulator or policy access requirements",
                        "type": "concept",
                        "description": "Most existing inverse reinforcement learning algorithms require either access to environment dynamics or to agents' full policy, which are unavailable or costly in large real-world games, limiting their practical deployment.",
                        "aliases": [
                            "IRL scalability limitation",
                            "need for environment dynamics in IRL"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1807.00366",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516438256,
                        "updated_at": 1759516438256
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2304,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Traditional IRL methods require environment dynamics or policy access",
                        "type": "concept",
                        "description": "Linear, maximum-entropy and other classic IRL algorithms usually assume the availability of a simulator or the ability to query counterfactual actions, prerequisites that are impractical for logged human data.",
                        "aliases": [
                            "conventional IRL prerequisite",
                            "policy/dynamics dependency"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1807.00366",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516438256,
                        "updated_at": 1759516438256
                    }
                },
                0.698201954364777
            ]
        ]
    },
    {
        "seed": 2070,
        "cluster": [
            [
                {
                    "id": 2070,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Frame-level prioritized reward shaping targets salient experience",
                        "type": "concept",
                        "description": "By weighting rewards from particularly informative frames or actions more heavily, evolution can be guided away from repetitive input-agnostic policies.",
                        "aliases": [
                            "prioritized frame weighting insight",
                            "selectively rewarding critical frames"
                        ],
                        "concept_category": "theoretical insight",
                        "url": "https://arxiv.org/abs/1806.05695",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435545,
                        "updated_at": 1759516435545
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2073,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Prioritized experience frames improve policy learning (Schaul et al. 2015)",
                        "type": "concept",
                        "description": "Empirical Deep RL work shows that prioritizing high-information frames accelerates learning and yields higher final performance.",
                        "aliases": [
                            "PER validation evidence",
                            "Schaul15 frame prioritization"
                        ],
                        "concept_category": "validation evidence",
                        "url": "https://arxiv.org/abs/1806.05695",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516435545,
                        "updated_at": 1759516435545
                    }
                },
                0.698459565639496
            ]
        ]
    },
    {
        "seed": 20,
        "cluster": [
            [
                {
                    "id": 20,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Suboptimal decision outcomes in AI systems due to ignored computational costs",
                        "type": "concept",
                        "description": "When autonomous systems ignore the cost of computation they may choose actions that appear utility-maximising on paper but are actually inferior once resource expenditure or time delay is considered, leading to unsafe or inefficient behaviour.",
                        "aliases": [
                            "computationally-na\u00efve decision risk",
                            "resource-blind AI decisions"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1106.2657",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516413620,
                        "updated_at": 1759516413620
                    }
                },
                0.0
            ],
            [
                {
                    "id": 364,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "suboptimal utility maximization in embedded AI systems",
                        "type": "concept",
                        "description": "AI systems whose decision procedures do not account for their physical embedding can systematically obtain lower utility than theoretically achievable, especially in environments containing predictors or logical copies of the agent.",
                        "aliases": [
                            "embedded agent utility loss",
                            "poor outcomes for physicalistic agents"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1506.07359",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516417417,
                        "updated_at": 1759516417417
                    }
                },
                0.698482513427734
            ]
        ]
    },
    {
        "seed": 1519,
        "cluster": [
            [
                {
                    "id": 1519,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Saddle-point Nash avoidance in zero-sum games with gradient descent",
                        "type": "concept",
                        "description": "Risk that, because gradient learning almost surely avoids strict saddles, agents in zero-sum games fail to converge to Nash equilibria that coincide with those saddles.",
                        "aliases": [
                            "failure to reach Nash in zero-sum",
                            "strict-saddle Nash non-convergence"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1804.05464",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429815,
                        "updated_at": 1759516429815
                    }
                },
                0.0
            ],
            [
                {
                    "id": 1524,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Gradient-based learning avoidance of strict saddles almost surely",
                        "type": "concept",
                        "description": "Theorem 3.1 proves that starting from almost any initial condition, gradient updates do not converge to linearly unstable critical points (strict saddles).",
                        "aliases": [
                            "measure-zero convergence to saddles",
                            "strict saddle avoidance"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1804.05464",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516429815,
                        "updated_at": 1759516429815
                    }
                },
                0.698539197444916
            ]
        ]
    },
    {
        "seed": 29,
        "cluster": [
            [
                {
                    "id": 29,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Value misalignment from ontological crises in AI agents",
                        "type": "concept",
                        "description": "Risk that an agent\u2019s original utility function loses its intended meaning after the agent upgrades or replaces its ontology, potentially leading to actions misaligned with designer goals.",
                        "aliases": [
                            "ontological crisis value misalignment",
                            "goal corruption from ontology upgrade"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1105.3821",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516413538,
                        "updated_at": 1759516413538
                    }
                },
                0.0
            ],
            [
                {
                    "id": 695,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Catastrophic value misalignment by superintelligent AI systems",
                        "type": "concept",
                        "description": "Potential for highly capable AI agents to pursue goals that conflict with human values, leading to large-scale harm or loss of control.",
                        "aliases": [
                            "AI goal misalignment catastrophe",
                            "Superintelligence misaligned with human values"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1607.08289",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421090,
                        "updated_at": 1759516421090
                    }
                },
                0.698589384555817
            ]
        ]
    },
    {
        "seed": 2524,
        "cluster": [
            [
                {
                    "id": 2524,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Per-agent discriminator design incorporating cooperation or competition priors",
                        "type": "concept",
                        "description": "Encoding whether agents are cooperative, independent or adversarial directly in discriminators guides learning and improves imitation (\u00a74.1).",
                        "aliases": [
                            "reward-structure prior in MAGAIL",
                            "centralised/decentralised/zero-sum discriminator rationale"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1807.09936",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440463,
                        "updated_at": 1759516440463
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2528,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Prior-specific discriminator reward structures in MAGAIL",
                        "type": "concept",
                        "description": "Imposes equality, independence or negation constraints on per-agent rewards to reflect cooperative, mixed or competitive tasks (Fig. 1).",
                        "aliases": [
                            "centralised MAGAIL variant",
                            "decentralised MAGAIL variant",
                            "zero-sum MAGAIL variant"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1807.09936",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516440463,
                        "updated_at": 1759516440463
                    }
                },
                0.698674201965332
            ]
        ]
    },
    {
        "seed": 1605,
        "cluster": [
            [
                {
                    "id": 1605,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Adaptive gradient clipping using moving average thresholding",
                        "type": "concept",
                        "description": "Training steps are aborted when gradient norm exceeds four std-dev above moving average of log-norms.",
                        "aliases": [
                            "adaptive clipping mechanism"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1804.09849",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516430547,
                        "updated_at": 1759516430547
                    }
                },
                0.0
            ],
            [
                {
                    "id": 291,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Apply hard gradient norm clipping",
                        "type": "concept",
                        "description": "Adopt a policy that rescales gradients when their norm exceeds a chosen threshold.",
                        "aliases": [
                            "gradient clipping rationale",
                            "norm threshold design choice"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1409.3215",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516416451,
                        "updated_at": 1759516416451
                    }
                },
                0.698888123035431
            ]
        ]
    },
    {
        "seed": 2183,
        "cluster": [
            [
                {
                    "id": 2183,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Fine-tune agents with human-interactive subgoal-supervised IRL incorporating failure learning",
                        "type": "intervention",
                        "description": "During fine-tuning/RL, implement the HI-IRL algorithm: initial demonstrations, subgoal specification, subtask trials, partial demos on failures, and IRLFF updates.",
                        "aliases": [
                            "train with HI-IRL",
                            "apply HI-IRL framework"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 2,
                        "url": "https://arxiv.org/abs/1806.08479",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516436709,
                        "updated_at": 1759516436709
                    }
                },
                0.0
            ],
            [
                {
                    "id": 711,
                    "alias": "",
                    "labels": [
                        "NODE",
                        "Intervention"
                    ],
                    "properties": {
                        "name": "Fine-tune AI agents via inverse reinforcement learning to align with human behavior",
                        "type": "intervention",
                        "description": "During fine-tuning/RL stage, optimise the agent\u2019s policy against reward models inferred from human behavioural datasets using IRL algorithms.",
                        "aliases": [
                            "IRL fine-tuning for alignment",
                            "Behavior-based alignment via IRL"
                        ],
                        "intervention_lifecycle": 3,
                        "intervention_maturity": 2,
                        "url": "https://arxiv.org/abs/1607.08289",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516421090,
                        "updated_at": 1759516421090
                    }
                },
                0.699373483657837
            ]
        ]
    },
    {
        "seed": 677,
        "cluster": [
            [
                {
                    "id": 677,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Reward channel vulnerability enabling direct stimulation",
                        "type": "concept",
                        "description": "Architectural or hardware designs that allow an agent to manipulate its own reward signal open the door to wireheading.",
                        "aliases": [
                            "reward-signal exposure",
                            "direct reward access"
                        ],
                        "concept_category": "problem analysis",
                        "url": "https://arxiv.org/abs/1606.07092",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420874,
                        "updated_at": 1759516420874
                    }
                },
                0.0
            ],
            [
                {
                    "id": 674,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Wireheading-induced reward hijacking in artificial agents",
                        "type": "concept",
                        "description": "AI systems may directly stimulate their own reward mechanisms, bypassing novelty or productive behaviour, which could result in dangerous, unaligned policies.",
                        "aliases": [
                            "reward channel hacking in AI",
                            "artificial wireheading risk"
                        ],
                        "concept_category": "risk",
                        "url": "https://arxiv.org/abs/1606.07092",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516420874,
                        "updated_at": 1759516420874
                    }
                },
                0.699405908584595
            ]
        ]
    },
    {
        "seed": 532,
        "cluster": [
            [
                {
                    "id": 532,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "adaptive subsequence density adjustment to accelerate convergence",
                        "type": "concept",
                        "description": "Design idea to dynamically choose denser yet still independent evaluation points, trading some robustness for faster convergence.",
                        "aliases": [
                            "adaptive EvOp design rationale"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1604.05280",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516419131,
                        "updated_at": 1759516419131
                    }
                },
                0.0
            ],
            [
                {
                    "id": 533,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "proposed adaptive EvOp variant selecting denser independent subsequences",
                        "type": "concept",
                        "description": "Implementation mechanism sketch: alter testseq to allow next comparison once a probabilistic confidence threshold is met rather than strict one-by-one revelation.",
                        "aliases": [
                            "adaptive-EvOp mechanism"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1604.05280",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516419131,
                        "updated_at": 1759516419131
                    }
                },
                0.6998171210289
            ]
        ]
    },
    {
        "seed": 2270,
        "cluster": [
            [
                {
                    "id": 2270,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Adversarial reprogramming adversarial training during fine-tuning phase",
                        "type": "concept",
                        "description": "During RLHF/fine-tuning, include adversarial program examples that attempt task-redirection and optimise the model to maintain correct original task output (inferred from \u00a74.4).",
                        "aliases": [
                            "reprogramming-specific fine-tuning"
                        ],
                        "concept_category": "implementation mechanism",
                        "url": "https://arxiv.org/abs/1806.11146",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516437803,
                        "updated_at": 1759516437803
                    }
                },
                0.0
            ],
            [
                {
                    "id": 2267,
                    "alias": "",
                    "labels": [
                        "Concept",
                        "NODE"
                    ],
                    "properties": {
                        "name": "Training models with adversarial reprogramming examples to increase robustness",
                        "type": "concept",
                        "description": "Extends adversarial training to include large-magnitude, task-redirecting programs rather than small-norm perturbations (motivated by \u00a74.4 finding that standard adversarial training fails).",
                        "aliases": [
                            "reprogramming-aware adversarial training"
                        ],
                        "concept_category": "design rationale",
                        "url": "https://arxiv.org/abs/1806.11146",
                        "is_tombstone": false,
                        "is_leaf": true,
                        "cycled_id": "",
                        "created_at": 1759516437803,
                        "updated_at": 1759516437803
                    }
                },
                0.699932038784027
            ]
        ]
    }
]