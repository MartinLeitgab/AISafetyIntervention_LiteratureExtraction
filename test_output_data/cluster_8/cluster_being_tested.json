[
    {
        "id": 362,
        "alias": "",
        "labels": [
            "Concept",
            "NODE"
        ],
        "properties": {
            "name": "Risk of mis-specifying reward models in RLHF",
            "type": "concept",
            "description": "Training a reward model on corrupted human feedback can cause the learned reward to diverge from intended human preferences.",
            "aliases": [
                "reward model poisoning",
                "alignment drift via bad feedback"
            ],
            "concept_category": "Risk",
            "paper_id": "arxiv__arxiv_org_abs_1606_05374",
            "method": "model",
            "is_tombstone": false
        }
    },
    {
        "id": 200,
        "alias": "",
        "labels": [
            "NODE",
            "Intervention"
        ],
        "properties": {
            "name": "staged AI architecture",
            "type": "intervention",
            "description": "Phase 1 passive model learner; Phase 2 boxed self-model learner with forced exploration; Phase 3 active deployment agent",
            "aliases": [
                "two-stage then three-stage design"
            ],
            "intervention_lifecycle": 1,
            "intervention_maturity": 2,
            "paper_id": "arxiv__arxiv_org_abs_1411_1373",
            "method": "model",
            "is_tombstone": false
        }
    },
    {
        "id": 279,
        "alias": "",
        "labels": [
            "Concept",
            "NODE"
        ],
        "properties": {
            "name": "reliance on brain-energy constraint to dismiss AI feasibility",
            "type": "concept",
            "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
            "aliases": [
                "energy-bound complacency",
                "brain power fallacy"
            ],
            "concept_category": "Problem",
            "paper_id": "arxiv__arxiv_org_abs_1602_04019",
            "method": "model",
            "is_tombstone": false
        }
    },
    {
        "id": 282,
        "alias": "",
        "labels": [
            "Concept",
            "NODE"
        ],
        "properties": {
            "name": "energy requirements of AI not tightly correlated with brain energy",
            "type": "concept",
            "description": "Because of architectural and hardware differences, AI could match or surpass human cognition at far lower or higher power levels.",
            "aliases": [
                "decoupled AI energy use",
                "weak brain-AI power link"
            ],
            "concept_category": "Claim",
            "paper_id": "arxiv__arxiv_org_abs_1602_04019",
            "method": "model",
            "is_tombstone": false
        }
    },
    {
        "id": 280,
        "alias": "",
        "labels": [
            "Concept",
            "NODE"
        ],
        "properties": {
            "name": "AI architectures can differ from brain and use higher-level representations",
            "type": "concept",
            "description": "Artificial systems are not constrained to neuron-level simulation and can operate on compressed higher-level data structures.",
            "aliases": [
                "non-biological cognitive pathways",
                "abstracted AI computation"
            ],
            "concept_category": "Principle",
            "paper_id": "arxiv__arxiv_org_abs_1602_04019",
            "method": "model",
            "is_tombstone": false
        }
    },
    {
        "id": 1034,
        "alias": "",
        "labels": [
            "Concept",
            "NODE"
        ],
        "properties": {
            "name": "standard RL ignores reward observation cost",
            "type": "concept",
            "description": "In conventional reinforcement learning the agent observes rewards for free, so the optimisation objective contains no term for the cost of acquiring those rewards.",
            "aliases": [
                "traditional RL no query price",
                "missing query cost in RL"
            ],
            "concept_category": "Problem",
            "paper_id": "arxiv__arxiv_org_abs_1803_04926",
            "method": "model",
            "is_tombstone": false
        }
    },
    {
        "id": 1035,
        "alias": "",
        "labels": [
            "Concept",
            "NODE"
        ],
        "properties": {
            "name": "human feedback labelling cost is significant in reward learning",
            "type": "concept",
            "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
            "aliases": [
                "expensive human labels",
                "costly human evaluations"
            ],
            "concept_category": "Observation",
            "paper_id": "arxiv__arxiv_org_abs_1803_04926",
            "method": "model",
            "is_tombstone": false
        }
    },
    {
        "id": 281,
        "alias": "",
        "labels": [
            "Concept",
            "NODE"
        ],
        "properties": {
            "name": "specialised hardware and abstraction reduce energy per operation by orders of magnitude",
            "type": "concept",
            "description": "Empirical evidence shows dedicated accelerators and model simplifications yield \u2265100\u00d7 energy savings compared to na\u00efve implementations.",
            "aliases": [
                "GPU/ASIC energy efficiency",
                "algorithmic compression benefit"
            ],
            "concept_category": "Finding",
            "paper_id": "arxiv__arxiv_org_abs_1602_04019",
            "method": "model",
            "is_tombstone": false
        }
    },
    {
        "id": 104,
        "alias": "",
        "labels": [
            "Concept",
            "NODE"
        ],
        "properties": {
            "name": "nonstationary_behavior_distribution",
            "type": "concept",
            "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
            "aliases": [
                "changing data distribution",
                "policy-induced non-stationarity"
            ],
            "concept_category": "Observation",
            "paper_id": "arxiv__arxiv_org_abs_1312_5602",
            "method": "model",
            "is_tombstone": false
        }
    }
]