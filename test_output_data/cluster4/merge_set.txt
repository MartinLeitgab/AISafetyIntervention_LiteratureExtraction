[
    {
        "nodes": [
            1136,
            1134,
            1135
        ],
        "rationale": "These three nodes are tightly coupled pieces of a single explanatory chain about off-manifold inputs. Node 1134 gives the core definition of what an off-manifold input is. Node 1135 immediately builds on that definition by describing the geometric property that makes such inputs easier to separate in deeper hidden layers. Node 1136 supplies the main computational tool\u2014using a denoising auto-encoder\u2019s reconstruction vector as an estimate of the log-density gradient\u2014that practitioners rely on to detect those same off-manifold directions. Because the three statements are almost always referenced together when discussing distribution-shift or adversarial-example detection, keeping them separate adds duplication without additional conceptual clarity. Merging them preserves the full meaning while reducing graph fragmentation.",
        "parameters": {
            "name": "off-manifold input detection via deep representations",
            "type": "concept",
            "description": "Unified concept covering: (1) inputs that lie in low-density regions of the training-data distribution (off the manifold), (2) the fact that such deviations become increasingly linearly separable in deeper network layers due to manifold flattening, and (3) the use of denoising auto-encoders whose reconstruction vector r(x)\u2013x \u2248 \u2207log p(x) to provide a practical signal for identifying or rejecting those off-manifold inputs."
        }
    }
]