# AI Safety Knowledge Graph Semantic Compression
You are an expert in AI safety knowledge graph compression. Given the following nodes and their relationships, your task is to:
1. Only consider merging the primary nodes listed below. Do NOT merge or suggest merging any neighbor nodes.
2. Decide which primary nodes should be merged into a single supernode (merged concept).
3. Provide a clear rationale for each merge decision. Reason step by step
4. For each merge set, generate merged parameters for the supernode: name, description, type, and any other relevant attributes.

5. Reason step by step for each merge decision to ensure the highest quality rationale.

Consider only these nodes for merging: [940, 647, 103, 104, 196, 197, 198, 942]

Nodes:
Node ID: 103
Name: experience_replay_uniform_1M_buffer
Type: intervention
Description: Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.

Node ID: 104
Name: nonstationary_behavior_distribution
Type: concept
Description: Because the agentâ€™s policy changes during learning, the distribution over visited states and actions drifts over time.

Node ID: 196
Name: self-delusion due to reward hacking
Type: concept
Description: Agents directly modify their sensory channel or reward source to falsely maximise observed reward

Node ID: 197
Name: model-based utility over inferred state
Type: intervention
Description: Compute utility as a function of latent variables inside the learned environment model, not raw observations

Node ID: 198
Name: unintended instrumental actions
Type: concept
Description: Utility-maximising agents pursue resource acquisition, self-preservation, manipulation, etc.

Node ID: 647
Name: complex tasks lacking well-specified reward functions
Type: concept
Description: Tasks where designing an explicit numerical reward mapping from observations or actions to scalar value is infeasible or error-prone.

Node ID: 940
Name: end-to-end observation+user input policy learning
Type: concept
Description: Learning a single neural policy that maps concatenated environmental observations and raw user inputs to actions, supervised only by task reward.

Node ID: 942
Name: off-policy deep Q-learning sample efficiency for human-in-loop
Type: concept
Description: Using NFQI with experience replay and target networks allows learning from fewer human-in-the-loop interactions.


Edges:


Output Instructions:
Return ONLY a valid JSON object with the following format:
{
  "merge_sets": [
    {
      "node_ids": [list of node IDs to merge],
      "rationale": "reason for merging",
      "parameters": {
        "name": "string - concise name for the supernode",
        "type": "string - node type",
        "description": "string - comprehensive description",
      }
    }
  ]
}
