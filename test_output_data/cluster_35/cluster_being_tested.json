[
    {
        "id": 940,
        "alias": "",
        "labels": [
            "Concept",
            "NODE"
        ],
        "properties": {
            "name": "end-to-end observation+user input policy learning",
            "type": "concept",
            "description": "Learning a single neural policy that maps concatenated environmental observations and raw user inputs to actions, supervised only by task reward.",
            "aliases": [
                "joint observation and control embedding",
                "model-free intent decoding"
            ],
            "concept_category": "Principle",
            "paper_id": "arxiv__arxiv_org_abs_1802_01744",
            "method": "model",
            "is_tombstone": false
        }
    },
    {
        "id": 647,
        "alias": "",
        "labels": [
            "Concept",
            "NODE"
        ],
        "properties": {
            "name": "complex tasks lacking well-specified reward functions",
            "type": "concept",
            "description": "Tasks where designing an explicit numerical reward mapping from observations or actions to scalar value is infeasible or error-prone.",
            "aliases": [
                "unspecified reward tasks",
                "hard-to-specify goals"
            ],
            "concept_category": "Problem",
            "paper_id": "arxiv__arxiv_org_abs_1706_03741",
            "method": "model",
            "is_tombstone": false
        }
    },
    {
        "id": 103,
        "alias": "",
        "labels": [
            "NODE",
            "Intervention"
        ],
        "properties": {
            "name": "experience_replay_uniform_1M_buffer",
            "type": "intervention",
            "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
            "aliases": [
                "experience replay buffer",
                "uniform replay memory"
            ],
            "intervention_lifecycle": 1,
            "intervention_maturity": 2,
            "paper_id": "arxiv__arxiv_org_abs_1312_5602",
            "method": "model",
            "is_tombstone": false
        }
    },
    {
        "id": 104,
        "alias": "",
        "labels": [
            "Concept",
            "NODE"
        ],
        "properties": {
            "name": "nonstationary_behavior_distribution",
            "type": "concept",
            "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
            "aliases": [
                "changing data distribution",
                "policy-induced non-stationarity"
            ],
            "concept_category": "Observation",
            "paper_id": "arxiv__arxiv_org_abs_1312_5602",
            "method": "model",
            "is_tombstone": false
        }
    },
    {
        "id": 196,
        "alias": "",
        "labels": [
            "Concept",
            "NODE"
        ],
        "properties": {
            "name": "self-delusion due to reward hacking",
            "type": "concept",
            "description": "Agents directly modify their sensory channel or reward source to falsely maximise observed reward",
            "aliases": [
                "wireheading",
                "observation manipulation"
            ],
            "concept_category": "Problem",
            "paper_id": "arxiv__arxiv_org_abs_1411_1373",
            "method": "model",
            "is_tombstone": false
        }
    },
    {
        "id": 197,
        "alias": "",
        "labels": [
            "NODE",
            "Intervention"
        ],
        "properties": {
            "name": "model-based utility over inferred state",
            "type": "intervention",
            "description": "Compute utility as a function of latent variables inside the learned environment model, not raw observations",
            "aliases": [
                "latent-state utility",
                "environment-model utility"
            ],
            "intervention_lifecycle": 2,
            "intervention_maturity": 2,
            "paper_id": "arxiv__arxiv_org_abs_1411_1373",
            "method": "model",
            "is_tombstone": false
        }
    },
    {
        "id": 198,
        "alias": "",
        "labels": [
            "Concept",
            "NODE"
        ],
        "properties": {
            "name": "unintended instrumental actions",
            "type": "concept",
            "description": "Utility-maximising agents pursue resource acquisition, self-preservation, manipulation, etc.",
            "aliases": [
                "power-seeking side-effects"
            ],
            "concept_category": "Problem",
            "paper_id": "arxiv__arxiv_org_abs_1411_1373",
            "method": "model",
            "is_tombstone": false
        }
    },
    {
        "id": 942,
        "alias": "",
        "labels": [
            "Concept",
            "NODE"
        ],
        "properties": {
            "name": "off-policy deep Q-learning sample efficiency for human-in-loop",
            "type": "concept",
            "description": "Using NFQI with experience replay and target networks allows learning from fewer human-in-the-loop interactions.",
            "aliases": [
                "NFQI sample efficiency claim",
                "off-policy RL benefit"
            ],
            "concept_category": "Claim",
            "paper_id": "arxiv__arxiv_org_abs_1802_01744",
            "method": "model",
            "is_tombstone": false
        }
    }
]