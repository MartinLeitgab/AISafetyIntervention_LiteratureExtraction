# AI Safety Knowledge Graph Semantic Compression
You are an expert in AI safety knowledge graph compression. Given the following nodes and their relationships, your task is to:
1. Only consider merging the primary nodes listed below. Do NOT merge or suggest merging any neighbor nodes.
2. Decide which primary nodes should be merged into a single supernode (merged concept).
3. Provide a clear rationale for each merge decision. Reason step by step
4. For each merge set, generate merged parameters for the supernode: name, description, type, and any other relevant attributes.

5. Reason step by step for each merge decision to ensure the highest quality rationale.

Consider only these nodes for merging: [197, 753, 1035, 1036, 1037, 1038, 279]

Nodes:
Node ID: 197
Name: model-based utility over inferred state
Type: intervention
Description: Compute utility as a function of latent variables inside the learned environment model, not raw observations

Node ID: 279
Name: reliance on brain-energy constraint to dismiss AI feasibility
Type: concept
Description: The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.

Node ID: 753
Name: restoration of positive expected utility under mis-specification
Type: concept
Description: With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.

Node ID: 1035
Name: human feedback labelling cost is significant in reward learning
Type: concept
Description: Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.

Node ID: 1036
Name: need to minimise number of costly reward queries while maintaining performance
Type: concept
Description: Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.

Node ID: 1037
Name: active reinforcement learning model with query cost c in MDP
Type: concept
Description: Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.

Node ID: 1038
Name: RL exploration heuristics cause zero querying in ARL
Type: concept
Description: Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.


Edges:


Output Instructions:
Return ONLY a valid JSON object with the following format:
{
  "merge_sets": [
    {
      "node_ids": [list of node IDs to merge],
      "rationale": "reason for merging",
      "parameters": {
        "name": "string - concise name for the supernode",
        "type": "string - node type",
        "description": "string - comprehensive description",
      }
    }
  ]
}
