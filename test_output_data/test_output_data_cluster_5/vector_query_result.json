[
    [
        {
            "id": 101,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "correlated_onpolicy_transition_sequences",
                "type": "concept",
                "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
                "aliases": [
                    "correlated data sequences",
                    "temporally correlated samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model"
            }
        },
        {
            "id": 863,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "General human action datasets are easier to collect than goal-aligned datasets",
                "type": "concept",
                "description": "Collecting large unlabeled logs of ordinary human actions is less expensive than collecting demonstrations aligned with the agent\u2019s specific objective function.",
                "aliases": [
                    "general human data accessible",
                    "low-cost human behavioural data"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model"
            }
        },
        {
            "id": 864,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Aggregated human policy derived via Bayesian integration of human state\u2013action frequency",
                "type": "concept",
                "description": "A stochastic policy Pr_H(a|s) is estimated from the frequency of human-taken actions in each state using a binomial/Bayesian model with confidence parameter C.",
                "aliases": [
                    "human policy via Griffith et al. aggregation",
                    "Bayesian human policy estimation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model"
            }
        },
        {
            "id": 941,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "model-free shared autonomy improves success and reduces crashes",
                "type": "concept",
                "description": "Empirical finding that pilot+copilot teams achieve higher success rates and lower crash/out-of-bounds rates than either component alone.",
                "aliases": [
                    "shared autonomy performance gains",
                    "copilot success boost"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model"
            }
        },
        {
            "id": 939,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "known-model assumptions limit shared autonomy adaptability",
                "type": "concept",
                "description": "Requiring known dynamics, a known goal set, and a known user policy makes shared-autonomy systems brittle and inflexible.",
                "aliases": [
                    "need to relax dynamics/goal/user-policy assumptions",
                    "fixed-model assumption problem"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model"
            }
        },
        {
            "id": 942,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "off-policy deep Q-learning sample efficiency for human-in-loop",
                "type": "concept",
                "description": "Using NFQI with experience replay and target networks allows learning from fewer human-in-the-loop interactions.",
                "aliases": [
                    "NFQI sample efficiency claim",
                    "off-policy RL benefit"
                ],
                "concept_category": "Claim",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model"
            }
        },
        {
            "id": 943,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ignoring user suggestions degrades human input quality",
                "type": "concept",
                "description": "Consistently overriding a human\u2019s suggested controls reduces their ability to provide meaningful inputs in real-time tasks.",
                "aliases": [
                    "feedback disruption issue",
                    "loss of human control feedback"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model"
            }
        },
        {
            "id": 940,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "end-to-end observation+user input policy learning",
                "type": "concept",
                "description": "Learning a single neural policy that maps concatenated environmental observations and raw user inputs to actions, supervised only by task reward.",
                "aliases": [
                    "joint observation and control embedding",
                    "model-free intent decoding"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model"
            }
        }
    ]
]