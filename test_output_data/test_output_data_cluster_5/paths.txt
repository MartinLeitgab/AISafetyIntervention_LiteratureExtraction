[
  {
    "nodes": [
      {
        "id": 197,
        "alias": "",
        "labels": [
          "NODE",
          "Intervention"
        ],
        "properties": {
          "name": "model-based utility over inferred state",
          "type": "intervention",
          "description": "Compute utility as a function of latent variables inside the learned environment model, not raw observations",
          "aliases": [
            "latent-state utility",
            "environment-model utility"
          ],
          "intervention_lifecycle": 2,
          "intervention_maturity": 2,
          "paper_id": "arxiv__arxiv_org_abs_1411_1373",
          "method": "model",
          "is_tombstone": false
        }
      }
    ],
    "edges": [],
    "length": 0
  },
  {
    "nodes": [
      {
        "id": 279,
        "alias": "",
        "labels": [
          "Concept",
          "NODE"
        ],
        "properties": {
          "name": "reliance on brain-energy constraint to dismiss AI feasibility",
          "type": "concept",
          "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
          "aliases": [
            "energy-bound complacency",
            "brain power fallacy"
          ],
          "concept_category": "Problem",
          "paper_id": "arxiv__arxiv_org_abs_1602_04019",
          "method": "model",
          "is_tombstone": false
        }
      }
    ],
    "edges": [],
    "length": 0
  },
  {
    "nodes": [
      {
        "id": 753,
        "alias": "",
        "labels": [
          "Concept",
          "NODE"
        ],
        "properties": {
          "name": "restoration of positive expected utility under mis-specification",
          "type": "concept",
          "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
          "aliases": [
            "positive expected reward after fix"
          ],
          "concept_category": "Outcome",
          "paper_id": "arxiv__arxiv_org_abs_1709_06275",
          "method": "model",
          "is_tombstone": false
        }
      }
    ],
    "edges": [],
    "length": 0
  },
  {
    "nodes": [
      {
        "id": 1035,
        "alias": "",
        "labels": [
          "Concept",
          "NODE"
        ],
        "properties": {
          "name": "human feedback labelling cost is significant in reward learning",
          "type": "concept",
          "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
          "aliases": [
            "expensive human labels",
            "costly human evaluations"
          ],
          "concept_category": "Observation",
          "paper_id": "arxiv__arxiv_org_abs_1803_04926",
          "method": "model",
          "is_tombstone": false
        }
      }
    ],
    "edges": [],
    "length": 0
  },
  {
    "nodes": [
      {
        "id": 1036,
        "alias": "",
        "labels": [
          "Concept",
          "NODE"
        ],
        "properties": {
          "name": "need to minimise number of costly reward queries while maintaining performance",
          "type": "concept",
          "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
          "aliases": [
            "query cost trade-off",
            "efficient supervision objective"
          ],
          "concept_category": "Opportunity",
          "paper_id": "arxiv__arxiv_org_abs_1803_04926",
          "method": "model",
          "is_tombstone": false
        }
      }
    ],
    "edges": [],
    "length": 0
  },
  {
    "nodes": [
      {
        "id": 1037,
        "alias": "",
        "labels": [
          "Concept",
          "NODE"
        ],
        "properties": {
          "name": "active reinforcement learning model with query cost c in MDP",
          "type": "concept",
          "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
          "aliases": [
            "ARL formalism",
            "MDP augmented with query indicator"
          ],
          "concept_category": "Method",
          "paper_id": "arxiv__arxiv_org_abs_1803_04926",
          "method": "model",
          "is_tombstone": false
        }
      }
    ],
    "edges": [],
    "length": 0
  },
  {
    "nodes": [
      {
        "id": 1038,
        "alias": "",
        "labels": [
          "Concept",
          "NODE"
        ],
        "properties": {
          "name": "RL exploration heuristics cause zero querying in ARL",
          "type": "concept",
          "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
          "aliases": [
            "optimism fails in ARL",
            "Thompson sampling pathology"
          ],
          "concept_category": "Finding",
          "paper_id": "arxiv__arxiv_org_abs_1803_04926",
          "method": "model",
          "is_tombstone": false
        }
      }
    ],
    "edges": [],
    "length": 0
  }
]