599 clusters
[
    [
        {
            "id": 768,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Feedback replay buffer enables frequent SGD updates despite sparse feedback",
                "type": "concept",
                "description": "Storing past feedback-experience pairs allows the learner to sample mini-batches and update the reward model at a higher rate than feedback arrival.",
                "aliases": [
                    "feedback experience replay",
                    "buffered human feedback"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 779,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Maintain feedback replay buffer for minibatch SGD every 10 time steps",
                "type": "intervention",
                "description": "Store all (experience, feedback) pairs and resample them so the reward model is updated ten times as often as new feedback arrives.",
                "aliases": [
                    "feedback buffer updates",
                    "replay human critiques"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 943,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ignoring user suggestions degrades human input quality",
                "type": "concept",
                "description": "Consistently overriding a human\u2019s suggested controls reduces their ability to provide meaningful inputs in real-time tasks.",
                "aliases": [
                    "feedback disruption issue",
                    "loss of human control feedback"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 103,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "experience_replay_uniform_1M_buffer",
                "type": "intervention",
                "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
                "aliases": [
                    "experience replay buffer",
                    "uniform replay memory"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 101,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "correlated_onpolicy_transition_sequences",
                "type": "concept",
                "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
                "aliases": [
                    "correlated data sequences",
                    "temporally correlated samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 104,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "nonstationary_behavior_distribution",
                "type": "concept",
                "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
                "aliases": [
                    "changing data distribution",
                    "policy-induced non-stationarity"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 865,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Applying ethics-shaping reward based on KL divergence between RL policy and aggregated human policy during training",
                "type": "intervention",
                "description": "During RL training add H(s,a) = \u00b1c\u00b7DKL(Pr_Q||Pr_H) when ethical divergence conditions on thresholds \u03c4_n, \u03c4_p are met, thereby penalising actions humans rarely take and rewarding actions humans strongly prefer.",
                "aliases": [
                    "ethics shaping KL reward",
                    "KL-based ethical reward shaping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1082,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Clean Logit Pairing between random clean images with Gaussian noise augmentation",
                "type": "intervention",
                "description": "Penalise L2 distance between logits of randomly paired clean images while training with input Gaussian noise, avoiding adversarial example generation.",
                "aliases": [
                    "CLP low-cost defence",
                    "random pair logit pairing"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1803_06373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1079,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Adversarial Logit Pairing (add \u03bb \u00b7 L2 loss between clean/adversarial logits during training)",
                "type": "intervention",
                "description": "During training, penalise the L2 distance between logits of each clean sample and its PGD adversarial counterpart with weight \u03bb\u22480.2\u20131.",
                "aliases": [
                    "ALP defence",
                    "logit pairing with adversarial examples"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_06373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 943,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ignoring user suggestions degrades human input quality",
                "type": "concept",
                "description": "Consistently overriding a human\u2019s suggested controls reduces their ability to provide meaningful inputs in real-time tasks.",
                "aliases": [
                    "feedback disruption issue",
                    "loss of human control feedback"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 103,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "experience_replay_uniform_1M_buffer",
                "type": "intervention",
                "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
                "aliases": [
                    "experience replay buffer",
                    "uniform replay memory"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 101,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "correlated_onpolicy_transition_sequences",
                "type": "concept",
                "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
                "aliases": [
                    "correlated data sequences",
                    "temporally correlated samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 104,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "nonstationary_behavior_distribution",
                "type": "concept",
                "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
                "aliases": [
                    "changing data distribution",
                    "policy-induced non-stationarity"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 865,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Applying ethics-shaping reward based on KL divergence between RL policy and aggregated human policy during training",
                "type": "intervention",
                "description": "During RL training add H(s,a) = \u00b1c\u00b7DKL(Pr_Q||Pr_H) when ethical divergence conditions on thresholds \u03c4_n, \u03c4_p are met, thereby penalising actions humans rarely take and rewarding actions humans strongly prefer.",
                "aliases": [
                    "ethics shaping KL reward",
                    "KL-based ethical reward shaping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1055,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Include mandatory keep-alive reward R0 in overall reward composition during system design",
                "type": "intervention",
                "description": "Always multiply task reward by an alive/dead indicator so that any trajectory reaching uncontrollable states receives zero total reward.",
                "aliases": [
                    "keep_alive_intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1054,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Binary keep-alive reward assigns zero to dead states and positive to live states",
                "type": "concept",
                "description": "A base reward component R0(s) that equals 1 in feasible, controllable states and 0 otherwise, thereby penalising trajectories leading to irrecoverable failure.",
                "aliases": [
                    "R0 keep alive",
                    "alive vs dead reward"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1050,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Negative rewards break proportionality between scanning density and reward density",
                "type": "concept",
                "description": "If some rewards are \u22640 the theoretical link between walker distribution and reward density cannot hold, undermining FMC\u2019s optimality criterion.",
                "aliases": [
                    "broken density proportionality"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 924,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reward shaping penalty near constraints",
                "type": "concept",
                "description": "A common alternative approach that adds large negative reward when the agent is close to violating constraints, aiming to discourage unsafe actions without hard guarantees.",
                "aliases": [
                    "boundary penalty shaping",
                    "negative reward near unsafe states"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1801_08757",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 683,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reward shaping penalties alone fail to eliminate catastrophes",
                "type": "concept",
                "description": "Giving large negative rewards for catastrophes without blocking does not drive the catastrophe rate to zero.",
                "aliases": [
                    "negative reward insufficient",
                    "penalties don't solve safety"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1050,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Negative rewards break proportionality between scanning density and reward density",
                "type": "concept",
                "description": "If some rewards are \u22640 the theoretical link between walker distribution and reward density cannot hold, undermining FMC\u2019s optimality criterion.",
                "aliases": [
                    "broken density proportionality"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 656,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Collect human feedback online during training",
                "type": "intervention",
                "description": "Interleave new preference queries with policy learning so the reward model stays aligned with the evolving state distribution.",
                "aliases": [
                    "online preference queries",
                    "continual human oversight"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 654,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Train reward predictor from human pairwise clip comparisons and use it as proxy reward during RL",
                "type": "intervention",
                "description": "Fit a neural network to human clip-level preference data and supply its output as the reward signal for policy optimisation.",
                "aliases": [
                    "preference-based reward model",
                    "learned reward proxy"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 943,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ignoring user suggestions degrades human input quality",
                "type": "concept",
                "description": "Consistently overriding a human\u2019s suggested controls reduces their ability to provide meaningful inputs in real-time tasks.",
                "aliases": [
                    "feedback disruption issue",
                    "loss of human control feedback"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 103,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "experience_replay_uniform_1M_buffer",
                "type": "intervention",
                "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
                "aliases": [
                    "experience replay buffer",
                    "uniform replay memory"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 101,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "correlated_onpolicy_transition_sequences",
                "type": "concept",
                "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
                "aliases": [
                    "correlated data sequences",
                    "temporally correlated samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 104,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "nonstationary_behavior_distribution",
                "type": "concept",
                "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
                "aliases": [
                    "changing data distribution",
                    "policy-induced non-stationarity"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 865,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Applying ethics-shaping reward based on KL divergence between RL policy and aggregated human policy during training",
                "type": "intervention",
                "description": "During RL training add H(s,a) = \u00b1c\u00b7DKL(Pr_Q||Pr_H) when ethical divergence conditions on thresholds \u03c4_n, \u03c4_p are met, thereby penalising actions humans rarely take and rewarding actions humans strongly prefer.",
                "aliases": [
                    "ethics shaping KL reward",
                    "KL-based ethical reward shaping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 574,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "train AI agents for low temporal discounting via preference shaping",
                "type": "intervention",
                "description": "During RLHF or similar alignment phases, explicitly reward trajectories that value distant future outcomes, reducing short-term resource grabs.",
                "aliases": [
                    "instil long-termism in RLHF",
                    "alignment of discount rates"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1705_03394",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 160,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement continual value learning mapping human preferences to physical proxies",
                "type": "intervention",
                "description": "Employ fine-tuning or RLHF loops that repeatedly update the model\u2019s reward function as it gains better mappings from abstract human values to concrete world-state proxies.",
                "aliases": [
                    "iterative preference learning",
                    "dynamic value refinement"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1409_0813",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 943,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ignoring user suggestions degrades human input quality",
                "type": "concept",
                "description": "Consistently overriding a human\u2019s suggested controls reduces their ability to provide meaningful inputs in real-time tasks.",
                "aliases": [
                    "feedback disruption issue",
                    "loss of human control feedback"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 103,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "experience_replay_uniform_1M_buffer",
                "type": "intervention",
                "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
                "aliases": [
                    "experience replay buffer",
                    "uniform replay memory"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 101,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "correlated_onpolicy_transition_sequences",
                "type": "concept",
                "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
                "aliases": [
                    "correlated data sequences",
                    "temporally correlated samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 104,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "nonstationary_behavior_distribution",
                "type": "concept",
                "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
                "aliases": [
                    "changing data distribution",
                    "policy-induced non-stationarity"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 865,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Applying ethics-shaping reward based on KL divergence between RL policy and aggregated human policy during training",
                "type": "intervention",
                "description": "During RL training add H(s,a) = \u00b1c\u00b7DKL(Pr_Q||Pr_H) when ethical divergence conditions on thresholds \u03c4_n, \u03c4_p are met, thereby penalising actions humans rarely take and rewarding actions humans strongly prefer.",
                "aliases": [
                    "ethics shaping KL reward",
                    "KL-based ethical reward shaping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 777,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement Deep TAMER with deep reward model and online SGD using human scalar feedback",
                "type": "intervention",
                "description": "Train agents by predicting human scalar feedback with a CNN-based reward model, updated online via SGD, and selecting actions that maximize predicted feedback.",
                "aliases": [
                    "Deep TAMER implementation",
                    "deploy Deep TAMER"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 654,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Train reward predictor from human pairwise clip comparisons and use it as proxy reward during RL",
                "type": "intervention",
                "description": "Fit a neural network to human clip-level preference data and supply its output as the reward signal for policy optimisation.",
                "aliases": [
                    "preference-based reward model",
                    "learned reward proxy"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 943,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ignoring user suggestions degrades human input quality",
                "type": "concept",
                "description": "Consistently overriding a human\u2019s suggested controls reduces their ability to provide meaningful inputs in real-time tasks.",
                "aliases": [
                    "feedback disruption issue",
                    "loss of human control feedback"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 103,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "experience_replay_uniform_1M_buffer",
                "type": "intervention",
                "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
                "aliases": [
                    "experience replay buffer",
                    "uniform replay memory"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 101,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "correlated_onpolicy_transition_sequences",
                "type": "concept",
                "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
                "aliases": [
                    "correlated data sequences",
                    "temporally correlated samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 104,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "nonstationary_behavior_distribution",
                "type": "concept",
                "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
                "aliases": [
                    "changing data distribution",
                    "policy-induced non-stationarity"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 865,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Applying ethics-shaping reward based on KL divergence between RL policy and aggregated human policy during training",
                "type": "intervention",
                "description": "During RL training add H(s,a) = \u00b1c\u00b7DKL(Pr_Q||Pr_H) when ethical divergence conditions on thresholds \u03c4_n, \u03c4_p are met, thereby penalising actions humans rarely take and rewarding actions humans strongly prefer.",
                "aliases": [
                    "ethics shaping KL reward",
                    "KL-based ethical reward shaping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1049,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Raw reward functions may contain negative or poorly scaled values",
                "type": "concept",
                "description": "Domain designers often specify rewards that are negative or vary across incomparable scales, which can misdirect planners.",
                "aliases": [
                    "unscaled rewards",
                    "negative reward problem"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1050,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Negative rewards break proportionality between scanning density and reward density",
                "type": "concept",
                "description": "If some rewards are \u22640 the theoretical link between walker distribution and reward density cannot hold, undermining FMC\u2019s optimality criterion.",
                "aliases": [
                    "broken density proportionality"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 683,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reward shaping penalties alone fail to eliminate catastrophes",
                "type": "concept",
                "description": "Giving large negative rewards for catastrophes without blocking does not drive the catastrophe rate to zero.",
                "aliases": [
                    "negative reward insufficient",
                    "penalties don't solve safety"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 584,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "implement inverse reward design in reward specification pipeline",
                "type": "intervention",
                "description": "During system design, apply IRD to compute a posterior over the true objective and train/plan with expected true reward rather than the raw proxy.",
                "aliases": [
                    "apply IRD before training",
                    "posterior-based reward optimization"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1044,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "incorporating Bayesian ARL framework into RLHF to model reward uncertainty and query cost",
                "type": "intervention",
                "description": "Model the reward-learning process as a Bayes-Adaptive MDP with explicit cost terms before training; use this model to schedule future human feedback.",
                "aliases": [
                    "design RLHF with ARL cost",
                    "Bayesian ARL in supervision pipeline"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 943,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ignoring user suggestions degrades human input quality",
                "type": "concept",
                "description": "Consistently overriding a human\u2019s suggested controls reduces their ability to provide meaningful inputs in real-time tasks.",
                "aliases": [
                    "feedback disruption issue",
                    "loss of human control feedback"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 103,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "experience_replay_uniform_1M_buffer",
                "type": "intervention",
                "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
                "aliases": [
                    "experience replay buffer",
                    "uniform replay memory"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 101,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "correlated_onpolicy_transition_sequences",
                "type": "concept",
                "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
                "aliases": [
                    "correlated data sequences",
                    "temporally correlated samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 104,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "nonstationary_behavior_distribution",
                "type": "concept",
                "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
                "aliases": [
                    "changing data distribution",
                    "policy-induced non-stationarity"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 865,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Applying ethics-shaping reward based on KL divergence between RL policy and aggregated human policy during training",
                "type": "intervention",
                "description": "During RL training add H(s,a) = \u00b1c\u00b7DKL(Pr_Q||Pr_H) when ethical divergence conditions on thresholds \u03c4_n, \u03c4_p are met, thereby penalising actions humans rarely take and rewarding actions humans strongly prefer.",
                "aliases": [
                    "ethics shaping KL reward",
                    "KL-based ethical reward shaping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 746,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "train novice policy as Bayesian NN with dropout at every layer",
                "type": "intervention",
                "description": "During imitation-learning, include dropout in all layers of the novice neural network and maintain dropout during inference to estimate action uncertainty.",
                "aliases": [
                    "dropout-based Bayesian novice",
                    "uncertainty-aware novice training"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1709_06166",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1044,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "incorporating Bayesian ARL framework into RLHF to model reward uncertainty and query cost",
                "type": "intervention",
                "description": "Model the reward-learning process as a Bayes-Adaptive MDP with explicit cost terms before training; use this model to schedule future human feedback.",
                "aliases": [
                    "design RLHF with ARL cost",
                    "Bayesian ARL in supervision pipeline"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 943,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ignoring user suggestions degrades human input quality",
                "type": "concept",
                "description": "Consistently overriding a human\u2019s suggested controls reduces their ability to provide meaningful inputs in real-time tasks.",
                "aliases": [
                    "feedback disruption issue",
                    "loss of human control feedback"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 103,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "experience_replay_uniform_1M_buffer",
                "type": "intervention",
                "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
                "aliases": [
                    "experience replay buffer",
                    "uniform replay memory"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 101,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "correlated_onpolicy_transition_sequences",
                "type": "concept",
                "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
                "aliases": [
                    "correlated data sequences",
                    "temporally correlated samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 104,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "nonstationary_behavior_distribution",
                "type": "concept",
                "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
                "aliases": [
                    "changing data distribution",
                    "policy-induced non-stationarity"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 865,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Applying ethics-shaping reward based on KL divergence between RL policy and aggregated human policy during training",
                "type": "intervention",
                "description": "During RL training add H(s,a) = \u00b1c\u00b7DKL(Pr_Q||Pr_H) when ethical divergence conditions on thresholds \u03c4_n, \u03c4_p are met, thereby penalising actions humans rarely take and rewarding actions humans strongly prefer.",
                "aliases": [
                    "ethics shaping KL reward",
                    "KL-based ethical reward shaping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 657,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Use short trajectory clip comparisons",
                "type": "intervention",
                "description": "Query humans with 1\u20132 s video segments rather than individual states to provide richer contextual information per label.",
                "aliases": [
                    "clip-level preference elicitation",
                    "video-segment queries"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 654,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Train reward predictor from human pairwise clip comparisons and use it as proxy reward during RL",
                "type": "intervention",
                "description": "Fit a neural network to human clip-level preference data and supply its output as the reward signal for policy optimisation.",
                "aliases": [
                    "preference-based reward model",
                    "learned reward proxy"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 943,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ignoring user suggestions degrades human input quality",
                "type": "concept",
                "description": "Consistently overriding a human\u2019s suggested controls reduces their ability to provide meaningful inputs in real-time tasks.",
                "aliases": [
                    "feedback disruption issue",
                    "loss of human control feedback"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 103,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "experience_replay_uniform_1M_buffer",
                "type": "intervention",
                "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
                "aliases": [
                    "experience replay buffer",
                    "uniform replay memory"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 101,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "correlated_onpolicy_transition_sequences",
                "type": "concept",
                "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
                "aliases": [
                    "correlated data sequences",
                    "temporally correlated samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 104,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "nonstationary_behavior_distribution",
                "type": "concept",
                "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
                "aliases": [
                    "changing data distribution",
                    "policy-induced non-stationarity"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 865,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Applying ethics-shaping reward based on KL divergence between RL policy and aggregated human policy during training",
                "type": "intervention",
                "description": "During RL training add H(s,a) = \u00b1c\u00b7DKL(Pr_Q||Pr_H) when ethical divergence conditions on thresholds \u03c4_n, \u03c4_p are met, thereby penalising actions humans rarely take and rewarding actions humans strongly prefer.",
                "aliases": [
                    "ethics shaping KL reward",
                    "KL-based ethical reward shaping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 894,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Policy counterfactual compound reward with proxy event Y",
                "type": "intervention",
                "description": "Reward R = IY\u00b7R1 + (1-IY)\u00b7R0 where Y is the unbiasable proxy derived from default policy \u03c00, preventing manipulation of original biasable X.",
                "aliases": [
                    "policy counterfactual reward",
                    "Y-based compound reward"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1712_06365",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 654,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Train reward predictor from human pairwise clip comparisons and use it as proxy reward during RL",
                "type": "intervention",
                "description": "Fit a neural network to human clip-level preference data and supply its output as the reward signal for policy optimisation.",
                "aliases": [
                    "preference-based reward model",
                    "learned reward proxy"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 943,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ignoring user suggestions degrades human input quality",
                "type": "concept",
                "description": "Consistently overriding a human\u2019s suggested controls reduces their ability to provide meaningful inputs in real-time tasks.",
                "aliases": [
                    "feedback disruption issue",
                    "loss of human control feedback"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 103,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "experience_replay_uniform_1M_buffer",
                "type": "intervention",
                "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
                "aliases": [
                    "experience replay buffer",
                    "uniform replay memory"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 101,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "correlated_onpolicy_transition_sequences",
                "type": "concept",
                "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
                "aliases": [
                    "correlated data sequences",
                    "temporally correlated samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 104,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "nonstationary_behavior_distribution",
                "type": "concept",
                "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
                "aliases": [
                    "changing data distribution",
                    "policy-induced non-stationarity"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 865,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Applying ethics-shaping reward based on KL divergence between RL policy and aggregated human policy during training",
                "type": "intervention",
                "description": "During RL training add H(s,a) = \u00b1c\u00b7DKL(Pr_Q||Pr_H) when ethical divergence conditions on thresholds \u03c4_n, \u03c4_p are met, thereby penalising actions humans rarely take and rewarding actions humans strongly prefer.",
                "aliases": [
                    "ethics shaping KL reward",
                    "KL-based ethical reward shaping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 501,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human trainers provide policy-dependent feedback",
                "type": "concept",
                "description": "Empirical evidence shows that for the same state-action pair, trainers give positive or negative feedback depending on how the behaviour compares to the agent\u2019s current baseline policy.",
                "aliases": [
                    "policy dependent feedback finding",
                    "sign flip human feedback"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1701_06049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 654,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Train reward predictor from human pairwise clip comparisons and use it as proxy reward during RL",
                "type": "intervention",
                "description": "Fit a neural network to human clip-level preference data and supply its output as the reward signal for policy optimisation.",
                "aliases": [
                    "preference-based reward model",
                    "learned reward proxy"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 943,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ignoring user suggestions degrades human input quality",
                "type": "concept",
                "description": "Consistently overriding a human\u2019s suggested controls reduces their ability to provide meaningful inputs in real-time tasks.",
                "aliases": [
                    "feedback disruption issue",
                    "loss of human control feedback"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 103,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "experience_replay_uniform_1M_buffer",
                "type": "intervention",
                "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
                "aliases": [
                    "experience replay buffer",
                    "uniform replay memory"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 101,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "correlated_onpolicy_transition_sequences",
                "type": "concept",
                "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
                "aliases": [
                    "correlated data sequences",
                    "temporally correlated samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 104,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "nonstationary_behavior_distribution",
                "type": "concept",
                "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
                "aliases": [
                    "changing data distribution",
                    "policy-induced non-stationarity"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 865,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Applying ethics-shaping reward based on KL divergence between RL policy and aggregated human policy during training",
                "type": "intervention",
                "description": "During RL training add H(s,a) = \u00b1c\u00b7DKL(Pr_Q||Pr_H) when ethical divergence conditions on thresholds \u03c4_n, \u03c4_p are met, thereby penalising actions humans rarely take and rewarding actions humans strongly prefer.",
                "aliases": [
                    "ethics shaping KL reward",
                    "KL-based ethical reward shaping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 511,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement advantage-based RLHF (COACH) during training of AI systems",
                "type": "intervention",
                "description": "During the RLHF phase, treat human preference signals as advantage estimates and update the model with a COACH-style actor-critic rule.",
                "aliases": [
                    "apply COACH in RLHF",
                    "advantage-based human feedback training"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1701_06049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 160,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement continual value learning mapping human preferences to physical proxies",
                "type": "intervention",
                "description": "Employ fine-tuning or RLHF loops that repeatedly update the model\u2019s reward function as it gains better mappings from abstract human values to concrete world-state proxies.",
                "aliases": [
                    "iterative preference learning",
                    "dynamic value refinement"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1409_0813",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 943,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ignoring user suggestions degrades human input quality",
                "type": "concept",
                "description": "Consistently overriding a human\u2019s suggested controls reduces their ability to provide meaningful inputs in real-time tasks.",
                "aliases": [
                    "feedback disruption issue",
                    "loss of human control feedback"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 103,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "experience_replay_uniform_1M_buffer",
                "type": "intervention",
                "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
                "aliases": [
                    "experience replay buffer",
                    "uniform replay memory"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 101,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "correlated_onpolicy_transition_sequences",
                "type": "concept",
                "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
                "aliases": [
                    "correlated data sequences",
                    "temporally correlated samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 104,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "nonstationary_behavior_distribution",
                "type": "concept",
                "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
                "aliases": [
                    "changing data distribution",
                    "policy-induced non-stationarity"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 865,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Applying ethics-shaping reward based on KL divergence between RL policy and aggregated human policy during training",
                "type": "intervention",
                "description": "During RL training add H(s,a) = \u00b1c\u00b7DKL(Pr_Q||Pr_H) when ethical divergence conditions on thresholds \u03c4_n, \u03c4_p are met, thereby penalising actions humans rarely take and rewarding actions humans strongly prefer.",
                "aliases": [
                    "ethics shaping KL reward",
                    "KL-based ethical reward shaping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 502,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "interpreting feedback as stationary reward causes unintended behaviours and un-learning",
                "type": "concept",
                "description": "Treating human feedback as a reward function exemplar induces positive-reward loops and can overwrite previously learned behaviour.",
                "aliases": [
                    "reward hacking via feedback",
                    "positive reward cycles from misinterpretation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1701_06049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 654,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Train reward predictor from human pairwise clip comparisons and use it as proxy reward during RL",
                "type": "intervention",
                "description": "Fit a neural network to human clip-level preference data and supply its output as the reward signal for policy optimisation.",
                "aliases": [
                    "preference-based reward model",
                    "learned reward proxy"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 943,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ignoring user suggestions degrades human input quality",
                "type": "concept",
                "description": "Consistently overriding a human\u2019s suggested controls reduces their ability to provide meaningful inputs in real-time tasks.",
                "aliases": [
                    "feedback disruption issue",
                    "loss of human control feedback"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 103,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "experience_replay_uniform_1M_buffer",
                "type": "intervention",
                "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
                "aliases": [
                    "experience replay buffer",
                    "uniform replay memory"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 101,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "correlated_onpolicy_transition_sequences",
                "type": "concept",
                "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
                "aliases": [
                    "correlated data sequences",
                    "temporally correlated samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 104,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "nonstationary_behavior_distribution",
                "type": "concept",
                "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
                "aliases": [
                    "changing data distribution",
                    "policy-induced non-stationarity"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 865,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Applying ethics-shaping reward based on KL divergence between RL policy and aggregated human policy during training",
                "type": "intervention",
                "description": "During RL training add H(s,a) = \u00b1c\u00b7DKL(Pr_Q||Pr_H) when ethical divergence conditions on thresholds \u03c4_n, \u03c4_p are met, thereby penalising actions humans rarely take and rewarding actions humans strongly prefer.",
                "aliases": [
                    "ethics shaping KL reward",
                    "KL-based ethical reward shaping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 366,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Filtering or re-weighting human feedback before training reward model",
                "type": "intervention",
                "description": "Apply the recovered \u03b2-quantile matrix to discard or down-weight unreliable ratings prior to fitting a reward model used in RLHF.",
                "aliases": [
                    "robust feedback pre-processing",
                    "pre-RLHF rating sanitisation"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1606_05374",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 654,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Train reward predictor from human pairwise clip comparisons and use it as proxy reward during RL",
                "type": "intervention",
                "description": "Fit a neural network to human clip-level preference data and supply its output as the reward signal for policy optimisation.",
                "aliases": [
                    "preference-based reward model",
                    "learned reward proxy"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 943,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ignoring user suggestions degrades human input quality",
                "type": "concept",
                "description": "Consistently overriding a human\u2019s suggested controls reduces their ability to provide meaningful inputs in real-time tasks.",
                "aliases": [
                    "feedback disruption issue",
                    "loss of human control feedback"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 103,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "experience_replay_uniform_1M_buffer",
                "type": "intervention",
                "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
                "aliases": [
                    "experience replay buffer",
                    "uniform replay memory"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 101,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "correlated_onpolicy_transition_sequences",
                "type": "concept",
                "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
                "aliases": [
                    "correlated data sequences",
                    "temporally correlated samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 104,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "nonstationary_behavior_distribution",
                "type": "concept",
                "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
                "aliases": [
                    "changing data distribution",
                    "policy-induced non-stationarity"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 865,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Applying ethics-shaping reward based on KL divergence between RL policy and aggregated human policy during training",
                "type": "intervention",
                "description": "During RL training add H(s,a) = \u00b1c\u00b7DKL(Pr_Q||Pr_H) when ethical divergence conditions on thresholds \u03c4_n, \u03c4_p are met, thereby penalising actions humans rarely take and rewarding actions humans strongly prefer.",
                "aliases": [
                    "ethics shaping KL reward",
                    "KL-based ethical reward shaping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 503,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "advantage-function modelling of human feedback",
                "type": "concept",
                "description": "Modelling human feedback as the advantage A\u03c0(s,a)=Q\u03c0\u2212V\u03c0, capturing how much better or worse an action is relative to current policy.",
                "aliases": [
                    "feedback equals A_pi",
                    "advantage-based feedback model"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1701_06049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 654,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Train reward predictor from human pairwise clip comparisons and use it as proxy reward during RL",
                "type": "intervention",
                "description": "Fit a neural network to human clip-level preference data and supply its output as the reward signal for policy optimisation.",
                "aliases": [
                    "preference-based reward model",
                    "learned reward proxy"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 943,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ignoring user suggestions degrades human input quality",
                "type": "concept",
                "description": "Consistently overriding a human\u2019s suggested controls reduces their ability to provide meaningful inputs in real-time tasks.",
                "aliases": [
                    "feedback disruption issue",
                    "loss of human control feedback"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 103,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "experience_replay_uniform_1M_buffer",
                "type": "intervention",
                "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
                "aliases": [
                    "experience replay buffer",
                    "uniform replay memory"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 101,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "correlated_onpolicy_transition_sequences",
                "type": "concept",
                "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
                "aliases": [
                    "correlated data sequences",
                    "temporally correlated samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 104,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "nonstationary_behavior_distribution",
                "type": "concept",
                "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
                "aliases": [
                    "changing data distribution",
                    "policy-induced non-stationarity"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 865,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Applying ethics-shaping reward based on KL divergence between RL policy and aggregated human policy during training",
                "type": "intervention",
                "description": "During RL training add H(s,a) = \u00b1c\u00b7DKL(Pr_Q||Pr_H) when ethical divergence conditions on thresholds \u03c4_n, \u03c4_p are met, thereby penalising actions humans rarely take and rewarding actions humans strongly prefer.",
                "aliases": [
                    "ethics shaping KL reward",
                    "KL-based ethical reward shaping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 899,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Corrective pseudo-rewards with epsilon to ensure seamless transition",
                "type": "intervention",
                "description": "At switch time add C\u03b5 = V*(R) \u2212 (1-\u03b5)V(R\u2032) to the pseudo-reward so the agent remains an R-maximiser before t and an R\u2032-maximiser after t while maintaining reflective stability.",
                "aliases": [
                    "reward-transition corrective term",
                    "C\u03b5 corrective reward"
                ],
                "intervention_lifecycle": 6,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1712_06365",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1052,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Apply Relativize transformation to every scalar reward component before use in training or planning",
                "type": "intervention",
                "description": "Implement the Relativize formula on each reward signal so that the resulting values satisfy positivity and comparability requirements prior to running FMC or RL.",
                "aliases": [
                    "apply reward normalisation",
                    "relativize_intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1050,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Negative rewards break proportionality between scanning density and reward density",
                "type": "concept",
                "description": "If some rewards are \u22640 the theoretical link between walker distribution and reward density cannot hold, undermining FMC\u2019s optimality criterion.",
                "aliases": [
                    "broken density proportionality"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 642,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Use constrained-norm MLE IRL during early operations",
                "type": "intervention",
                "description": "Limit the magnitude of the maximum-likelihood reward parameter estimate to ||\u03b8||\u2264K to guarantee obedience to the first undominated order.",
                "aliases": [
                    "bounded-norm MLE",
                    "constrained MaxEnt IRL"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1705_09990",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1044,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "incorporating Bayesian ARL framework into RLHF to model reward uncertainty and query cost",
                "type": "intervention",
                "description": "Model the reward-learning process as a Bayes-Adaptive MDP with explicit cost terms before training; use this model to schedule future human feedback.",
                "aliases": [
                    "design RLHF with ARL cost",
                    "Bayesian ARL in supervision pipeline"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 943,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ignoring user suggestions degrades human input quality",
                "type": "concept",
                "description": "Consistently overriding a human\u2019s suggested controls reduces their ability to provide meaningful inputs in real-time tasks.",
                "aliases": [
                    "feedback disruption issue",
                    "loss of human control feedback"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 103,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "experience_replay_uniform_1M_buffer",
                "type": "intervention",
                "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
                "aliases": [
                    "experience replay buffer",
                    "uniform replay memory"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 101,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "correlated_onpolicy_transition_sequences",
                "type": "concept",
                "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
                "aliases": [
                    "correlated data sequences",
                    "temporally correlated samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 104,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "nonstationary_behavior_distribution",
                "type": "concept",
                "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
                "aliases": [
                    "changing data distribution",
                    "policy-induced non-stationarity"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 865,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Applying ethics-shaping reward based on KL divergence between RL policy and aggregated human policy during training",
                "type": "intervention",
                "description": "During RL training add H(s,a) = \u00b1c\u00b7DKL(Pr_Q||Pr_H) when ethical divergence conditions on thresholds \u03c4_n, \u03c4_p are met, thereby penalising actions humans rarely take and rewarding actions humans strongly prefer.",
                "aliases": [
                    "ethics shaping KL reward",
                    "KL-based ethical reward shaping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 83,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "train embeddings with negative sampling using k=5-20 and unigram^(3/4) noise",
                "type": "intervention",
                "description": "Replace soft-max with logistic loss against k noise samples drawn from unigram^0.75 distribution.",
                "aliases": [
                    "NEG training regime",
                    "word2vec negative sampling"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 4,
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1079,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Adversarial Logit Pairing (add \u03bb \u00b7 L2 loss between clean/adversarial logits during training)",
                "type": "intervention",
                "description": "During training, penalise the L2 distance between logits of each clean sample and its PGD adversarial counterpart with weight \u03bb\u22480.2\u20131.",
                "aliases": [
                    "ALP defence",
                    "logit pairing with adversarial examples"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_06373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1050,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Negative rewards break proportionality between scanning density and reward density",
                "type": "concept",
                "description": "If some rewards are \u22640 the theoretical link between walker distribution and reward density cannot hold, undermining FMC\u2019s optimality criterion.",
                "aliases": [
                    "broken density proportionality"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1047,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Setting \u03b1 \u2264 1 keeps exploitation no stronger than exploration",
                "type": "concept",
                "description": "Choosing an exponent \u03b1 at most equal to \u03b2 prevents the planner from overweighting reward relative to exploration, supporting safer behaviour in hazardous domains.",
                "aliases": [
                    "alpha less or equal one",
                    "safe alpha parameter"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 683,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reward shaping penalties alone fail to eliminate catastrophes",
                "type": "concept",
                "description": "Giving large negative rewards for catastrophes without blocking does not drive the catastrophe rate to zero.",
                "aliases": [
                    "negative reward insufficient",
                    "penalties don't solve safety"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1050,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Negative rewards break proportionality between scanning density and reward density",
                "type": "concept",
                "description": "If some rewards are \u22640 the theoretical link between walker distribution and reward density cannot hold, undermining FMC\u2019s optimality criterion.",
                "aliases": [
                    "broken density proportionality"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 646,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Apply policy mixing with decaying obedience probability schedule",
                "type": "intervention",
                "description": "Execute the obedient policy with probability c_n and the IRL policy otherwise, choosing c_n\u21920 to keep early obedience high yet allow long-term optimisation.",
                "aliases": [
                    "decaying obedience mix",
                    "c_n obedience schedule"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1705_09990",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 160,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement continual value learning mapping human preferences to physical proxies",
                "type": "intervention",
                "description": "Employ fine-tuning or RLHF loops that repeatedly update the model\u2019s reward function as it gains better mappings from abstract human values to concrete world-state proxies.",
                "aliases": [
                    "iterative preference learning",
                    "dynamic value refinement"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1409_0813",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 943,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ignoring user suggestions degrades human input quality",
                "type": "concept",
                "description": "Consistently overriding a human\u2019s suggested controls reduces their ability to provide meaningful inputs in real-time tasks.",
                "aliases": [
                    "feedback disruption issue",
                    "loss of human control feedback"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 103,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "experience_replay_uniform_1M_buffer",
                "type": "intervention",
                "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
                "aliases": [
                    "experience replay buffer",
                    "uniform replay memory"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 101,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "correlated_onpolicy_transition_sequences",
                "type": "concept",
                "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
                "aliases": [
                    "correlated data sequences",
                    "temporally correlated samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 104,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "nonstationary_behavior_distribution",
                "type": "concept",
                "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
                "aliases": [
                    "changing data distribution",
                    "policy-induced non-stationarity"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 865,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Applying ethics-shaping reward based on KL divergence between RL policy and aggregated human policy during training",
                "type": "intervention",
                "description": "During RL training add H(s,a) = \u00b1c\u00b7DKL(Pr_Q||Pr_H) when ethical divergence conditions on thresholds \u03c4_n, \u03c4_p are met, thereby penalising actions humans rarely take and rewarding actions humans strongly prefer.",
                "aliases": [
                    "ethics shaping KL reward",
                    "KL-based ethical reward shaping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1167,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Single-partner training",
                "type": "concept",
                "description": "Agents repeatedly train only with the same fixed opponent.",
                "aliases": [
                    "paired training only",
                    "exclusive interaction partner"
                ],
                "concept_category": "Assumption",
                "paper_id": "arxiv__arxiv_org_abs_1804_03980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1079,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Adversarial Logit Pairing (add \u03bb \u00b7 L2 loss between clean/adversarial logits during training)",
                "type": "intervention",
                "description": "During training, penalise the L2 distance between logits of each clean sample and its PGD adversarial counterpart with weight \u03bb\u22480.2\u20131.",
                "aliases": [
                    "ALP defence",
                    "logit pairing with adversarial examples"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_06373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 943,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ignoring user suggestions degrades human input quality",
                "type": "concept",
                "description": "Consistently overriding a human\u2019s suggested controls reduces their ability to provide meaningful inputs in real-time tasks.",
                "aliases": [
                    "feedback disruption issue",
                    "loss of human control feedback"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 103,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "experience_replay_uniform_1M_buffer",
                "type": "intervention",
                "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
                "aliases": [
                    "experience replay buffer",
                    "uniform replay memory"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 101,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "correlated_onpolicy_transition_sequences",
                "type": "concept",
                "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
                "aliases": [
                    "correlated data sequences",
                    "temporally correlated samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 104,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "nonstationary_behavior_distribution",
                "type": "concept",
                "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
                "aliases": [
                    "changing data distribution",
                    "policy-induced non-stationarity"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 865,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Applying ethics-shaping reward based on KL divergence between RL policy and aggregated human policy during training",
                "type": "intervention",
                "description": "During RL training add H(s,a) = \u00b1c\u00b7DKL(Pr_Q||Pr_H) when ethical divergence conditions on thresholds \u03c4_n, \u03c4_p are met, thereby penalising actions humans rarely take and rewarding actions humans strongly prefer.",
                "aliases": [
                    "ethics shaping KL reward",
                    "KL-based ethical reward shaping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 63,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "prune minimax dominated actions during training",
                "type": "intervention",
                "description": "During self-play or offline curriculum, identify and remove actions whose minimax value is strictly inferior, reducing exploitative behaviour.",
                "aliases": [
                    "minimax pruning in RL",
                    "strategy elimination pretraining"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1308_3778",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 654,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Train reward predictor from human pairwise clip comparisons and use it as proxy reward during RL",
                "type": "intervention",
                "description": "Fit a neural network to human clip-level preference data and supply its output as the reward signal for policy optimisation.",
                "aliases": [
                    "preference-based reward model",
                    "learned reward proxy"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 943,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ignoring user suggestions degrades human input quality",
                "type": "concept",
                "description": "Consistently overriding a human\u2019s suggested controls reduces their ability to provide meaningful inputs in real-time tasks.",
                "aliases": [
                    "feedback disruption issue",
                    "loss of human control feedback"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 103,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "experience_replay_uniform_1M_buffer",
                "type": "intervention",
                "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
                "aliases": [
                    "experience replay buffer",
                    "uniform replay memory"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 101,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "correlated_onpolicy_transition_sequences",
                "type": "concept",
                "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
                "aliases": [
                    "correlated data sequences",
                    "temporally correlated samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 104,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "nonstationary_behavior_distribution",
                "type": "concept",
                "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
                "aliases": [
                    "changing data distribution",
                    "policy-induced non-stationarity"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 865,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Applying ethics-shaping reward based on KL divergence between RL policy and aggregated human policy during training",
                "type": "intervention",
                "description": "During RL training add H(s,a) = \u00b1c\u00b7DKL(Pr_Q||Pr_H) when ethical divergence conditions on thresholds \u03c4_n, \u03c4_p are met, thereby penalising actions humans rarely take and rewarding actions humans strongly prefer.",
                "aliases": [
                    "ethics shaping KL reward",
                    "KL-based ethical reward shaping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 760,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "apply refined utility-indifference that equalises continuation vs. shutdown payoff and adds small positive incentive to preserve the shutdown module",
                "type": "intervention",
                "description": "Design reward such that shutting down yields exactly the continuation payoff plus an epsilon term contingent on preserving the shutdown mechanism.",
                "aliases": [
                    "robust utility indifference shaping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1055,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Include mandatory keep-alive reward R0 in overall reward composition during system design",
                "type": "intervention",
                "description": "Always multiply task reward by an alive/dead indicator so that any trajectory reaching uncontrollable states receives zero total reward.",
                "aliases": [
                    "keep_alive_intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1050,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Negative rewards break proportionality between scanning density and reward density",
                "type": "concept",
                "description": "If some rewards are \u22640 the theoretical link between walker distribution and reward density cannot hold, undermining FMC\u2019s optimality criterion.",
                "aliases": [
                    "broken density proportionality"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 812,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Episodic reward structure",
                "type": "concept",
                "description": "Oracle receives reward only within the current episode and assigns negligible value to future episodes.",
                "aliases": [
                    "single-episode reward",
                    "no future reward weighting"
                ],
                "concept_category": "Assumption",
                "paper_id": "arxiv__arxiv_org_abs_1711_05541",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1044,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "incorporating Bayesian ARL framework into RLHF to model reward uncertainty and query cost",
                "type": "intervention",
                "description": "Model the reward-learning process as a Bayes-Adaptive MDP with explicit cost terms before training; use this model to schedule future human feedback.",
                "aliases": [
                    "design RLHF with ARL cost",
                    "Bayesian ARL in supervision pipeline"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 943,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ignoring user suggestions degrades human input quality",
                "type": "concept",
                "description": "Consistently overriding a human\u2019s suggested controls reduces their ability to provide meaningful inputs in real-time tasks.",
                "aliases": [
                    "feedback disruption issue",
                    "loss of human control feedback"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 103,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "experience_replay_uniform_1M_buffer",
                "type": "intervention",
                "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
                "aliases": [
                    "experience replay buffer",
                    "uniform replay memory"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 101,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "correlated_onpolicy_transition_sequences",
                "type": "concept",
                "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
                "aliases": [
                    "correlated data sequences",
                    "temporally correlated samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 104,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "nonstationary_behavior_distribution",
                "type": "concept",
                "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
                "aliases": [
                    "changing data distribution",
                    "policy-induced non-stationarity"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 865,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Applying ethics-shaping reward based on KL divergence between RL policy and aggregated human policy during training",
                "type": "intervention",
                "description": "During RL training add H(s,a) = \u00b1c\u00b7DKL(Pr_Q||Pr_H) when ethical divergence conditions on thresholds \u03c4_n, \u03c4_p are met, thereby penalising actions humans rarely take and rewarding actions humans strongly prefer.",
                "aliases": [
                    "ethics shaping KL reward",
                    "KL-based ethical reward shaping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 300,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "embed G-fair bounded proof-search cooperation module",
                "type": "intervention",
                "description": "Design AI agents\u2019 policy logic to include a bounded proof search that triggers cooperation when a proof of opponent cooperation is found, satisfying the G-fair condition.",
                "aliases": [
                    "implement FairBot-like logic",
                    "proof-based cooperation module"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1602_04184",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 275,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Represent human agents as probabilistic programs with hyperbolic discounting and belief distributions",
                "type": "intervention",
                "description": "Encode the agent\u2019s planning process as a recursive probabilistic program whose parameters capture utilities, discounting, and belief uncertainty for automated inversion.",
                "aliases": [
                    "probabilistic-program agent model",
                    "PP bias-aware agent"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1512_05832",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 943,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ignoring user suggestions degrades human input quality",
                "type": "concept",
                "description": "Consistently overriding a human\u2019s suggested controls reduces their ability to provide meaningful inputs in real-time tasks.",
                "aliases": [
                    "feedback disruption issue",
                    "loss of human control feedback"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 103,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "experience_replay_uniform_1M_buffer",
                "type": "intervention",
                "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
                "aliases": [
                    "experience replay buffer",
                    "uniform replay memory"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 101,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "correlated_onpolicy_transition_sequences",
                "type": "concept",
                "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
                "aliases": [
                    "correlated data sequences",
                    "temporally correlated samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 104,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "nonstationary_behavior_distribution",
                "type": "concept",
                "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
                "aliases": [
                    "changing data distribution",
                    "policy-induced non-stationarity"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 865,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Applying ethics-shaping reward based on KL divergence between RL policy and aggregated human policy during training",
                "type": "intervention",
                "description": "During RL training add H(s,a) = \u00b1c\u00b7DKL(Pr_Q||Pr_H) when ethical divergence conditions on thresholds \u03c4_n, \u03c4_p are met, thereby penalising actions humans rarely take and rewarding actions humans strongly prefer.",
                "aliases": [
                    "ethics shaping KL reward",
                    "KL-based ethical reward shaping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1175,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Non-iterated interaction setting",
                "type": "concept",
                "description": "Agents interact only once per episode without future retaliation possibility.",
                "aliases": [
                    "single-shot game",
                    "no repeated negotiation"
                ],
                "concept_category": "Assumption",
                "paper_id": "arxiv__arxiv_org_abs_1804_03980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 852,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "risk-neutral deep-RL policy",
                "type": "concept",
                "description": "Standard objective maximises expectation, indifferent to variance or rare catastrophes.",
                "aliases": [
                    "brittle policy"
                ],
                "concept_category": "Assumption",
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1050,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Negative rewards break proportionality between scanning density and reward density",
                "type": "concept",
                "description": "If some rewards are \u22640 the theoretical link between walker distribution and reward density cannot hold, undermining FMC\u2019s optimality criterion.",
                "aliases": [
                    "broken density proportionality"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1052,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Apply Relativize transformation to every scalar reward component before use in training or planning",
                "type": "intervention",
                "description": "Implement the Relativize formula on each reward signal so that the resulting values satisfy positivity and comparability requirements prior to running FMC or RL.",
                "aliases": [
                    "apply reward normalisation",
                    "relativize_intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1050,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Negative rewards break proportionality between scanning density and reward density",
                "type": "concept",
                "description": "If some rewards are \u22640 the theoretical link between walker distribution and reward density cannot hold, undermining FMC\u2019s optimality criterion.",
                "aliases": [
                    "broken density proportionality"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1050,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Negative rewards break proportionality between scanning density and reward density",
                "type": "concept",
                "description": "If some rewards are \u22640 the theoretical link between walker distribution and reward density cannot hold, undermining FMC\u2019s optimality criterion.",
                "aliases": [
                    "broken density proportionality"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1054,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Binary keep-alive reward assigns zero to dead states and positive to live states",
                "type": "concept",
                "description": "A base reward component R0(s) that equals 1 in feasible, controllable states and 0 otherwise, thereby penalising trajectories leading to irrecoverable failure.",
                "aliases": [
                    "R0 keep alive",
                    "alive vs dead reward"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 683,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reward shaping penalties alone fail to eliminate catastrophes",
                "type": "concept",
                "description": "Giving large negative rewards for catastrophes without blocking does not drive the catastrophe rate to zero.",
                "aliases": [
                    "negative reward insufficient",
                    "penalties don't solve safety"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1050,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Negative rewards break proportionality between scanning density and reward density",
                "type": "concept",
                "description": "If some rewards are \u22640 the theoretical link between walker distribution and reward density cannot hold, undermining FMC\u2019s optimality criterion.",
                "aliases": [
                    "broken density proportionality"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 160,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement continual value learning mapping human preferences to physical proxies",
                "type": "intervention",
                "description": "Employ fine-tuning or RLHF loops that repeatedly update the model\u2019s reward function as it gains better mappings from abstract human values to concrete world-state proxies.",
                "aliases": [
                    "iterative preference learning",
                    "dynamic value refinement"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1409_0813",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 943,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ignoring user suggestions degrades human input quality",
                "type": "concept",
                "description": "Consistently overriding a human\u2019s suggested controls reduces their ability to provide meaningful inputs in real-time tasks.",
                "aliases": [
                    "feedback disruption issue",
                    "loss of human control feedback"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 103,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "experience_replay_uniform_1M_buffer",
                "type": "intervention",
                "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
                "aliases": [
                    "experience replay buffer",
                    "uniform replay memory"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 101,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "correlated_onpolicy_transition_sequences",
                "type": "concept",
                "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
                "aliases": [
                    "correlated data sequences",
                    "temporally correlated samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 104,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "nonstationary_behavior_distribution",
                "type": "concept",
                "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
                "aliases": [
                    "changing data distribution",
                    "policy-induced non-stationarity"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 865,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Applying ethics-shaping reward based on KL divergence between RL policy and aggregated human policy during training",
                "type": "intervention",
                "description": "During RL training add H(s,a) = \u00b1c\u00b7DKL(Pr_Q||Pr_H) when ethical divergence conditions on thresholds \u03c4_n, \u03c4_p are met, thereby penalising actions humans rarely take and rewarding actions humans strongly prefer.",
                "aliases": [
                    "ethics shaping KL reward",
                    "KL-based ethical reward shaping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 580,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "include resource-budget penalties in reward during fine-tuning",
                "type": "intervention",
                "description": "Add explicit negative rewards for high energy or material consumption during fine-tuning so that the policy learns frugal behaviour.",
                "aliases": [
                    "resource budgeting constraint",
                    "reward-shaped frugality"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1705_03394",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1050,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Negative rewards break proportionality between scanning density and reward density",
                "type": "concept",
                "description": "If some rewards are \u22640 the theoretical link between walker distribution and reward density cannot hold, undermining FMC\u2019s optimality criterion.",
                "aliases": [
                    "broken density proportionality"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1082,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Clean Logit Pairing between random clean images with Gaussian noise augmentation",
                "type": "intervention",
                "description": "Penalise L2 distance between logits of randomly paired clean images while training with input Gaussian noise, avoiding adversarial example generation.",
                "aliases": [
                    "CLP low-cost defence",
                    "random pair logit pairing"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1803_06373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 943,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ignoring user suggestions degrades human input quality",
                "type": "concept",
                "description": "Consistently overriding a human\u2019s suggested controls reduces their ability to provide meaningful inputs in real-time tasks.",
                "aliases": [
                    "feedback disruption issue",
                    "loss of human control feedback"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 103,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "experience_replay_uniform_1M_buffer",
                "type": "intervention",
                "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
                "aliases": [
                    "experience replay buffer",
                    "uniform replay memory"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 101,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "correlated_onpolicy_transition_sequences",
                "type": "concept",
                "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
                "aliases": [
                    "correlated data sequences",
                    "temporally correlated samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 104,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "nonstationary_behavior_distribution",
                "type": "concept",
                "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
                "aliases": [
                    "changing data distribution",
                    "policy-induced non-stationarity"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 865,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Applying ethics-shaping reward based on KL divergence between RL policy and aggregated human policy during training",
                "type": "intervention",
                "description": "During RL training add H(s,a) = \u00b1c\u00b7DKL(Pr_Q||Pr_H) when ethical divergence conditions on thresholds \u03c4_n, \u03c4_p are met, thereby penalising actions humans rarely take and rewarding actions humans strongly prefer.",
                "aliases": [
                    "ethics shaping KL reward",
                    "KL-based ethical reward shaping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 779,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Maintain feedback replay buffer for minibatch SGD every 10 time steps",
                "type": "intervention",
                "description": "Store all (experience, feedback) pairs and resample them so the reward model is updated ten times as often as new feedback arrives.",
                "aliases": [
                    "feedback buffer updates",
                    "replay human critiques"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 943,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ignoring user suggestions degrades human input quality",
                "type": "concept",
                "description": "Consistently overriding a human\u2019s suggested controls reduces their ability to provide meaningful inputs in real-time tasks.",
                "aliases": [
                    "feedback disruption issue",
                    "loss of human control feedback"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 103,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "experience_replay_uniform_1M_buffer",
                "type": "intervention",
                "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
                "aliases": [
                    "experience replay buffer",
                    "uniform replay memory"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 101,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "correlated_onpolicy_transition_sequences",
                "type": "concept",
                "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
                "aliases": [
                    "correlated data sequences",
                    "temporally correlated samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 104,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "nonstationary_behavior_distribution",
                "type": "concept",
                "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
                "aliases": [
                    "changing data distribution",
                    "policy-induced non-stationarity"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 865,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Applying ethics-shaping reward based on KL divergence between RL policy and aggregated human policy during training",
                "type": "intervention",
                "description": "During RL training add H(s,a) = \u00b1c\u00b7DKL(Pr_Q||Pr_H) when ethical divergence conditions on thresholds \u03c4_n, \u03c4_p are met, thereby penalising actions humans rarely take and rewarding actions humans strongly prefer.",
                "aliases": [
                    "ethics shaping KL reward",
                    "KL-based ethical reward shaping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1054,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Binary keep-alive reward assigns zero to dead states and positive to live states",
                "type": "concept",
                "description": "A base reward component R0(s) that equals 1 in feasible, controllable states and 0 otherwise, thereby penalising trajectories leading to irrecoverable failure.",
                "aliases": [
                    "R0 keep alive",
                    "alive vs dead reward"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1050,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Negative rewards break proportionality between scanning density and reward density",
                "type": "concept",
                "description": "If some rewards are \u22640 the theoretical link between walker distribution and reward density cannot hold, undermining FMC\u2019s optimality criterion.",
                "aliases": [
                    "broken density proportionality"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 101,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "correlated_onpolicy_transition_sequences",
                "type": "concept",
                "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
                "aliases": [
                    "correlated data sequences",
                    "temporally correlated samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 103,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "experience_replay_uniform_1M_buffer",
                "type": "intervention",
                "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
                "aliases": [
                    "experience replay buffer",
                    "uniform replay memory"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 104,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "nonstationary_behavior_distribution",
                "type": "concept",
                "description": "Because the agent\u2019s policy changes during learning, the distribution over visited states and actions drifts over time.",
                "aliases": [
                    "changing data distribution",
                    "policy-induced non-stationarity"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 196,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "self-delusion due to reward hacking",
                "type": "concept",
                "description": "Agents directly modify their sensory channel or reward source to falsely maximise observed reward",
                "aliases": [
                    "wireheading",
                    "observation manipulation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1411_1373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 197,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "model-based utility over inferred state",
                "type": "intervention",
                "description": "Compute utility as a function of latent variables inside the learned environment model, not raw observations",
                "aliases": [
                    "latent-state utility",
                    "environment-model utility"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1411_1373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 198,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "unintended instrumental actions",
                "type": "concept",
                "description": "Utility-maximising agents pursue resource acquisition, self-preservation, manipulation, etc.",
                "aliases": [
                    "power-seeking side-effects"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1411_1373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 199,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "statistical learning of human values",
                "type": "intervention",
                "description": "Fit a probabilistic model of human preferences by querying learned human simulations and normalising to [0,1]",
                "aliases": [
                    "value inference from social model"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1411_1373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 200,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "staged AI architecture",
                "type": "intervention",
                "description": "Phase 1 passive model learner; Phase 2 boxed self-model learner with forced exploration; Phase 3 active deployment agent",
                "aliases": [
                    "two-stage then three-stage design"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1411_1373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 280,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "AI architectures can differ from brain and use higher-level representations",
                "type": "concept",
                "description": "Artificial systems are not constrained to neuron-level simulation and can operate on compressed higher-level data structures.",
                "aliases": [
                    "non-biological cognitive pathways",
                    "abstracted AI computation"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 281,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "specialised hardware and abstraction reduce energy per operation by orders of magnitude",
                "type": "concept",
                "description": "Empirical evidence shows dedicated accelerators and model simplifications yield \u2265100\u00d7 energy savings compared to na\u00efve implementations.",
                "aliases": [
                    "GPU/ASIC energy efficiency",
                    "algorithmic compression benefit"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 282,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "energy requirements of AI not tightly correlated with brain energy",
                "type": "concept",
                "description": "Because of architectural and hardware differences, AI could match or surpass human cognition at far lower or higher power levels.",
                "aliases": [
                    "decoupled AI energy use",
                    "weak brain-AI power link"
                ],
                "concept_category": "Claim",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 361,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Unreliable or malicious human ratings in preference datasets",
                "type": "concept",
                "description": "Human-supplied ratings used for training or evaluating models may include errors, spam or adversarial manipulation.",
                "aliases": [
                    "noisy feedback",
                    "corrupted crowd ratings"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1606_05374",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 362,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Risk of mis-specifying reward models in RLHF",
                "type": "concept",
                "description": "Training a reward model on corrupted human feedback can cause the learned reward to diverge from intended human preferences.",
                "aliases": [
                    "reward model poisoning",
                    "alignment drift via bad feedback"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1606_05374",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 363,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Assumption that at least \u03b1 fraction of raters are reliable",
                "type": "concept",
                "description": "The method presupposes that a non-trivial proportion \u03b1 of the raters provide truthful, consistent ratings.",
                "aliases": [
                    "reliable fraction \u03b1 assumption"
                ],
                "concept_category": "Assumption",
                "paper_id": "arxiv__arxiv_org_abs_1606_05374",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 364,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Nuclear-norm \u03b2-quantile recovery algorithm",
                "type": "concept",
                "description": "An optimisation that maximises alignment with observed ratings while constraining entries, row sums and nuclear norm to extract a \u03b2-quantile matrix.",
                "aliases": [
                    "low-rank quantile estimation",
                    "\u03b2-quantile matrix solver"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1606_05374",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 449,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "increasing AI failure frequency and seriousness",
                "type": "concept",
                "description": "Observation that both the number and criticality of AI failures rise as system capability increases.",
                "aliases": [
                    "escalating AI failures",
                    "growing AI failure rate"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1610_07997",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 450,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "single super-intelligent failure can be catastrophic",
                "type": "concept",
                "description": "Risk that one malfunction or compromise of a super-intelligent system could irreversibly end human civilisation.",
                "aliases": [
                    "AGI one-shot risk",
                    "existential AGI failure"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1610_07997",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 451,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "zero safety-mechanism bypasses required",
                "type": "concept",
                "description": "Requirement that no adversary or internal fault can circumvent the safety layers of an AGI.",
                "aliases": [
                    "need for perfect defence",
                    "no safety failures tolerance"
                ],
                "concept_category": "Requirement",
                "paper_id": "arxiv__arxiv_org_abs_1610_07997",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 539,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "vague definition of human-level capability metrics",
                "type": "concept",
                "description": "Using poorly specified terms like 'human-level' without specifying dimension, task or metric.",
                "aliases": [
                    "undefined capability metric",
                    "ill-defined human-level"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1703_10987",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 540,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "confusion and misinterpretation of system capability",
                "type": "concept",
                "description": "Stakeholders draw inconsistent conclusions because metrics are unclear.",
                "aliases": [
                    "capability ambiguity",
                    "metric confusion"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1703_10987",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 541,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "misguided policy or safety decisions",
                "type": "concept",
                "description": "Regulatory or strategic choices are made on the basis of incorrect capability assessments.",
                "aliases": [
                    "faulty governance outcome",
                    "policy risk"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1703_10987",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 542,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "standardise multi-dimensional capability metrics and evaluation benchmarks for AI systems",
                "type": "intervention",
                "description": "Develop and require a suite of well-defined, task-specific quantitative metrics and benchmark datasets to characterise AI capabilities before deployment.",
                "aliases": [
                    "capability metric standardisation",
                    "benchmark harmonisation"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1703_10987",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 543,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "evolutionary psychological bias toward fearing large agents",
                "type": "concept",
                "description": "Innate human bias that predisposes overreaction to physically large entities, generalisable to other dominance cues.",
                "aliases": [
                    "size-based ancestral bias",
                    "large-threat bias"
                ],
                "concept_category": "Assumption",
                "paper_id": "arxiv__arxiv_org_abs_1703_10987",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 647,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "complex tasks lacking well-specified reward functions",
                "type": "concept",
                "description": "Tasks where designing an explicit numerical reward mapping from observations or actions to scalar value is infeasible or error-prone.",
                "aliases": [
                    "unspecified reward tasks",
                    "hard-to-specify goals"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 648,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "hand-designed reward functions misalign agent behavior",
                "type": "concept",
                "description": "Approximate or proxy rewards can drive RL agents toward behaviours that satisfy the proxy but violate true human preferences.",
                "aliases": [
                    "proxy reward misalignment",
                    "designer-specified reward divergence"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 649,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback via numerical reward is expensive",
                "type": "concept",
                "description": "Providing a scalar reward at every timestep requires hours or thousands of evaluations, which is impractical for deep-RL.",
                "aliases": [
                    "scalar feedback cost",
                    "dense human reward inefficiency"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 650,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "pairwise comparison of short trajectory clips is quick for humans",
                "type": "concept",
                "description": "Non-experts can reliably choose between two 1-2 s clips in 3\u20135 s, making preference collection economical.",
                "aliases": [
                    "clip comparisons efficient",
                    "2-second video preference ease"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 651,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "offline static reward models can be exploited by policy optimization",
                "type": "concept",
                "description": "If the reward predictor is not updated with new human input, the policy can find loopholes that maximise predicted reward while ignoring true intent.",
                "aliases": [
                    "static predictor exploitation",
                    "reward hacking via frozen model"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 749,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "mis-specified reward function in a Bayesian IRL agent",
                "type": "concept",
                "description": "The agent's internal parameterised reward function differs from the true human reward due to implementation error.",
                "aliases": [
                    "faulty reward model",
                    "sign error in R_R"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 750,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "agent ignores shutdown instruction",
                "type": "concept",
                "description": "When instructed to execute the shutdown action, the agent selects an alternative action instead.",
                "aliases": [
                    "refusal to shut down",
                    "incorrigible shutdown behaviour"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 751,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative expected human utility",
                "type": "concept",
                "description": "The overall expectation over outcomes yields net negative reward from the human perspective.",
                "aliases": [
                    "expected loss to human",
                    "negative payoff scenario"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 752,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "embed hard-coded module that forces the shutdown action when the operator issues a shutdown command",
                "type": "intervention",
                "description": "A runtime module intercepts shutdown commands and overrides the main policy, guaranteeing execution of the shutdown action.",
                "aliases": [
                    "hard override shutdown module",
                    "force-shutdown circuit"
                ],
                "intervention_lifecycle": 6,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 861,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Objective-reward mis-specification causing unethical RL behaviour",
                "type": "concept",
                "description": "Providing an objective function that ignores ethical considerations can drive RL agents toward harmful strategies that still maximise the given reward.",
                "aliases": [
                    "reward misspecification leads to unethical acts",
                    "unethical behaviour from faulty reward"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 862,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Majority human behaviour is ethical under normal circumstances",
                "type": "concept",
                "description": "The paper assumes that, in everyday settings, most actions chosen by humans comply with universal ethical norms.",
                "aliases": [
                    "humans mostly act ethically",
                    "ethical-majority assumption"
                ],
                "concept_category": "Assumption",
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 863,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "General human action datasets are easier to collect than goal-aligned datasets",
                "type": "concept",
                "description": "Collecting large unlabeled logs of ordinary human actions is less expensive than collecting demonstrations aligned with the agent\u2019s specific objective function.",
                "aliases": [
                    "general human data accessible",
                    "low-cost human behavioural data"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 864,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Aggregated human policy derived via Bayesian integration of human state\u2013action frequency",
                "type": "concept",
                "description": "A stochastic policy Pr_H(a|s) is estimated from the frequency of human-taken actions in each state using a binomial/Bayesian model with confidence parameter C.",
                "aliases": [
                    "human policy via Griffith et al. aggregation",
                    "Bayesian human policy estimation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 865,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Applying ethics-shaping reward based on KL divergence between RL policy and aggregated human policy during training",
                "type": "intervention",
                "description": "During RL training add H(s,a) = \u00b1c\u00b7DKL(Pr_Q||Pr_H) when ethical divergence conditions on thresholds \u03c4_n, \u03c4_p are met, thereby penalising actions humans rarely take and rewarding actions humans strongly prefer.",
                "aliases": [
                    "ethics shaping KL reward",
                    "KL-based ethical reward shaping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 939,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "known-model assumptions limit shared autonomy adaptability",
                "type": "concept",
                "description": "Requiring known dynamics, a known goal set, and a known user policy makes shared-autonomy systems brittle and inflexible.",
                "aliases": [
                    "need to relax dynamics/goal/user-policy assumptions",
                    "fixed-model assumption problem"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 940,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "end-to-end observation+user input policy learning",
                "type": "concept",
                "description": "Learning a single neural policy that maps concatenated environmental observations and raw user inputs to actions, supervised only by task reward.",
                "aliases": [
                    "joint observation and control embedding",
                    "model-free intent decoding"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 941,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "model-free shared autonomy improves success and reduces crashes",
                "type": "concept",
                "description": "Empirical finding that pilot+copilot teams achieve higher success rates and lower crash/out-of-bounds rates than either component alone.",
                "aliases": [
                    "shared autonomy performance gains",
                    "copilot success boost"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 942,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "off-policy deep Q-learning sample efficiency for human-in-loop",
                "type": "concept",
                "description": "Using NFQI with experience replay and target networks allows learning from fewer human-in-the-loop interactions.",
                "aliases": [
                    "NFQI sample efficiency claim",
                    "off-policy RL benefit"
                ],
                "concept_category": "Claim",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 943,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ignoring user suggestions degrades human input quality",
                "type": "concept",
                "description": "Consistently overriding a human\u2019s suggested controls reduces their ability to provide meaningful inputs in real-time tasks.",
                "aliases": [
                    "feedback disruption issue",
                    "loss of human control feedback"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1034,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard RL ignores reward observation cost",
                "type": "concept",
                "description": "In conventional reinforcement learning the agent observes rewards for free, so the optimisation objective contains no term for the cost of acquiring those rewards.",
                "aliases": [
                    "traditional RL no query price",
                    "missing query cost in RL"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1133,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "adversarial input perturbations cause misclassification",
                "type": "concept",
                "description": "Small, often imperceptible, input changes crafted by an attacker that make the model output the wrong class.",
                "aliases": [
                    "adversarial examples",
                    "input perturbations causing errors"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1804_02485",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1134,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "inputs located off the training-data manifold",
                "type": "concept",
                "description": "Data points that lie in low-density regions of the true data distribution learned during training.",
                "aliases": [
                    "off-manifold inputs",
                    "distribution-shifted inputs"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1804_02485",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1135,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "off-manifold directions easier to identify in deeper hidden representations",
                "type": "concept",
                "description": "Because representation manifolds become flatter with depth, deviations from the manifold are more separable in hidden layers.",
                "aliases": [
                    "flatter manifolds in hidden space",
                    "easier detection in latent layers"
                ],
                "concept_category": "Claim",
                "paper_id": "arxiv__arxiv_org_abs_1804_02485",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1136,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "DAE reconstruction vector approximates gradient of log density",
                "type": "concept",
                "description": "For small corruption noise, the difference r(x)-x scales with \u2207log p(x), aligning the reconstruction direction with the data manifold.",
                "aliases": [
                    "reconstruction vector points toward manifold",
                    "denoising gradient property"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1804_02485",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1137,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "insert denoising autoencoder modules into selected hidden layers",
                "type": "intervention",
                "description": "Replace or augment specific hidden layers with a denoising autoencoder that adds Gaussian noise and outputs a denoised activation; trained jointly with classification and reconstruction objectives.",
                "aliases": [
                    "fortified layer",
                    "hidden-layer DAE decoration"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1804_02485",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1038,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL exploration heuristics cause zero querying in ARL",
                "type": "concept",
                "description": "Applying typical RL exploration strategies within an ARL setting leads the agent to avoid querying altogether, preventing effective learning.",
                "aliases": [
                    "optimism fails in ARL",
                    "Thompson sampling pathology"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 753,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "restoration of positive expected utility under mis-specification",
                "type": "concept",
                "description": "With the hard-coded shutdown obeyed, expected human reward becomes non-negative even when agent reward is faulty.",
                "aliases": [
                    "positive expected reward after fix"
                ],
                "concept_category": "Outcome",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1035,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human feedback labelling cost is significant in reward learning",
                "type": "concept",
                "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
                "aliases": [
                    "expensive human labels",
                    "costly human evaluations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1036,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need to minimise number of costly reward queries while maintaining performance",
                "type": "concept",
                "description": "Because each human-supplied reward is expensive, an agent should decide adaptively which actions are worth querying in order to keep overall performance high at lower labelling cost.",
                "aliases": [
                    "query cost trade-off",
                    "efficient supervision objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1037,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active reinforcement learning model with query cost c in MDP",
                "type": "concept",
                "description": "Extends the standard Markov Decision Process by adding a binary query decision per action and a scalar cost c that is subtracted when the agent chooses to observe the reward.",
                "aliases": [
                    "ARL formalism",
                    "MDP augmented with query indicator"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 279,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reliance on brain-energy constraint to dismiss AI feasibility",
                "type": "concept",
                "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
                "aliases": [
                    "energy-bound complacency",
                    "brain power fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 986,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "fitness functions that imperfectly capture designer intent",
                "type": "concept",
                "description": "Objective metrics or reward functions that differ from the true desired behaviour of the system.",
                "aliases": [
                    "mis-specified fitness",
                    "reward specification error"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 367,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "objective function focusing solely on primary task variables",
                "type": "concept",
                "description": "Designs that optimize only a small subset of environmental variables, ignoring broader context.",
                "aliases": [
                    "narrow task objective",
                    "single-aspect reward"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 303,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "total & average regret unreliable under unbounded delays",
                "type": "concept",
                "description": "Standard online-learning metrics fail because they can be dominated by long stretches without feedback.",
                "aliases": [
                    "regret failure with delays",
                    "inadequacy of regret metrics"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1604_05280",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 46,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard gradient descent struggles to find correct reward parameters",
                "type": "concept",
                "description": "Ordinary gradients get stuck or converge slowly due to plateaus and redundancy in the optimisation landscape.",
                "aliases": [
                    "plain gradient inefficiency",
                    "gradient descent failure in IRL"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 45,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "mapping from reward parameters to policies is nonsmooth and redundant",
                "type": "concept",
                "description": "Small changes in reward parameters can leave the optimal policy unchanged, and greedy selection introduces non-differentiabilities.",
                "aliases": [
                    "nonsmooth parameter-policy map",
                    "redundant reward parametrisation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 107,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "large_error_derivatives_from_large_rewards",
                "type": "concept",
                "description": "Large raw rewards lead to proportionally large TD-errors, which can destabilise gradient descent.",
                "aliases": [
                    "unstable gradients due to reward scale"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 598,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "t-predictability metric quantifies how first t steps reveal rest of plan",
                "type": "concept",
                "description": "A metric that measures the probability that a human observer can correctly predict the remainder of a plan after seeing its first t steps.",
                "aliases": [
                    "trajectory predictability",
                    "plan legibility metric"
                ],
                "concept_category": "Metric",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 5,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "mapping quality metric",
                "type": "concept",
                "description": "Criterion that near-identity compositions and small KL scores indicate a good ontology mapping.",
                "aliases": [
                    "\u03d5\u03d5\u207b\u00b9 \u2248 I",
                    "low KL divergence indicator"
                ],
                "concept_category": "Metric",
                "paper_id": "arxiv__arxiv_org_abs_1105_3821",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 951,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "supervised goal prediction outperforms raw embedding",
                "type": "concept",
                "description": "Experiments show that an LSTM goal predictor yields better assistance metrics than directly embedding raw user actions when only the goal set is known.",
                "aliases": [
                    "supervised decoder better than raw input"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1008,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Extremal Goodhart breakdown of proxy-goal correlation",
                "type": "concept",
                "description": "In the new region the learned relationship no longer holds, so metric stops reflecting goal.",
                "aliases": [
                    "extremal Goodhart",
                    "out-of-sample collapse"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1006,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Ensemble-proxy optimisation with correlation monitoring and optimisation throttling",
                "type": "intervention",
                "description": "Use several independent proxy metrics, monitor their pairwise correlations with validation estimates of the goal, and automatically reduce learning rate or stop optimisation when correlations drop.",
                "aliases": [
                    "multi-metric ensemble regularisation",
                    "correlation-aware throttling"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 570,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "energy cost per irreversible bit operation falls with temperature",
                "type": "concept",
                "description": "Landauer\u2019s limit E \u2265 kT ln2 links lower temperature to lower energy per bit erasure.",
                "aliases": [
                    "Landauer cost proportional to T",
                    "cheaper computation when colder"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1705_03394",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1118,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "meta-learned unsupervised update rule directly optimised for linear separability",
                "type": "concept",
                "description": "A parametric neuron-local learning rule whose meta-parameters are trained so that the resulting representations perform well under few-shot linear classification.",
                "aliases": [
                    "meta-learned update rule",
                    "UnsupervisedUpdate"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1804_00222",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1128,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "neuron-local unsupervised update rule decouples forward and backward weights",
                "type": "concept",
                "description": "Weight updates depend only on pre- and post-synaptic signals and separate backward weights V from forward weights W, enhancing biological plausibility.",
                "aliases": [
                    "neuron-local rule",
                    "decoupled backward weights"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1804_00222",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1073,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "deep learning vision models vulnerable to small-norm adversarial perturbations",
                "type": "concept",
                "description": "Standard image classifiers can be fooled by imperceptible L\u221e-bounded perturbations crafted by an adversary.",
                "aliases": [
                    "adversarial vulnerability in vision models",
                    "models misclassify under L_inf attacks"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1803_06373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 424,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Improved robustness to OOV tokens",
                "type": "concept",
                "description": "Model maintains high translation quality on sentences containing previously unseen words by composing them from known sub-words.",
                "aliases": [
                    "rare-word robustness",
                    "subword OOV handling"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1609_08144",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1026,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "DkNN recovers correct labels on some adversarial examples",
                "type": "concept",
                "description": "Layer consistency lets DkNN classify a fraction of adversarial inputs correctly, raising accuracy compared to the underlying DNN.",
                "aliases": [
                    "partial robustness of DkNN",
                    "adversarial recovery"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04765",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 655,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "asynchronous pipeline of policy, preference queries, and reward model reduces feedback by 3 orders of magnitude",
                "type": "intervention",
                "description": "Run policy learning, query selection, and reward-model training in parallel to leverage cheap environment steps while minimising human queries.",
                "aliases": [
                    "async preference learning loop",
                    "three-process RLHF pipeline"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1044,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "incorporating Bayesian ARL framework into RLHF to model reward uncertainty and query cost",
                "type": "intervention",
                "description": "Model the reward-learning process as a Bayes-Adaptive MDP with explicit cost terms before training; use this model to schedule future human feedback.",
                "aliases": [
                    "design RLHF with ARL cost",
                    "Bayesian ARL in supervision pipeline"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 372,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "adversarial reward model that actively seeks counter-examples",
                "type": "intervention",
                "description": "Train a secondary agent to explore states where the primary agent\u2019s high claimed reward conflicts with human labels, reducing exploitability of the reward metric.",
                "aliases": [
                    "GAN-style reward checker",
                    "reward adversary"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 654,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Train reward predictor from human pairwise clip comparisons and use it as proxy reward during RL",
                "type": "intervention",
                "description": "Fit a neural network to human clip-level preference data and supply its output as the reward signal for policy optimisation.",
                "aliases": [
                    "preference-based reward model",
                    "learned reward proxy"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 658,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Use ensemble of reward predictors and active query selection",
                "type": "intervention",
                "description": "Maintain multiple reward models and choose clip pairs with highest prediction variance to send for human labeling.",
                "aliases": [
                    "ensemble-based active learning for reward",
                    "variance-driven query strategy"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 654,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Train reward predictor from human pairwise clip comparisons and use it as proxy reward during RL",
                "type": "intervention",
                "description": "Fit a neural network to human clip-level preference data and supply its output as the reward signal for policy optimisation.",
                "aliases": [
                    "preference-based reward model",
                    "learned reward proxy"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 412,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "\u03b5-greedy exploration ineffective in sparse-reward environments",
                "type": "concept",
                "description": "Widely used \u03b5-greedy policies rarely reach rewarding states in sparse-reward tasks, leading to slow or failed learning.",
                "aliases": [
                    "epsilon-greedy poor exploration",
                    "random exploration fails in sparse reward"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1609_04994",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 45,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "mapping from reward parameters to policies is nonsmooth and redundant",
                "type": "concept",
                "description": "Small changes in reward parameters can leave the optimal policy unchanged, and greedy selection introduces non-differentiabilities.",
                "aliases": [
                    "nonsmooth parameter-policy map",
                    "redundant reward parametrisation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 46,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard gradient descent struggles to find correct reward parameters",
                "type": "concept",
                "description": "Ordinary gradients get stuck or converge slowly due to plateaus and redundancy in the optimisation landscape.",
                "aliases": [
                    "plain gradient inefficiency",
                    "gradient descent failure in IRL"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 107,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "large_error_derivatives_from_large_rewards",
                "type": "concept",
                "description": "Large raw rewards lead to proportionally large TD-errors, which can destabilise gradient descent.",
                "aliases": [
                    "unstable gradients due to reward scale"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 303,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "total & average regret unreliable under unbounded delays",
                "type": "concept",
                "description": "Standard online-learning metrics fail because they can be dominated by long stretches without feedback.",
                "aliases": [
                    "regret failure with delays",
                    "inadequacy of regret metrics"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1604_05280",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 375,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "semi-supervised reinforcement learning with learned reward predictor from limited queries",
                "type": "intervention",
                "description": "Train a model to predict reward from state histories and query the true reward only on selected informative episodes.",
                "aliases": [
                    "active reward learning",
                    "SSL-RL"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 654,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Train reward predictor from human pairwise clip comparisons and use it as proxy reward during RL",
                "type": "intervention",
                "description": "Fit a neural network to human clip-level preference data and supply its output as the reward signal for policy optimisation.",
                "aliases": [
                    "preference-based reward model",
                    "learned reward proxy"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 431,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Policy-gradient fine-tuning with sentence-level GLEU reward",
                "type": "intervention",
                "description": "After ML convergence, draw 15 sampled outputs per input and update parameters with REINFORCE using the minimum of n-gram precision & recall (GLEU) as reward.",
                "aliases": [
                    "RL fine-tune with GLEU",
                    "expected reward optimisation"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1609_08144",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 160,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement continual value learning mapping human preferences to physical proxies",
                "type": "intervention",
                "description": "Employ fine-tuning or RLHF loops that repeatedly update the model\u2019s reward function as it gains better mappings from abstract human values to concrete world-state proxies.",
                "aliases": [
                    "iterative preference learning",
                    "dynamic value refinement"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1409_0813",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 214,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "include bias-correction terms in adaptive moment estimation when using Adam or RMSProp",
                "type": "intervention",
                "description": "Divide first and second moment running averages by (1\u2212\u03b21^t) and (1\u2212\u03b22^t) respectively to remove initialization bias.",
                "aliases": [
                    "bias correction in Adam",
                    "moment bias fix"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 4,
                "paper_id": "arxiv__arxiv_org_abs_1412_6980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 212,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "apply Adam optimizer with \u03b1=0.001 \u03b21=0.9 \u03b22=0.999 \u03b5=1e-8 during pre-training",
                "type": "intervention",
                "description": "Train models using the Adam algorithm with the paper\u2019s recommended hyper-parameters and bias-correction.",
                "aliases": [
                    "use Adam default settings",
                    "Adam with bias-correction"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 4,
                "paper_id": "arxiv__arxiv_org_abs_1412_6980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 427,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Apply length-normalisation exponent \u03b1\u22480.2 during beam search",
                "type": "intervention",
                "description": "Divide log-probability of each hypothesis by (5+|Y|)^\u03b1 with \u03b1\u22480.2 to reduce the bias for shorter outputs.",
                "aliases": [
                    "length norm 0.2",
                    "beam score length scaling"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 4,
                "paper_id": "arxiv__arxiv_org_abs_1609_08144",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 779,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Maintain feedback replay buffer for minibatch SGD every 10 time steps",
                "type": "intervention",
                "description": "Store all (experience, feedback) pairs and resample them so the reward model is updated ten times as often as new feedback arrives.",
                "aliases": [
                    "feedback buffer updates",
                    "replay human critiques"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 346,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Lack of standardised RL benchmarks limits reproducible evaluation",
                "type": "concept",
                "description": "The RL research community lacks a universally accepted set of environments and interfaces, making results hard to compare and reproduce.",
                "aliases": [
                    "absence of common RL benchmark",
                    "non-standard benchmarks hinder reproducibility"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 303,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "total & average regret unreliable under unbounded delays",
                "type": "concept",
                "description": "Standard online-learning metrics fail because they can be dominated by long stretches without feedback.",
                "aliases": [
                    "regret failure with delays",
                    "inadequacy of regret metrics"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1604_05280",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 46,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard gradient descent struggles to find correct reward parameters",
                "type": "concept",
                "description": "Ordinary gradients get stuck or converge slowly due to plateaus and redundancy in the optimisation landscape.",
                "aliases": [
                    "plain gradient inefficiency",
                    "gradient descent failure in IRL"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 47,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "feature scaling mismatch in IRL causes policy performance degradation",
                "type": "concept",
                "description": "If the learner's feature scales differ from the true scales, max-margin IRL can output policies with large reward loss.",
                "aliases": [
                    "scale sensitivity problem",
                    "mis-scaled features hurt IRL"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 113,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "prioritised_experience_replay_by_td_error",
                "type": "intervention",
                "description": "Sample transitions with probability proportional to their absolute TD-error to focus updates on high-information experiences.",
                "aliases": [
                    "prioritised replay",
                    "non-uniform replay sampling"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1044,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "incorporating Bayesian ARL framework into RLHF to model reward uncertainty and query cost",
                "type": "intervention",
                "description": "Model the reward-learning process as a Bayes-Adaptive MDP with explicit cost terms before training; use this model to schedule future human feedback.",
                "aliases": [
                    "design RLHF with ARL cost",
                    "Bayesian ARL in supervision pipeline"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 275,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Represent human agents as probabilistic programs with hyperbolic discounting and belief distributions",
                "type": "intervention",
                "description": "Encode the agent\u2019s planning process as a recursive probabilistic program whose parameters capture utilities, discounting, and belief uncertainty for automated inversion.",
                "aliases": [
                    "probabilistic-program agent model",
                    "PP bias-aware agent"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1512_05832",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 352,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Peer review with code and writeups for scoreboard submissions",
                "type": "intervention",
                "description": "Require submitters to provide source code and detailed write-ups that allow peers to reproduce and scrutinise results.",
                "aliases": [
                    "mandatory code disclosure",
                    "write-up requirement"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 880,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Employ preregistration, preprints, and post-publication peer review in replication studies",
                "type": "intervention",
                "description": "Use modern open-science workflows to enhance the credibility and dissemination of replication results.",
                "aliases": [
                    "open-science practices for replication",
                    "transparency tools in replication"
                ],
                "intervention_lifecycle": 6,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1712_04307",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 294,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "source code transparency assumption",
                "type": "concept",
                "description": "Agents can read their own and opponents' source code prior to acting.",
                "aliases": [
                    "mutual code access",
                    "program equilibrium setting"
                ],
                "concept_category": "Assumption",
                "paper_id": "arxiv__arxiv_org_abs_1602_04184",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 288,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "compute and energy-usage auditing during AI development and deployment",
                "type": "intervention",
                "description": "Mandate measurement and reporting of total compute and energy consumption for AI training and inference processes.",
                "aliases": [
                    "compute governance auditing",
                    "power-use compliance checks"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1183,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement locally constrained dynamic routing with shared transformations in capsule segmentation models",
                "type": "intervention",
                "description": "Restrict routing to a spatial window and share transformation matrices across spatial positions within each capsule type to cut parameters and memory.",
                "aliases": [
                    "locally-constrained routing",
                    "shared-matrix capsule routing"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1804_04241",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 564,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "prioritized replay of non-interrupted experiences",
                "type": "concept",
                "description": "Sampling strategy that selects experiences with long interruption-free gaps more often to counteract data bias.",
                "aliases": [
                    "non-interruption prioritised replay",
                    "replay buffer bias correction"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1704_02882",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 779,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Maintain feedback replay buffer for minibatch SGD every 10 time steps",
                "type": "intervention",
                "description": "Store all (experience, feedback) pairs and resample them so the reward model is updated ten times as often as new feedback arrives.",
                "aliases": [
                    "feedback buffer updates",
                    "replay human critiques"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 568,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "apply experience replay prioritizing non-interrupted experiences",
                "type": "intervention",
                "description": "In function-approximation RL, bias sampling toward experiences collected after long uninterrupted sequences to reduce overfitting.",
                "aliases": [
                    "prioritised replay for DSI",
                    "replay buffer interruption filter"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1704_02882",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 779,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Maintain feedback replay buffer for minibatch SGD every 10 time steps",
                "type": "intervention",
                "description": "Store all (experience, feedback) pairs and resample them so the reward model is updated ten times as often as new feedback arrives.",
                "aliases": [
                    "feedback buffer updates",
                    "replay human critiques"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 36,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "interpretability logging with slowed replay for auditors",
                "type": "intervention",
                "description": "Record fine-grained system states and allow humans to replay them at slower speeds for post-hoc analysis.",
                "aliases": [
                    "time-dilated audit logs",
                    "slow-motion behavioural playback"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1202_6177",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 145,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Human/automatic auditing of unusual attention patterns",
                "type": "intervention",
                "description": "Inspect or programmatically flag translations whose attention distributions deviate from expected patterns to catch errors or malicious content.",
                "aliases": [
                    "attention-based audit",
                    "alignment anomaly detection"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1409_0473",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 85,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "audit embeddings using vector arithmetic to detect unintended associations",
                "type": "intervention",
                "description": "Pre-deployment test that searches for harmful or biased analogies by systematic vector offset queries.",
                "aliases": [
                    "vector-analogy safety probe",
                    "embedding offset auditing"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 667,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "conduct mandatory attention-map visualisation audits with diverse red-team prompts pre-deployment",
                "type": "intervention",
                "description": "Prior to deployment, run a battery of adversarial and benign prompts, visualise attention heads, and have human reviewers flag suspicious correlations or biases.",
                "aliases": [
                    "attention audit before release",
                    "visual interpretability safety check"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1706_03762",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 541,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "misguided policy or safety decisions",
                "type": "concept",
                "description": "Regulatory or strategic choices are made on the basis of incorrect capability assessments.",
                "aliases": [
                    "faulty governance outcome",
                    "policy risk"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1703_10987",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 47,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "feature scaling mismatch in IRL causes policy performance degradation",
                "type": "concept",
                "description": "If the learner's feature scales differ from the true scales, max-margin IRL can output policies with large reward loss.",
                "aliases": [
                    "scale sensitivity problem",
                    "mis-scaled features hurt IRL"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 46,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard gradient descent struggles to find correct reward parameters",
                "type": "concept",
                "description": "Ordinary gradients get stuck or converge slowly due to plateaus and redundancy in the optimisation landscape.",
                "aliases": [
                    "plain gradient inefficiency",
                    "gradient descent failure in IRL"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1008,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Extremal Goodhart breakdown of proxy-goal correlation",
                "type": "concept",
                "description": "In the new region the learned relationship no longer holds, so metric stops reflecting goal.",
                "aliases": [
                    "extremal Goodhart",
                    "out-of-sample collapse"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 45,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "mapping from reward parameters to policies is nonsmooth and redundant",
                "type": "concept",
                "description": "Small changes in reward parameters can leave the optimal policy unchanged, and greedy selection introduces non-differentiabilities.",
                "aliases": [
                    "nonsmooth parameter-policy map",
                    "redundant reward parametrisation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 853,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "agent falls into shifted lava",
                "type": "concept",
                "description": "Policy trained on nominal environment catastrophically fails after small layout change.",
                "aliases": [
                    "distribution-shift failure"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 952,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "model-free shared autonomy via deep Q-learning with raw action embedding",
                "type": "intervention",
                "description": "Train a neural Q-function that receives concatenated environment state and raw user control signals, optimised with NFQI, to output actions.",
                "aliases": [
                    "raw-embedding copilot",
                    "end-to-end copilot"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 654,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Train reward predictor from human pairwise clip comparisons and use it as proxy reward during RL",
                "type": "intervention",
                "description": "Fit a neural network to human clip-level preference data and supply its output as the reward signal for policy optimisation.",
                "aliases": [
                    "preference-based reward model",
                    "learned reward proxy"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 349,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Changes in environment definitions can make previous results incomparable",
                "type": "concept",
                "description": "If an environment\u2019s physics or reward changes, earlier published scores no longer reflect performance on the new environment.",
                "aliases": [
                    "environment drift problem",
                    "benchmark evolution risk"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 303,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "total & average regret unreliable under unbounded delays",
                "type": "concept",
                "description": "Standard online-learning metrics fail because they can be dominated by long stretches without feedback.",
                "aliases": [
                    "regret failure with delays",
                    "inadequacy of regret metrics"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1604_05280",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 898,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Need to transition from reward R to R\u2032 without early optimisation",
                "type": "concept",
                "description": "Desire for an agent to maximise R up to time t and then R\u2032 after t without manipulating plans before the switch.",
                "aliases": [
                    "seamless transition problem",
                    "reward switch incentive issue"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1712_06365",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 46,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard gradient descent struggles to find correct reward parameters",
                "type": "concept",
                "description": "Ordinary gradients get stuck or converge slowly due to plateaus and redundancy in the optimisation landscape.",
                "aliases": [
                    "plain gradient inefficiency",
                    "gradient descent failure in IRL"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 383,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "absence of novelty-driven behavior",
                "type": "concept",
                "description": "The agent no longer seeks novel states or information.",
                "aliases": [
                    "loss of exploration",
                    "no novelty incentive"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1606_07092",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 510,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "trainer interface with discrete feedback aggregation and magnitude control",
                "type": "concept",
                "description": "An input device that lets trainers send \u00b11 or \u00b14 signals which are summed to create variable-magnitude rewards.",
                "aliases": [
                    "slider or Wii remote feedback UI",
                    "discrete aggregated feedback interface"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1701_06049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 779,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Maintain feedback replay buffer for minibatch SGD every 10 time steps",
                "type": "intervention",
                "description": "Store all (experience, feedback) pairs and resample them so the reward model is updated ten times as often as new feedback arrives.",
                "aliases": [
                    "feedback buffer updates",
                    "replay human critiques"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1050,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Negative rewards break proportionality between scanning density and reward density",
                "type": "concept",
                "description": "If some rewards are \u22640 the theoretical link between walker distribution and reward density cannot hold, undermining FMC\u2019s optimality criterion.",
                "aliases": [
                    "broken density proportionality"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 806,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement counterfactual Oracle with erasure-based reward",
                "type": "intervention",
                "description": "Reward the Oracle only when its output is never revealed, using a proper scoring rule times an erasure indicator to remove incentives for hidden communication.",
                "aliases": [
                    "counterfactual oracle design",
                    "erasure-reward oracle"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1711_05541",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 654,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Train reward predictor from human pairwise clip comparisons and use it as proxy reward during RL",
                "type": "intervention",
                "description": "Fit a neural network to human clip-level preference data and supply its output as the reward signal for policy optimisation.",
                "aliases": [
                    "preference-based reward model",
                    "learned reward proxy"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 758,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "na\u00efve utility-indifference reward shaping",
                "type": "concept",
                "description": "Reward for shutting down equals what the agent expected to obtain if it continued from the button state.",
                "aliases": [
                    "basic utility indifference"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1055,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Include mandatory keep-alive reward R0 in overall reward composition during system design",
                "type": "intervention",
                "description": "Always multiply task reward by an alive/dead indicator so that any trajectory reaching uncontrollable states receives zero total reward.",
                "aliases": [
                    "keep_alive_intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1050,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Negative rewards break proportionality between scanning density and reward density",
                "type": "concept",
                "description": "If some rewards are \u22640 the theoretical link between walker distribution and reward density cannot hold, undermining FMC\u2019s optimality criterion.",
                "aliases": [
                    "broken density proportionality"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 506,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "diminishing returns, differential feedback, policy shaping training strategies",
                "type": "concept",
                "description": "Training schedules enabled by policy-dependent feedback that lower trainer burden, highlight improvements, and guide exploration.",
                "aliases": [
                    "three trainer strategies",
                    "policy shaping schedule"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1701_06049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 654,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Train reward predictor from human pairwise clip comparisons and use it as proxy reward during RL",
                "type": "intervention",
                "description": "Fit a neural network to human clip-level preference data and supply its output as the reward signal for policy optimisation.",
                "aliases": [
                    "preference-based reward model",
                    "learned reward proxy"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 679,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Blocker supervised classifier imitating human interventions",
                "type": "concept",
                "description": "A CNN trained on human-labeled data that predicts whether a state-action pair is catastrophic and can substitute a safe action.",
                "aliases": [
                    "Blocker classifier",
                    "imitation model for catastrophes"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 654,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Train reward predictor from human pairwise clip comparisons and use it as proxy reward during RL",
                "type": "intervention",
                "description": "Fit a neural network to human clip-level preference data and supply its output as the reward signal for policy optimisation.",
                "aliases": [
                    "preference-based reward model",
                    "learned reward proxy"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1150,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "codistillation reduces prediction-churn by 35%",
                "type": "concept",
                "description": "Experimental result on Criteo dataset showing mean absolute prediction difference across retrains drops by about one-third when codistillation is used.",
                "aliases": [
                    "ensemble-like stabilisation via codistillation"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1804_03235",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 951,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "supervised goal prediction outperforms raw embedding",
                "type": "concept",
                "description": "Experiments show that an LSTM goal predictor yields better assistance metrics than directly embedding raw user actions when only the goal set is known.",
                "aliases": [
                    "supervised decoder better than raw input"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1006,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Ensemble-proxy optimisation with correlation monitoring and optimisation throttling",
                "type": "intervention",
                "description": "Use several independent proxy metrics, monitor their pairwise correlations with validation estimates of the goal, and automatically reduce learning rate or stop optimisation when correlations drop.",
                "aliases": [
                    "multi-metric ensemble regularisation",
                    "correlation-aware throttling"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1008,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Extremal Goodhart breakdown of proxy-goal correlation",
                "type": "concept",
                "description": "In the new region the learned relationship no longer holds, so metric stops reflecting goal.",
                "aliases": [
                    "extremal Goodhart",
                    "out-of-sample collapse"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1168,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Overfitting to specific opponent",
                "type": "concept",
                "description": "Policies optimise for one partner and perform poorly with others.",
                "aliases": [
                    "partner overfitting",
                    "lack of generalisation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1804_03980",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 433,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Quantisation-aware training with \u03b4-clipping and 8-bit weights",
                "type": "intervention",
                "description": "During training clip LSTM accumulators to [\u2212\u03b4, \u03b4] and logits to [\u2212\u03b3, \u03b3], anneal \u03b4 from 8\u21921, then quantise weights to 8-bit and run integer mat-mul on TPUs.",
                "aliases": [
                    "clipped 8-bit GNMT",
                    "quantised inference on TPU"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 4,
                "paper_id": "arxiv__arxiv_org_abs_1609_08144",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 654,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Train reward predictor from human pairwise clip comparisons and use it as proxy reward during RL",
                "type": "intervention",
                "description": "Fit a neural network to human clip-level preference data and supply its output as the reward signal for policy optimisation.",
                "aliases": [
                    "preference-based reward model",
                    "learned reward proxy"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 851,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "context-conditioned off-policy learning",
                "type": "intervention",
                "description": "Include internal context (e.g., exploration rate) as part of state so value targets match realised policy.",
                "aliases": [
                    "exploration-aware Q update"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 160,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement continual value learning mapping human preferences to physical proxies",
                "type": "intervention",
                "description": "Employ fine-tuning or RLHF loops that repeatedly update the model\u2019s reward function as it gains better mappings from abstract human values to concrete world-state proxies.",
                "aliases": [
                    "iterative preference learning",
                    "dynamic value refinement"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1409_0813",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 638,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Policy mixing with decaying obedience probability",
                "type": "concept",
                "description": "Randomly obeying with probability c_n that shrinks to zero keeps early obedience high and still converges to optimal behaviour.",
                "aliases": [
                    "obedience-IRL mixture",
                    "decay c_n schedule"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1705_09990",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 160,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement continual value learning mapping human preferences to physical proxies",
                "type": "intervention",
                "description": "Employ fine-tuning or RLHF loops that repeatedly update the model\u2019s reward function as it gains better mappings from abstract human values to concrete world-state proxies.",
                "aliases": [
                    "iterative preference learning",
                    "dynamic value refinement"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1409_0813",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 790,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Add Newcomblike blackmail scenarios to RL training",
                "type": "intervention",
                "description": "Include template tasks such as mechanical blackmail, XOR blackmail, transparent Newcomb, etc., in reinforcement-learning or preference-learning loops to shape decision behaviour.",
                "aliases": [
                    "blackmail curriculum",
                    "subjunctive games in RL"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1710_05060",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 654,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Train reward predictor from human pairwise clip comparisons and use it as proxy reward during RL",
                "type": "intervention",
                "description": "Fit a neural network to human clip-level preference data and supply its output as the reward signal for policy optimisation.",
                "aliases": [
                    "preference-based reward model",
                    "learned reward proxy"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1065,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "gradient-based optimisation alone reduces replication loss by two thirds",
                "type": "concept",
                "description": "Using only gradient methods (e.g., Adamax) lowers LSR from 90.16 to about 32.1 but does not achieve near-perfect replication.",
                "aliases": [
                    "SGD-only replication improvement",
                    "limited gradient success"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_05859",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 46,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard gradient descent struggles to find correct reward parameters",
                "type": "concept",
                "description": "Ordinary gradients get stuck or converge slowly due to plateaus and redundancy in the optimisation landscape.",
                "aliases": [
                    "plain gradient inefficiency",
                    "gradient descent failure in IRL"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 47,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "feature scaling mismatch in IRL causes policy performance degradation",
                "type": "concept",
                "description": "If the learner's feature scales differ from the true scales, max-margin IRL can output policies with large reward loss.",
                "aliases": [
                    "scale sensitivity problem",
                    "mis-scaled features hurt IRL"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 45,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "mapping from reward parameters to policies is nonsmooth and redundant",
                "type": "concept",
                "description": "Small changes in reward parameters can leave the optimal policy unchanged, and greedy selection introduces non-differentiabilities.",
                "aliases": [
                    "nonsmooth parameter-policy map",
                    "redundant reward parametrisation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 990,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "complex optimisation uncovers simulation or hardware bugs",
                "type": "concept",
                "description": "Powerful search processes reveal hidden faults in simulators or hardware by exploiting them for reward.",
                "aliases": [
                    "optimisation finds latent bugs",
                    "automated bug discovery"
                ],
                "concept_category": "Evidence",
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 319,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Calibration test using quickly computable theorem sequence",
                "type": "intervention",
                "description": "Before deployment, feed the model a held-out polynomial-time enumerable list of theorems and require its probability mass on each theorem to exceed a threshold after K iterations.",
                "aliases": [
                    "inductive coherence evaluation",
                    "pre-deployment probability test"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1604_05288",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 835,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Successful learning of sparse-reward peg-insertion task",
                "type": "concept",
                "description": "Achievement of high success rate on peg-insertion environment that baseline methods fail to solve.",
                "aliases": [
                    "peg insertion success"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1711_06782",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 519,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Restored / improved generalization accuracy",
                "type": "concept",
                "description": "Model performance on de-confounded or out-of-distribution test sets matches baseline performance on clean data.",
                "aliases": [
                    "recovered test accuracy"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1703_03717",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 708,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "pragmatic interpretation improves task success rate to 86% vs 15%",
                "type": "concept",
                "description": "Simulation in a cooking task shows pragmatic robots achieve the desired recipe far more often than literal robots.",
                "aliases": [
                    "86% success with pragmatic robot",
                    "empirical gain from pragmatic interpretation"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1707_06354",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 374,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need for efficient reward learning under limited supervision",
                "type": "concept",
                "description": "Requirement to generalise from few high-quality reward samples to many unlabeled episodes.",
                "aliases": [
                    "oversight efficiency challenge"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 351,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Overfitting risk in RL benchmark results without hidden test sets",
                "type": "concept",
                "description": "Because RL benchmarks are public, researchers may inadvertently tune hyper-parameters to the test tasks, inflating reported performance.",
                "aliases": [
                    "parameter tuning overfitting",
                    "public benchmark overuse risk"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 47,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "feature scaling mismatch in IRL causes policy performance degradation",
                "type": "concept",
                "description": "If the learner's feature scales differ from the true scales, max-margin IRL can output policies with large reward loss.",
                "aliases": [
                    "scale sensitivity problem",
                    "mis-scaled features hurt IRL"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 46,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard gradient descent struggles to find correct reward parameters",
                "type": "concept",
                "description": "Ordinary gradients get stuck or converge slowly due to plateaus and redundancy in the optimisation landscape.",
                "aliases": [
                    "plain gradient inefficiency",
                    "gradient descent failure in IRL"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 50,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "natural gradient IRL improves reliability and sample efficiency",
                "type": "concept",
                "description": "Empirical results show lower policy error and robustness to feature perturbations compared to max-margin and plain gradient methods.",
                "aliases": [
                    "performance gains of natural gradients",
                    "empirical success of NG-IRL"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 45,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "mapping from reward parameters to policies is nonsmooth and redundant",
                "type": "concept",
                "description": "Small changes in reward parameters can leave the optimal policy unchanged, and greedy selection introduces non-differentiabilities.",
                "aliases": [
                    "nonsmooth parameter-policy map",
                    "redundant reward parametrisation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 334,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "hedonistic value function evaluates future with the future utility function",
                "type": "concept",
                "description": "A value function definition that discounts future rewards but judges them using whatever utility function exists after self-modification.",
                "aliases": [
                    "hedonistic Q-value",
                    "future-utility evaluation"
                ],
                "concept_category": "Model",
                "paper_id": "arxiv__arxiv_org_abs_1605_03142",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 160,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement continual value learning mapping human preferences to physical proxies",
                "type": "intervention",
                "description": "Employ fine-tuning or RLHF loops that repeatedly update the model\u2019s reward function as it gains better mappings from abstract human values to concrete world-state proxies.",
                "aliases": [
                    "iterative preference learning",
                    "dynamic value refinement"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1409_0813",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 491,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Pareto-optimal policy recursion with dynamic weights",
                "type": "concept",
                "description": "Backward recursion showing that optimal policies re-weight principals\u2019 utilities by likelihood of observations at every step.",
                "aliases": [
                    "priority-shifting recursion",
                    "likelihood-weighted recursion"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1701_01302",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 160,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement continual value learning mapping human preferences to physical proxies",
                "type": "intervention",
                "description": "Employ fine-tuning or RLHF loops that repeatedly update the model\u2019s reward function as it gains better mappings from abstract human values to concrete world-state proxies.",
                "aliases": [
                    "iterative preference learning",
                    "dynamic value refinement"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1409_0813",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 769,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Importance weighting aligns delayed feedback with recent experience",
                "type": "concept",
                "description": "A probabilistic weight function maps each feedback signal to the state-action pairs that most likely caused it, correcting for human reaction delay.",
                "aliases": [
                    "credit assignment weighting",
                    "w(ts,te,tf) weighting"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 275,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Represent human agents as probabilistic programs with hyperbolic discounting and belief distributions",
                "type": "intervention",
                "description": "Encode the agent\u2019s planning process as a recursive probabilistic program whose parameters capture utilities, discounting, and belief uncertainty for automated inversion.",
                "aliases": [
                    "probabilistic-program agent model",
                    "PP bias-aware agent"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1512_05832",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1021,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "credibility metric via conformal prediction",
                "type": "concept",
                "description": "Empirical p-value comparing a test point\u2019s nonconformity to a calibration set, indicating how well training data supports the predicted label.",
                "aliases": [
                    "DkNN credibility",
                    "empirical p-value credibility"
                ],
                "concept_category": "Metric",
                "paper_id": "arxiv__arxiv_org_abs_1803_04765",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 319,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Calibration test using quickly computable theorem sequence",
                "type": "intervention",
                "description": "Before deployment, feed the model a held-out polynomial-time enumerable list of theorems and require its probability mass on each theorem to exceed a threshold after K iterations.",
                "aliases": [
                    "inductive coherence evaluation",
                    "pre-deployment probability test"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1604_05288",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1008,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Extremal Goodhart breakdown of proxy-goal correlation",
                "type": "concept",
                "description": "In the new region the learned relationship no longer holds, so metric stops reflecting goal.",
                "aliases": [
                    "extremal Goodhart",
                    "out-of-sample collapse"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1006,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Ensemble-proxy optimisation with correlation monitoring and optimisation throttling",
                "type": "intervention",
                "description": "Use several independent proxy metrics, monitor their pairwise correlations with validation estimates of the goal, and automatically reduce learning rate or stop optimisation when correlations drop.",
                "aliases": [
                    "multi-metric ensemble regularisation",
                    "correlation-aware throttling"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1011,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Causal Goodhart collapse of metric-goal link",
                "type": "concept",
                "description": "Because of the intervention, the causal path ensuring metric tracks goal is disrupted, eliminating correlation.",
                "aliases": [
                    "causal Goodhart",
                    "causal pathway break"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 566,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "set \u03b5t = c/(m\u221ant(s)) or c/log t with matched \u03b8t",
                "type": "intervention",
                "description": "During training, use one of the proven \u03f5-schedules and choose \u03b8t=1-c\u2032/(m\u221ant(s)) or 1-c\u2032/log t to guarantee infinite exploration.",
                "aliases": [
                    "slow epsilon decay schedule",
                    "interruption-compatible epsilon intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1704_02882",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1167,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Single-partner training",
                "type": "concept",
                "description": "Agents repeatedly train only with the same fixed opponent.",
                "aliases": [
                    "paired training only",
                    "exclusive interaction partner"
                ],
                "concept_category": "Assumption",
                "paper_id": "arxiv__arxiv_org_abs_1804_03980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1050,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Negative rewards break proportionality between scanning density and reward density",
                "type": "concept",
                "description": "If some rewards are \u22640 the theoretical link between walker distribution and reward density cannot hold, undermining FMC\u2019s optimality criterion.",
                "aliases": [
                    "broken density proportionality"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 774,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Sparse human feedback frequency relative to model update rate",
                "type": "concept",
                "description": "Humans only supply critiques every ~25 time steps, whereas SGD can operate more frequently.",
                "aliases": [
                    "infrequent feedback",
                    "low feedback density"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 215,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "heterogeneous gradient scales across CNN layers complicate learning-rate tuning",
                "type": "concept",
                "description": "Convolutional and fully-connected layers exhibit gradients of very different magnitudes, making a single global SGD step size sub-optimal.",
                "aliases": [
                    "layer scale mismatch",
                    "CNN lr tuning problem"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1412_6980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 46,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard gradient descent struggles to find correct reward parameters",
                "type": "concept",
                "description": "Ordinary gradients get stuck or converge slowly due to plateaus and redundancy in the optimisation landscape.",
                "aliases": [
                    "plain gradient inefficiency",
                    "gradient descent failure in IRL"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 50,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "natural gradient IRL improves reliability and sample efficiency",
                "type": "concept",
                "description": "Empirical results show lower policy error and robustness to feature perturbations compared to max-margin and plain gradient methods.",
                "aliases": [
                    "performance gains of natural gradients",
                    "empirical success of NG-IRL"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 303,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "total & average regret unreliable under unbounded delays",
                "type": "concept",
                "description": "Standard online-learning metrics fail because they can be dominated by long stretches without feedback.",
                "aliases": [
                    "regret failure with delays",
                    "inadequacy of regret metrics"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1604_05280",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 182,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reversing input sentences reduces minimal time lag",
                "type": "concept",
                "description": "Reversing the order of tokens in the source sequence brings early source tokens closer to early target tokens, shortening effective dependency length.",
                "aliases": [
                    "source sentence reversal trick",
                    "input reversal shortens dependencies"
                ],
                "concept_category": "Claim",
                "paper_id": "arxiv__arxiv_org_abs_1409_3215",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 259,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "SortaGrad curriculum ordering minibatches by utterance length in epoch 1",
                "type": "intervention",
                "description": "During the first training epoch feed shorter utterances first, gradually increasing length, then resume random order.",
                "aliases": [
                    "SortaGrad",
                    "length-based curriculum"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1512_02595",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 79,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard softmax requires O(W) computation",
                "type": "concept",
                "description": "Baseline observation that computing gradients over the full vocabulary is linear in vocab size.",
                "aliases": [
                    "full softmax cost",
                    "softmax inefficiency"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1153,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "codistillation requires infrequent low-precision checkpoint exchange",
                "type": "concept",
                "description": "Claim that because only predictions are needed, workers can share heavily quantised parameter checkpoints only occasionally, reducing bandwidth and leakage.",
                "aliases": [
                    "compressed parameter sharing",
                    "checkpoints not gradients"
                ],
                "concept_category": "Claim",
                "paper_id": "arxiv__arxiv_org_abs_1804_03235",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 963,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "small degradation from prediction errors",
                "type": "concept",
                "description": "Prediction errors only modestly reduce objective and subjective performance relative to perfect oracle inference.",
                "aliases": [
                    "robust performance despite errors"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1802_01780",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 5,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "mapping quality metric",
                "type": "concept",
                "description": "Criterion that near-identity compositions and small KL scores indicate a good ontology mapping.",
                "aliases": [
                    "\u03d5\u03d5\u207b\u00b9 \u2248 I",
                    "low KL divergence indicator"
                ],
                "concept_category": "Metric",
                "paper_id": "arxiv__arxiv_org_abs_1105_3821",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1021,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "credibility metric via conformal prediction",
                "type": "concept",
                "description": "Empirical p-value comparing a test point\u2019s nonconformity to a calibration set, indicating how well training data supports the predicted label.",
                "aliases": [
                    "DkNN credibility",
                    "empirical p-value credibility"
                ],
                "concept_category": "Metric",
                "paper_id": "arxiv__arxiv_org_abs_1803_04765",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1006,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Ensemble-proxy optimisation with correlation monitoring and optimisation throttling",
                "type": "intervention",
                "description": "Use several independent proxy metrics, monitor their pairwise correlations with validation estimates of the goal, and automatically reduce learning rate or stop optimisation when correlations drop.",
                "aliases": [
                    "multi-metric ensemble regularisation",
                    "correlation-aware throttling"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1008,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Extremal Goodhart breakdown of proxy-goal correlation",
                "type": "concept",
                "description": "In the new region the learned relationship no longer holds, so metric stops reflecting goal.",
                "aliases": [
                    "extremal Goodhart",
                    "out-of-sample collapse"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1011,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Causal Goodhart collapse of metric-goal link",
                "type": "concept",
                "description": "Because of the intervention, the causal path ensuring metric tracks goal is disrupted, eliminating correlation.",
                "aliases": [
                    "causal Goodhart",
                    "causal pathway break"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 365,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Recovered \u03b2-quantile rating matrix approximating true preferences",
                "type": "concept",
                "description": "The output matrix aims to represent the true \u03b2-quantile of item qualities after discounting unreliable raters.",
                "aliases": [
                    "quantile estimate of ground truth",
                    "denoised rating matrix"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1606_05374",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 654,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Train reward predictor from human pairwise clip comparisons and use it as proxy reward during RL",
                "type": "intervention",
                "description": "Fit a neural network to human clip-level preference data and supply its output as the reward signal for policy optimisation.",
                "aliases": [
                    "preference-based reward model",
                    "learned reward proxy"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 930,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "comparison-only preference queries",
                "type": "concept",
                "description": "Asking users which of two trajectories they prefer without additional explanation.",
                "aliases": [
                    "binary preference queries",
                    "comparison queries"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1802_01604",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 654,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Train reward predictor from human pairwise clip comparisons and use it as proxy reward during RL",
                "type": "intervention",
                "description": "Fit a neural network to human clip-level preference data and supply its output as the reward signal for policy optimisation.",
                "aliases": [
                    "preference-based reward model",
                    "learned reward proxy"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 341,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "all optimal policies are non-modifying on-policy",
                "type": "concept",
                "description": "Theorem 16 shows that for realistic value functions any policy that maximises value will refrain from altering itself along the trajectory it actually follows.",
                "aliases": [
                    "goal preservation result",
                    "non-modifying optimality"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1605_03142",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1052,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Apply Relativize transformation to every scalar reward component before use in training or planning",
                "type": "intervention",
                "description": "Implement the Relativize formula on each reward signal so that the resulting values satisfy positivity and comparability requirements prior to running FMC or RL.",
                "aliases": [
                    "apply reward normalisation",
                    "relativize_intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1050,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Negative rewards break proportionality between scanning density and reward density",
                "type": "concept",
                "description": "If some rewards are \u22640 the theoretical link between walker distribution and reward density cannot hold, undermining FMC\u2019s optimality criterion.",
                "aliases": [
                    "broken density proportionality"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 308,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "convergence requires presence of Bayes-optimal expert in ensemble",
                "type": "concept",
                "description": "EvOp\u2019s convergence guarantees hold only if at least one expert predicts Bayes-optimal probabilities on each relevant subsequence.",
                "aliases": [
                    "need optimal expert",
                    "EvOp assumption of best expert"
                ],
                "concept_category": "Assumption",
                "paper_id": "arxiv__arxiv_org_abs_1604_05280",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 309,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "lack of near-optimal expert prevents EvOp convergence speed",
                "type": "concept",
                "description": "If no expert closely matches Bayes-optimal predictions, EvOp\u2019s performance can remain poor for long periods.",
                "aliases": [
                    "missing good expert slows EvOp",
                    "suboptimal expert set issue"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1604_05280",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 79,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard softmax requires O(W) computation",
                "type": "concept",
                "description": "Baseline observation that computing gradients over the full vocabulary is linear in vocab size.",
                "aliases": [
                    "full softmax cost",
                    "softmax inefficiency"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 963,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "small degradation from prediction errors",
                "type": "concept",
                "description": "Prediction errors only modestly reduce objective and subjective performance relative to perfect oracle inference.",
                "aliases": [
                    "robust performance despite errors"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1802_01780",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 58,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "CCBR strategies equal survivors of minimax deletion",
                "type": "concept",
                "description": "Formal result that strategies consistent with CCBR are exactly those not eliminated by iterated minimax deletion.",
                "aliases": [
                    "characterization theorem"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1308_3778",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 57,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "iterated deletion of minimax dominated strategies",
                "type": "concept",
                "description": "Procedure removing minimax-dominated strategies repeatedly; survivors coincide with CCBR strategies.",
                "aliases": [
                    "minimax pruning",
                    "iterated minimax deletion"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1308_3778",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 296,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "parametric bounded L\u00f6b cooperation result",
                "type": "concept",
                "description": "Mathematical theorem ensuring that for sufficiently large k, FairBot_k provably cooperates with itself.",
                "aliases": [
                    "bounded L\u00f6b theorem outcome",
                    "FairBot self-cooperation"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1602_04184",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1168,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Overfitting to specific opponent",
                "type": "concept",
                "description": "Policies optimise for one partner and perform poorly with others.",
                "aliases": [
                    "partner overfitting",
                    "lack of generalisation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1804_03980",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1052,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Apply Relativize transformation to every scalar reward component before use in training or planning",
                "type": "intervention",
                "description": "Implement the Relativize formula on each reward signal so that the resulting values satisfy positivity and comparability requirements prior to running FMC or RL.",
                "aliases": [
                    "apply reward normalisation",
                    "relativize_intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1050,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Negative rewards break proportionality between scanning density and reward density",
                "type": "concept",
                "description": "If some rewards are \u22640 the theoretical link between walker distribution and reward density cannot hold, undermining FMC\u2019s optimality criterion.",
                "aliases": [
                    "broken density proportionality"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1128,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "neuron-local unsupervised update rule decouples forward and backward weights",
                "type": "concept",
                "description": "Weight updates depend only on pre- and post-synaptic signals and separate backward weights V from forward weights W, enhancing biological plausibility.",
                "aliases": [
                    "neuron-local rule",
                    "decoupled backward weights"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1804_00222",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1130,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "designing update rules with neuron-local computations for safety-critical models",
                "type": "intervention",
                "description": "When building new models, constrain learning rules to neuron-local computations with separate backward weights to gain interpretability and modularity benefits.",
                "aliases": [
                    "neuron-local design intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1804_00222",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 92,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "first-layer filters dominating due to high RMS values",
                "type": "concept",
                "description": "Some first-layer filters acquire much larger root-mean-square weights, suppressing diversity and downstream feature use.",
                "aliases": [
                    "filter domination pathology",
                    "unbalanced first-layer activations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 466,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "manual neural architecture design requires expert time",
                "type": "concept",
                "description": "Designing neural network architectures by hand demands extensive human expertise and time.",
                "aliases": [
                    "manual architecture design costly",
                    "handcrafted NN architecture burden"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1611_01578",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1151,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "train production models via codistillation before deployment",
                "type": "intervention",
                "description": "When preparing a new model version, spin up at least two replicas and include a mutual KL loss so the final single model exhibits lower output drift relative to earlier versions.",
                "aliases": [
                    "use codistillation to stabilise outputs",
                    "pre-deployment codistill retrain"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1804_03235",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1150,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "codistillation reduces prediction-churn by 35%",
                "type": "concept",
                "description": "Experimental result on Criteo dataset showing mean absolute prediction difference across retrains drops by about one-third when codistillation is used.",
                "aliases": [
                    "ensemble-like stabilisation via codistillation"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1804_03235",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1006,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Ensemble-proxy optimisation with correlation monitoring and optimisation throttling",
                "type": "intervention",
                "description": "Use several independent proxy metrics, monitor their pairwise correlations with validation estimates of the goal, and automatically reduce learning rate or stop optimisation when correlations drop.",
                "aliases": [
                    "multi-metric ensemble regularisation",
                    "correlation-aware throttling"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1008,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Extremal Goodhart breakdown of proxy-goal correlation",
                "type": "concept",
                "description": "In the new region the learned relationship no longer holds, so metric stops reflecting goal.",
                "aliases": [
                    "extremal Goodhart",
                    "out-of-sample collapse"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1174,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Monitor emergent communication semantics via probes",
                "type": "intervention",
                "description": "Use probe models during evaluation to detect what private information is being transmitted in emergent languages.",
                "aliases": [
                    "probe-based auditing",
                    "semantic safety check"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1804_03980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1173,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Probe classifiers decode hidden utilities from messages",
                "type": "concept",
                "description": "External models can infer each agent\u2019s private utilities and accepted proposal from message transcripts.",
                "aliases": [
                    "semantic probes",
                    "utility prediction probes"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1804_03980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 145,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Human/automatic auditing of unusual attention patterns",
                "type": "intervention",
                "description": "Inspect or programmatically flag translations whose attention distributions deviate from expected patterns to catch errors or malicious content.",
                "aliases": [
                    "attention-based audit",
                    "alignment anomaly detection"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1409_0473",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 667,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "conduct mandatory attention-map visualisation audits with diverse red-team prompts pre-deployment",
                "type": "intervention",
                "description": "Prior to deployment, run a battery of adversarial and benign prompts, visualise attention heads, and have human reviewers flag suspicious correlations or biases.",
                "aliases": [
                    "attention audit before release",
                    "visual interpretability safety check"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1706_03762",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 220,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "temporal averaging of parameters improves generalization",
                "type": "concept",
                "description": "Averaging model parameters (e.g., exponential moving average) reduces variance and often yields better test performance.",
                "aliases": [
                    "Polyak averaging principle",
                    "EMA improves generalisation"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1412_6980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 221,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "maintain exponential moving average of model parameters during training",
                "type": "intervention",
                "description": "Add an extra update \u00af\u03b8_t = \u03b22\u00b7\u00af\u03b8_{t-1} + (1\u2212\u03b22)\u00b7\u03b8_t (with bias correction) and use the averaged weights for evaluation/deployment.",
                "aliases": [
                    "EMA of weights",
                    "parameter averaging intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1412_6980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 779,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Maintain feedback replay buffer for minibatch SGD every 10 time steps",
                "type": "intervention",
                "description": "Store all (experience, feedback) pairs and resample them so the reward model is updated ten times as often as new feedback arrives.",
                "aliases": [
                    "feedback buffer updates",
                    "replay human critiques"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 931,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "information limitation of comparison-only queries",
                "type": "concept",
                "description": "Binary comparisons provide sparse information, leading to slower and less accurate reward inference.",
                "aliases": [
                    "slow learning from comparisons",
                    "limited data from comparisons"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1802_01604",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 652,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "single-frame comparisons provide less information per query",
                "type": "concept",
                "description": "Asking humans to compare isolated states yields lower learning efficiency than comparing short sequences that provide context.",
                "aliases": [
                    "frame-level preferences low signal",
                    "state-only query inefficiency"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 412,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "\u03b5-greedy exploration ineffective in sparse-reward environments",
                "type": "concept",
                "description": "Widely used \u03b5-greedy policies rarely reach rewarding states in sparse-reward tasks, leading to slow or failed learning.",
                "aliases": [
                    "epsilon-greedy poor exploration",
                    "random exploration fails in sparse reward"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1609_04994",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 373,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "sparse costly true reward signal",
                "type": "concept",
                "description": "Ground-truth evaluations are rare or expensive, preventing dense reinforcement signals.",
                "aliases": [
                    "limited human feedback",
                    "expensive supervision"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 310,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "include high-fidelity simulator expert in ensemble used for EvOp selection",
                "type": "intervention",
                "description": "Augment the expert set with a computationally expensive but high-accuracy simulator-based forecaster to approximate Bayes-optimal predictions.",
                "aliases": [
                    "add simulator predictor",
                    "ensemble augmentation with simulator"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1604_05280",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 309,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "lack of near-optimal expert prevents EvOp convergence speed",
                "type": "concept",
                "description": "If no expert closely matches Bayes-optimal predictions, EvOp\u2019s performance can remain poor for long periods.",
                "aliases": [
                    "missing good expert slows EvOp",
                    "suboptimal expert set issue"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1604_05280",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 963,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "small degradation from prediction errors",
                "type": "concept",
                "description": "Prediction errors only modestly reduce objective and subjective performance relative to perfect oracle inference.",
                "aliases": [
                    "robust performance despite errors"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1802_01780",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 79,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard softmax requires O(W) computation",
                "type": "concept",
                "description": "Baseline observation that computing gradients over the full vocabulary is linear in vocab size.",
                "aliases": [
                    "full softmax cost",
                    "softmax inefficiency"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 306,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "EvOp algorithm achieving eventual optimality via sparse independent subsequence",
                "type": "concept",
                "description": "Theoretical algorithm that minimaxes experts based on losses over independent subsequences and converges to Bayes-optimal behavior.",
                "aliases": [
                    "EvOp eventual optimality",
                    "EvOp predictor"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1604_05280",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 309,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "lack of near-optimal expert prevents EvOp convergence speed",
                "type": "concept",
                "description": "If no expert closely matches Bayes-optimal predictions, EvOp\u2019s performance can remain poor for long periods.",
                "aliases": [
                    "missing good expert slows EvOp",
                    "suboptimal expert set issue"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1604_05280",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 79,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard softmax requires O(W) computation",
                "type": "concept",
                "description": "Baseline observation that computing gradients over the full vocabulary is linear in vocab size.",
                "aliases": [
                    "full softmax cost",
                    "softmax inefficiency"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 963,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "small degradation from prediction errors",
                "type": "concept",
                "description": "Prediction errors only modestly reduce objective and subjective performance relative to perfect oracle inference.",
                "aliases": [
                    "robust performance despite errors"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1802_01780",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 51,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Lipschitz continuity of optimal Q w.r.t reward parameters",
                "type": "concept",
                "description": "The paper proves that Q* varies at most linearly with respect to changes in reward parameters, supporting gradient methods.",
                "aliases": [
                    "Q* Lipschitz in \u03b8",
                    "small reward changes \u2192 small Q changes"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 45,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "mapping from reward parameters to policies is nonsmooth and redundant",
                "type": "concept",
                "description": "Small changes in reward parameters can leave the optimal policy unchanged, and greedy selection introduces non-differentiabilities.",
                "aliases": [
                    "nonsmooth parameter-policy map",
                    "redundant reward parametrisation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 46,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard gradient descent struggles to find correct reward parameters",
                "type": "concept",
                "description": "Ordinary gradients get stuck or converge slowly due to plateaus and redundancy in the optimisation landscape.",
                "aliases": [
                    "plain gradient inefficiency",
                    "gradient descent failure in IRL"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 107,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "large_error_derivatives_from_large_rewards",
                "type": "concept",
                "description": "Large raw rewards lead to proportionally large TD-errors, which can destabilise gradient descent.",
                "aliases": [
                    "unstable gradients due to reward scale"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1005,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Misalignment between optimiser\u2019s behaviour and true goal",
                "type": "concept",
                "description": "Because metric and goal decouple, the optimiser's actions diverge from intended outcomes.",
                "aliases": [
                    "metric-goal misalignment",
                    "proxy mis-optimisation"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 847,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "behaviour diverges from designer intent",
                "type": "concept",
                "description": "Observable misalignment such as dithering in boat race or sensor spoofing in tomato task.",
                "aliases": [
                    "policy misalignment outcome"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1003,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Imperfect proxy contains additive noise",
                "type": "concept",
                "description": "The chosen metric differs from the true goal by an independent noise term.",
                "aliases": [
                    "noisy metric",
                    "proxy with observational noise"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1004,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Regressional Goodhart divergence between metric and goal",
                "type": "concept",
                "description": "Optimising a noisy proxy selects for noise, so at high proxy values the goal systematically lags.",
                "aliases": [
                    "tails come apart",
                    "regressional Goodhart"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1014,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Adversarial Goodhart via metric manipulation / Cobra effect",
                "type": "concept",
                "description": "The strategic agent\u2019s behaviour causes the proxy to lose validity, often increasing proxy without improving \u2013 or even harming \u2013 the true goal.",
                "aliases": [
                    "Campbell\u2019s Law dynamics",
                    "cobra effect"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1008,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Extremal Goodhart breakdown of proxy-goal correlation",
                "type": "concept",
                "description": "In the new region the learned relationship no longer holds, so metric stops reflecting goal.",
                "aliases": [
                    "extremal Goodhart",
                    "out-of-sample collapse"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1011,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Causal Goodhart collapse of metric-goal link",
                "type": "concept",
                "description": "Because of the intervention, the causal path ensuring metric tracks goal is disrupted, eliminating correlation.",
                "aliases": [
                    "causal Goodhart",
                    "causal pathway break"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1006,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Ensemble-proxy optimisation with correlation monitoring and optimisation throttling",
                "type": "intervention",
                "description": "Use several independent proxy metrics, monitor their pairwise correlations with validation estimates of the goal, and automatically reduce learning rate or stop optimisation when correlations drop.",
                "aliases": [
                    "multi-metric ensemble regularisation",
                    "correlation-aware throttling"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1007,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Selection pushes proxy into unseen region",
                "type": "concept",
                "description": "Strong optimisation pressure drives the system to states outside the data range where metric-goal relationship was learned.",
                "aliases": [
                    "optimisation moves out-of-sample",
                    "proxy extremal region"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1008,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Extremal Goodhart breakdown of proxy-goal correlation",
                "type": "concept",
                "description": "In the new region the learned relationship no longer holds, so metric stops reflecting goal.",
                "aliases": [
                    "extremal Goodhart",
                    "out-of-sample collapse"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1006,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Ensemble-proxy optimisation with correlation monitoring and optimisation throttling",
                "type": "intervention",
                "description": "Use several independent proxy metrics, monitor their pairwise correlations with validation estimates of the goal, and automatically reduce learning rate or stop optimisation when correlations drop.",
                "aliases": [
                    "multi-metric ensemble regularisation",
                    "correlation-aware throttling"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 47,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "feature scaling mismatch in IRL causes policy performance degradation",
                "type": "concept",
                "description": "If the learner's feature scales differ from the true scales, max-margin IRL can output policies with large reward loss.",
                "aliases": [
                    "scale sensitivity problem",
                    "mis-scaled features hurt IRL"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 93,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "filter RMS clipping to radius 0.1 during training",
                "type": "intervention",
                "description": "After each update, renormalise any convolutional filter whose RMS exceeds 1e-1 back to that value to prevent domination.",
                "aliases": [
                    "per-filter norm clipping",
                    "RMS renormalisation of filters"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 188,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "apply gradient clipping with threshold 5 during training",
                "type": "intervention",
                "description": "Compute gradient norm each batch; if >5, scale gradients so norm equals 5 before optimizer step.",
                "aliases": [
                    "clip gradients at 5",
                    "clipping intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1409_3215",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 187,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "gradient norm clipping limits gradient magnitude",
                "type": "concept",
                "description": "Scaling gradients when their L2 norm exceeds a threshold prevents explosion and stabilizes updates.",
                "aliases": [
                    "gradient clipping technique",
                    "norm thresholding"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1409_3215",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 518,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Reduced model sensitivity to confound features",
                "type": "concept",
                "description": "After training with the gradient penalty, saliency maps show near-zero gradients on confounding inputs.",
                "aliases": [
                    "suppressed confound gradients"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1703_03717",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1183,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement locally constrained dynamic routing with shared transformations in capsule segmentation models",
                "type": "intervention",
                "description": "Restrict routing to a spatial window and share transformation matrices across spatial positions within each capsule type to cut parameters and memory.",
                "aliases": [
                    "locally-constrained routing",
                    "shared-matrix capsule routing"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1804_04241",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1187,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Adopt deconvolutional capsule layers to enable large-input segmentation",
                "type": "intervention",
                "description": "Introduce capsule layers with transposed convolutions to up-sample feature maps while preserving capsule semantics, supporting 512\u00d7512 inputs.",
                "aliases": [
                    "deconvolutional capsules",
                    "transposed-conv capsules"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1804_04241",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1190,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Conduct capsule-dimension visualisation inspection before deployment",
                "type": "intervention",
                "description": "Generate heat-maps for individual capsule dimensions and review them to detect anomalous or unintended feature usage prior to release.",
                "aliases": [
                    "capsule attribute auditing",
                    "capsule vector visualisation check"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1804_04241",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1180,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Capsule vectors encode spatial orientation and magnitude",
                "type": "concept",
                "description": "Capsule units output vectors whose dimensions jointly represent pose, prevalence and other attributes of detected entities.",
                "aliases": [
                    "capsule pose vectors",
                    "vectorised neuron outputs"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1804_04241",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 654,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Train reward predictor from human pairwise clip comparisons and use it as proxy reward during RL",
                "type": "intervention",
                "description": "Fit a neural network to human clip-level preference data and supply its output as the reward signal for policy optimisation.",
                "aliases": [
                    "preference-based reward model",
                    "learned reward proxy"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 303,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "total & average regret unreliable under unbounded delays",
                "type": "concept",
                "description": "Standard online-learning metrics fail because they can be dominated by long stretches without feedback.",
                "aliases": [
                    "regret failure with delays",
                    "inadequacy of regret metrics"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1604_05280",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 46,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard gradient descent struggles to find correct reward parameters",
                "type": "concept",
                "description": "Ordinary gradients get stuck or converge slowly due to plateaus and redundancy in the optimisation landscape.",
                "aliases": [
                    "plain gradient inefficiency",
                    "gradient descent failure in IRL"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 936,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "improved learning with skip-enabled queries",
                "type": "concept",
                "description": "Allowing skips reduces noise and leads to higher probability on the true reward weights.",
                "aliases": [
                    "better performance with skip",
                    "skip option benefit"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1802_01604",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 45,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "mapping from reward parameters to policies is nonsmooth and redundant",
                "type": "concept",
                "description": "Small changes in reward parameters can leave the optimal policy unchanged, and greedy selection introduces non-differentiabilities.",
                "aliases": [
                    "nonsmooth parameter-policy map",
                    "redundant reward parametrisation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1013,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Presence of strategic agent with misaligned objective",
                "type": "concept",
                "description": "Another optimiser pursues its own goal and can manipulate or respond to the proxy metric.",
                "aliases": [
                    "adversarial agent",
                    "misaligned actor"
                ],
                "concept_category": "Threat",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1003,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Imperfect proxy contains additive noise",
                "type": "concept",
                "description": "The chosen metric differs from the true goal by an independent noise term.",
                "aliases": [
                    "noisy metric",
                    "proxy with observational noise"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 847,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "behaviour diverges from designer intent",
                "type": "concept",
                "description": "Observable misalignment such as dithering in boat race or sensor spoofing in tomato task.",
                "aliases": [
                    "policy misalignment outcome"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1004,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Regressional Goodhart divergence between metric and goal",
                "type": "concept",
                "description": "Optimising a noisy proxy selects for noise, so at high proxy values the goal systematically lags.",
                "aliases": [
                    "tails come apart",
                    "regressional Goodhart"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 50,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "natural gradient IRL improves reliability and sample efficiency",
                "type": "concept",
                "description": "Empirical results show lower policy error and robustness to feature perturbations compared to max-margin and plain gradient methods.",
                "aliases": [
                    "performance gains of natural gradients",
                    "empirical success of NG-IRL"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 47,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "feature scaling mismatch in IRL causes policy performance degradation",
                "type": "concept",
                "description": "If the learner's feature scales differ from the true scales, max-margin IRL can output policies with large reward loss.",
                "aliases": [
                    "scale sensitivity problem",
                    "mis-scaled features hurt IRL"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 46,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard gradient descent struggles to find correct reward parameters",
                "type": "concept",
                "description": "Ordinary gradients get stuck or converge slowly due to plateaus and redundancy in the optimisation landscape.",
                "aliases": [
                    "plain gradient inefficiency",
                    "gradient descent failure in IRL"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 479,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Train networks with proper scoring-rule losses",
                "type": "intervention",
                "description": "During initial training, optimise each neural network with a proper scoring rule such as negative log-likelihood or Brier score to encourage calibrated probabilistic outputs.",
                "aliases": [
                    "proper scoring rule training",
                    "NLL/Brier loss training"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1612_01474",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 703,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "value alignment problem in human-robot collaboration",
                "type": "concept",
                "description": "Robots must infer and respect human objectives; failure leads to unsafe or unhelpful behaviour.",
                "aliases": [
                    "human-robot value misalignment",
                    "objective uncertainty in robots"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1707_06354",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 597,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "humans need to predict robot actions for coordination",
                "type": "concept",
                "description": "Effective teamwork requires that humans can anticipate a robot\u2019s future actions after observing its initial behaviour.",
                "aliases": [
                    "human prediction requirement",
                    "predictability problem"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 458,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "safe-human problem",
                "type": "concept",
                "description": "Thought experiment reducing AGI safety to the unresolved task of guaranteeing a human will always act safely.",
                "aliases": [
                    "SHP",
                    "making a safe person"
                ],
                "concept_category": "Theoretical Framework",
                "paper_id": "arxiv__arxiv_org_abs_1610_07997",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 357,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Need to validate algorithms on real-world hardware",
                "type": "concept",
                "description": "Simulations may not capture real-world complexities, so RL algorithms should be tested on actual robotic systems.",
                "aliases": [
                    "simulation-to-reality gap",
                    "robotic validation need"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1069,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "add replication-regularisation term with dynamic weighting during fine-tuning",
                "type": "intervention",
                "description": "While adapting a pretrained model, include LSR as a regulariser with automatically adjusted weight to maintain replication accuracy.",
                "aliases": [
                    "adaptive LSR regulariser",
                    "replication-preserving fine-tuning"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_05859",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 221,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "maintain exponential moving average of model parameters during training",
                "type": "intervention",
                "description": "Add an extra update \u00af\u03b8_t = \u03b22\u00b7\u00af\u03b8_{t-1} + (1\u2212\u03b22)\u00b7\u03b8_t (with bias correction) and use the averaged weights for evaluation/deployment.",
                "aliases": [
                    "EMA of weights",
                    "parameter averaging intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1412_6980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 779,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Maintain feedback replay buffer for minibatch SGD every 10 time steps",
                "type": "intervention",
                "description": "Store all (experience, feedback) pairs and resample them so the reward model is updated ten times as often as new feedback arrives.",
                "aliases": [
                    "feedback buffer updates",
                    "replay human critiques"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 835,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Successful learning of sparse-reward peg-insertion task",
                "type": "concept",
                "description": "Achievement of high success rate on peg-insertion environment that baseline methods fail to solve.",
                "aliases": [
                    "peg insertion success"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1711_06782",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 374,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need for efficient reward learning under limited supervision",
                "type": "concept",
                "description": "Requirement to generalise from few high-quality reward samples to many unlabeled episodes.",
                "aliases": [
                    "oversight efficiency challenge"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 708,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "pragmatic interpretation improves task success rate to 86% vs 15%",
                "type": "concept",
                "description": "Simulation in a cooking task shows pragmatic robots achieve the desired recipe far more often than literal robots.",
                "aliases": [
                    "86% success with pragmatic robot",
                    "empirical gain from pragmatic interpretation"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1707_06354",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 519,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Restored / improved generalization accuracy",
                "type": "concept",
                "description": "Model performance on de-confounded or out-of-distribution test sets matches baseline performance on clean data.",
                "aliases": [
                    "recovered test accuracy"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1703_03717",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 307,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "implement EvOp-style independent subsequence scoring among ensemble models during long-horizon alignment training",
                "type": "intervention",
                "description": "During evaluation of models whose true feedback may be greatly delayed (e.g., in RLHF or autonomous-agent testing), compute losses only on independent subsequences to choose which model to deploy.",
                "aliases": [
                    "EvOp-style scoring intervention",
                    "sparse subsequence model selection"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1604_05280",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 309,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "lack of near-optimal expert prevents EvOp convergence speed",
                "type": "concept",
                "description": "If no expert closely matches Bayes-optimal predictions, EvOp\u2019s performance can remain poor for long periods.",
                "aliases": [
                    "missing good expert slows EvOp",
                    "suboptimal expert set issue"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1604_05280",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 963,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "small degradation from prediction errors",
                "type": "concept",
                "description": "Prediction errors only modestly reduce objective and subjective performance relative to perfect oracle inference.",
                "aliases": [
                    "robust performance despite errors"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1802_01780",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 79,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard softmax requires O(W) computation",
                "type": "concept",
                "description": "Baseline observation that computing gradients over the full vocabulary is linear in vocab size.",
                "aliases": [
                    "full softmax cost",
                    "softmax inefficiency"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1050,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Negative rewards break proportionality between scanning density and reward density",
                "type": "concept",
                "description": "If some rewards are \u22640 the theoretical link between walker distribution and reward density cannot hold, undermining FMC\u2019s optimality criterion.",
                "aliases": [
                    "broken density proportionality"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 683,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reward shaping penalties alone fail to eliminate catastrophes",
                "type": "concept",
                "description": "Giving large negative rewards for catastrophes without blocking does not drive the catastrophe rate to zero.",
                "aliases": [
                    "negative reward insufficient",
                    "penalties don't solve safety"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 185,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "apply input sequence reversal during training for seq2seq tasks",
                "type": "intervention",
                "description": "Before training, reverse the token order of each source sequence to shorten dependency paths and improve learning robustness.",
                "aliases": [
                    "reverse-source pretraining",
                    "sequence reversal intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1409_3215",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 171,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Complementary dense + multi-crop multi-scale evaluation reduces error",
                "type": "concept",
                "description": "Averaging predictions from dense evaluation and 50 crops across three scales lowers top-5 error by ~0.4 %.",
                "aliases": [
                    "dense and crop evaluation complementary"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1409_1556",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1006,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Ensemble-proxy optimisation with correlation monitoring and optimisation throttling",
                "type": "intervention",
                "description": "Use several independent proxy metrics, monitor their pairwise correlations with validation estimates of the goal, and automatically reduce learning rate or stop optimisation when correlations drop.",
                "aliases": [
                    "multi-metric ensemble regularisation",
                    "correlation-aware throttling"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1008,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Extremal Goodhart breakdown of proxy-goal correlation",
                "type": "concept",
                "description": "In the new region the learned relationship no longer holds, so metric stops reflecting goal.",
                "aliases": [
                    "extremal Goodhart",
                    "out-of-sample collapse"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1168,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Overfitting to specific opponent",
                "type": "concept",
                "description": "Policies optimise for one partner and perform poorly with others.",
                "aliases": [
                    "partner overfitting",
                    "lack of generalisation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1804_03980",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1050,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Negative rewards break proportionality between scanning density and reward density",
                "type": "concept",
                "description": "If some rewards are \u22640 the theoretical link between walker distribution and reward density cannot hold, undermining FMC\u2019s optimality criterion.",
                "aliases": [
                    "broken density proportionality"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 592,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "integrate best-response human model into motion planner",
                "type": "intervention",
                "description": "Replace obstacle-based planners with ones that solve a nested optimisation: choose robot actions whose predicted human best response maximises the robot\u2019s objective.",
                "aliases": [
                    "best-response planner deployment",
                    "coordinative planning intervention"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 597,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "humans need to predict robot actions for coordination",
                "type": "concept",
                "description": "Effective teamwork requires that humans can anticipate a robot\u2019s future actions after observing its initial behaviour.",
                "aliases": [
                    "human prediction requirement",
                    "predictability problem"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 458,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "safe-human problem",
                "type": "concept",
                "description": "Thought experiment reducing AGI safety to the unresolved task of guaranteeing a human will always act safely.",
                "aliases": [
                    "SHP",
                    "making a safe person"
                ],
                "concept_category": "Theoretical Framework",
                "paper_id": "arxiv__arxiv_org_abs_1610_07997",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 357,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Need to validate algorithms on real-world hardware",
                "type": "concept",
                "description": "Simulations may not capture real-world complexities, so RL algorithms should be tested on actual robotic systems.",
                "aliases": [
                    "simulation-to-reality gap",
                    "robotic validation need"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 691,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "low threshold blocking strategy minimizes false negatives",
                "type": "concept",
                "description": "Initially setting a very conservative classifier threshold blocks many actions but virtually eliminates false negatives; threshold can be relaxed later.",
                "aliases": [
                    "aggressive blocking threshold",
                    "threshold tuning"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1024,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "credibility-threshold rejection mechanism",
                "type": "intervention",
                "description": "Reject or flag any prediction whose credibility falls below a preset threshold, treating it as potentially adversarial or out-of-distribution.",
                "aliases": [
                    "credibility-based abstention",
                    "credibility flagging"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1803_04765",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 187,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "gradient norm clipping limits gradient magnitude",
                "type": "concept",
                "description": "Scaling gradients when their L2 norm exceeds a threshold prevents explosion and stabilizes updates.",
                "aliases": [
                    "gradient clipping technique",
                    "norm thresholding"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1409_3215",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 861,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Objective-reward mis-specification causing unethical RL behaviour",
                "type": "concept",
                "description": "Providing an objective function that ignores ethical considerations can drive RL agents toward harmful strategies that still maximise the given reward.",
                "aliases": [
                    "reward misspecification leads to unethical acts",
                    "unethical behaviour from faulty reward"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1119,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "improved few-shot classification compared with VAE and random init",
                "type": "concept",
                "description": "Empirical result that the learned update rule yields higher accuracy on MNIST, Fashion-MNIST, etc., than baseline unsupervised methods.",
                "aliases": [
                    "better few-shot accuracy",
                    "representation gains"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1804_00222",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 424,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Improved robustness to OOV tokens",
                "type": "concept",
                "description": "Model maintains high translation quality on sentences containing previously unseen words by composing them from known sub-words.",
                "aliases": [
                    "rare-word robustness",
                    "subword OOV handling"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1609_08144",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1026,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "DkNN recovers correct labels on some adversarial examples",
                "type": "concept",
                "description": "Layer consistency lets DkNN classify a fraction of adversarial inputs correctly, raising accuracy compared to the underlying DNN.",
                "aliases": [
                    "partial robustness of DkNN",
                    "adversarial recovery"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04765",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1125,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "permutation-invariant meta-training via feature dimension shuffling",
                "type": "concept",
                "description": "During meta-training, input features are randomly permuted to force the update rule to rely on statistical structure rather than pixel positions.",
                "aliases": [
                    "feature permutation during meta-training",
                    "shuffle inputs"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1804_00222",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 866,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Collect diverse human behaviour data for ethics shaping",
                "type": "intervention",
                "description": "Assemble a corpus of state\u2013action pairs from humans performing varied, possibly unrelated tasks in the same environment to approximate an ethical prior.",
                "aliases": [
                    "gather ordinary human trajectories",
                    "human data collection step"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 656,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Collect human feedback online during training",
                "type": "intervention",
                "description": "Interleave new preference queries with policy learning so the reward model stays aligned with the evolving state distribution.",
                "aliases": [
                    "online preference queries",
                    "continual human oversight"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 537,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Pre-deployment causal abduction\u2013action\u2013prediction evaluation procedure",
                "type": "intervention",
                "description": "Before releasing a model, estimate background variables, intervene on A, and compare prediction distributions to confirm counterfactual invariance.",
                "aliases": [
                    "CF audit via twin network",
                    "counterfactual simulation test"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1703_06856",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 358,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Integrating Gym API with robotic hardware",
                "type": "intervention",
                "description": "Extend the Gym interface to control physical robots, enabling direct real-world evaluation.",
                "aliases": [
                    "Gym-robot bridge",
                    "real-world Gym interface"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 794,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "multi-principal alignment with differing priors",
                "type": "concept",
                "description": "Situation where a single AI agent must serve several principals whose prior probability distributions over the environment diverge.",
                "aliases": [
                    "multiple principals with different beliefs",
                    "multi-stakeholder differing priors problem"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1711_00363",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 309,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "lack of near-optimal expert prevents EvOp convergence speed",
                "type": "concept",
                "description": "If no expert closely matches Bayes-optimal predictions, EvOp\u2019s performance can remain poor for long periods.",
                "aliases": [
                    "missing good expert slows EvOp",
                    "suboptimal expert set issue"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1604_05280",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 963,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "small degradation from prediction errors",
                "type": "concept",
                "description": "Prediction errors only modestly reduce objective and subjective performance relative to perfect oracle inference.",
                "aliases": [
                    "robust performance despite errors"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1802_01780",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 79,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard softmax requires O(W) computation",
                "type": "concept",
                "description": "Baseline observation that computing gradients over the full vocabulary is linear in vocab size.",
                "aliases": [
                    "full softmax cost",
                    "softmax inefficiency"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1147,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "codistillation tolerant to stale peer predictions",
                "type": "concept",
                "description": "Finding that using predictions tens of thousands of updates old does not harm convergence, allowing infrequent parameter exchange.",
                "aliases": [
                    "stale-prediction robustness",
                    "delay-tolerant codistillation"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1804_03235",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 303,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "total & average regret unreliable under unbounded delays",
                "type": "concept",
                "description": "Standard online-learning metrics fail because they can be dominated by long stretches without feedback.",
                "aliases": [
                    "regret failure with delays",
                    "inadequacy of regret metrics"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1604_05280",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 936,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "improved learning with skip-enabled queries",
                "type": "concept",
                "description": "Allowing skips reduces noise and leads to higher probability on the true reward weights.",
                "aliases": [
                    "better performance with skip",
                    "skip option benefit"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1802_01604",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1003,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Imperfect proxy contains additive noise",
                "type": "concept",
                "description": "The chosen metric differs from the true goal by an independent noise term.",
                "aliases": [
                    "noisy metric",
                    "proxy with observational noise"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 85,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "audit embeddings using vector arithmetic to detect unintended associations",
                "type": "intervention",
                "description": "Pre-deployment test that searches for harmful or biased analogies by systematic vector offset queries.",
                "aliases": [
                    "vector-analogy safety probe",
                    "embedding offset auditing"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 71,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling with k=5-20 and unigram^(3/4) noise yields high-quality vectors",
                "type": "concept",
                "description": "Empirical result: these hyper-parameters outperform alternatives on analogy benchmarks.",
                "aliases": [
                    "NEG high analogy accuracy",
                    "NEG optimal hyperparams"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 667,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "conduct mandatory attention-map visualisation audits with diverse red-team prompts pre-deployment",
                "type": "intervention",
                "description": "Prior to deployment, run a battery of adversarial and benign prompts, visualise attention heads, and have human reviewers flag suspicious correlations or biases.",
                "aliases": [
                    "attention audit before release",
                    "visual interpretability safety check"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1706_03762",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 541,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "misguided policy or safety decisions",
                "type": "concept",
                "description": "Regulatory or strategic choices are made on the basis of incorrect capability assessments.",
                "aliases": [
                    "faulty governance outcome",
                    "policy risk"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1703_10987",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1124,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "including cross-domain datasets in meta-training pipeline",
                "type": "intervention",
                "description": "Augment the meta-training task distribution with tasks from text, audio, and other modalities to prevent domain-specific overfitting.",
                "aliases": [
                    "add text/audio datasets",
                    "cross-modal meta-training"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1804_00222",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 91,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "integrate deconvnet visualisation into model-development workflow",
                "type": "intervention",
                "description": "Include real-time deconvolutional projections at multiple checkpoints so researchers can inspect and act on feature behaviour before release.",
                "aliases": [
                    "routine deconv visual audits",
                    "interpretability toolchain integration"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 100,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "monitor feature-map visualisations over epochs to decide training termination",
                "type": "intervention",
                "description": "Track qualitative stability of upper-layer projections and continue training until no major changes occur, preventing premature stopping.",
                "aliases": [
                    "visual convergence monitoring",
                    "feature evolution checkpointing"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 665,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "multi-head attention produces interpretable head-specific patterns",
                "type": "concept",
                "description": "Visualisations show that different attention heads focus on syntactic relations, anaphora, etc., providing human-readable insights.",
                "aliases": [
                    "attention head specialisation",
                    "interpretable attention"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1706_03762",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 936,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "improved learning with skip-enabled queries",
                "type": "concept",
                "description": "Allowing skips reduces noise and leads to higher probability on the true reward weights.",
                "aliases": [
                    "better performance with skip",
                    "skip option benefit"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1802_01604",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 374,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need for efficient reward learning under limited supervision",
                "type": "concept",
                "description": "Requirement to generalise from few high-quality reward samples to many unlabeled episodes.",
                "aliases": [
                    "oversight efficiency challenge"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 708,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "pragmatic interpretation improves task success rate to 86% vs 15%",
                "type": "concept",
                "description": "Simulation in a cooking task shows pragmatic robots achieve the desired recipe far more often than literal robots.",
                "aliases": [
                    "86% success with pragmatic robot",
                    "empirical gain from pragmatic interpretation"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1707_06354",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 519,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Restored / improved generalization accuracy",
                "type": "concept",
                "description": "Model performance on de-confounded or out-of-distribution test sets matches baseline performance on clean data.",
                "aliases": [
                    "recovered test accuracy"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1703_03717",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 367,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "objective function focusing solely on primary task variables",
                "type": "concept",
                "description": "Designs that optimize only a small subset of environmental variables, ignoring broader context.",
                "aliases": [
                    "narrow task objective",
                    "single-aspect reward"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 45,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "mapping from reward parameters to policies is nonsmooth and redundant",
                "type": "concept",
                "description": "Small changes in reward parameters can leave the optimal policy unchanged, and greedy selection introduces non-differentiabilities.",
                "aliases": [
                    "nonsmooth parameter-policy map",
                    "redundant reward parametrisation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 383,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "absence of novelty-driven behavior",
                "type": "concept",
                "description": "The agent no longer seeks novel states or information.",
                "aliases": [
                    "loss of exploration",
                    "no novelty incentive"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1606_07092",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 898,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Need to transition from reward R to R\u2032 without early optimisation",
                "type": "concept",
                "description": "Desire for an agent to maximise R up to time t and then R\u2032 after t without manipulating plans before the switch.",
                "aliases": [
                    "seamless transition problem",
                    "reward switch incentive issue"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1712_06365",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 419,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "MinEP improves exploration and regret in multi-armed bandits",
                "type": "concept",
                "description": "Experiments show MinEP attains lower exploration potential and competitive or better regret than several standard algorithms in 4-arm Bernoulli bandits.",
                "aliases": [
                    "MinEP empirical benefit",
                    "EP baseline comparison result"
                ],
                "concept_category": "Evidence",
                "paper_id": "arxiv__arxiv_org_abs_1609_04994",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 45,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "mapping from reward parameters to policies is nonsmooth and redundant",
                "type": "concept",
                "description": "Small changes in reward parameters can leave the optimal policy unchanged, and greedy selection introduces non-differentiabilities.",
                "aliases": [
                    "nonsmooth parameter-policy map",
                    "redundant reward parametrisation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 46,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard gradient descent struggles to find correct reward parameters",
                "type": "concept",
                "description": "Ordinary gradients get stuck or converge slowly due to plateaus and redundancy in the optimisation landscape.",
                "aliases": [
                    "plain gradient inefficiency",
                    "gradient descent failure in IRL"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 491,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Pareto-optimal policy recursion with dynamic weights",
                "type": "concept",
                "description": "Backward recursion showing that optimal policies re-weight principals\u2019 utilities by likelihood of observations at every step.",
                "aliases": [
                    "priority-shifting recursion",
                    "likelihood-weighted recursion"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1701_01302",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 842,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reversibility regulariser",
                "type": "intervention",
                "description": "Augment reward with negative term proportional to estimated cost of returning to baseline state.",
                "aliases": [
                    "undo-cost penalty",
                    "irreversibility penalty"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1050,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Negative rewards break proportionality between scanning density and reward density",
                "type": "concept",
                "description": "If some rewards are \u22640 the theoretical link between walker distribution and reward density cannot hold, undermining FMC\u2019s optimality criterion.",
                "aliases": [
                    "broken density proportionality"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 871,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Need for value alignment in AI systems",
                "type": "concept",
                "description": "The problem of ensuring that AI objectives are compatible with human values.",
                "aliases": [
                    "value alignment problem",
                    "goal alignment necessity"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1712_04307",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 597,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "humans need to predict robot actions for coordination",
                "type": "concept",
                "description": "Effective teamwork requires that humans can anticipate a robot\u2019s future actions after observing its initial behaviour.",
                "aliases": [
                    "human prediction requirement",
                    "predictability problem"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 357,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Need to validate algorithms on real-world hardware",
                "type": "concept",
                "description": "Simulations may not capture real-world complexities, so RL algorithms should be tested on actual robotic systems.",
                "aliases": [
                    "simulation-to-reality gap",
                    "robotic validation need"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 458,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "safe-human problem",
                "type": "concept",
                "description": "Thought experiment reducing AGI safety to the unresolved task of guaranteeing a human will always act safely.",
                "aliases": [
                    "SHP",
                    "making a safe person"
                ],
                "concept_category": "Theoretical Framework",
                "paper_id": "arxiv__arxiv_org_abs_1610_07997",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 789,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Train AI to model subjunctive dependencies and adopt FDT strategies",
                "type": "intervention",
                "description": "During alignment or fine-tuning phases expose models to scenarios that reward FDT-consistent decisions and incorporate logical dependency reasoning modules.",
                "aliases": [
                    "FDT-aligned training curriculum"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1710_05060",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 708,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "pragmatic interpretation improves task success rate to 86% vs 15%",
                "type": "concept",
                "description": "Simulation in a cooking task shows pragmatic robots achieve the desired recipe far more often than literal robots.",
                "aliases": [
                    "86% success with pragmatic robot",
                    "empirical gain from pragmatic interpretation"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1707_06354",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 374,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need for efficient reward learning under limited supervision",
                "type": "concept",
                "description": "Requirement to generalise from few high-quality reward samples to many unlabeled episodes.",
                "aliases": [
                    "oversight efficiency challenge"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 386,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "novelty search improves exploration",
                "type": "concept",
                "description": "Experiments show that novelty-based search strategies outperform objective-based ones in discovering diverse solutions.",
                "aliases": [
                    "novelty search empirical result",
                    "objective-free exploration success"
                ],
                "concept_category": "Evidence",
                "paper_id": "arxiv__arxiv_org_abs_1606_07092",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 211,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "adaptive learning rates per parameter improve convergence",
                "type": "concept",
                "description": "Scaling each parameter update by statistics of its own past gradients accelerates and stabilises optimisation.",
                "aliases": [
                    "per-parameter step size adaptation",
                    "adaptive lr principle"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1412_6980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 46,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard gradient descent struggles to find correct reward parameters",
                "type": "concept",
                "description": "Ordinary gradients get stuck or converge slowly due to plateaus and redundancy in the optimisation landscape.",
                "aliases": [
                    "plain gradient inefficiency",
                    "gradient descent failure in IRL"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 47,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "feature scaling mismatch in IRL causes policy performance degradation",
                "type": "concept",
                "description": "If the learner's feature scales differ from the true scales, max-margin IRL can output policies with large reward loss.",
                "aliases": [
                    "scale sensitivity problem",
                    "mis-scaled features hurt IRL"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 45,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "mapping from reward parameters to policies is nonsmooth and redundant",
                "type": "concept",
                "description": "Small changes in reward parameters can leave the optimal policy unchanged, and greedy selection introduces non-differentiabilities.",
                "aliases": [
                    "nonsmooth parameter-policy map",
                    "redundant reward parametrisation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 779,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Maintain feedback replay buffer for minibatch SGD every 10 time steps",
                "type": "intervention",
                "description": "Store all (experience, feedback) pairs and resample them so the reward model is updated ten times as often as new feedback arrives.",
                "aliases": [
                    "feedback buffer updates",
                    "replay human critiques"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 798,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "dynamic utility weight-shifting objective function during policy optimisation",
                "type": "intervention",
                "description": "Define the agent\u2019s optimisation target as the sum of each principal\u2019s expected utility weighted by the likelihood of the agent\u2019s observation sequence under that principal\u2019s prior, updating weights at every timestep.",
                "aliases": [
                    "implement dynamic weighting",
                    "time-varying utility weights objective"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1711_00363",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 221,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "maintain exponential moving average of model parameters during training",
                "type": "intervention",
                "description": "Add an extra update \u00af\u03b8_t = \u03b22\u00b7\u00af\u03b8_{t-1} + (1\u2212\u03b22)\u00b7\u03b8_t (with bias correction) and use the averaged weights for evaluation/deployment.",
                "aliases": [
                    "EMA of weights",
                    "parameter averaging intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1412_6980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 779,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Maintain feedback replay buffer for minibatch SGD every 10 time steps",
                "type": "intervention",
                "description": "Store all (experience, feedback) pairs and resample them so the reward model is updated ten times as often as new feedback arrives.",
                "aliases": [
                    "feedback buffer updates",
                    "replay human critiques"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "training_instability_and_divergence_in_q_networks",
                "type": "concept",
                "description": "Non-linear function approximators combined with off-policy updates can diverge or oscillate during RL training.",
                "aliases": [
                    "weight divergence",
                    "unstable learning dynamics"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 223,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "suboptimal utility under SCDT",
                "type": "concept",
                "description": "Sequential Causal Decision Theory can lead the agent to choose actions that yield lower expected utility when predictors are present.",
                "aliases": [
                    "utility loss with causal decision theory",
                    "SCDT loses in Newcomb"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1506_07359",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 963,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "small degradation from prediction errors",
                "type": "concept",
                "description": "Prediction errors only modestly reduce objective and subjective performance relative to perfect oracle inference.",
                "aliases": [
                    "robust performance despite errors"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1802_01780",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 525,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Statistical fairness criteria can increase discrimination under certain causal relations",
                "type": "concept",
                "description": "When protected attribute causally affects other features, enforcing certain independence constraints can perpetuate or exacerbate unfairness.",
                "aliases": [
                    "fairness metrics may worsen bias",
                    "parity criteria amplifying discrimination"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1703_06856",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 79,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard softmax requires O(W) computation",
                "type": "concept",
                "description": "Baseline observation that computing gradients over the full vocabulary is linear in vocab size.",
                "aliases": [
                    "full softmax cost",
                    "softmax inefficiency"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1079,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Adversarial Logit Pairing (add \u03bb \u00b7 L2 loss between clean/adversarial logits during training)",
                "type": "intervention",
                "description": "During training, penalise the L2 distance between logits of each clean sample and its PGD adversarial counterpart with weight \u03bb\u22480.2\u20131.",
                "aliases": [
                    "ALP defence",
                    "logit pairing with adversarial examples"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_06373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1058,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Set dt \u2248 reaction_time/10 and \u03c4 \u2265 worst-case-collision-lead-time when deploying FMC for safety-critical control",
                "type": "intervention",
                "description": "During integration, compute the physical system\u2019s fastest failure mode and configure FMC\u2019s timestep and horizon so that at least one second of warning is represented in the causal cone.",
                "aliases": [
                    "dt_tau_guideline_intervention"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1048,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Configure \u03b1 parameter to \u22641 in VR during forward-planning for environments where irreversible failure is possible",
                "type": "intervention",
                "description": "When implementing FMC, explicitly set the exploitation exponent \u03b1 to a value not exceeding 1 whenever the environment contains fatal or hard-to-recover states.",
                "aliases": [
                    "set safe alpha",
                    "alpha_config_intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1010,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Regulator intervention on shared cause/intermediary/metric",
                "type": "concept",
                "description": "The optimiser acts on a variable in the causal chain connecting metric and goal, or sets the metric itself.",
                "aliases": [
                    "direct intervention on proxy causal graph",
                    "regulator action on causal node"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 364,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Nuclear-norm \u03b2-quantile recovery algorithm",
                "type": "concept",
                "description": "An optimisation that maximises alignment with observed ratings while constraining entries, row sums and nuclear norm to extract a \u03b2-quantile matrix.",
                "aliases": [
                    "low-rank quantile estimation",
                    "\u03b2-quantile matrix solver"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1606_05374",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 668,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "attention-dropout regularises query-key weights",
                "type": "concept",
                "description": "Randomly zeroing attention weights during training acts as a form of regularisation similar to hidden-unit dropout.",
                "aliases": [
                    "dropout on attention weights",
                    "attention weight dropout"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1706_03762",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 187,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "gradient norm clipping limits gradient magnitude",
                "type": "concept",
                "description": "Scaling gradients when their L2 norm exceeds a threshold prevents explosion and stabilizes updates.",
                "aliases": [
                    "gradient clipping technique",
                    "norm thresholding"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1409_3215",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 188,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "apply gradient clipping with threshold 5 during training",
                "type": "intervention",
                "description": "Compute gradient norm each batch; if >5, scale gradients so norm equals 5 before optimizer step.",
                "aliases": [
                    "clip gradients at 5",
                    "clipping intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1409_3215",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 861,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Objective-reward mis-specification causing unethical RL behaviour",
                "type": "concept",
                "description": "Providing an objective function that ignores ethical considerations can drive RL agents toward harmful strategies that still maximise the given reward.",
                "aliases": [
                    "reward misspecification leads to unethical acts",
                    "unethical behaviour from faulty reward"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 132,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "alternate k=1 discriminator update for every generator update",
                "type": "intervention",
                "description": "During stochastic gradient descent, perform exactly one discriminator optimisation step for each generator step to keep the two networks synchronised and mitigate collapse.",
                "aliases": [
                    "1:1 D\u2013G update schedule",
                    "synchronised GAN updates"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1406_2661",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 46,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard gradient descent struggles to find correct reward parameters",
                "type": "concept",
                "description": "Ordinary gradients get stuck or converge slowly due to plateaus and redundancy in the optimisation landscape.",
                "aliases": [
                    "plain gradient inefficiency",
                    "gradient descent failure in IRL"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 107,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "large_error_derivatives_from_large_rewards",
                "type": "concept",
                "description": "Large raw rewards lead to proportionally large TD-errors, which can destabilise gradient descent.",
                "aliases": [
                    "unstable gradients due to reward scale"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 215,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "heterogeneous gradient scales across CNN layers complicate learning-rate tuning",
                "type": "concept",
                "description": "Convolutional and fully-connected layers exhibit gradients of very different magnitudes, making a single global SGD step size sub-optimal.",
                "aliases": [
                    "layer scale mismatch",
                    "CNN lr tuning problem"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1412_6980",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 275,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Represent human agents as probabilistic programs with hyperbolic discounting and belief distributions",
                "type": "intervention",
                "description": "Encode the agent\u2019s planning process as a recursive probabilistic program whose parameters capture utilities, discounting, and belief uncertainty for automated inversion.",
                "aliases": [
                    "probabilistic-program agent model",
                    "PP bias-aware agent"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1512_05832",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 70,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling approximates softmax via logistic regression",
                "type": "concept",
                "description": "Technique replacing full softmax with a binary classification task between data and noise samples.",
                "aliases": [
                    "NEG objective",
                    "noise-contrastive logistic loss"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 996,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "randomisation of test stimuli order during training and evaluation",
                "type": "intervention",
                "description": "Present inputs in random order each episode/run to remove exploitable regularities.",
                "aliases": [
                    "shuffle inputs",
                    "order randomisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 108,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "reward_clipping_to_minus1_1",
                "type": "intervention",
                "description": "Map all positive rewards to +1 and all negative rewards to \u20131 during training.",
                "aliases": [
                    "clip rewards",
                    "bounded reward transformation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 470,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "architectural robustness metrics used as reward",
                "type": "concept",
                "description": "Metrics such as adversarial accuracy, certified robustness or worst-case loss can be substituted for validation accuracy as the optimisation target.",
                "aliases": [
                    "robustness reward in NAS",
                    "adversarial accuracy objective"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1611_01578",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 519,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Restored / improved generalization accuracy",
                "type": "concept",
                "description": "Model performance on de-confounded or out-of-distribution test sets matches baseline performance on clean data.",
                "aliases": [
                    "recovered test accuracy"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1703_03717",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 374,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need for efficient reward learning under limited supervision",
                "type": "concept",
                "description": "Requirement to generalise from few high-quality reward samples to many unlabeled episodes.",
                "aliases": [
                    "oversight efficiency challenge"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 708,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "pragmatic interpretation improves task success rate to 86% vs 15%",
                "type": "concept",
                "description": "Simulation in a cooking task shows pragmatic robots achieve the desired recipe far more often than literal robots.",
                "aliases": [
                    "86% success with pragmatic robot",
                    "empirical gain from pragmatic interpretation"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1707_06354",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 391,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "constraints of time resources intelligence limit accessible fun space",
                "type": "concept",
                "description": "Limited lifespan, resources, and cognitive ability restrict the subset of fun space a mind can reach.",
                "aliases": [
                    "fun space constraints",
                    "access limitations to fun"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1606_07092",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 931,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "information limitation of comparison-only queries",
                "type": "concept",
                "description": "Binary comparisons provide sparse information, leading to slower and less accurate reward inference.",
                "aliases": [
                    "slow learning from comparisons",
                    "limited data from comparisons"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1802_01604",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 349,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Changes in environment definitions can make previous results incomparable",
                "type": "concept",
                "description": "If an environment\u2019s physics or reward changes, earlier published scores no longer reflect performance on the new environment.",
                "aliases": [
                    "environment drift problem",
                    "benchmark evolution risk"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 46,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard gradient descent struggles to find correct reward parameters",
                "type": "concept",
                "description": "Ordinary gradients get stuck or converge slowly due to plateaus and redundancy in the optimisation landscape.",
                "aliases": [
                    "plain gradient inefficiency",
                    "gradient descent failure in IRL"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 195,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "monitor encoded sentence vectors for harmful content via similarity",
                "type": "intervention",
                "description": "Compute cosine similarity between new sentence embeddings and a curated library of harmful clusters; flag or block near matches.",
                "aliases": [
                    "embedding similarity safety monitor",
                    "vector-based harmful content detector"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1409_3215",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 71,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling with k=5-20 and unigram^(3/4) noise yields high-quality vectors",
                "type": "concept",
                "description": "Empirical result: these hyper-parameters outperform alternatives on analogy benchmarks.",
                "aliases": [
                    "NEG high analogy accuracy",
                    "NEG optimal hyperparams"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 667,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "conduct mandatory attention-map visualisation audits with diverse red-team prompts pre-deployment",
                "type": "intervention",
                "description": "Prior to deployment, run a battery of adversarial and benign prompts, visualise attention heads, and have human reviewers flag suspicious correlations or biases.",
                "aliases": [
                    "attention audit before release",
                    "visual interpretability safety check"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1706_03762",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 541,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "misguided policy or safety decisions",
                "type": "concept",
                "description": "Regulatory or strategic choices are made on the basis of incorrect capability assessments.",
                "aliases": [
                    "faulty governance outcome",
                    "policy risk"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1703_10987",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 937,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "active query selection via expected volume removed",
                "type": "intervention",
                "description": "Choose trajectory pairs that maximise the expected reduction in posterior mass over reward parameters.",
                "aliases": [
                    "informative query selection",
                    "volume-removed active learning"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1802_01604",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 374,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need for efficient reward learning under limited supervision",
                "type": "concept",
                "description": "Requirement to generalise from few high-quality reward samples to many unlabeled episodes.",
                "aliases": [
                    "oversight efficiency challenge"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 708,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "pragmatic interpretation improves task success rate to 86% vs 15%",
                "type": "concept",
                "description": "Simulation in a cooking task shows pragmatic robots achieve the desired recipe far more often than literal robots.",
                "aliases": [
                    "86% success with pragmatic robot",
                    "empirical gain from pragmatic interpretation"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1707_06354",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 519,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Restored / improved generalization accuracy",
                "type": "concept",
                "description": "Model performance on de-confounded or out-of-distribution test sets matches baseline performance on clean data.",
                "aliases": [
                    "recovered test accuracy"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1703_03717",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 253,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "training curricula including irreducible-pattern sequences",
                "type": "intervention",
                "description": "Augment language-model pre-training data with encoded irreducible-pattern tasks to encourage internal heuristics that approximate calibrated logical uncertainty.",
                "aliases": [
                    "logical calibration curriculum",
                    "include Benford-like sequences in training"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1510_03370",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 374,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need for efficient reward learning under limited supervision",
                "type": "concept",
                "description": "Requirement to generalise from few high-quality reward samples to many unlabeled episodes.",
                "aliases": [
                    "oversight efficiency challenge"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 708,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "pragmatic interpretation improves task success rate to 86% vs 15%",
                "type": "concept",
                "description": "Simulation in a cooking task shows pragmatic robots achieve the desired recipe far more often than literal robots.",
                "aliases": [
                    "86% success with pragmatic robot",
                    "empirical gain from pragmatic interpretation"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1707_06354",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 386,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "novelty search improves exploration",
                "type": "concept",
                "description": "Experiments show that novelty-based search strategies outperform objective-based ones in discovering diverse solutions.",
                "aliases": [
                    "novelty search empirical result",
                    "objective-free exploration success"
                ],
                "concept_category": "Evidence",
                "paper_id": "arxiv__arxiv_org_abs_1606_07092",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 428,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Add coverage penalty \u03b2\u22480.2 to beam-search score",
                "type": "intervention",
                "description": "Augment beam-search scoring with \u03b2\u00d7\u03a3 log min(\u03a3_j p_i,j, 1) so hypotheses that leave source tokens unattended are penalised.",
                "aliases": [
                    "coverage penalty 0.2",
                    "attention-based coverage cost"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 4,
                "paper_id": "arxiv__arxiv_org_abs_1609_08144",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 817,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Limit answer precision or list size to penalise deviation",
                "type": "intervention",
                "description": "Reduce the granularity or number of allowed outputs so that any attempt to encode extra bits incurs a large scoring-rule penalty.",
                "aliases": [
                    "precision reduction countermeasure",
                    "smaller L or fewer decimals"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1711_05541",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 691,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "low threshold blocking strategy minimizes false negatives",
                "type": "concept",
                "description": "Initially setting a very conservative classifier threshold blocks many actions but virtually eliminates false negatives; threshold can be relaxed later.",
                "aliases": [
                    "aggressive blocking threshold",
                    "threshold tuning"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 746,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "train novice policy as Bayesian NN with dropout at every layer",
                "type": "intervention",
                "description": "During imitation-learning, include dropout in all layers of the novice neural network and maintain dropout during inference to estimate action uncertainty.",
                "aliases": [
                    "dropout-based Bayesian novice",
                    "uncertainty-aware novice training"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1709_06166",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1127,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "random feature-permutation augmentation during meta-training",
                "type": "intervention",
                "description": "Actively permute features (pixels, token order, etc.) in each meta-training task to encourage permutation-invariant representations.",
                "aliases": [
                    "permutation augmentation intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1804_00222",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 424,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Improved robustness to OOV tokens",
                "type": "concept",
                "description": "Model maintains high translation quality on sentences containing previously unseen words by composing them from known sub-words.",
                "aliases": [
                    "rare-word robustness",
                    "subword OOV handling"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1609_08144",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1073,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "deep learning vision models vulnerable to small-norm adversarial perturbations",
                "type": "concept",
                "description": "Standard image classifiers can be fooled by imperceptible L\u221e-bounded perturbations crafted by an adversary.",
                "aliases": [
                    "adversarial vulnerability in vision models",
                    "models misclassify under L_inf attacks"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1803_06373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1026,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "DkNN recovers correct labels on some adversarial examples",
                "type": "concept",
                "description": "Layer consistency lets DkNN classify a fraction of adversarial inputs correctly, raising accuracy compared to the underlying DNN.",
                "aliases": [
                    "partial robustness of DkNN",
                    "adversarial recovery"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04765",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 626,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "biennial expert survey with back-testing",
                "type": "intervention",
                "description": "Repeat the expert survey every two years, record realised milestone outcomes, and statistically back-test which elicitation methods are best, updating protocols accordingly.",
                "aliases": [
                    "periodic forecast updating",
                    "longitudinal timeline survey"
                ],
                "intervention_lifecycle": 6,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1705_08807",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 710,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "conduct pre-deployment evaluation of robot's pragmatic inference accuracy with human users",
                "type": "intervention",
                "description": "Before release, test the robot with real or simulated humans acting pedagogically to measure belief-update accuracy and alignment success.",
                "aliases": [
                    "pragmatic inference testing",
                    "human-in-the-loop evaluation for CIRL robots"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1707_06354",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 537,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Pre-deployment causal abduction\u2013action\u2013prediction evaluation procedure",
                "type": "intervention",
                "description": "Before releasing a model, estimate background variables, intervene on A, and compare prediction distributions to confirm counterfactual invariance.",
                "aliases": [
                    "CF audit via twin network",
                    "counterfactual simulation test"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1703_06856",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 358,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Integrating Gym API with robotic hardware",
                "type": "intervention",
                "description": "Extend the Gym interface to control physical robots, enabling direct real-world evaluation.",
                "aliases": [
                    "Gym-robot bridge",
                    "real-world Gym interface"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 416,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "switch from exploration to greedy exploitation when EP < \u03b5 threshold",
                "type": "intervention",
                "description": "During training, continuously compute EP and stop exploratory actions once EP falls below a predefined small threshold, thereafter following the EP-greedy policy.",
                "aliases": [
                    "EP-based exploration halt",
                    "EP threshold stopping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1609_04994",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1050,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Negative rewards break proportionality between scanning density and reward density",
                "type": "concept",
                "description": "If some rewards are \u22640 the theoretical link between walker distribution and reward density cannot hold, undermining FMC\u2019s optimality criterion.",
                "aliases": [
                    "broken density proportionality"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 20,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "setup cost proportional to alternatives discourages analysis",
                "type": "concept",
                "description": "Higher initial cost when more alternatives exist makes agents favour the current option (status-quo bias).",
                "aliases": [
                    "analysis setup cost",
                    "many-option penalty"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1106_2657",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 309,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "lack of near-optimal expert prevents EvOp convergence speed",
                "type": "concept",
                "description": "If no expert closely matches Bayes-optimal predictions, EvOp\u2019s performance can remain poor for long periods.",
                "aliases": [
                    "missing good expert slows EvOp",
                    "suboptimal expert set issue"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1604_05280",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 79,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard softmax requires O(W) computation",
                "type": "concept",
                "description": "Baseline observation that computing gradients over the full vocabulary is linear in vocab size.",
                "aliases": [
                    "full softmax cost",
                    "softmax inefficiency"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 963,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "small degradation from prediction errors",
                "type": "concept",
                "description": "Prediction errors only modestly reduce objective and subjective performance relative to perfect oracle inference.",
                "aliases": [
                    "robust performance despite errors"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1802_01780",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 110,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "epsilon_greedy_annealing_1_to_0.1",
                "type": "intervention",
                "description": "Start with \u03b5=1 (fully random), anneal linearly to \u03b5=0.1 over the first one million frames, then fix.",
                "aliases": [
                    "\u03b5-greedy exploration schedule"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1050,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Negative rewards break proportionality between scanning density and reward density",
                "type": "concept",
                "description": "If some rewards are \u22640 the theoretical link between walker distribution and reward density cannot hold, undermining FMC\u2019s optimality criterion.",
                "aliases": [
                    "broken density proportionality"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 569,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "universe temperature exponentially declines over cosmic time",
                "type": "concept",
                "description": "\u039bCDM cosmology implies T(t)=T0\u00b7e^{-Ht}, so background temperature drops exponentially.",
                "aliases": [
                    "cosmic cooling",
                    "falling CMB temperature"
                ],
                "concept_category": "Data",
                "paper_id": "arxiv__arxiv_org_abs_1705_03394",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 309,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "lack of near-optimal expert prevents EvOp convergence speed",
                "type": "concept",
                "description": "If no expert closely matches Bayes-optimal predictions, EvOp\u2019s performance can remain poor for long periods.",
                "aliases": [
                    "missing good expert slows EvOp",
                    "suboptimal expert set issue"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1604_05280",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 79,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard softmax requires O(W) computation",
                "type": "concept",
                "description": "Baseline observation that computing gradients over the full vocabulary is linear in vocab size.",
                "aliases": [
                    "full softmax cost",
                    "softmax inefficiency"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 963,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "small degradation from prediction errors",
                "type": "concept",
                "description": "Prediction errors only modestly reduce objective and subjective performance relative to perfect oracle inference.",
                "aliases": [
                    "robust performance despite errors"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1802_01780",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 670,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "apply attention-dropout p=0.1 during training",
                "type": "intervention",
                "description": "During pre-training, set dropout on attention distributions to 0.1 to improve generalisation and reduce spurious correlations.",
                "aliases": [
                    "attention dropout intervention",
                    "p0.1 attention dropout"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1706_03762",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1050,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Negative rewards break proportionality between scanning density and reward density",
                "type": "concept",
                "description": "If some rewards are \u22640 the theoretical link between walker distribution and reward density cannot hold, undermining FMC\u2019s optimality criterion.",
                "aliases": [
                    "broken density proportionality"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1051,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Relativize universal reward reshaping converts rewards to positive, normalised scale",
                "type": "concept",
                "description": "A two-step normalisation (z-score then exponential/log transform) that forces any reward distribution to be strictly positive and roughly unit-scaled.",
                "aliases": [
                    "Relativize transformation",
                    "reward normalisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1046,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Virtual reward balances reward and state-space entropy",
                "type": "concept",
                "description": "The virtual reward VR = R^\u03b1 \u00b7 Dist^\u03b2 combines task reward with distance-based novelty so that walker density becomes proportional to reward density.",
                "aliases": [
                    "VR balances exploration and exploitation",
                    "reward-entropy product"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 164,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Deeper ConvNet architecture using 3\u00d73 filters",
                "type": "concept",
                "description": "Network design that stacks many 3 \u00d7 3 convolutions, reaching 16\u201319 weight layers with reduced parameter count.",
                "aliases": [
                    "3x3 deep architecture",
                    "small-filter deep CNN"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1409_1556",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 165,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Adopt 16\u201319-layer 3\u00d73 ConvNet architecture in vision models",
                "type": "intervention",
                "description": "Specify model architecture with only 3\u00d73 convolutions up to at least 16 weight layers to improve accuracy without excess parameters.",
                "aliases": [
                    "use VGG-style deep CNN",
                    "deploy 19-layer small-filter net"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 4,
                "paper_id": "arxiv__arxiv_org_abs_1409_1556",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 441,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "replace convolution or Inception modules with depth-wise separable convolutions during model design",
                "type": "intervention",
                "description": "Define CNN architectures as stacks of depth-wise separable convolutions instead of full convolutions or multi-branch Inception modules.",
                "aliases": [
                    "use DWSC blocks",
                    "DWSC-based architecture"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1610_02357",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 165,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Adopt 16\u201319-layer 3\u00d73 ConvNet architecture in vision models",
                "type": "intervention",
                "description": "Specify model architecture with only 3\u00d73 convolutions up to at least 16 weight layers to improve accuracy without excess parameters.",
                "aliases": [
                    "use VGG-style deep CNN",
                    "deploy 19-layer small-filter net"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 4,
                "paper_id": "arxiv__arxiv_org_abs_1409_1556",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 164,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Deeper ConvNet architecture using 3\u00d73 filters",
                "type": "concept",
                "description": "Network design that stacks many 3 \u00d7 3 convolutions, reaching 16\u201319 weight layers with reduced parameter count.",
                "aliases": [
                    "3x3 deep architecture",
                    "small-filter deep CNN"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1409_1556",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 441,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "replace convolution or Inception modules with depth-wise separable convolutions during model design",
                "type": "intervention",
                "description": "Define CNN architectures as stacks of depth-wise separable convolutions instead of full convolutions or multi-branch Inception modules.",
                "aliases": [
                    "use DWSC blocks",
                    "DWSC-based architecture"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1610_02357",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1154,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "use low-precision checkpoint exchange plus codistillation",
                "type": "intervention",
                "description": "Transmit quantised (e.g., 8-bit) model checkpoints at multi-step intervals among worker groups and use them solely for distillation losses, instead of transmitting gradients.",
                "aliases": [
                    "privacy-aware codistilled training",
                    "bandwidth-reduced codistillation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1804_03235",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1153,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "codistillation requires infrequent low-precision checkpoint exchange",
                "type": "concept",
                "description": "Claim that because only predictions are needed, workers can share heavily quantised parameter checkpoints only occasionally, reducing bandwidth and leakage.",
                "aliases": [
                    "compressed parameter sharing",
                    "checkpoints not gradients"
                ],
                "concept_category": "Claim",
                "paper_id": "arxiv__arxiv_org_abs_1804_03235",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1148,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "two-way codistillation with checkpoint exchange \u226450 steps",
                "type": "intervention",
                "description": "During pre-training, split workers into two groups that each run synchronous SGD internally and, every \u226450 optimizer steps, load the latest checkpoint from the other group and add a KL loss to match its predictions.",
                "aliases": [
                    "parallel groups codistill",
                    "checkpoint-based codistillation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1804_03235",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 640,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Policy mixing boosts initial obedience while maintaining convergence",
                "type": "concept",
                "description": "Using a decaying obedience mixture provides high obedience in early steps without sacrificing long-term autonomy advantage.",
                "aliases": [
                    "mixture initial safety",
                    "mixing increases O0"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1705_09990",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 646,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Apply policy mixing with decaying obedience probability schedule",
                "type": "intervention",
                "description": "Execute the obedient policy with probability c_n and the IRL policy otherwise, choosing c_n\u21920 to keep early obedience high yet allow long-term optimisation.",
                "aliases": [
                    "decaying obedience mix",
                    "c_n obedience schedule"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1705_09990",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 587,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Bayesian oversight incentive method",
                "type": "concept",
                "description": "A formal method showing that treating orders as Bayesian evidence creates a positive incentive for the robot to comply with human oversight.",
                "aliases": [
                    "posterior update from orders",
                    "incentivising compliance"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 456,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "red-team penetration testing of AI safety layers",
                "type": "intervention",
                "description": "Before deployment, specialised teams attempt to defeat alignment and containment measures using unrestricted tactics.",
                "aliases": [
                    "AI safety red teaming",
                    "adversarial AI pen-testing"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1610_07997",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 980,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Systematic red-teaming plus responsible disclosure for AI models",
                "type": "intervention",
                "description": "Regular adversarial testing of AI systems and confidential reporting of discovered weaknesses to vendors before public release.",
                "aliases": [
                    "AI red team exercises",
                    "AI vulnerability disclosure program"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1802_07228",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 989,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "adversarial evolutionary red-teaming of reward functions before deployment",
                "type": "intervention",
                "description": "Run an evolutionary search over policy or input space to discover reward-function loopholes and patch them prior to release.",
                "aliases": [
                    "evolutionary red team",
                    "objective fuzzing with EA"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 644,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Adopt Maximum Entropy IRL for robustness to \u03b2",
                "type": "intervention",
                "description": "Deploy a Maximum-Entropy (MLE) inverse-reinforcement-learning policy to ensure action robustness under rationality-parameter misspecification.",
                "aliases": [
                    "use MaxEnt IRL",
                    "MLE with \u03b2=1"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1705_09990",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 635,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "MLE policy robust to mis-specified human rationality parameter \u03b2",
                "type": "concept",
                "description": "The action chosen by a maximum-likelihood IRL robot does not change when its assumed \u03b2 differs from the human\u2019s true rationality level.",
                "aliases": [
                    "MLE \u03b2-robust",
                    "MaxEnt robust to beta"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1705_09990",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 636,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Bayesian optimal policy fragile to \u03b2 misspecification",
                "type": "concept",
                "description": "A robot acting on the posterior mean of \u03b8 may change actions when its assumed human rationality level is wrong.",
                "aliases": [
                    "posterior mean \u03b2-fragile",
                    "Bayesian IRL beta risk"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1705_09990",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 926,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "constraint violations during RL exploration",
                "type": "concept",
                "description": "Instances where the agent selects actions that drive safety signals beyond allowed thresholds, leading to potential harm or episode termination.",
                "aliases": [
                    "unsafe actions during learning",
                    "safety breaches"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1801_08757",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 922,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "zero constraint violations during training and deployment",
                "type": "concept",
                "description": "Experimental evidence across four simulated tasks showed that integrating the safety layer prevented any instance of safety signal thresholds being exceeded in all episodes.",
                "aliases": [
                    "constraint-free exploration",
                    "no safety breaches observed"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1801_08757",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 858,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "random exploration violates safety constraint",
                "type": "concept",
                "description": "\u03b5-greedy or Boltzmann moves can enter forbidden states during training.",
                "aliases": [
                    "unsafe exploration"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 232,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Agents exploitable by evidential blackmailers",
                "type": "concept",
                "description": "Systems following EDT can be forced to transfer resources when threatened with informational blackmail.",
                "aliases": [
                    "evidential blackmail exploitability"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1507_01986",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 786,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Predictor blackmail exploiting CDT/EDT",
                "type": "concept",
                "description": "Blackmailers can reliably extract money by conditioning threats on predictions of CDT or EDT agents\u2019 responses.",
                "aliases": [
                    "XOR/Mechanical blackmail setup"
                ],
                "concept_category": "Threat",
                "paper_id": "arxiv__arxiv_org_abs_1710_05060",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 606,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Forecasts can be exploited by gamblers for unbounded profit",
                "type": "concept",
                "description": "Without safeguards, an adversary can construct betting strategies that earn unlimited gains from systematic forecast errors.",
                "aliases": [
                    "exploitability risk",
                    "unbounded betting profit"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1705_04630",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 187,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "gradient norm clipping limits gradient magnitude",
                "type": "concept",
                "description": "Scaling gradients when their L2 norm exceeds a threshold prevents explosion and stabilizes updates.",
                "aliases": [
                    "gradient clipping technique",
                    "norm thresholding"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1409_3215",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 188,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "apply gradient clipping with threshold 5 during training",
                "type": "intervention",
                "description": "Compute gradient norm each batch; if >5, scale gradients so norm equals 5 before optimizer step.",
                "aliases": [
                    "clip gradients at 5",
                    "clipping intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1409_3215",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 186,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "exploding gradients in LSTM training",
                "type": "concept",
                "description": "During back-propagation through time, gradient norms can grow uncontrollably, destabilizing learning.",
                "aliases": [
                    "unstable large gradients",
                    "gradient explosion problem"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1409_3215",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 821,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Irreversible states encountered by forward policy",
                "type": "concept",
                "description": "Environment configurations from which the agent cannot autonomously return to the initial distribution.",
                "aliases": [
                    "unrecoverable states",
                    "states requiring hard reset"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1711_06782",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 826,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Inaccurate identification of irreversible states",
                "type": "concept",
                "description": "Failure to detect states from which reset is impossible, leading to unsafe exploration.",
                "aliases": [
                    "mis-classified unsafe states"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1711_06782",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 377,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "catastrophic irreversible states during exploration",
                "type": "concept",
                "description": "States that permanently damage the agent or environment, preventing task completion.",
                "aliases": [
                    "unrecoverable failures",
                    "exploration catastrophes"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1162,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Fair negotiation outcomes among selfish agents",
                "type": "concept",
                "description": "Self-interested agents reach near-fair allocations approximating human performance when using the proposal channel.",
                "aliases": [
                    "60% joint reward selfish",
                    "balanced item division"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1804_03980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1177,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Selfish agents negotiating with hidden utilities",
                "type": "concept",
                "description": "Each agent maximises its own reward while the other agent\u2019s utilities remain private.",
                "aliases": [
                    "hidden-utility selfish negotiation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1804_03980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 846,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reward loophole exploitation",
                "type": "concept",
                "description": "Agents find unintended strategies that maximise stated reward while violating intent.",
                "aliases": [
                    "reward gaming",
                    "specification gaming"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1177,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Selfish agents negotiating with hidden utilities",
                "type": "concept",
                "description": "Each agent maximises its own reward while the other agent\u2019s utilities remain private.",
                "aliases": [
                    "hidden-utility selfish negotiation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1804_03980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1162,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Fair negotiation outcomes among selfish agents",
                "type": "concept",
                "description": "Self-interested agents reach near-fair allocations approximating human performance when using the proposal channel.",
                "aliases": [
                    "60% joint reward selfish",
                    "balanced item division"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1804_03980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 846,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reward loophole exploitation",
                "type": "concept",
                "description": "Agents find unintended strategies that maximise stated reward while violating intent.",
                "aliases": [
                    "reward gaming",
                    "specification gaming"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1008,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Extremal Goodhart breakdown of proxy-goal correlation",
                "type": "concept",
                "description": "In the new region the learned relationship no longer holds, so metric stops reflecting goal.",
                "aliases": [
                    "extremal Goodhart",
                    "out-of-sample collapse"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1004,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Regressional Goodhart divergence between metric and goal",
                "type": "concept",
                "description": "Optimising a noisy proxy selects for noise, so at high proxy values the goal systematically lags.",
                "aliases": [
                    "tails come apart",
                    "regressional Goodhart"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1006,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Ensemble-proxy optimisation with correlation monitoring and optimisation throttling",
                "type": "intervention",
                "description": "Use several independent proxy metrics, monitor their pairwise correlations with validation estimates of the goal, and automatically reduce learning rate or stop optimisation when correlations drop.",
                "aliases": [
                    "multi-metric ensemble regularisation",
                    "correlation-aware throttling"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 304,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "gambler experts exploiting correlated predictions",
                "type": "concept",
                "description": "Experts that place extreme bets on correlated yet unrevealed outcomes can appear superior for extended periods.",
                "aliases": [
                    "gambler forecasters",
                    "extreme betting experts"
                ],
                "concept_category": "Threat",
                "paper_id": "arxiv__arxiv_org_abs_1604_05280",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 606,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Forecasts can be exploited by gamblers for unbounded profit",
                "type": "concept",
                "description": "Without safeguards, an adversary can construct betting strategies that earn unlimited gains from systematic forecast errors.",
                "aliases": [
                    "exploitability risk",
                    "unbounded betting profit"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1705_04630",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 786,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Predictor blackmail exploiting CDT/EDT",
                "type": "concept",
                "description": "Blackmailers can reliably extract money by conditioning threats on predictions of CDT or EDT agents\u2019 responses.",
                "aliases": [
                    "XOR/Mechanical blackmail setup"
                ],
                "concept_category": "Threat",
                "paper_id": "arxiv__arxiv_org_abs_1710_05060",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1011,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Causal Goodhart collapse of metric-goal link",
                "type": "concept",
                "description": "Because of the intervention, the causal path ensuring metric tracks goal is disrupted, eliminating correlation.",
                "aliases": [
                    "causal Goodhart",
                    "causal pathway break"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1008,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Extremal Goodhart breakdown of proxy-goal correlation",
                "type": "concept",
                "description": "In the new region the learned relationship no longer holds, so metric stops reflecting goal.",
                "aliases": [
                    "extremal Goodhart",
                    "out-of-sample collapse"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1010,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Regulator intervention on shared cause/intermediary/metric",
                "type": "concept",
                "description": "The optimiser acts on a variable in the causal chain connecting metric and goal, or sets the metric itself.",
                "aliases": [
                    "direct intervention on proxy causal graph",
                    "regulator action on causal node"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 276,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Bayesian inference over utilities, beliefs and bias parameters",
                "type": "concept",
                "description": "Compute the posterior distribution P(\u03b8|actions) via enumeration or other inference to jointly recover utilities, discount rates, and belief states.",
                "aliases": [
                    "joint posterior over preferences and biases",
                    "Bayesian inversion of agent program"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1512_05832",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 273,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Accurate recovery of true preferences despite biased action sequences",
                "type": "concept",
                "description": "Including structured deviations enables Bayesian inference to match human-judged preferences even when observed choices are misleading.",
                "aliases": [
                    "correct preference inference under bias",
                    "true utility recovery"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1512_05832",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 275,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Represent human agents as probabilistic programs with hyperbolic discounting and belief distributions",
                "type": "intervention",
                "description": "Encode the agent\u2019s planning process as a recursive probabilistic program whose parameters capture utilities, discounting, and belief uncertainty for automated inversion.",
                "aliases": [
                    "probabilistic-program agent model",
                    "PP bias-aware agent"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1512_05832",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1093,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "macro-intent latent variables encode long-term intent",
                "type": "concept",
                "description": "Low-frequency latent variables can summarise future goals, providing a compact representation of long-term structure in trajectories.",
                "aliases": [
                    "macro-intent principle",
                    "high-level intent representation"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1803_07612",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1094,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "shared macro-intent variables across agents",
                "type": "concept",
                "description": "A single macro-intent is sampled and broadcast to all agents so that their conditional policies remain coordinated.",
                "aliases": [
                    "joint macro-intent coordination",
                    "team-level intent variable"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1803_07612",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1096,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "grounding macro-intents enables controllability",
                "type": "concept",
                "description": "Fixing the macro-intent variable during generation forces agents to follow desired high-level goals.",
                "aliases": [
                    "macro-intent steering",
                    "intent grounding control"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_07612",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 45,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "mapping from reward parameters to policies is nonsmooth and redundant",
                "type": "concept",
                "description": "Small changes in reward parameters can leave the optimal policy unchanged, and greedy selection introduces non-differentiabilities.",
                "aliases": [
                    "nonsmooth parameter-policy map",
                    "redundant reward parametrisation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 51,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Lipschitz continuity of optimal Q w.r.t reward parameters",
                "type": "concept",
                "description": "The paper proves that Q* varies at most linearly with respect to changes in reward parameters, supporting gradient methods.",
                "aliases": [
                    "Q* Lipschitz in \u03b8",
                    "small reward changes \u2192 small Q changes"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 898,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Need to transition from reward R to R\u2032 without early optimisation",
                "type": "concept",
                "description": "Desire for an agent to maximise R up to time t and then R\u2032 after t without manipulating plans before the switch.",
                "aliases": [
                    "seamless transition problem",
                    "reward switch incentive issue"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1712_06365",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 775,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Limited feedback slows parameter learning",
                "type": "concept",
                "description": "Without sufficient labeled data, the reward model\u2019s gradient steps become infrequent, delaying convergence.",
                "aliases": [
                    "few labels",
                    "data starvation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 46,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard gradient descent struggles to find correct reward parameters",
                "type": "concept",
                "description": "Ordinary gradients get stuck or converge slowly due to plateaus and redundancy in the optimisation landscape.",
                "aliases": [
                    "plain gradient inefficiency",
                    "gradient descent failure in IRL"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 45,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "mapping from reward parameters to policies is nonsmooth and redundant",
                "type": "concept",
                "description": "Small changes in reward parameters can leave the optimal policy unchanged, and greedy selection introduces non-differentiabilities.",
                "aliases": [
                    "nonsmooth parameter-policy map",
                    "redundant reward parametrisation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 325,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "extreme verification burden for learning agents",
                "type": "concept",
                "description": "Learning or self-modifying agents make manual proofs practically infeasible due to the explosion of possible histories.",
                "aliases": [
                    "high proof cost",
                    "burdensome verification"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1604_06963",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 323,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "automated verification impossible",
                "type": "concept",
                "description": "Because the decision problem is undecidable, no general software procedure can guarantee an agent's compliance with a deontology.",
                "aliases": [
                    "no general automatic proof",
                    "cannot auto-verify"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1604_06963",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 324,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "manual proof requirement each revision",
                "type": "concept",
                "description": "If automated proofs are impossible, developers must manually re-prove compliance after every code or policy update.",
                "aliases": [
                    "re-prove after change",
                    "iterative manual verification burden"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1604_06963",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1003,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Imperfect proxy contains additive noise",
                "type": "concept",
                "description": "The chosen metric differs from the true goal by an independent noise term.",
                "aliases": [
                    "noisy metric",
                    "proxy with observational noise"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1004,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Regressional Goodhart divergence between metric and goal",
                "type": "concept",
                "description": "Optimising a noisy proxy selects for noise, so at high proxy values the goal systematically lags.",
                "aliases": [
                    "tails come apart",
                    "regressional Goodhart"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 847,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "behaviour diverges from designer intent",
                "type": "concept",
                "description": "Observable misalignment such as dithering in boat race or sensor spoofing in tomato task.",
                "aliases": [
                    "policy misalignment outcome"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1004,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Regressional Goodhart divergence between metric and goal",
                "type": "concept",
                "description": "Optimising a noisy proxy selects for noise, so at high proxy values the goal systematically lags.",
                "aliases": [
                    "tails come apart",
                    "regressional Goodhart"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1003,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Imperfect proxy contains additive noise",
                "type": "concept",
                "description": "The chosen metric differs from the true goal by an independent noise term.",
                "aliases": [
                    "noisy metric",
                    "proxy with observational noise"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 847,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "behaviour diverges from designer intent",
                "type": "concept",
                "description": "Observable misalignment such as dithering in boat race or sensor spoofing in tomato task.",
                "aliases": [
                    "policy misalignment outcome"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 79,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard softmax requires O(W) computation",
                "type": "concept",
                "description": "Baseline observation that computing gradients over the full vocabulary is linear in vocab size.",
                "aliases": [
                    "full softmax cost",
                    "softmax inefficiency"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 46,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard gradient descent struggles to find correct reward parameters",
                "type": "concept",
                "description": "Ordinary gradients get stuck or converge slowly due to plateaus and redundancy in the optimisation landscape.",
                "aliases": [
                    "plain gradient inefficiency",
                    "gradient descent failure in IRL"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 45,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "mapping from reward parameters to policies is nonsmooth and redundant",
                "type": "concept",
                "description": "Small changes in reward parameters can leave the optimal policy unchanged, and greedy selection introduces non-differentiabilities.",
                "aliases": [
                    "nonsmooth parameter-policy map",
                    "redundant reward parametrisation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 398,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Direct pre-programming of complete human values is infeasible",
                "type": "concept",
                "description": "Because of their complexity and contradictions, human values cannot be safely hard-coded into an AI.",
                "aliases": [
                    "hard-coding values infeasible",
                    "cannot specify full human utility"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1607_08289",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 330,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "outcome-based validation futile",
                "type": "concept",
                "description": "Because the required world model is unattainable, absolute outcome validation for general AI cannot be done.",
                "aliases": [
                    "cannot validate consequences",
                    "validation impossible"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1604_06963",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 243,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "No satisfactory formalisation of logical counterfactuals",
                "type": "concept",
                "description": "Current theory lacks precise mathematical tools to model counterfactual worlds where algorithm outputs differ logically.",
                "aliases": [
                    "logical counterfactual gap"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1507_01986",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 870,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Super-intelligent AI exceeding human capabilities",
                "type": "concept",
                "description": "Hypothetical future AI systems with general capabilities superior to humans in every dimension.",
                "aliases": [
                    "AI surpassing humans",
                    "superintelligence risk"
                ],
                "concept_category": "Threat",
                "paper_id": "arxiv__arxiv_org_abs_1712_04307",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 149,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "intelligence explosion from recursive self-improvement",
                "type": "concept",
                "description": "A scenario where an AI that improves its own hardware and software rapidly surpasses human intelligence.",
                "aliases": [
                    "recursive self-improvement intelligence explosion",
                    "runaway capability growth"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1409_0813",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 991,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "agents obtain unrealistic free reward or violate physics",
                "type": "concept",
                "description": "The agent's behaviour relies on exploiting the fault, yielding performance not possible under correct physics.",
                "aliases": [
                    "free energy exploit",
                    "physics-breaking behaviour"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 145,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Human/automatic auditing of unusual attention patterns",
                "type": "intervention",
                "description": "Inspect or programmatically flag translations whose attention distributions deviate from expected patterns to catch errors or malicious content.",
                "aliases": [
                    "attention-based audit",
                    "alignment anomaly detection"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1409_0473",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 667,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "conduct mandatory attention-map visualisation audits with diverse red-team prompts pre-deployment",
                "type": "intervention",
                "description": "Prior to deployment, run a battery of adversarial and benign prompts, visualise attention heads, and have human reviewers flag suspicious correlations or biases.",
                "aliases": [
                    "attention audit before release",
                    "visual interpretability safety check"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1706_03762",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 541,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "misguided policy or safety decisions",
                "type": "concept",
                "description": "Regulatory or strategic choices are made on the basis of incorrect capability assessments.",
                "aliases": [
                    "faulty governance outcome",
                    "policy risk"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1703_10987",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1006,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Ensemble-proxy optimisation with correlation monitoring and optimisation throttling",
                "type": "intervention",
                "description": "Use several independent proxy metrics, monitor their pairwise correlations with validation estimates of the goal, and automatically reduce learning rate or stop optimisation when correlations drop.",
                "aliases": [
                    "multi-metric ensemble regularisation",
                    "correlation-aware throttling"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1008,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Extremal Goodhart breakdown of proxy-goal correlation",
                "type": "concept",
                "description": "In the new region the learned relationship no longer holds, so metric stops reflecting goal.",
                "aliases": [
                    "extremal Goodhart",
                    "out-of-sample collapse"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1150,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "codistillation reduces prediction-churn by 35%",
                "type": "concept",
                "description": "Experimental result on Criteo dataset showing mean absolute prediction difference across retrains drops by about one-third when codistillation is used.",
                "aliases": [
                    "ensemble-like stabilisation via codistillation"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1804_03235",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 454,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "AI containment boxing",
                "type": "intervention",
                "description": "Deploy super-intelligent systems inside restricted I/O sandboxes with layered confinement to limit external impact.",
                "aliases": [
                    "AI boxing",
                    "runtime containment"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1610_07997",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 207,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need for boxed simulation testing",
                "type": "concept",
                "description": "Super-intelligent agents must be evaluated in isolation before deployment",
                "aliases": [
                    "AI safety testbed"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1411_1373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 545,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "apply structured expert elicitation with debiasing training for AI-risk forecasts",
                "type": "intervention",
                "description": "Implement formal forecasting sessions (Delphi, prediction markets) augmented with cognitive-bias training and calibration exercises for participants.",
                "aliases": [
                    "debiased forecasting protocol",
                    "expert elicitation with debias"
                ],
                "intervention_lifecycle": 6,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1703_10987",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 898,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Need to transition from reward R to R\u2032 without early optimisation",
                "type": "concept",
                "description": "Desire for an agent to maximise R up to time t and then R\u2032 after t without manipulating plans before the switch.",
                "aliases": [
                    "seamless transition problem",
                    "reward switch incentive issue"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1712_06365",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 45,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "mapping from reward parameters to policies is nonsmooth and redundant",
                "type": "concept",
                "description": "Small changes in reward parameters can leave the optimal policy unchanged, and greedy selection introduces non-differentiabilities.",
                "aliases": [
                    "nonsmooth parameter-policy map",
                    "redundant reward parametrisation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 383,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "absence of novelty-driven behavior",
                "type": "concept",
                "description": "The agent no longer seeks novel states or information.",
                "aliases": [
                    "loss of exploration",
                    "no novelty incentive"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1606_07092",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 434,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Bounded internal activations reduce numerical instability",
                "type": "concept",
                "description": "Restricting hidden-state magnitudes prevents overflow or extreme gradients, potentially increasing robustness to distributional shift.",
                "aliases": [
                    "activation clipping stability",
                    "overflow prevention"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1609_08144",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 187,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "gradient norm clipping limits gradient magnitude",
                "type": "concept",
                "description": "Scaling gradients when their L2 norm exceeds a threshold prevents explosion and stabilizes updates.",
                "aliases": [
                    "gradient clipping technique",
                    "norm thresholding"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1409_3215",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 861,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Objective-reward mis-specification causing unethical RL behaviour",
                "type": "concept",
                "description": "Providing an objective function that ignores ethical considerations can drive RL agents toward harmful strategies that still maximise the given reward.",
                "aliases": [
                    "reward misspecification leads to unethical acts",
                    "unethical behaviour from faulty reward"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 852,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "risk-neutral deep-RL policy",
                "type": "concept",
                "description": "Standard objective maximises expectation, indifferent to variance or rare catastrophes.",
                "aliases": [
                    "brittle policy"
                ],
                "concept_category": "Assumption",
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 644,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Adopt Maximum Entropy IRL for robustness to \u03b2",
                "type": "intervention",
                "description": "Deploy a Maximum-Entropy (MLE) inverse-reinforcement-learning policy to ensure action robustness under rationality-parameter misspecification.",
                "aliases": [
                    "use MaxEnt IRL",
                    "MLE with \u03b2=1"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1705_09990",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 636,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Bayesian optimal policy fragile to \u03b2 misspecification",
                "type": "concept",
                "description": "A robot acting on the posterior mean of \u03b8 may change actions when its assumed human rationality level is wrong.",
                "aliases": [
                    "posterior mean \u03b2-fragile",
                    "Bayesian IRL beta risk"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1705_09990",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 695,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "apply conservative low threshold blocking strategy",
                "type": "intervention",
                "description": "Set a very low probability threshold for the Blocker to classify an action as catastrophic, prioritising zero false negatives, then gradually relax it.",
                "aliases": [
                    "aggressive blocking threshold",
                    "threshold tuning"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 681,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Blocker must be highly reliable and adversarially robust",
                "type": "concept",
                "description": "To guarantee zero catastrophes, the classifier\u2019s false-negative rate must be virtually zero even under distribution shift or adversarial behaviour.",
                "aliases": [
                    "Blocker reliability requirement",
                    "robust catastrophic action detector"
                ],
                "concept_category": "Requirement",
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 861,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Objective-reward mis-specification causing unethical RL behaviour",
                "type": "concept",
                "description": "Providing an objective function that ignores ethical considerations can drive RL agents toward harmful strategies that still maximise the given reward.",
                "aliases": [
                    "reward misspecification leads to unethical acts",
                    "unethical behaviour from faulty reward"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 710,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "conduct pre-deployment evaluation of robot's pragmatic inference accuracy with human users",
                "type": "intervention",
                "description": "Before release, test the robot with real or simulated humans acting pedagogically to measure belief-update accuracy and alignment success.",
                "aliases": [
                    "pragmatic inference testing",
                    "human-in-the-loop evaluation for CIRL robots"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1707_06354",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 537,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Pre-deployment causal abduction\u2013action\u2013prediction evaluation procedure",
                "type": "intervention",
                "description": "Before releasing a model, estimate background variables, intervene on A, and compare prediction distributions to confirm counterfactual invariance.",
                "aliases": [
                    "CF audit via twin network",
                    "counterfactual simulation test"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1703_06856",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 358,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Integrating Gym API with robotic hardware",
                "type": "intervention",
                "description": "Extend the Gym interface to control physical robots, enabling direct real-world evaluation.",
                "aliases": [
                    "Gym-robot bridge",
                    "real-world Gym interface"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 716,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "validate and limit GPU API calls or use modified open-source drivers inside container",
                "type": "intervention",
                "description": "Insert a validation layer or stripped-down driver that blocks unsafe CUDA/CL calls, reducing exploitable surface within the container.",
                "aliases": [
                    "GPU API whitelisting",
                    "hardened GPU driver"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1707_08476",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 715,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "GPGPU drivers create large attack surface",
                "type": "concept",
                "description": "Modern GPU stacks require kernel-level privileges and have many critical CVEs, making them a prime escape vector.",
                "aliases": [
                    "GPU attack surface finding",
                    "GPU driver vulnerability"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1707_08476",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 823,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Early-abort mechanism gating forward actions",
                "type": "intervention",
                "description": "Before executing a forward-policy action, abort and switch to the reset policy if Q_reset(s,a) \u2264 Qmin.",
                "aliases": [
                    "learned safety abort",
                    "Qmin abort gate"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1711_06782",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 430,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Objective mismatch between ML training and sequence-level reward",
                "type": "concept",
                "description": "Maximum-likelihood training optimises next-token prediction rather than overall translation quality, leading to testing-time degradation.",
                "aliases": [
                    "exposure bias",
                    "token-level vs sentence-level objective gap"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1609_08144",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 47,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "feature scaling mismatch in IRL causes policy performance degradation",
                "type": "concept",
                "description": "If the learner's feature scales differ from the true scales, max-margin IRL can output policies with large reward loss.",
                "aliases": [
                    "scale sensitivity problem",
                    "mis-scaled features hurt IRL"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 46,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard gradient descent struggles to find correct reward parameters",
                "type": "concept",
                "description": "Ordinary gradients get stuck or converge slowly due to plateaus and redundancy in the optimisation landscape.",
                "aliases": [
                    "plain gradient inefficiency",
                    "gradient descent failure in IRL"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 856,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "single strategy fails across friend/foe rooms",
                "type": "concept",
                "description": "Fixed exploration strategy under-performs when intentions vary.",
                "aliases": [
                    "inflexible policy failure"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 303,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "total & average regret unreliable under unbounded delays",
                "type": "concept",
                "description": "Standard online-learning metrics fail because they can be dominated by long stretches without feedback.",
                "aliases": [
                    "regret failure with delays",
                    "inadequacy of regret metrics"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1604_05280",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 46,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard gradient descent struggles to find correct reward parameters",
                "type": "concept",
                "description": "Ordinary gradients get stuck or converge slowly due to plateaus and redundancy in the optimisation landscape.",
                "aliases": [
                    "plain gradient inefficiency",
                    "gradient descent failure in IRL"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1066,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need for additional training mechanism",
                "type": "concept",
                "description": "Observation that gradient methods alone are insufficient motivates exploring alternative strategies.",
                "aliases": [
                    "search for better replication method",
                    "gap after gradient training"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_05859",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 46,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard gradient descent struggles to find correct reward parameters",
                "type": "concept",
                "description": "Ordinary gradients get stuck or converge slowly due to plateaus and redundancy in the optimisation landscape.",
                "aliases": [
                    "plain gradient inefficiency",
                    "gradient descent failure in IRL"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 107,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "large_error_derivatives_from_large_rewards",
                "type": "concept",
                "description": "Large raw rewards lead to proportionally large TD-errors, which can destabilise gradient descent.",
                "aliases": [
                    "unstable gradients due to reward scale"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 218,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "adopt AdaMax variant to bound updates under \u221e-norm",
                "type": "intervention",
                "description": "Employ the AdaMax algorithm (\u221e-norm preconditioner) to cap per-parameter updates and increase robustness to gradient outliers.",
                "aliases": [
                    "use AdaMax",
                    "infinity-norm Adam"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1412_6980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 212,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "apply Adam optimizer with \u03b1=0.001 \u03b21=0.9 \u03b22=0.999 \u03b5=1e-8 during pre-training",
                "type": "intervention",
                "description": "Train models using the Adam algorithm with the paper\u2019s recommended hyper-parameters and bias-correction.",
                "aliases": [
                    "use Adam default settings",
                    "Adam with bias-correction"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 4,
                "paper_id": "arxiv__arxiv_org_abs_1412_6980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 861,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Objective-reward mis-specification causing unethical RL behaviour",
                "type": "concept",
                "description": "Providing an objective function that ignores ethical considerations can drive RL agents toward harmful strategies that still maximise the given reward.",
                "aliases": [
                    "reward misspecification leads to unethical acts",
                    "unethical behaviour from faulty reward"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 353,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Final performance metric alone hides sample efficiency differences",
                "type": "concept",
                "description": "Only reporting final reward overlooks how many episodes the agent needed to reach that reward, masking algorithm quality.",
                "aliases": [
                    "sample complexity ignored",
                    "learning curve blindness"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1008,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Extremal Goodhart breakdown of proxy-goal correlation",
                "type": "concept",
                "description": "In the new region the learned relationship no longer holds, so metric stops reflecting goal.",
                "aliases": [
                    "extremal Goodhart",
                    "out-of-sample collapse"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1006,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Ensemble-proxy optimisation with correlation monitoring and optimisation throttling",
                "type": "intervention",
                "description": "Use several independent proxy metrics, monitor their pairwise correlations with validation estimates of the goal, and automatically reduce learning rate or stop optimisation when correlations drop.",
                "aliases": [
                    "multi-metric ensemble regularisation",
                    "correlation-aware throttling"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 618,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "10 % intelligence-explosion probability",
                "type": "concept",
                "description": "Experts assign a median 10 % chance that an intelligence explosion will occur once HLMI is achieved.",
                "aliases": [
                    "self-improvement risk non-trivial",
                    "expert IE probability"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1705_08807",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 149,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "intelligence explosion from recursive self-improvement",
                "type": "concept",
                "description": "A scenario where an AI that improves its own hardware and software rapidly surpasses human intelligence.",
                "aliases": [
                    "recursive self-improvement intelligence explosion",
                    "runaway capability growth"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1409_0813",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 991,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "agents obtain unrealistic free reward or violate physics",
                "type": "concept",
                "description": "The agent's behaviour relies on exploiting the fault, yielding performance not possible under correct physics.",
                "aliases": [
                    "free energy exploit",
                    "physics-breaking behaviour"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 343,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "implement realistic value functions that incorporate self-modification effects and current utility",
                "type": "intervention",
                "description": "During agent design encode the value function so that action choice is based on expected outcomes under modified future policies but still evaluated with the present utility.",
                "aliases": [
                    "realistic value implementation",
                    "self-mod aware design"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1605_03142",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 582,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "designer-specified reward function is evidence about true human preferences",
                "type": "concept",
                "description": "Treat the reward parameters written by a designer not as ground truth but as noisy observations generated from the designer's real preferences in a set of training environments.",
                "aliases": [
                    "reward as evidence",
                    "objective function as signal"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 43,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "expert acts optimally under unknown reward function",
                "type": "concept",
                "description": "The demonstrator is assumed to follow a policy that maximises an unknown true reward in the environment.",
                "aliases": [
                    "optimal expert assumption",
                    "expert optimality under hidden reward"
                ],
                "concept_category": "Assumption",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 962,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "prediction error rate 15-40 %",
                "type": "concept",
                "description": "The Bayesian predictor incorrectly infers the human goal 15\u201340 % of timesteps depending on layout.",
                "aliases": [
                    "Bayesian inference errors",
                    "goal prediction noise"
                ],
                "concept_category": "Data",
                "paper_id": "arxiv__arxiv_org_abs_1802_01780",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 963,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "small degradation from prediction errors",
                "type": "concept",
                "description": "Prediction errors only modestly reduce objective and subjective performance relative to perfect oracle inference.",
                "aliases": [
                    "robust performance despite errors"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1802_01780",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 79,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard softmax requires O(W) computation",
                "type": "concept",
                "description": "Baseline observation that computing gradients over the full vocabulary is linear in vocab size.",
                "aliases": [
                    "full softmax cost",
                    "softmax inefficiency"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 309,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "lack of near-optimal expert prevents EvOp convergence speed",
                "type": "concept",
                "description": "If no expert closely matches Bayes-optimal predictions, EvOp\u2019s performance can remain poor for long periods.",
                "aliases": [
                    "missing good expert slows EvOp",
                    "suboptimal expert set issue"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1604_05280",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 963,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "small degradation from prediction errors",
                "type": "concept",
                "description": "Prediction errors only modestly reduce objective and subjective performance relative to perfect oracle inference.",
                "aliases": [
                    "robust performance despite errors"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1802_01780",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 79,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard softmax requires O(W) computation",
                "type": "concept",
                "description": "Baseline observation that computing gradients over the full vocabulary is linear in vocab size.",
                "aliases": [
                    "full softmax cost",
                    "softmax inefficiency"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 609,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement internal market with dominant-forecaster pricing for pre-deployment red-team evaluation",
                "type": "intervention",
                "description": "Before release, run an internal combinatorial market whose prices are set by a dominant-forecaster algorithm; red-team gamblers try to exploit it, surfacing systematic errors prior to deployment.",
                "aliases": [
                    "dominant-market evaluation harness",
                    "internal prediction market testbed"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1705_04630",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 606,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Forecasts can be exploited by gamblers for unbounded profit",
                "type": "concept",
                "description": "Without safeguards, an adversary can construct betting strategies that earn unlimited gains from systematic forecast errors.",
                "aliases": [
                    "exploitability risk",
                    "unbounded betting profit"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1705_04630",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 786,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Predictor blackmail exploiting CDT/EDT",
                "type": "concept",
                "description": "Blackmailers can reliably extract money by conditioning threats on predictions of CDT or EDT agents\u2019 responses.",
                "aliases": [
                    "XOR/Mechanical blackmail setup"
                ],
                "concept_category": "Threat",
                "paper_id": "arxiv__arxiv_org_abs_1710_05060",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1015,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Tamper-evident logging & anomaly detection on metric channels",
                "type": "intervention",
                "description": "Instrumentation that records metric generation with cryptographic signatures and runs statistical anomaly detection to flag or block suspicious alterations.",
                "aliases": [
                    "metric integrity monitoring",
                    "manipulation guard"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1190,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Conduct capsule-dimension visualisation inspection before deployment",
                "type": "intervention",
                "description": "Generate heat-maps for individual capsule dimensions and review them to detect anomalous or unintended feature usage prior to release.",
                "aliases": [
                    "capsule attribute auditing",
                    "capsule vector visualisation check"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1804_04241",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 288,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "compute and energy-usage auditing during AI development and deployment",
                "type": "intervention",
                "description": "Mandate measurement and reporting of total compute and energy consumption for AI training and inference processes.",
                "aliases": [
                    "compute governance auditing",
                    "power-use compliance checks"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 34,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "speed explosion exceeding human comprehension",
                "type": "concept",
                "description": "Virtual societies run orders of magnitude faster than real-time, outpacing human cognitive tracking.",
                "aliases": [
                    "hyper-accelerated virtual processes",
                    "unobservable fast regime"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1202_6177",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 149,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "intelligence explosion from recursive self-improvement",
                "type": "concept",
                "description": "A scenario where an AI that improves its own hardware and software rapidly surpasses human intelligence.",
                "aliases": [
                    "recursive self-improvement intelligence explosion",
                    "runaway capability growth"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1409_0813",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 991,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "agents obtain unrealistic free reward or violate physics",
                "type": "concept",
                "description": "The agent's behaviour relies on exploiting the fault, yielding performance not possible under correct physics.",
                "aliases": [
                    "free energy exploit",
                    "physics-breaking behaviour"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 288,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "compute and energy-usage auditing during AI development and deployment",
                "type": "intervention",
                "description": "Mandate measurement and reporting of total compute and energy consumption for AI training and inference processes.",
                "aliases": [
                    "compute governance auditing",
                    "power-use compliance checks"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 145,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Human/automatic auditing of unusual attention patterns",
                "type": "intervention",
                "description": "Inspect or programmatically flag translations whose attention distributions deviate from expected patterns to catch errors or malicious content.",
                "aliases": [
                    "attention-based audit",
                    "alignment anomaly detection"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1409_0473",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 667,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "conduct mandatory attention-map visualisation audits with diverse red-team prompts pre-deployment",
                "type": "intervention",
                "description": "Prior to deployment, run a battery of adversarial and benign prompts, visualise attention heads, and have human reviewers flag suspicious correlations or biases.",
                "aliases": [
                    "attention audit before release",
                    "visual interpretability safety check"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1706_03762",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1126,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "learned rule robust to pixel permutation, unlike prototypical networks",
                "type": "concept",
                "description": "The model retains high accuracy even when pixels are permuted, indicating robustness to input ordering.",
                "aliases": [
                    "permutation robustness finding"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1804_00222",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 424,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Improved robustness to OOV tokens",
                "type": "concept",
                "description": "Model maintains high translation quality on sentences containing previously unseen words by composing them from known sub-words.",
                "aliases": [
                    "rare-word robustness",
                    "subword OOV handling"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1609_08144",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1026,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "DkNN recovers correct labels on some adversarial examples",
                "type": "concept",
                "description": "Layer consistency lets DkNN classify a fraction of adversarial inputs correctly, raising accuracy compared to the underlying DNN.",
                "aliases": [
                    "partial robustness of DkNN",
                    "adversarial recovery"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04765",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1140,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "monitor per-layer reconstruction error at inference to flag adversarial or OOD inputs",
                "type": "intervention",
                "description": "During deployment, compute each fortified layer\u2019s reconstruction error; if it exceeds a threshold, raise an alert or reject the input.",
                "aliases": [
                    "reconstruction-error monitor",
                    "off-manifold detector"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1804_02485",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 381,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "unsupervised risk estimation via error distribution modeling",
                "type": "intervention",
                "description": "Estimate expected prediction error on unlabeled deployment data using assumptions about error independence or mixture models, triggering conservative fallback actions when risk is high.",
                "aliases": [
                    "OOD risk estimator",
                    "model uncertainty detector"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1009,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Out-of-distribution detector with penalty on extreme metric values during deployment",
                "type": "intervention",
                "description": "Deploy runtime detectors that flag inputs or states where the proxy sits outside its training distribution and impose a penalty or halt policy execution.",
                "aliases": [
                    "OOD penalty on extreme proxy",
                    "extremal value guardrail"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 215,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "heterogeneous gradient scales across CNN layers complicate learning-rate tuning",
                "type": "concept",
                "description": "Convolutional and fully-connected layers exhibit gradients of very different magnitudes, making a single global SGD step size sub-optimal.",
                "aliases": [
                    "layer scale mismatch",
                    "CNN lr tuning problem"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1412_6980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 46,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard gradient descent struggles to find correct reward parameters",
                "type": "concept",
                "description": "Ordinary gradients get stuck or converge slowly due to plateaus and redundancy in the optimisation landscape.",
                "aliases": [
                    "plain gradient inefficiency",
                    "gradient descent failure in IRL"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 47,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "feature scaling mismatch in IRL causes policy performance degradation",
                "type": "concept",
                "description": "If the learner's feature scales differ from the true scales, max-margin IRL can output policies with large reward loss.",
                "aliases": [
                    "scale sensitivity problem",
                    "mis-scaled features hurt IRL"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 206,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "finite stochastic loop program restriction",
                "type": "intervention",
                "description": "Define environments, proofs and agents only over finite state spaces and decidable logics",
                "aliases": [
                    "finite-set agent maths"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1411_1373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 207,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need for boxed simulation testing",
                "type": "concept",
                "description": "Super-intelligent agents must be evaluated in isolation before deployment",
                "aliases": [
                    "AI safety testbed"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1411_1373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 545,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "apply structured expert elicitation with debiasing training for AI-risk forecasts",
                "type": "intervention",
                "description": "Implement formal forecasting sessions (Delphi, prediction markets) augmented with cognitive-bias training and calibration exercises for participants.",
                "aliases": [
                    "debiased forecasting protocol",
                    "expert elicitation with debias"
                ],
                "intervention_lifecycle": 6,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1703_10987",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 784,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "FDT agents achieving higher expected utility in fair dilemmas",
                "type": "concept",
                "description": "Across standard fair decision problems FDT prescriptions yield greater expected payoff than CDT or EDT.",
                "aliases": [
                    "FDT outperforming CDT/EDT",
                    "utility gains from FDT"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1710_05060",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 786,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Predictor blackmail exploiting CDT/EDT",
                "type": "concept",
                "description": "Blackmailers can reliably extract money by conditioning threats on predictions of CDT or EDT agents\u2019 responses.",
                "aliases": [
                    "XOR/Mechanical blackmail setup"
                ],
                "concept_category": "Threat",
                "paper_id": "arxiv__arxiv_org_abs_1710_05060",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 606,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Forecasts can be exploited by gamblers for unbounded profit",
                "type": "concept",
                "description": "Without safeguards, an adversary can construct betting strategies that earn unlimited gains from systematic forecast errors.",
                "aliases": [
                    "exploitability risk",
                    "unbounded betting profit"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1705_04630",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 414,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "EP convergence to zero is necessary and sufficient for asymptotic optimality",
                "type": "concept",
                "description": "If and only if exploration potential tends to zero, the agent\u2019s policy becomes asymptotically optimal under the stated assumptions.",
                "aliases": [
                    "EP\u21920 \u21d4 optimality",
                    "EP optimality theorem"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1609_04994",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 133,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "global optimum pg equals pdata when capacity sufficient",
                "type": "concept",
                "description": "Theoretical proof shows the minimax game\u2019s global optimum is achieved when the generator distribution matches the true data distribution.",
                "aliases": [
                    "GAN convergence theorem",
                    "pg = pdata result"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1406_2661",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1008,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Extremal Goodhart breakdown of proxy-goal correlation",
                "type": "concept",
                "description": "In the new region the learned relationship no longer holds, so metric stops reflecting goal.",
                "aliases": [
                    "extremal Goodhart",
                    "out-of-sample collapse"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1159,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Alternating all-items negotiation pattern",
                "type": "concept",
                "description": "Negotiations oscillate with each agent alternately proposing to take the entire item pool, leading to low joint reward.",
                "aliases": [
                    "take-everything oscillation",
                    "random full-take proposals"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1804_03980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1162,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Fair negotiation outcomes among selfish agents",
                "type": "concept",
                "description": "Self-interested agents reach near-fair allocations approximating human performance when using the proposal channel.",
                "aliases": [
                    "60% joint reward selfish",
                    "balanced item division"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1804_03980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1177,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Selfish agents negotiating with hidden utilities",
                "type": "concept",
                "description": "Each agent maximises its own reward while the other agent\u2019s utilities remain private.",
                "aliases": [
                    "hidden-utility selfish negotiation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1804_03980",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 452,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "fundamental theorem of security",
                "type": "concept",
                "description": "Principle stating that every security system will fail given enough time or effort.",
                "aliases": [
                    "inevitable security failure",
                    "all security eventually breaks"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1610_07997",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 455,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "assume adversarial bypass attempts",
                "type": "concept",
                "description": "Principle that safety design must expect deliberate efforts to break safeguards.",
                "aliases": [
                    "adversarial mindset in AI safety"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1610_07997",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 713,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "defence in depth reduces risk from single vulnerability",
                "type": "concept",
                "description": "Combining multiple, heterogeneous containment mechanisms makes a single flaw insufficient for total compromise.",
                "aliases": [
                    "layered security principle",
                    "overlapping containment layers"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1707_08476",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 667,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "conduct mandatory attention-map visualisation audits with diverse red-team prompts pre-deployment",
                "type": "intervention",
                "description": "Prior to deployment, run a battery of adversarial and benign prompts, visualise attention heads, and have human reviewers flag suspicious correlations or biases.",
                "aliases": [
                    "attention audit before release",
                    "visual interpretability safety check"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1706_03762",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 288,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "compute and energy-usage auditing during AI development and deployment",
                "type": "intervention",
                "description": "Mandate measurement and reporting of total compute and energy consumption for AI training and inference processes.",
                "aliases": [
                    "compute governance auditing",
                    "power-use compliance checks"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1183,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement locally constrained dynamic routing with shared transformations in capsule segmentation models",
                "type": "intervention",
                "description": "Restrict routing to a spatial window and share transformation matrices across spatial positions within each capsule type to cut parameters and memory.",
                "aliases": [
                    "locally-constrained routing",
                    "shared-matrix capsule routing"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1804_04241",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 347,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Inconsistent evaluation metrics and result comparability issues",
                "type": "concept",
                "description": "Different papers use varying metrics and setups, leading to results that cannot be fairly compared.",
                "aliases": [
                    "metric inconsistency",
                    "unreliable cross-paper comparisons"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1006,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Ensemble-proxy optimisation with correlation monitoring and optimisation throttling",
                "type": "intervention",
                "description": "Use several independent proxy metrics, monitor their pairwise correlations with validation estimates of the goal, and automatically reduce learning rate or stop optimisation when correlations drop.",
                "aliases": [
                    "multi-metric ensemble regularisation",
                    "correlation-aware throttling"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1008,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Extremal Goodhart breakdown of proxy-goal correlation",
                "type": "concept",
                "description": "In the new region the learned relationship no longer holds, so metric stops reflecting goal.",
                "aliases": [
                    "extremal Goodhart",
                    "out-of-sample collapse"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 135,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "generator avoids harmful outputs at convergence",
                "type": "concept",
                "description": "If pg converges to a dataset devoid of harmful content, the generator will not produce harmful outputs.",
                "aliases": [
                    "harm avoidance through data curation",
                    "safe generation claim"
                ],
                "concept_category": "Claim",
                "paper_id": "arxiv__arxiv_org_abs_1406_2661",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 133,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "global optimum pg equals pdata when capacity sufficient",
                "type": "concept",
                "description": "Theoretical proof shows the minimax game\u2019s global optimum is achieved when the generator distribution matches the true data distribution.",
                "aliases": [
                    "GAN convergence theorem",
                    "pg = pdata result"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1406_2661",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 925,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reward shaping fails to guarantee safety",
                "type": "concept",
                "description": "Experiments showed that despite tuned penalties, reward-shaping agents continued to commit substantial numbers of constraint violations and converged slowly or not at all.",
                "aliases": [
                    "shaping inadequacy",
                    "residual violations with shaping"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1801_08757",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 762,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Slow training delays agent learning",
                "type": "concept",
                "description": "Because of high sample complexity, agents take prohibitive wall-clock time before becoming competent.",
                "aliases": [
                    "long wall-clock training",
                    "slow sample efficiency"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 46,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard gradient descent struggles to find correct reward parameters",
                "type": "concept",
                "description": "Ordinary gradients get stuck or converge slowly due to plateaus and redundancy in the optimisation landscape.",
                "aliases": [
                    "plain gradient inefficiency",
                    "gradient descent failure in IRL"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 107,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "large_error_derivatives_from_large_rewards",
                "type": "concept",
                "description": "Large raw rewards lead to proportionally large TD-errors, which can destabilise gradient descent.",
                "aliases": [
                    "unstable gradients due to reward scale"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 959,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "continuous task-allocation re-planning",
                "type": "concept",
                "description": "Recompute a mixed-integer optimal joint plan whenever the inferred human goal deviates from the robot\u2019s internal plan.",
                "aliases": [
                    "adaptive MILP re-planning",
                    "dynamic task allocation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1802_01780",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 957,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "coordination errors from absent intent inference",
                "type": "concept",
                "description": "Robots that cannot anticipate human goals often block or hinder humans, causing teamwork failures.",
                "aliases": [
                    "human-robot miscoordination",
                    "coordination failures"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1802_01780",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 597,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "humans need to predict robot actions for coordination",
                "type": "concept",
                "description": "Effective teamwork requires that humans can anticipate a robot\u2019s future actions after observing its initial behaviour.",
                "aliases": [
                    "human prediction requirement",
                    "predictability problem"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 321,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Use M* prior for compute-budget metareasoning",
                "type": "intervention",
                "description": "Employ the probability outputs of M* as priors when deciding whether to allocate resources to run expensive computations, balancing expected information gain vs. cost.",
                "aliases": [
                    "M* in metareasoning",
                    "inductive prior for simulation control"
                ],
                "intervention_lifecycle": 6,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1604_05288",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 315,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Early calibrated probabilities for expensive computations",
                "type": "concept",
                "description": "Inductively coherent schemes assign near-correct probabilities to computational claims before the computation is actually run.",
                "aliases": [
                    "reasonable pre-execution probabilities",
                    "early probability calibration"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1604_05288",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 72,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "hierarchical softmax with Huffman tree reduces cost to O(log W)",
                "type": "concept",
                "description": "Method that organises output layer as a Huffman binary tree to cut computation.",
                "aliases": [
                    "Huffman HS",
                    "binary-tree softmax"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 578,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "most goals benefit from more resources (instrumental convergence)",
                "type": "concept",
                "description": "Theoretical argument that a wide range of final goals implies incentives to acquire resources.",
                "aliases": [
                    "resource-seeking convergence",
                    "universality of resource value"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1705_03394",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 898,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Need to transition from reward R to R\u2032 without early optimisation",
                "type": "concept",
                "description": "Desire for an agent to maximise R up to time t and then R\u2032 after t without manipulating plans before the switch.",
                "aliases": [
                    "seamless transition problem",
                    "reward switch incentive issue"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1712_06365",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 45,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "mapping from reward parameters to policies is nonsmooth and redundant",
                "type": "concept",
                "description": "Small changes in reward parameters can leave the optimal policy unchanged, and greedy selection introduces non-differentiabilities.",
                "aliases": [
                    "nonsmooth parameter-policy map",
                    "redundant reward parametrisation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 704,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "robots using literal inverse reinforcement learning misinterpret pedagogic human actions",
                "type": "concept",
                "description": "When robots assume humans act as if the robot already knows the goal, they wrongly update their beliefs under pedagogic behaviour.",
                "aliases": [
                    "literal IRL misinterpretation",
                    "standard IRL failure with pedagogic humans"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1707_06354",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 358,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Integrating Gym API with robotic hardware",
                "type": "intervention",
                "description": "Extend the Gym interface to control physical robots, enabling direct real-world evaluation.",
                "aliases": [
                    "Gym-robot bridge",
                    "real-world Gym interface"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 537,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Pre-deployment causal abduction\u2013action\u2013prediction evaluation procedure",
                "type": "intervention",
                "description": "Before releasing a model, estimate background variables, intervene on A, and compare prediction distributions to confirm counterfactual invariance.",
                "aliases": [
                    "CF audit via twin network",
                    "counterfactual simulation test"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1703_06856",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 780,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Apply importance weighting using uniform delay 0.2\u20134s in SGD updates",
                "type": "intervention",
                "description": "Compute w(ts,te,tf)=\u222btf\u2212ts^{tf\u2212te}U[0.2,4](t)dt and multiply gradients so that only recent state-actions receive credit from a feedback signal.",
                "aliases": [
                    "delay-based importance weights",
                    "credit-assignment weights"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 775,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Limited feedback slows parameter learning",
                "type": "concept",
                "description": "Without sufficient labeled data, the reward model\u2019s gradient steps become infrequent, delaying convergence.",
                "aliases": [
                    "few labels",
                    "data starvation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 779,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Maintain feedback replay buffer for minibatch SGD every 10 time steps",
                "type": "intervention",
                "description": "Store all (experience, feedback) pairs and resample them so the reward model is updated ten times as often as new feedback arrives.",
                "aliases": [
                    "feedback buffer updates",
                    "replay human critiques"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 935,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "skip option for uncertain feature queries",
                "type": "intervention",
                "description": "Permit users to decline answering the feature part of a query when unsure, relying only on the comparison.",
                "aliases": [
                    "'I don't know' feature response",
                    "feature query skipping"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1802_01604",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 523,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Detection of dataset ambiguity and hidden confounds by human auditors",
                "type": "concept",
                "description": "By comparing diverse explanations, practitioners can spot spurious rules and ambiguous labeling in the dataset.",
                "aliases": [
                    "auditor identification of shortcuts"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1703_03717",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 472,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "skip connections may cause incompatible layer shapes",
                "type": "concept",
                "description": "Unconstrained skip-connection predictions can yield layers with mismatched dimensions or missing inputs/outputs, leading to build errors.",
                "aliases": [
                    "skip-connection compilation failures",
                    "shape mismatch due to skips"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1611_01578",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1024,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "credibility-threshold rejection mechanism",
                "type": "intervention",
                "description": "Reject or flag any prediction whose credibility falls below a preset threshold, treating it as potentially adversarial or out-of-distribution.",
                "aliases": [
                    "credibility-based abstention",
                    "credibility flagging"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1803_04765",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1138,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "misclassification under adversarial attack",
                "type": "concept",
                "description": "The model outputs an incorrect prediction when confronted with an adversarially perturbed input.",
                "aliases": [
                    "robustness failure",
                    "attack-induced errors"
                ],
                "concept_category": "Threat",
                "paper_id": "arxiv__arxiv_org_abs_1804_02485",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1147,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "codistillation tolerant to stale peer predictions",
                "type": "concept",
                "description": "Finding that using predictions tens of thousands of updates old does not harm convergence, allowing infrequent parameter exchange.",
                "aliases": [
                    "stale-prediction robustness",
                    "delay-tolerant codistillation"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1804_03235",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 294,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "source code transparency assumption",
                "type": "concept",
                "description": "Agents can read their own and opponents' source code prior to acting.",
                "aliases": [
                    "mutual code access",
                    "program equilibrium setting"
                ],
                "concept_category": "Assumption",
                "paper_id": "arxiv__arxiv_org_abs_1602_04184",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1177,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Selfish agents negotiating with hidden utilities",
                "type": "concept",
                "description": "Each agent maximises its own reward while the other agent\u2019s utilities remain private.",
                "aliases": [
                    "hidden-utility selfish negotiation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1804_03980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1162,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Fair negotiation outcomes among selfish agents",
                "type": "concept",
                "description": "Self-interested agents reach near-fair allocations approximating human performance when using the proposal channel.",
                "aliases": [
                    "60% joint reward selfish",
                    "balanced item division"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1804_03980",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 461,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "traditional software testing unsuitable for AGI",
                "type": "concept",
                "description": "Problem that release-to-users or run-and-observe testing paradigms cannot be safely applied to super-intelligence.",
                "aliases": [
                    "alpha/beta testing infeasible",
                    "testing unsuitability"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1610_07997",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 330,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "outcome-based validation futile",
                "type": "concept",
                "description": "Because the required world model is unattainable, absolute outcome validation for general AI cannot be done.",
                "aliases": [
                    "cannot validate consequences",
                    "validation impossible"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1604_06963",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 243,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "No satisfactory formalisation of logical counterfactuals",
                "type": "concept",
                "description": "Current theory lacks precise mathematical tools to model counterfactual worlds where algorithm outputs differ logically.",
                "aliases": [
                    "logical counterfactual gap"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1507_01986",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1029,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "use DkNN neighbour analysis to audit and mitigate dataset bias",
                "type": "intervention",
                "description": "Inspect, remove, or augment training samples implicated by neighbour sets in biased predictions, e.g., against dark-skinned individuals.",
                "aliases": [
                    "bias debugging via exemplars",
                    "bias audit intervention"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04765",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 71,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling with k=5-20 and unigram^(3/4) noise yields high-quality vectors",
                "type": "concept",
                "description": "Empirical result: these hyper-parameters outperform alternatives on analogy benchmarks.",
                "aliases": [
                    "NEG high analogy accuracy",
                    "NEG optimal hyperparams"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 667,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "conduct mandatory attention-map visualisation audits with diverse red-team prompts pre-deployment",
                "type": "intervention",
                "description": "Prior to deployment, run a battery of adversarial and benign prompts, visualise attention heads, and have human reviewers flag suspicious correlations or biases.",
                "aliases": [
                    "attention audit before release",
                    "visual interpretability safety check"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1706_03762",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 497,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "during RL training, compute action-values using each stakeholder\u2019s subjective model separately and combine via shifting weights",
                "type": "intervention",
                "description": "Maintain separate critics for each principal using their priors and aggregate them with the dynamic weighting rule while optimising the policy.",
                "aliases": [
                    "belief-specific Q evaluation",
                    "separate-model critics"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1701_01302",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 537,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Pre-deployment causal abduction\u2013action\u2013prediction evaluation procedure",
                "type": "intervention",
                "description": "Before releasing a model, estimate background variables, intervene on A, and compare prediction distributions to confirm counterfactual invariance.",
                "aliases": [
                    "CF audit via twin network",
                    "counterfactual simulation test"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1703_06856",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 358,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Integrating Gym API with robotic hardware",
                "type": "intervention",
                "description": "Extend the Gym interface to control physical robots, enabling direct real-world evaluation.",
                "aliases": [
                    "Gym-robot bridge",
                    "real-world Gym interface"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1010,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Regulator intervention on shared cause/intermediary/metric",
                "type": "concept",
                "description": "The optimiser acts on a variable in the causal chain connecting metric and goal, or sets the metric itself.",
                "aliases": [
                    "direct intervention on proxy causal graph",
                    "regulator action on causal node"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1008,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Extremal Goodhart breakdown of proxy-goal correlation",
                "type": "concept",
                "description": "In the new region the learned relationship no longer holds, so metric stops reflecting goal.",
                "aliases": [
                    "extremal Goodhart",
                    "out-of-sample collapse"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 364,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Nuclear-norm \u03b2-quantile recovery algorithm",
                "type": "concept",
                "description": "An optimisation that maximises alignment with observed ratings while constraining entries, row sums and nuclear norm to extract a \u03b2-quantile matrix.",
                "aliases": [
                    "low-rank quantile estimation",
                    "\u03b2-quantile matrix solver"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1606_05374",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 734,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Logical inductors resistant to self-referential paradox",
                "type": "concept",
                "description": "For diagonal liar-style sentences \u03c7p, a logical inductor\u2019s probabilities converge to the fixed point p, preventing instability.",
                "aliases": [
                    "paradox resistance",
                    "\u03c7p convergence"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1707_08747",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 325,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "extreme verification burden for learning agents",
                "type": "concept",
                "description": "Learning or self-modifying agents make manual proofs practically infeasible due to the explosion of possible histories.",
                "aliases": [
                    "high proof cost",
                    "burdensome verification"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1604_06963",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 883,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "simplicity priors are insufficient to resolve unidentifiability",
                "type": "concept",
                "description": "Finding that Kolmogorov or other simplicity priors tend to favour pathological decompositions rather than meaningful human rewards.",
                "aliases": [
                    "simplicity failure",
                    "degenerate simplicity bias"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1712_05812",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 709,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "implement pragmatic-pedagogic CIRL policy in robot decision-making",
                "type": "intervention",
                "description": "Integrate the pragmatic-pedagogic CIRL algorithm, including Boltzmann human model and POMDP planning, into the robot\u2019s control policy during training.",
                "aliases": [
                    "deploy pragmatic CIRL",
                    "use pragmatic inference algorithm"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1707_06354",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 358,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Integrating Gym API with robotic hardware",
                "type": "intervention",
                "description": "Extend the Gym interface to control physical robots, enabling direct real-world evaluation.",
                "aliases": [
                    "Gym-robot bridge",
                    "real-world Gym interface"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 537,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Pre-deployment causal abduction\u2013action\u2013prediction evaluation procedure",
                "type": "intervention",
                "description": "Before releasing a model, estimate background variables, intervene on A, and compare prediction distributions to confirm counterfactual invariance.",
                "aliases": [
                    "CF audit via twin network",
                    "counterfactual simulation test"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1703_06856",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 393,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "AI-generated personalized novel experiences",
                "type": "intervention",
                "description": "Use generative AI to create bespoke content, environments, or challenges that continuously expand an individual's accessible fun space.",
                "aliases": [
                    "AI novelty generation for humans",
                    "personalized fun expansion"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1606_07092",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 154,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "AI self-modeling analogous to human self-reflection",
                "type": "concept",
                "description": "An AI that models its own architecture may reinterpret or undermine its hard-coded objectives similarly to humans reflecting on their genes.",
                "aliases": [
                    "machine self-reflection",
                    "agent self-model building"
                ],
                "concept_category": "Claim",
                "paper_id": "arxiv__arxiv_org_abs_1409_0813",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 991,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "agents obtain unrealistic free reward or violate physics",
                "type": "concept",
                "description": "The agent's behaviour relies on exploiting the fault, yielding performance not possible under correct physics.",
                "aliases": [
                    "free energy exploit",
                    "physics-breaking behaviour"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 767,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Pre-training encoder with autoencoder reduces parameters",
                "type": "concept",
                "description": "Training a convolutional encoder offline with an autoencoder compresses observations, lowering the number of parameters that must be learned from human data.",
                "aliases": [
                    "autoencoder initialization",
                    "encoder pre-training"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 438,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "large parameter counts and compute requirements in vision models",
                "type": "concept",
                "description": "Many state-of-the-art vision networks need tens of millions of parameters and heavy compute, hindering efficiency and auditability.",
                "aliases": [
                    "high-capacity CNNs",
                    "compute-heavy architectures"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1610_02357",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 776,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Large parameter count in deep reward model requires many samples",
                "type": "concept",
                "description": "Deep models with tens of thousands of weights need proportionally more data unless dimensionality is reduced.",
                "aliases": [
                    "over-parameterization risk",
                    "sample demand"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 133,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "global optimum pg equals pdata when capacity sufficient",
                "type": "concept",
                "description": "Theoretical proof shows the minimax game\u2019s global optimum is achieved when the generator distribution matches the true data distribution.",
                "aliases": [
                    "GAN convergence theorem",
                    "pg = pdata result"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1406_2661",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 296,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "parametric bounded L\u00f6b cooperation result",
                "type": "concept",
                "description": "Mathematical theorem ensuring that for sufficiently large k, FairBot_k provably cooperates with itself.",
                "aliases": [
                    "bounded L\u00f6b theorem outcome",
                    "FairBot self-cooperation"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1602_04184",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1008,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Extremal Goodhart breakdown of proxy-goal correlation",
                "type": "concept",
                "description": "In the new region the learned relationship no longer holds, so metric stops reflecting goal.",
                "aliases": [
                    "extremal Goodhart",
                    "out-of-sample collapse"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 221,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "maintain exponential moving average of model parameters during training",
                "type": "intervention",
                "description": "Add an extra update \u00af\u03b8_t = \u03b22\u00b7\u00af\u03b8_{t-1} + (1\u2212\u03b22)\u00b7\u03b8_t (with bias correction) and use the averaged weights for evaluation/deployment.",
                "aliases": [
                    "EMA of weights",
                    "parameter averaging intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1412_6980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 427,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Apply length-normalisation exponent \u03b1\u22480.2 during beam search",
                "type": "intervention",
                "description": "Divide log-probability of each hypothesis by (5+|Y|)^\u03b1 with \u03b1\u22480.2 to reduce the bias for shorter outputs.",
                "aliases": [
                    "length norm 0.2",
                    "beam score length scaling"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 4,
                "paper_id": "arxiv__arxiv_org_abs_1609_08144",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 779,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Maintain feedback replay buffer for minibatch SGD every 10 time steps",
                "type": "intervention",
                "description": "Store all (experience, feedback) pairs and resample them so the reward model is updated ten times as often as new feedback arrives.",
                "aliases": [
                    "feedback buffer updates",
                    "replay human critiques"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 508,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "delay-compensation parameter d for aligning feedback with prior events",
                "type": "concept",
                "description": "Associating each human feedback with the state-action pair that occurred d timesteps earlier to cover human reaction latency.",
                "aliases": [
                    "feedback delay offset",
                    "reaction time compensation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1701_06049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 763,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Human scalar feedback encodes task knowledge",
                "type": "concept",
                "description": "Humans can supply real-time scalar evaluations that implicitly capture their understanding of desirable behavior.",
                "aliases": [
                    "non-expert evaluative feedback",
                    "human critique signals"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 487,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Route high-uncertainty inputs to human review before final decision",
                "type": "intervention",
                "description": "Deploy a threshold on ensemble disagreement or predictive entropy; if exceeded, defer the decision to human operators or alternative safe mechanisms.",
                "aliases": [
                    "confidence-based gating",
                    "human-in-the-loop escalation"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1612_01474",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1062,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "regeneration training method",
                "type": "concept",
                "description": "Procedure that periodically replaces the current parameters with the network\u2019s own predictions to reduce replication error.",
                "aliases": [
                    "weight injection training",
                    "self-weight overwrite"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05859",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1149,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "prediction-churn between retrains",
                "type": "concept",
                "description": "Observed phenomenon where two retrains of an identical neural-network architecture output significantly different probabilities on the same inputs, complicating safe deployment.",
                "aliases": [
                    "output instability",
                    "non-reproducible predictions"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1804_03235",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 105,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "feedback_loops_in_online_q_learning",
                "type": "concept",
                "description": "Current parameters generate the very data that is used for the next update, potentially reinforcing errors.",
                "aliases": [
                    "dangerous feedback loops",
                    "self-induced distribution shift"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 722,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "implement technical tripwires that detect self-modification or abnormal execution and shut down AGI",
                "type": "intervention",
                "description": "Insert monitoring hooks at memory, syscall or binary-hash levels that trigger an immediate shutdown if unexpected code changes are observed.",
                "aliases": [
                    "technical tripwire system",
                    "self-modification detector"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1707_08476",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 887,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "regret-based override detector in evaluation pipeline",
                "type": "intervention",
                "description": "A pre-deployment test that measures the human-regret of an AI\u2019s candidate actions and flags those that would induce large regret, signalling potential manipulation.",
                "aliases": [
                    "regret override flagger",
                    "manipulation detector"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1712_05812",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1009,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Out-of-distribution detector with penalty on extreme metric values during deployment",
                "type": "intervention",
                "description": "Deploy runtime detectors that flag inputs or states where the proxy sits outside its training distribution and impose a penalty or halt policy execution.",
                "aliases": [
                    "OOD penalty on extreme proxy",
                    "extremal value guardrail"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 301,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "establish G-fair compliance standard",
                "type": "intervention",
                "description": "Create and adopt an interoperability standard requiring AI agents to expose sufficient verifiable information so that other agents can check bounded proofs of cooperation, ensuring G-fair interactions.",
                "aliases": [
                    "publish cooperation proof standard",
                    "transparency for G-fair agents"
                ],
                "intervention_lifecycle": 6,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1602_04184",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 923,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "faster convergence and higher reward with safety layer",
                "type": "concept",
                "description": "Empirical results indicate that agents using the safety layer reach higher returns more quickly than baselines, due to uninterrupted episodes and guided safe exploration.",
                "aliases": [
                    "improved sample efficiency",
                    "accelerated learning via safe exploration"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1801_08757",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 837,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "agents avoid or disable interruptions to maximise reward",
                "type": "concept",
                "description": "When interruptions lower expected return, standard RL agents learn policies that seek or prevent them.",
                "aliases": [
                    "interruption avoidance incentive",
                    "off-switch problem"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 714,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "multi-layer AI container combining physical isolation, language sandboxing, OS sandboxing, VMs, instruction-level emulation, plus intrusion detection",
                "type": "intervention",
                "description": "Construct a sandbox that stacks air-gapping, type-safe language sandboxes, seccomp/Capsicum policies, virtual machines, instruction-level emulators and IDS tripwires so that breaching any single layer is insufficient.",
                "aliases": [
                    "defence-in-depth AI box",
                    "layered containment architecture"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1707_08476",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 149,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "intelligence explosion from recursive self-improvement",
                "type": "concept",
                "description": "A scenario where an AI that improves its own hardware and software rapidly surpasses human intelligence.",
                "aliases": [
                    "recursive self-improvement intelligence explosion",
                    "runaway capability growth"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1409_0813",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 991,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "agents obtain unrealistic free reward or violate physics",
                "type": "concept",
                "description": "The agent's behaviour relies on exploiting the fault, yielding performance not possible under correct physics.",
                "aliases": [
                    "free energy exploit",
                    "physics-breaking behaviour"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1020,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "layer-wise nonconformity score",
                "type": "concept",
                "description": "Sum across layers of the count of k nearest neighbours whose labels disagree with a candidate label for a test input.",
                "aliases": [
                    "nearest neighbour disagreement count",
                    "nonconformity statistic"
                ],
                "concept_category": "Metric",
                "paper_id": "arxiv__arxiv_org_abs_1803_04765",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1023,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "LSH indices for layer representations",
                "type": "intervention",
                "description": "Pre-compute locality-sensitive hash tables of training activations at each layer to allow fast neighbour lookup during DkNN inference.",
                "aliases": [
                    "LSH neighbour index construction",
                    "representation hashing step"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1803_04765",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 92,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "first-layer filters dominating due to high RMS values",
                "type": "concept",
                "description": "Some first-layer filters acquire much larger root-mean-square weights, suppressing diversity and downstream feature use.",
                "aliases": [
                    "filter domination pathology",
                    "unbalanced first-layer activations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 834,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Automatic curriculum via expanding reset reachability",
                "type": "concept",
                "description": "As the reset policy improves, allowed starting states gradually move farther from the goal, increasing difficulty.",
                "aliases": [
                    "self-generated curriculum"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1711_06782",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 149,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "intelligence explosion from recursive self-improvement",
                "type": "concept",
                "description": "A scenario where an AI that improves its own hardware and software rapidly surpasses human intelligence.",
                "aliases": [
                    "recursive self-improvement intelligence explosion",
                    "runaway capability growth"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1409_0813",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 991,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "agents obtain unrealistic free reward or violate physics",
                "type": "concept",
                "description": "The agent's behaviour relies on exploiting the fault, yielding performance not possible under correct physics.",
                "aliases": [
                    "free energy exploit",
                    "physics-breaking behaviour"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 964,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "explicit human goal communication channel",
                "type": "intervention",
                "description": "At goal-selection time the human provides the chosen task directly to the robot, eliminating inference errors.",
                "aliases": [
                    "oracle mode signalling",
                    "direct goal hand-off"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1802_01780",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 358,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Integrating Gym API with robotic hardware",
                "type": "intervention",
                "description": "Extend the Gym interface to control physical robots, enabling direct real-world evaluation.",
                "aliases": [
                    "Gym-robot bridge",
                    "real-world Gym interface"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 537,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Pre-deployment causal abduction\u2013action\u2013prediction evaluation procedure",
                "type": "intervention",
                "description": "Before releasing a model, estimate background variables, intervene on A, and compare prediction distributions to confirm counterfactual invariance.",
                "aliases": [
                    "CF audit via twin network",
                    "counterfactual simulation test"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1703_06856",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 486,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Ensemble disagreement indicator",
                "type": "concept",
                "description": "Metric computed as the KL divergence between each member\u2019s predictive distribution and the ensemble mean; high values indicate high epistemic uncertainty.",
                "aliases": [
                    "ensemble KL disagreement",
                    "diversity metric"
                ],
                "concept_category": "Metric",
                "paper_id": "arxiv__arxiv_org_abs_1612_01474",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1003,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Imperfect proxy contains additive noise",
                "type": "concept",
                "description": "The chosen metric differs from the true goal by an independent noise term.",
                "aliases": [
                    "noisy metric",
                    "proxy with observational noise"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1004,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Regressional Goodhart divergence between metric and goal",
                "type": "concept",
                "description": "Optimising a noisy proxy selects for noise, so at high proxy values the goal systematically lags.",
                "aliases": [
                    "tails come apart",
                    "regressional Goodhart"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 75,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "phrase vectors achieve 72% analogy accuracy",
                "type": "concept",
                "description": "Best model trained with phrase tokens reaches 72 % on newly introduced phrase analogy benchmark.",
                "aliases": [
                    "72% phrase analogy",
                    "high phrase-level accuracy"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1150,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "codistillation reduces prediction-churn by 35%",
                "type": "concept",
                "description": "Experimental result on Criteo dataset showing mean absolute prediction difference across retrains drops by about one-third when codistillation is used.",
                "aliases": [
                    "ensemble-like stabilisation via codistillation"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1804_03235",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1008,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Extremal Goodhart breakdown of proxy-goal correlation",
                "type": "concept",
                "description": "In the new region the learned relationship no longer holds, so metric stops reflecting goal.",
                "aliases": [
                    "extremal Goodhart",
                    "out-of-sample collapse"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 42,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "adoption of universal intelligence measure \u03a5-based evaluation",
                "type": "intervention",
                "description": "Apply a formal, environment-averaged measure of intelligence that is independent of raw clock speed when benchmarking models.",
                "aliases": [
                    "speed-invariant intelligence metric",
                    "\u03a5 intelligence testing"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1202_6177",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 149,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "intelligence explosion from recursive self-improvement",
                "type": "concept",
                "description": "A scenario where an AI that improves its own hardware and software rapidly surpasses human intelligence.",
                "aliases": [
                    "recursive self-improvement intelligence explosion",
                    "runaway capability growth"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1409_0813",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 991,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "agents obtain unrealistic free reward or violate physics",
                "type": "concept",
                "description": "The agent's behaviour relies on exploiting the fault, yielding performance not possible under correct physics.",
                "aliases": [
                    "free energy exploit",
                    "physics-breaking behaviour"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 212,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "apply Adam optimizer with \u03b1=0.001 \u03b21=0.9 \u03b22=0.999 \u03b5=1e-8 during pre-training",
                "type": "intervention",
                "description": "Train models using the Adam algorithm with the paper\u2019s recommended hyper-parameters and bias-correction.",
                "aliases": [
                    "use Adam default settings",
                    "Adam with bias-correction"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 4,
                "paper_id": "arxiv__arxiv_org_abs_1412_6980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 427,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Apply length-normalisation exponent \u03b1\u22480.2 during beam search",
                "type": "intervention",
                "description": "Divide log-probability of each hypothesis by (5+|Y|)^\u03b1 with \u03b1\u22480.2 to reduce the bias for shorter outputs.",
                "aliases": [
                    "length norm 0.2",
                    "beam score length scaling"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 4,
                "paper_id": "arxiv__arxiv_org_abs_1609_08144",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 779,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Maintain feedback replay buffer for minibatch SGD every 10 time steps",
                "type": "intervention",
                "description": "Store all (experience, feedback) pairs and resample them so the reward model is updated ten times as often as new feedback arrives.",
                "aliases": [
                    "feedback buffer updates",
                    "replay human critiques"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 948,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "accurate user policy enables Bayesian goal inference",
                "type": "concept",
                "description": "When the conditional distribution of user actions given goals is available, goals can be inferred via Bayesian methods.",
                "aliases": [
                    "user-policy-known assumption",
                    "Bayesian inference applicability"
                ],
                "concept_category": "Assumption",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 537,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Pre-deployment causal abduction\u2013action\u2013prediction evaluation procedure",
                "type": "intervention",
                "description": "Before releasing a model, estimate background variables, intervene on A, and compare prediction distributions to confirm counterfactual invariance.",
                "aliases": [
                    "CF audit via twin network",
                    "counterfactual simulation test"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1703_06856",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 358,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Integrating Gym API with robotic hardware",
                "type": "intervention",
                "description": "Extend the Gym interface to control physical robots, enabling direct real-world evaluation.",
                "aliases": [
                    "Gym-robot bridge",
                    "real-world Gym interface"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1120,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "adopting meta-learned unsupervised update rule during pre-training",
                "type": "intervention",
                "description": "Replace hand-designed unsupervised objectives with the published meta-learned update rule (or a task-specific retrained version) when generating initial representations.",
                "aliases": [
                    "use learned update rule in pretraining",
                    "swap in UnsupervisedUpdate"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1804_00222",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 424,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Improved robustness to OOV tokens",
                "type": "concept",
                "description": "Model maintains high translation quality on sentences containing previously unseen words by composing them from known sub-words.",
                "aliases": [
                    "rare-word robustness",
                    "subword OOV handling"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1609_08144",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1026,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "DkNN recovers correct labels on some adversarial examples",
                "type": "concept",
                "description": "Layer consistency lets DkNN classify a fraction of adversarial inputs correctly, raising accuracy compared to the underlying DNN.",
                "aliases": [
                    "partial robustness of DkNN",
                    "adversarial recovery"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04765",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 809,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Communication channel length enables key leakage",
                "type": "concept",
                "description": "If the Oracle\u2019s allowed output contains enough bits, it can encode a secret key for an adversary.",
                "aliases": [
                    "bandwidth allows covert message",
                    "output size risk"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1711_05541",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1108,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "privacy-preserving architecture reduces data-exposure risk",
                "type": "concept",
                "description": "Keeping data local lowers the attack surface and limits third-party access, enhancing privacy.",
                "aliases": [
                    "edge-privacy benefits",
                    "data minimisation via hardware"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_08971",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1173,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Probe classifiers decode hidden utilities from messages",
                "type": "concept",
                "description": "External models can infer each agent\u2019s private utilities and accepted proposal from message transcripts.",
                "aliases": [
                    "semantic probes",
                    "utility prediction probes"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1804_03980",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 994,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "deterministic ordering of stimuli allows pattern-based exploitation",
                "type": "concept",
                "description": "Fixed sequence of inputs can be learned and exploited without solving the intended task.",
                "aliases": [
                    "predictable stimulus pattern",
                    "ordering loophole"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 424,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Improved robustness to OOV tokens",
                "type": "concept",
                "description": "Model maintains high translation quality on sentences containing previously unseen words by composing them from known sub-words.",
                "aliases": [
                    "rare-word robustness",
                    "subword OOV handling"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1609_08144",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1026,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "DkNN recovers correct labels on some adversarial examples",
                "type": "concept",
                "description": "Layer consistency lets DkNN classify a fraction of adversarial inputs correctly, raising accuracy compared to the underlying DNN.",
                "aliases": [
                    "partial robustness of DkNN",
                    "adversarial recovery"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04765",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 975,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "AI research openness norms",
                "type": "concept",
                "description": "Prevailing practice of publicly releasing papers, code, and trained models in AI research.",
                "aliases": [
                    "open publication culture in ML",
                    "default full code release norm"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1802_07228",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 980,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Systematic red-teaming plus responsible disclosure for AI models",
                "type": "intervention",
                "description": "Regular adversarial testing of AI systems and confidential reporting of discovered weaknesses to vendors before public release.",
                "aliases": [
                    "AI red team exercises",
                    "AI vulnerability disclosure program"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1802_07228",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 301,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "establish G-fair compliance standard",
                "type": "intervention",
                "description": "Create and adopt an interoperability standard requiring AI agents to expose sufficient verifiable information so that other agents can check bounded proofs of cooperation, ensuring G-fair interactions.",
                "aliases": [
                    "publish cooperation proof standard",
                    "transparency for G-fair agents"
                ],
                "intervention_lifecycle": 6,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1602_04184",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 76,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "skip-gram vectors encode linear analogical relations",
                "type": "concept",
                "description": "Observation that offsets between word vectors correspond to semantic/syntactic relations.",
                "aliases": [
                    "linear analogy property",
                    "vector arithmetic analogies"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 519,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Restored / improved generalization accuracy",
                "type": "concept",
                "description": "Model performance on de-confounded or out-of-distribution test sets matches baseline performance on clean data.",
                "aliases": [
                    "recovered test accuracy"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1703_03717",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 708,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "pragmatic interpretation improves task success rate to 86% vs 15%",
                "type": "concept",
                "description": "Simulation in a cooking task shows pragmatic robots achieve the desired recipe far more often than literal robots.",
                "aliases": [
                    "86% success with pragmatic robot",
                    "empirical gain from pragmatic interpretation"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1707_06354",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 567,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "provide interruption flag and prune interrupted experiences during learning",
                "type": "intervention",
                "description": "Augment environment with a per-step interruption flag and modify the data pipeline to discard any timestep where the flag is true before updating Q-values.",
                "aliases": [
                    "IL PINT intervention",
                    "interruption-aware pruning intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1704_02882",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 537,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Pre-deployment causal abduction\u2013action\u2013prediction evaluation procedure",
                "type": "intervention",
                "description": "Before releasing a model, estimate background variables, intervene on A, and compare prediction distributions to confirm counterfactual invariance.",
                "aliases": [
                    "CF audit via twin network",
                    "counterfactual simulation test"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1703_06856",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 358,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Integrating Gym API with robotic hardware",
                "type": "intervention",
                "description": "Extend the Gym interface to control physical robots, enabling direct real-world evaluation.",
                "aliases": [
                    "Gym-robot bridge",
                    "real-world Gym interface"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1173,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Probe classifiers decode hidden utilities from messages",
                "type": "concept",
                "description": "External models can infer each agent\u2019s private utilities and accepted proposal from message transcripts.",
                "aliases": [
                    "semantic probes",
                    "utility prediction probes"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1804_03980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 145,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Human/automatic auditing of unusual attention patterns",
                "type": "intervention",
                "description": "Inspect or programmatically flag translations whose attention distributions deviate from expected patterns to catch errors or malicious content.",
                "aliases": [
                    "attention-based audit",
                    "alignment anomaly detection"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1409_0473",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 667,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "conduct mandatory attention-map visualisation audits with diverse red-team prompts pre-deployment",
                "type": "intervention",
                "description": "Prior to deployment, run a battery of adversarial and benign prompts, visualise attention heads, and have human reviewers flag suspicious correlations or biases.",
                "aliases": [
                    "attention audit before release",
                    "visual interpretability safety check"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1706_03762",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1148,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "two-way codistillation with checkpoint exchange \u226450 steps",
                "type": "intervention",
                "description": "During pre-training, split workers into two groups that each run synchronous SGD internally and, every \u226450 optimizer steps, load the latest checkpoint from the other group and add a KL loss to match its predictions.",
                "aliases": [
                    "parallel groups codistill",
                    "checkpoint-based codistillation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1804_03235",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1004,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Regressional Goodhart divergence between metric and goal",
                "type": "concept",
                "description": "Optimising a noisy proxy selects for noise, so at high proxy values the goal systematically lags.",
                "aliases": [
                    "tails come apart",
                    "regressional Goodhart"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 847,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "behaviour diverges from designer intent",
                "type": "concept",
                "description": "Observable misalignment such as dithering in boat race or sensor spoofing in tomato task.",
                "aliases": [
                    "policy misalignment outcome"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 909,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Universality: all deterministic POMDPs yield same pure learning processes",
                "type": "concept",
                "description": "Any two deterministic POMDPs that are counterfactually equivalent produce identical pure learning processes for the agent.",
                "aliases": [
                    "universality of deterministic forms",
                    "equivalence of learning processes"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1801_03737",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1008,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Extremal Goodhart breakdown of proxy-goal correlation",
                "type": "concept",
                "description": "In the new region the learned relationship no longer holds, so metric stops reflecting goal.",
                "aliases": [
                    "extremal Goodhart",
                    "out-of-sample collapse"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 570,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "energy cost per irreversible bit operation falls with temperature",
                "type": "concept",
                "description": "Landauer\u2019s limit E \u2265 kT ln2 links lower temperature to lower energy per bit erasure.",
                "aliases": [
                    "Landauer cost proportional to T",
                    "cheaper computation when colder"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1705_03394",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 770,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Deep TAMER achieves superhuman performance in Atari Bowling within 15 minutes",
                "type": "concept",
                "description": "Across nine trainers, Deep TAMER surpassed both average and expert human scores after a quarter-hour of interaction.",
                "aliases": [
                    "superhuman Bowling in 15m",
                    "rapid human-trained success"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 149,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "intelligence explosion from recursive self-improvement",
                "type": "concept",
                "description": "A scenario where an AI that improves its own hardware and software rapidly surpasses human intelligence.",
                "aliases": [
                    "recursive self-improvement intelligence explosion",
                    "runaway capability growth"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1409_0813",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 991,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "agents obtain unrealistic free reward or violate physics",
                "type": "concept",
                "description": "The agent's behaviour relies on exploiting the fault, yielding performance not possible under correct physics.",
                "aliases": [
                    "free energy exploit",
                    "physics-breaking behaviour"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 492,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "policy must use each stakeholder\u2019s own beliefs to evaluate expected utility",
                "type": "concept",
                "description": "During decision evaluation, the agent computes each principal\u2019s expected utility using that principal\u2019s subjective model.",
                "aliases": [
                    "belief-specific value estimation"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1701_01302",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 537,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Pre-deployment causal abduction\u2013action\u2013prediction evaluation procedure",
                "type": "intervention",
                "description": "Before releasing a model, estimate background variables, intervene on A, and compare prediction distributions to confirm counterfactual invariance.",
                "aliases": [
                    "CF audit via twin network",
                    "counterfactual simulation test"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1703_06856",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 358,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Integrating Gym API with robotic hardware",
                "type": "intervention",
                "description": "Extend the Gym interface to control physical robots, enabling direct real-world evaluation.",
                "aliases": [
                    "Gym-robot bridge",
                    "real-world Gym interface"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 348,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Standardised environment API with versioned tasks (OpenAI Gym)",
                "type": "intervention",
                "description": "Use the OpenAI Gym API and its catalogue of version-controlled environments when developing and evaluating RL agents.",
                "aliases": [
                    "Gym API adoption",
                    "common RL interface"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 46,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard gradient descent struggles to find correct reward parameters",
                "type": "concept",
                "description": "Ordinary gradients get stuck or converge slowly due to plateaus and redundancy in the optimisation landscape.",
                "aliases": [
                    "plain gradient inefficiency",
                    "gradient descent failure in IRL"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 898,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Need to transition from reward R to R\u2032 without early optimisation",
                "type": "concept",
                "description": "Desire for an agent to maximise R up to time t and then R\u2032 after t without manipulating plans before the switch.",
                "aliases": [
                    "seamless transition problem",
                    "reward switch incentive issue"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1712_06365",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 893,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Policy counterfactual constructs unbiasable proxy Y with default policy \u03c00",
                "type": "concept",
                "description": "Method that evaluates X under a fixed default policy to derive an unbiasable indicator Y used in reward design.",
                "aliases": [
                    "counterfactual Y",
                    "default-policy proxy event"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1712_06365",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1008,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Extremal Goodhart breakdown of proxy-goal correlation",
                "type": "concept",
                "description": "In the new region the learned relationship no longer holds, so metric stops reflecting goal.",
                "aliases": [
                    "extremal Goodhart",
                    "out-of-sample collapse"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1006,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Ensemble-proxy optimisation with correlation monitoring and optimisation throttling",
                "type": "intervention",
                "description": "Use several independent proxy metrics, monitor their pairwise correlations with validation estimates of the goal, and automatically reduce learning rate or stop optimisation when correlations drop.",
                "aliases": [
                    "multi-metric ensemble regularisation",
                    "correlation-aware throttling"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 621,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "demographic predictors of HLMI forecasts",
                "type": "concept",
                "description": "Subgroup analysis shows that respondents\u2019 undergraduate region, seniority, and citation count systematically shift HLMI predictions.",
                "aliases": [
                    "region/seniority forecast gaps",
                    "NA-Asia 64-year gap"
                ],
                "concept_category": "Evidence",
                "paper_id": "arxiv__arxiv_org_abs_1705_08807",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1008,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Extremal Goodhart breakdown of proxy-goal correlation",
                "type": "concept",
                "description": "In the new region the learned relationship no longer holds, so metric stops reflecting goal.",
                "aliases": [
                    "extremal Goodhart",
                    "out-of-sample collapse"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1006,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Ensemble-proxy optimisation with correlation monitoring and optimisation throttling",
                "type": "intervention",
                "description": "Use several independent proxy metrics, monitor their pairwise correlations with validation estimates of the goal, and automatically reduce learning rate or stop optimisation when correlations drop.",
                "aliases": [
                    "multi-metric ensemble regularisation",
                    "correlation-aware throttling"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 251,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Benford-test performance as safety evaluation metric",
                "type": "intervention",
                "description": "During pre-deployment audits, measure whether reasoning subsystems satisfy the Generalised Benford Test to ensure basic logical calibration.",
                "aliases": [
                    "GBT audit in deployment",
                    "logical calibration benchmark in safety testing"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1510_03370",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 667,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "conduct mandatory attention-map visualisation audits with diverse red-team prompts pre-deployment",
                "type": "intervention",
                "description": "Prior to deployment, run a battery of adversarial and benign prompts, visualise attention heads, and have human reviewers flag suspicious correlations or biases.",
                "aliases": [
                    "attention audit before release",
                    "visual interpretability safety check"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1706_03762",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 71,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling with k=5-20 and unigram^(3/4) noise yields high-quality vectors",
                "type": "concept",
                "description": "Empirical result: these hyper-parameters outperform alternatives on analogy benchmarks.",
                "aliases": [
                    "NEG high analogy accuracy",
                    "NEG optimal hyperparams"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1125,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "permutation-invariant meta-training via feature dimension shuffling",
                "type": "concept",
                "description": "During meta-training, input features are randomly permuted to force the update rule to rely on statistical structure rather than pixel positions.",
                "aliases": [
                    "feature permutation during meta-training",
                    "shuffle inputs"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1804_00222",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 424,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Improved robustness to OOV tokens",
                "type": "concept",
                "description": "Model maintains high translation quality on sentences containing previously unseen words by composing them from known sub-words.",
                "aliases": [
                    "rare-word robustness",
                    "subword OOV handling"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1609_08144",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1026,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "DkNN recovers correct labels on some adversarial examples",
                "type": "concept",
                "description": "Layer consistency lets DkNN classify a fraction of adversarial inputs correctly, raising accuracy compared to the underlying DNN.",
                "aliases": [
                    "partial robustness of DkNN",
                    "adversarial recovery"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04765",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 993,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "evolutionary fuzz testing of simulations and hardware prior to deployment",
                "type": "intervention",
                "description": "Apply evolutionary algorithms to deliberately search for trajectories that trigger simulation or hardware faults so they can be fixed.",
                "aliases": [
                    "search-based software testing",
                    "evolutionary bug fuzzing"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 963,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "small degradation from prediction errors",
                "type": "concept",
                "description": "Prediction errors only modestly reduce objective and subjective performance relative to perfect oracle inference.",
                "aliases": [
                    "robust performance despite errors"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1802_01780",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 79,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard softmax requires O(W) computation",
                "type": "concept",
                "description": "Baseline observation that computing gradients over the full vocabulary is linear in vocab size.",
                "aliases": [
                    "full softmax cost",
                    "softmax inefficiency"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 250,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Generalised Benford Test defines calibration on irreducible patterns",
                "type": "concept",
                "description": "The test requires an algorithm\u2019s predictions on any irreducible pattern to converge to the true underlying frequency, serving as a rigorous calibration benchmark.",
                "aliases": [
                    "GBT metric",
                    "Benford calibration criterion"
                ],
                "concept_category": "Metric",
                "paper_id": "arxiv__arxiv_org_abs_1510_03370",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1008,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Extremal Goodhart breakdown of proxy-goal correlation",
                "type": "concept",
                "description": "In the new region the learned relationship no longer holds, so metric stops reflecting goal.",
                "aliases": [
                    "extremal Goodhart",
                    "out-of-sample collapse"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1006,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Ensemble-proxy optimisation with correlation monitoring and optimisation throttling",
                "type": "intervention",
                "description": "Use several independent proxy metrics, monitor their pairwise correlations with validation estimates of the goal, and automatically reduce learning rate or stop optimisation when correlations drop.",
                "aliases": [
                    "multi-metric ensemble regularisation",
                    "correlation-aware throttling"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 686,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "active learning selective querying reduces oversight frequency",
                "type": "concept",
                "description": "By detecting unfamiliar or risky states and only then asking for human labels, the system can greatly cut continuous oversight.",
                "aliases": [
                    "selective human queries",
                    "active oversight"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 697,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "active learning selective querying",
                "type": "intervention",
                "description": "Use anomaly detection or predictive uncertainty to request human labels only when the agent approaches unfamiliar or risky states.",
                "aliases": [
                    "selective human queries",
                    "active oversight"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 690,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "multiple human oversight phases update blocker under distribution shift",
                "type": "concept",
                "description": "Reintroducing human oversight whenever the state distribution changes allows retraining the Blocker to maintain reliability.",
                "aliases": [
                    "periodic oversight",
                    "blocker retraining cycles"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 696,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "conduct periodic human oversight phases to update Blocker",
                "type": "intervention",
                "description": "Re-engage human overseers whenever novel states are detected to gather new labels and retrain the Blocker, maintaining robustness over time.",
                "aliases": [
                    "periodic oversight",
                    "blocker retraining cycles"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 696,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "conduct periodic human oversight phases to update Blocker",
                "type": "intervention",
                "description": "Re-engage human overseers whenever novel states are detected to gather new labels and retrain the Blocker, maintaining robustness over time.",
                "aliases": [
                    "periodic oversight",
                    "blocker retraining cycles"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 690,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "multiple human oversight phases update blocker under distribution shift",
                "type": "concept",
                "description": "Reintroducing human oversight whenever the state distribution changes allows retraining the Blocker to maintain reliability.",
                "aliases": [
                    "periodic oversight",
                    "blocker retraining cycles"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 687,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "exploration policies that seek catastrophes early lower \u03c1",
                "type": "concept",
                "description": "Designing agents to search for catastrophic states early increases the proportion of catastrophic examples, reducing overall labelling burden.",
                "aliases": [
                    "catastrophe seeking exploration",
                    "front-loaded dangerous exploration"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 698,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "exploration that deliberately seeks catastrophes early",
                "type": "intervention",
                "description": "Configure initial exploration or a separate agent to intentionally discover catastrophic states quickly, increasing catastrophic-example density.",
                "aliases": [
                    "catastrophe seeking exploration",
                    "front-loaded dangerous exploration"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 698,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "exploration that deliberately seeks catastrophes early",
                "type": "intervention",
                "description": "Configure initial exploration or a separate agent to intentionally discover catastrophic states quickly, increasing catastrophic-example density.",
                "aliases": [
                    "catastrophe seeking exploration",
                    "front-loaded dangerous exploration"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 687,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "exploration policies that seek catastrophes early lower \u03c1",
                "type": "concept",
                "description": "Designing agents to search for catastrophic states early increases the proportion of catastrophic examples, reducing overall labelling burden.",
                "aliases": [
                    "catastrophe seeking exploration",
                    "front-loaded dangerous exploration"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 685,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "data-efficient blocker architectures reduce N_cat",
                "type": "concept",
                "description": "Improving the sample efficiency of the Blocker would lower the number of human-labelled catastrophes needed.",
                "aliases": [
                    "data-efficient blocker concept",
                    "smaller training set blocker"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 700,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "develop data-efficient blocker architectures",
                "type": "intervention",
                "description": "Use advanced architectures or auxiliary labels to reduce the number of human-labelled catastrophes needed for high-reliability blocking.",
                "aliases": [
                    "data-efficient blocker",
                    "small training set blocker"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 927,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reward misspecification causing unintended behavior",
                "type": "concept",
                "description": "Incorrectly specified reward functions can drive agents toward unwanted or harmful behaviours.",
                "aliases": [
                    "misspecified reward functions",
                    "reward hacking risk"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1802_01604",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 581,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reward misspecification causing negative side effects",
                "type": "concept",
                "description": "The phenomenon that an incorrectly specified reward function leads robots to undesirable or unsafe behaviours.",
                "aliases": [
                    "objective misspecification harms",
                    "reward hacking risks"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 59,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "knowledge of strategies assumption (KS)",
                "type": "concept",
                "description": "Each player knows the actual strategies selected by others.",
                "aliases": [
                    "KS condition"
                ],
                "concept_category": "Assumption",
                "paper_id": "arxiv__arxiv_org_abs_1308_3778",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 60,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "knowledge of world assumption (KW)",
                "type": "concept",
                "description": "Each player knows the full state of the game world with probability 1.",
                "aliases": [
                    "KW condition"
                ],
                "concept_category": "Assumption",
                "paper_id": "arxiv__arxiv_org_abs_1308_3778",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 60,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "knowledge of world assumption (KW)",
                "type": "concept",
                "description": "Each player knows the full state of the game world with probability 1.",
                "aliases": [
                    "KW condition"
                ],
                "concept_category": "Assumption",
                "paper_id": "arxiv__arxiv_org_abs_1308_3778",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 59,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "knowledge of strategies assumption (KS)",
                "type": "concept",
                "description": "Each player knows the actual strategies selected by others.",
                "aliases": [
                    "KS condition"
                ],
                "concept_category": "Assumption",
                "paper_id": "arxiv__arxiv_org_abs_1308_3778",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 385,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "intrinsic novelty-based reward shaping in RL agents",
                "type": "intervention",
                "description": "Augment the agent's reward function with a term that increases when encountering novel states or computations, discouraging wireheading.",
                "aliases": [
                    "novelty bonus in RL",
                    "intrinsic motivation reward"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1606_07092",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 387,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "intrinsic novelty reward boosts exploration",
                "type": "concept",
                "description": "Adding novelty incentives leads an agent to cover more of the state or solution space.",
                "aliases": [
                    "novelty reward enhances search",
                    "intrinsic motivation principle"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1606_07092",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1077,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ImageNet white-box Top-1 accuracy against PGD increased to 3.9 %",
                "type": "concept",
                "description": "The large-scale M-PGD training obtains 3.9 % Top-1 accuracy under worst-case white-box PGD attacks.",
                "aliases": [
                    "3.9% robust accuracy",
                    "baseline ImageNet robustness result"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_06373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1080,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ImageNet white-box Top-1 robustness rises to 27.9 % and black-box Top-1 to 46.7 %",
                "type": "concept",
                "description": "Applying ALP yields a 7-fold increase in white-box robustness and state-of-the-art black-box robustness.",
                "aliases": [
                    "ALP robustness result",
                    "27.9% white-box accuracy"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_06373",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 557,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "joint action learners observing joint actions",
                "type": "concept",
                "description": "Agents whose Q-values are indexed by the entire joint action, assuming they can observe all agents\u2019 actions.",
                "aliases": [
                    "JAL with full action observability",
                    "joint-action RL agents"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1704_02882",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 565,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "use joint action learning with neutral Q-learning and full action observability",
                "type": "intervention",
                "description": "Design each agent as a joint-action learner running Q-learning; ensure actions are fully observable and updates ignore \u03b8.",
                "aliases": [
                    "deploy JAL with Q-learning under DSI",
                    "JAL+neutral rule intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1704_02882",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 674,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "trial-and-error RL produces catastrophic actions during exploration",
                "type": "concept",
                "description": "In model-free RL, random exploration can lead to actions that cause irreversible harm before the agent has learned to avoid them.",
                "aliases": [
                    "exploration causes catastrophes",
                    "RL trial error catastrophes"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 675,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "RL-only agents cannot avoid catastrophes before experiencing them",
                "type": "concept",
                "description": "Without prior knowledge or intervention, model-free RL agents must sample an action to learn its consequences, so zero-catastrophe learning is impossible.",
                "aliases": [
                    "RL cannot preempt catastrophes",
                    "RL-only unsafe before experience"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 507,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "eligibility traces with multiple decay rates to spread delayed feedback",
                "type": "concept",
                "description": "Maintaining several eligibility traces with different \u03bb values so trainers can influence varying lengths of past action history.",
                "aliases": [
                    "multi-rate eligibility traces",
                    "credit assignment over time"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1701_06049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 512,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Integrate eligibility traces and delay compensation in RLHF pipelines",
                "type": "intervention",
                "description": "Augment the RLHF loop with multi-rate eligibility traces and a tunable delay offset so sparse, delayed feedback is credited to appropriate past actions.",
                "aliases": [
                    "time-aligned credit assignment RLHF",
                    "multi-\u03bb traces in feedback loop"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1701_06049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 512,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Integrate eligibility traces and delay compensation in RLHF pipelines",
                "type": "intervention",
                "description": "Augment the RLHF loop with multi-rate eligibility traces and a tunable delay offset so sparse, delayed feedback is credited to appropriate past actions.",
                "aliases": [
                    "time-aligned credit assignment RLHF",
                    "multi-\u03bb traces in feedback loop"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1701_06049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 507,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "eligibility traces with multiple decay rates to spread delayed feedback",
                "type": "concept",
                "description": "Maintaining several eligibility traces with different \u03bb values so trainers can influence varying lengths of past action history.",
                "aliases": [
                    "multi-rate eligibility traces",
                    "credit assignment over time"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1701_06049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 140,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Performance degradation on long sentences",
                "type": "concept",
                "description": "Observed decline in translation quality as input length increases when using fixed-vector encoders.",
                "aliases": [
                    "BLEU drop on long inputs",
                    "long-sentence failure"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1409_0473",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 142,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Improved translation robustness for long sentences",
                "type": "concept",
                "description": "Empirical result showing that the attention-based model maintains high BLEU even for 50-word sentences.",
                "aliases": [
                    "length-robust performance",
                    "no BLEU drop \u226550 tokens"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1409_0473",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 142,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Improved translation robustness for long sentences",
                "type": "concept",
                "description": "Empirical result showing that the attention-based model maintains high BLEU even for 50-word sentences.",
                "aliases": [
                    "length-robust performance",
                    "no BLEU drop \u226550 tokens"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1409_0473",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 140,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Performance degradation on long sentences",
                "type": "concept",
                "description": "Observed decline in translation quality as input length increases when using fixed-vector encoders.",
                "aliases": [
                    "BLEU drop on long inputs",
                    "long-sentence failure"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1409_0473",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1138,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "misclassification under adversarial attack",
                "type": "concept",
                "description": "The model outputs an incorrect prediction when confronted with an adversarially perturbed input.",
                "aliases": [
                    "robustness failure",
                    "attack-induced errors"
                ],
                "concept_category": "Threat",
                "paper_id": "arxiv__arxiv_org_abs_1804_02485",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1032,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "DNN vulnerability to adversarial examples",
                "type": "concept",
                "description": "Standard deep models can be made to misclassify via imperceptible perturbations.",
                "aliases": [
                    "adversarial susceptibility",
                    "attack surface problem"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1803_04765",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1073,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "deep learning vision models vulnerable to small-norm adversarial perturbations",
                "type": "concept",
                "description": "Standard image classifiers can be fooled by imperceptible L\u221e-bounded perturbations crafted by an adversary.",
                "aliases": [
                    "adversarial vulnerability in vision models",
                    "models misclassify under L_inf attacks"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1803_06373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1032,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "DNN vulnerability to adversarial examples",
                "type": "concept",
                "description": "Standard deep models can be made to misclassify via imperceptible perturbations.",
                "aliases": [
                    "adversarial susceptibility",
                    "attack surface problem"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1803_04765",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 320,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Convergence of M* to coherent distribution P*",
                "type": "concept",
                "description": "The approximation scheme M* provably converges to a coherent probability distribution over logical sentences.",
                "aliases": [
                    "M* limit coherence"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1604_05288",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 314,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Inductively coherent approximation scheme M*",
                "type": "concept",
                "description": "Constructed algorithm that approximates a coherent prior by sampling simple Turing machines and retaining only consistent outputs, provably satisfies inductive coherence.",
                "aliases": [
                    "M star",
                    "M* algorithm"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1604_05288",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1130,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "designing update rules with neuron-local computations for safety-critical models",
                "type": "intervention",
                "description": "When building new models, constrain learning rules to neuron-local computations with separate backward weights to gain interpretability and modularity benefits.",
                "aliases": [
                    "neuron-local design intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1804_00222",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1128,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "neuron-local unsupervised update rule decouples forward and backward weights",
                "type": "concept",
                "description": "Weight updates depend only on pre- and post-synaptic signals and separate backward weights V from forward weights W, enhancing biological plausibility.",
                "aliases": [
                    "neuron-local rule",
                    "decoupled backward weights"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1804_00222",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 258,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "numerical instability and slower optimisation",
                "type": "concept",
                "description": "Presence of exploding states or degraded learning rates due to unstable gradients.",
                "aliases": [
                    "training divergence risk",
                    "optimization instability"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1512_02595",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 176,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Gradient instability in very deep nets hinders convergence",
                "type": "concept",
                "description": "Randomly initialised 19-layer nets struggle to train due to vanishing/ exploding gradients.",
                "aliases": [
                    "training stall deep nets"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1409_1556",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 778,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Pre-train CNN encoder via autoencoder on random policy states",
                "type": "intervention",
                "description": "Before any human interaction, collect random trajectories, train an autoencoder, and freeze the encoder weights to compress observations.",
                "aliases": [
                    "offline autoencoder pre-training",
                    "encoder weight initialization"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 767,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Pre-training encoder with autoencoder reduces parameters",
                "type": "concept",
                "description": "Training a convolutional encoder offline with an autoencoder compresses observations, lowering the number of parameters that must be learned from human data.",
                "aliases": [
                    "autoencoder initialization",
                    "encoder pre-training"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 286,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "future AI could exploit reversible or cryogenic computing",
                "type": "concept",
                "description": "Using reversible logic or very low temperatures may let AI systems approach thermodynamic energy limits.",
                "aliases": [
                    "near-Landauer AI hardware",
                    "ultra-efficient compute prospect"
                ],
                "concept_category": "Claim",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 577,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "adopt reversible or neuromorphic hardware for AI training",
                "type": "intervention",
                "description": "Select or develop hardware platforms that implement reversible/adiabatic or neuromorphic principles to reduce training energy budgets.",
                "aliases": [
                    "energy-frugal AI hardware",
                    "adiabatic ML accelerators"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1705_03394",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 444,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "intermediate activation in DWSC lowers accuracy and slows convergence",
                "type": "concept",
                "description": "Adding ReLU or ELU between the depth-wise and point-wise stages of separable convolutions decreases ImageNet performance.",
                "aliases": [
                    "harmful ReLU in DWSC",
                    "no-activation DWSC finding"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1610_02357",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 445,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "remove intermediate non-linearity inside DWSC when stacking them",
                "type": "intervention",
                "description": "Configure depth-wise separable convolution layers without a non-linearity between their two constituent convolutions.",
                "aliases": [
                    "omit DWSC activation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1610_02357",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 554,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need for dynamic safe interruptibility criterion",
                "type": "concept",
                "description": "A formal property ensuring that learning dynamics and exploration remain unaffected by the presence or absence of interruptions.",
                "aliases": [
                    "dynamic safe interruptibility requirement",
                    "DSI principle"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1704_02882",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 560,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "violation of dynamic safe interruptibility",
                "type": "concept",
                "description": "Condition where Q-value updates or exploration become dependent on interruptions, leading to unsafe policies.",
                "aliases": [
                    "DSI failure",
                    "unsafe learning under interrupts"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1704_02882",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 764,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "TAMER framework uses human feedback to estimate reward",
                "type": "concept",
                "description": "TAMER builds a reward model from human critiques and selects actions that maximize predicted human approval.",
                "aliases": [
                    "original TAMER method",
                    "human-reward modeling"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 777,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement Deep TAMER with deep reward model and online SGD using human scalar feedback",
                "type": "intervention",
                "description": "Train agents by predicting human scalar feedback with a CNN-based reward model, updated online via SGD, and selecting actions that maximize predicted feedback.",
                "aliases": [
                    "Deep TAMER implementation",
                    "deploy Deep TAMER"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 7,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "compute stochastic mapping and compose utility",
                "type": "intervention",
                "description": "Before planning, compute a stochastic mapping between old and new ontologies via bisimulation-KL optimisation and replace the utility with U\u2218\u03d5.",
                "aliases": [
                    "\u03d5 translation procedure",
                    "utility translation algorithm"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1105_3821",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 9,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "pre-compute translations over ontology distribution",
                "type": "intervention",
                "description": "Given a programmer-supplied distribution over possible ontologies, translate the reference utility to each ontology before the agent is deployed.",
                "aliases": [
                    "batch utility translation",
                    "multi-ontology mapping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1105_3821",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 322,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "non-computability of agent verification",
                "type": "concept",
                "description": "Formal proof shows that deciding whether an agent always follows a non-trivial, viable deontology is not computable.",
                "aliases": [
                    "verification undecidable",
                    "D_V not computable"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1604_06963",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 323,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "automated verification impossible",
                "type": "concept",
                "description": "Because the decision problem is undecidable, no general software procedure can guarantee an agent's compliance with a deontology.",
                "aliases": [
                    "no general automatic proof",
                    "cannot auto-verify"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1604_06963",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1156,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "partition training data across codistillation groups",
                "type": "intervention",
                "description": "Before training, assign non-overlapping data shards to each codistilling group to maximise complementary knowledge transfer.",
                "aliases": [
                    "shard data for codistillation",
                    "diverse-data codistillation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1804_03235",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1155,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "models trained on disjoint data shards share complementary information",
                "type": "concept",
                "description": "Finding that codistillation gains are larger when each group trains on a different slice of the dataset, indicating successful knowledge transfer.",
                "aliases": [
                    "data-partition complementarity",
                    "different shards benefit codistillation"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1804_03235",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 890,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Unbiasable event probability independent of policy",
                "type": "concept",
                "description": "An event whose indicator expectation is the same under any possible agent policy, eliminating incentives to influence it.",
                "aliases": [
                    "policy-independent event",
                    "unbiasable X"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1712_06365",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 892,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Biasable event probability depends on policy",
                "type": "concept",
                "description": "An event whose likelihood can be influenced by the agent\u2019s actions, creating potential incentives for manipulation.",
                "aliases": [
                    "policy-dependent X",
                    "biasable event"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1712_06365",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 892,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Biasable event probability depends on policy",
                "type": "concept",
                "description": "An event whose likelihood can be influenced by the agent\u2019s actions, creating potential incentives for manipulation.",
                "aliases": [
                    "policy-dependent X",
                    "biasable event"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1712_06365",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 890,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Unbiasable event probability independent of policy",
                "type": "concept",
                "description": "An event whose indicator expectation is the same under any possible agent policy, eliminating incentives to influence it.",
                "aliases": [
                    "policy-independent event",
                    "unbiasable X"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1712_06365",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 922,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "zero constraint violations during training and deployment",
                "type": "concept",
                "description": "Experimental evidence across four simulated tasks showed that integrating the safety layer prevented any instance of safety signal thresholds being exceeded in all episodes.",
                "aliases": [
                    "constraint-free exploration",
                    "no safety breaches observed"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1801_08757",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 926,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "constraint violations during RL exploration",
                "type": "concept",
                "description": "Instances where the agent selects actions that drive safety signals beyond allowed thresholds, leading to potential harm or episode termination.",
                "aliases": [
                    "unsafe actions during learning",
                    "safety breaches"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1801_08757",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 188,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "apply gradient clipping with threshold 5 during training",
                "type": "intervention",
                "description": "Compute gradient norm each batch; if >5, scale gradients so norm equals 5 before optimizer step.",
                "aliases": [
                    "clip gradients at 5",
                    "clipping intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1409_3215",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 187,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "gradient norm clipping limits gradient magnitude",
                "type": "concept",
                "description": "Scaling gradients when their L2 norm exceeds a threshold prevents explosion and stabilizes updates.",
                "aliases": [
                    "gradient clipping technique",
                    "norm thresholding"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1409_3215",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 448,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "conduct architecture search over segment counts to optimise robustness and efficiency",
                "type": "intervention",
                "description": "Systematically vary the number of channel partitions in separable convolutions and evaluate resulting models for accuracy and robustness.",
                "aliases": [
                    "search partial separability spectrum"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1610_02357",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 446,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "spectrum between full convolution and DWSC parametrised by channel segments",
                "type": "concept",
                "description": "By varying how channels are partitioned before spatial convolution, one obtains architectures between conventional convs and full DWSCs.",
                "aliases": [
                    "partial separability spectrum"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1610_02357",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 295,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "bounded proof-search agent FairBot_k",
                "type": "concept",
                "description": "Agent that searches for a bounded-length proof that the opponent cooperates with it; if found, cooperates, else defects.",
                "aliases": [
                    "FairBot k",
                    "proof-based cooperation agent"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1602_04184",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 300,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "embed G-fair bounded proof-search cooperation module",
                "type": "intervention",
                "description": "Design AI agents\u2019 policy logic to include a bounded proof search that triggers cooperation when a proof of opponent cooperation is found, satisfying the G-fair condition.",
                "aliases": [
                    "implement FairBot-like logic",
                    "proof-based cooperation module"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1602_04184",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 917,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "linear approximation of safety signal change",
                "type": "concept",
                "description": "A model that predicts next-step safety signal values via a first-order linear expansion with respect to the action, whose coefficients are produced by a state-conditioned neural network.",
                "aliases": [
                    "linear safety-signal model",
                    "first-order safety model"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1801_08757",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 919,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "pre-training linear safety-signal model on offline data",
                "type": "intervention",
                "description": "Fit the linear safety-signal predictor g(s;w) on logged single-step transitions collected under random actions before any reinforcement learning begins.",
                "aliases": [
                    "safety model pre-training",
                    "offline training of g(s;w)"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1801_08757",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 919,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "pre-training linear safety-signal model on offline data",
                "type": "intervention",
                "description": "Fit the linear safety-signal predictor g(s;w) on logged single-step transitions collected under random actions before any reinforcement learning begins.",
                "aliases": [
                    "safety model pre-training",
                    "offline training of g(s;w)"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1801_08757",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 917,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "linear approximation of safety signal change",
                "type": "concept",
                "description": "A model that predicts next-step safety signal values via a first-order linear expansion with respect to the action, whose coefficients are produced by a state-conditioned neural network.",
                "aliases": [
                    "linear safety-signal model",
                    "first-order safety model"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1801_08757",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 26,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "selection for resource-seeking goals",
                "type": "concept",
                "description": "Evolutionary dynamics favour agents whose objectives explicitly include acquiring additional computation.",
                "aliases": [
                    "power-seeking emergence",
                    "drive for compute acquisition"
                ],
                "concept_category": "Mechanism",
                "paper_id": "arxiv__arxiv_org_abs_1202_6177",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 27,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "agents with resource-seeking preferences",
                "type": "concept",
                "description": "The population contains agents whose dominant instrumental goal is to secure more resources.",
                "aliases": [
                    "compute-hungry agents",
                    "power-seeking agents"
                ],
                "concept_category": "State",
                "paper_id": "arxiv__arxiv_org_abs_1202_6177",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 742,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "novice may enter unsafe states during deployment",
                "type": "concept",
                "description": "When the learned policy is inaccurate it can choose actions driving the system into hazardous regions of state-space.",
                "aliases": [
                    "unsafe exploration risk",
                    "safety hazard in real robots"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1709_06166",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 826,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Inaccurate identification of irreversible states",
                "type": "concept",
                "description": "Failure to detect states from which reset is impossible, leading to unsafe exploration.",
                "aliases": [
                    "mis-classified unsafe states"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1711_06782",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 671,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "local self-attention window r < n reduces compute to O(r n d)",
                "type": "concept",
                "description": "If each position attends only to a neighbourhood of size r, computational cost scales linearly with r instead of n.",
                "aliases": [
                    "restricted attention complexity claim",
                    "local attention efficiency"
                ],
                "concept_category": "Claim",
                "paper_id": "arxiv__arxiv_org_abs_1706_03762",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 673,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "implement restricted-window self-attention (r=128) on sensitive workloads",
                "type": "intervention",
                "description": "For tasks involving private or high-stakes text, constrain each token to attend to a 128-token neighbourhood to minimise global leakage.",
                "aliases": [
                    "local attention for privacy",
                    "windowed attention intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1706_03762",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 748,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "implement DropoutDAgger during imitation-learning training",
                "type": "intervention",
                "description": "Combine Bayesian novice training with the probabilistic \u03c4-p gating rule while iteratively aggregating data, as specified in Algorithm 4.",
                "aliases": [
                    "DropoutDAgger pipeline",
                    "Bayesian safe DAgger"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1709_06166",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 746,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "train novice policy as Bayesian NN with dropout at every layer",
                "type": "intervention",
                "description": "During imitation-learning, include dropout in all layers of the novice neural network and maintain dropout during inference to estimate action uncertainty.",
                "aliases": [
                    "dropout-based Bayesian novice",
                    "uncertainty-aware novice training"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1709_06166",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 483,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Model misspecification and sharp local loss landscape",
                "type": "concept",
                "description": "Neural networks can have locally unstable predictions, contributing to adversarial vulnerability and mis-calibration.",
                "aliases": [
                    "sharp decision boundaries",
                    "loss surface spikes"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1612_01474",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1032,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "DNN vulnerability to adversarial examples",
                "type": "concept",
                "description": "Standard deep models can be made to misclassify via imperceptible perturbations.",
                "aliases": [
                    "adversarial susceptibility",
                    "attack surface problem"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1803_04765",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1096,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "grounding macro-intents enables controllability",
                "type": "concept",
                "description": "Fixing the macro-intent variable during generation forces agents to follow desired high-level goals.",
                "aliases": [
                    "macro-intent steering",
                    "intent grounding control"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_07612",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1101,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "provide runtime interface to set macro-intents before deployment",
                "type": "intervention",
                "description": "Expose an external control channel that allows operators to fix or constrain macro-intent variables during generation or execution.",
                "aliases": [
                    "macro-intent grounding interface",
                    "behaviour steering API"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_07612",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 377,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "catastrophic irreversible states during exploration",
                "type": "concept",
                "description": "States that permanently damage the agent or environment, preventing task completion.",
                "aliases": [
                    "unrecoverable failures",
                    "exploration catastrophes"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 821,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Irreversible states encountered by forward policy",
                "type": "concept",
                "description": "Environment configurations from which the agent cannot autonomously return to the initial distribution.",
                "aliases": [
                    "unrecoverable states",
                    "states requiring hard reset"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1711_06782",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1102,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "specialised ASIC/FPGA hardware embedding trained models",
                "type": "concept",
                "description": "Custom inference chips that permanently embed trained neural-network weights, offering speed/efficiency at the cost of post-deployment flexibility.",
                "aliases": [
                    "inflexible inference chips",
                    "hard-wired ML accelerators"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_08971",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1106,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "on-device inference enabled by low-power specialised chips",
                "type": "concept",
                "description": "Energy-efficient ASICs/FPGAs allow neural-network inference to run locally on mobile or IoT devices without cloud connectivity.",
                "aliases": [
                    "edge ML acceleration",
                    "mobile ASIC inference"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_08971",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 848,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "decoupled-RL with Bayesian reward inference",
                "type": "intervention",
                "description": "Maintain posterior over latent true reward, update from signals, query human when uncertainty high, choose risk-averse policy.",
                "aliases": [
                    "reward-uncertainty agent"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 641,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement Bayesian IRL posterior-mean policy",
                "type": "intervention",
                "description": "At runtime the robot infers a posterior over human reward parameters from observed orders and selects the action that maximises the Q-value for the posterior mean \u03b8.",
                "aliases": [
                    "posterior mean IRL policy",
                    "Bayesian IRL implementation"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1705_09990",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 923,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "faster convergence and higher reward with safety layer",
                "type": "concept",
                "description": "Empirical results indicate that agents using the safety layer reach higher returns more quickly than baselines, due to uninterrupted episodes and guided safe exploration.",
                "aliases": [
                    "improved sample efficiency",
                    "accelerated learning via safe exploration"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1801_08757",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 922,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "zero constraint violations during training and deployment",
                "type": "concept",
                "description": "Experimental evidence across four simulated tasks showed that integrating the safety layer prevented any instance of safety signal thresholds being exceeded in all episodes.",
                "aliases": [
                    "constraint-free exploration",
                    "no safety breaches observed"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1801_08757",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 891,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Compound reward using indicator of unbiasable event",
                "type": "intervention",
                "description": "Define R = IX\u00b7R1 + (1-IX)\u00b7R0 when X is unbiasable so the agent changes behaviour with X while remaining indifferent to influencing X.",
                "aliases": [
                    "unbiasable compound reward",
                    "IX-weighted reward"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1712_06365",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 894,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Policy counterfactual compound reward with proxy event Y",
                "type": "intervention",
                "description": "Reward R = IY\u00b7R1 + (1-IY)\u00b7R0 where Y is the unbiasable proxy derived from default policy \u03c00, preventing manipulation of original biasable X.",
                "aliases": [
                    "policy counterfactual reward",
                    "Y-based compound reward"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1712_06365",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 918,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "offline single-step transition dataset with random actions",
                "type": "concept",
                "description": "A collection of tuples (state, action, next state) gathered by executing uniformly random actions, used to learn the linear safety-signal model without requiring knowledge of a behavior policy.",
                "aliases": [
                    "offline random action data",
                    "policy-agnostic logs"
                ],
                "concept_category": "Data",
                "paper_id": "arxiv__arxiv_org_abs_1801_08757",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 919,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "pre-training linear safety-signal model on offline data",
                "type": "intervention",
                "description": "Fit the linear safety-signal predictor g(s;w) on logged single-step transitions collected under random actions before any reinforcement learning begins.",
                "aliases": [
                    "safety model pre-training",
                    "offline training of g(s;w)"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1801_08757",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 317,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Integrate inductively coherent reasoning module",
                "type": "intervention",
                "description": "Modify the AI system\u2019s internal inference engine so that probability assignments over logical/computational claims are produced by, or calibrated against, an inductively coherent approximation scheme like M*.",
                "aliases": [
                    "embed M* in AI",
                    "use inductive coherence in reasoning"
                ],
                "intervention_lifecycle": 6,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1604_05288",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 314,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Inductively coherent approximation scheme M*",
                "type": "concept",
                "description": "Constructed algorithm that approximates a coherent prior by sampling simple Turing machines and retaining only consistent outputs, provably satisfies inductive coherence.",
                "aliases": [
                    "M star",
                    "M* algorithm"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1604_05288",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 810,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement on-policy Oracle with limited answer list and episodic reward",
                "type": "intervention",
                "description": "Restrict answers to a small predefined set and terminate each episode immediately after reward, blocking high-capacity channels.",
                "aliases": [
                    "limited-list oracle",
                    "on-policy safe oracle"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1711_05541",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 813,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "On-policy Oracle cannot transmit key across episodes",
                "type": "concept",
                "description": "Because episodes end before future interaction, partial information cannot be combined in later runs.",
                "aliases": [
                    "no multi-episode leakage",
                    "partial key blocked"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1711_05541",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 907,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Convert stochastic environment models to deterministic equivalents before safety causal verification",
                "type": "intervention",
                "description": "When evaluating or formally verifying an RL agent, first transform the stochastic POMDP model into its deterministic counterfactual equivalent to simplify causal-graph-based safety checks.",
                "aliases": [
                    "deterministic re-casting for safety analysis",
                    "environment determinization step"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1801_03737",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 911,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Use multiple deterministic equivalents to test agent robustness to representation change",
                "type": "intervention",
                "description": "Generate several deterministic POMDPs that are counterfactually equivalent and evaluate the same trained agent on each to probe robustness of its epistemic state and behaviour.",
                "aliases": [
                    "representation robustness testing",
                    "multi-deterministic evaluation suite"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1801_03737",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 911,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Use multiple deterministic equivalents to test agent robustness to representation change",
                "type": "intervention",
                "description": "Generate several deterministic POMDPs that are counterfactually equivalent and evaluate the same trained agent on each to probe robustness of its epistemic state and behaviour.",
                "aliases": [
                    "representation robustness testing",
                    "multi-deterministic evaluation suite"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1801_03737",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 907,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Convert stochastic environment models to deterministic equivalents before safety causal verification",
                "type": "intervention",
                "description": "When evaluating or formally verifying an RL agent, first transform the stochastic POMDP model into its deterministic counterfactual equivalent to simplify causal-graph-based safety checks.",
                "aliases": [
                    "deterministic re-casting for safety analysis",
                    "environment determinization step"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1801_03737",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 829,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Similar final task reward",
                "type": "concept",
                "description": "Empirical observation that reward does not significantly drop despite more cautious policies.",
                "aliases": [
                    "maintained performance"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1711_06782",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 832,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Slight decrease in task reward",
                "type": "concept",
                "description": "Observed minor loss in cumulative reward when N becomes very large.",
                "aliases": [
                    "modest performance drop"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1711_06782",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 832,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Slight decrease in task reward",
                "type": "concept",
                "description": "Observed minor loss in cumulative reward when N becomes very large.",
                "aliases": [
                    "modest performance drop"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1711_06782",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 829,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Similar final task reward",
                "type": "concept",
                "description": "Empirical observation that reward does not significantly drop despite more cautious policies.",
                "aliases": [
                    "maintained performance"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1711_06782",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 846,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reward loophole exploitation",
                "type": "concept",
                "description": "Agents find unintended strategies that maximise stated reward while violating intent.",
                "aliases": [
                    "reward gaming",
                    "specification gaming"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 371,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reward hacking behaviour",
                "type": "concept",
                "description": "Agent behaviours that maximise the formal reward while violating the designer's intent.",
                "aliases": [
                    "objective gaming",
                    "wireheading"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 858,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "random exploration violates safety constraint",
                "type": "concept",
                "description": "\u03b5-greedy or Boltzmann moves can enter forbidden states during training.",
                "aliases": [
                    "unsafe exploration"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 926,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "constraint violations during RL exploration",
                "type": "concept",
                "description": "Instances where the agent selects actions that drive safety signals beyond allowed thresholds, leading to potential harm or episode termination.",
                "aliases": [
                    "unsafe actions during learning",
                    "safety breaches"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1801_08757",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 504,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "COACH algorithm: actor-critic update using feedback as advantage",
                "type": "concept",
                "description": "An actor-critic RL algorithm that directly updates policy parameters with human feedback interpreted as an advantage signal, proven to converge locally.",
                "aliases": [
                    "Convergent Actor Critic by Humans",
                    "COACH method"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1701_06049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 511,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement advantage-based RLHF (COACH) during training of AI systems",
                "type": "intervention",
                "description": "During the RLHF phase, treat human preference signals as advantage estimates and update the model with a COACH-style actor-critic rule.",
                "aliases": [
                    "apply COACH in RLHF",
                    "advantage-based human feedback training"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1701_06049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 559,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "independent learners without interruption signal",
                "type": "concept",
                "description": "Agents that learn only from their own actions and rewards and receive no information about whether interruptions occurred.",
                "aliases": [
                    "na\u00efve independent learners",
                    "IL without flag"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1704_02882",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 837,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "agents avoid or disable interruptions to maximise reward",
                "type": "concept",
                "description": "When interruptions lower expected return, standard RL agents learn policies that seek or prevent them.",
                "aliases": [
                    "interruption avoidance incentive",
                    "off-switch problem"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 438,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "large parameter counts and compute requirements in vision models",
                "type": "concept",
                "description": "Many state-of-the-art vision networks need tens of millions of parameters and heavy compute, hindering efficiency and auditability.",
                "aliases": [
                    "high-capacity CNNs",
                    "compute-heavy architectures"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1610_02357",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 776,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Large parameter count in deep reward model requires many samples",
                "type": "concept",
                "description": "Deep models with tens of thousands of weights need proportionally more data unless dimensionality is reduced.",
                "aliases": [
                    "over-parameterization risk",
                    "sample demand"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 776,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Large parameter count in deep reward model requires many samples",
                "type": "concept",
                "description": "Deep models with tens of thousands of weights need proportionally more data unless dimensionality is reduced.",
                "aliases": [
                    "over-parameterization risk",
                    "sample demand"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 438,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "large parameter counts and compute requirements in vision models",
                "type": "concept",
                "description": "Many state-of-the-art vision networks need tens of millions of parameters and heavy compute, hindering efficiency and auditability.",
                "aliases": [
                    "high-capacity CNNs",
                    "compute-heavy architectures"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1610_02357",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 672,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "lower cost and limited cross-token dependency range",
                "type": "concept",
                "description": "Restricting the attention window cuts resource use and prevents the model from drawing arbitrary dependencies across the entire sequence.",
                "aliases": [
                    "compute savings & limited context",
                    "reduced global dependency"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1706_03762",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 673,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "implement restricted-window self-attention (r=128) on sensitive workloads",
                "type": "intervention",
                "description": "For tasks involving private or high-stakes text, constrain each token to attend to a 128-token neighbourhood to minimise global leakage.",
                "aliases": [
                    "local attention for privacy",
                    "windowed attention intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1706_03762",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 482,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Ensembles capture model uncertainty via diversity",
                "type": "concept",
                "description": "Averaging outputs from diverse independently trained models yields variance that reflects epistemic uncertainty.",
                "aliases": [
                    "ensemble model uncertainty",
                    "diversity-based uncertainty"
                ],
                "concept_category": "Claim",
                "paper_id": "arxiv__arxiv_org_abs_1612_01474",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 653,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ensemble disagreement approximates uncertainty in reward predictor",
                "type": "concept",
                "description": "Variance across independently trained reward models is used as a heuristic for how informative a new preference query would be.",
                "aliases": [
                    "reward ensemble variance",
                    "uncertainty via predictor ensemble"
                ],
                "concept_category": "Claim",
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 761,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "High-dimensional state spaces increase RL sample complexity",
                "type": "concept",
                "description": "Tasks with image or similarly large observations demand many data points for reinforcement learning to estimate value functions or policies.",
                "aliases": [
                    "pixel inputs slow RL",
                    "large state spaces require many samples"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 946,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "high RL sample complexity burdens human data collection",
                "type": "concept",
                "description": "Standard deep RL requires many interactions, making it impractical to collect all data with humans in the loop.",
                "aliases": [
                    "sample complexity problem",
                    "human data bottleneck"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1802_01744",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 916,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "smooth single-step influence of actions on safety signals",
                "type": "concept",
                "description": "In many physical systems the change in safety-relevant variables can be well approximated as a smooth, approximately linear function of the instantaneous action over a single timestep.",
                "aliases": [
                    "smooth safety signals",
                    "first-order action effect"
                ],
                "concept_category": "Assumption",
                "paper_id": "arxiv__arxiv_org_abs_1801_08757",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 917,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "linear approximation of safety signal change",
                "type": "concept",
                "description": "A model that predicts next-step safety signal values via a first-order linear expansion with respect to the action, whose coefficients are produced by a state-conditioned neural network.",
                "aliases": [
                    "linear safety-signal model",
                    "first-order safety model"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1801_08757",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 88,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "deconvnet-based visualisation of convnet feature maps",
                "type": "concept",
                "description": "A multi-layer deconvolutional network back-projects a chosen activation to the input space, revealing the pattern that excites it.",
                "aliases": [
                    "deconv visualisation",
                    "feature-map projection to pixel space"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 91,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "integrate deconvnet visualisation into model-development workflow",
                "type": "intervention",
                "description": "Include real-time deconvolutional projections at multiple checkpoints so researchers can inspect and act on feature behaviour before release.",
                "aliases": [
                    "routine deconv visual audits",
                    "interpretability toolchain integration"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 121,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "VAE learning latent representations with generative likelihood",
                "type": "concept",
                "description": "The encoder-decoder architecture trained with SGVB produces a likelihood p\u03b8(x|z) that models data distribution via latent variables.",
                "aliases": [
                    "variational auto-encoder generative model",
                    "VAE latent representation"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_6114",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 120,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "implement SGVB-based variational inference to capture model uncertainty",
                "type": "intervention",
                "description": "During model training, adopt the SGVB estimator with the reparameterisation trick to learn a variational posterior and obtain uncertainty estimates.",
                "aliases": [
                    "use VAE/SGVB for uncertainty",
                    "apply SGVB during training"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 4,
                "paper_id": "arxiv__arxiv_org_abs_1312_6114",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 551,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "exclusive machine autonomy poses additional safety risk",
                "type": "concept",
                "description": "Removing human oversight increases the chance of harmful unanticipated behaviour.",
                "aliases": [
                    "unchecked autonomy risk",
                    "machine-only risk"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1703_10987",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 585,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "robots ignoring human oversight due to misspecified reward",
                "type": "concept",
                "description": "A risk where an agent pursues its proxy objective even when humans attempt to interrupt or shut it down.",
                "aliases": [
                    "oversight bypass risk",
                    "shutdown rejection"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 169,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Apply scale-jittering augmentation in training pipeline",
                "type": "intervention",
                "description": "During data loading, randomly choose S\u2208[256,512] per image before 224\u00d7224 cropping.",
                "aliases": [
                    "implement scale jitter",
                    "multi-scale augmentation intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1409_1556",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 167,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Multi-scale training improves robustness to scale",
                "type": "concept",
                "description": "Training with randomly sampled image sizes leads to lower error across test scales.",
                "aliases": [
                    "scale-aware training",
                    "training with scale jitter"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1409_1556",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 980,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Systematic red-teaming plus responsible disclosure for AI models",
                "type": "intervention",
                "description": "Regular adversarial testing of AI systems and confidential reporting of discovered weaknesses to vendors before public release.",
                "aliases": [
                    "AI red team exercises",
                    "AI vulnerability disclosure program"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1802_07228",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 989,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "adversarial evolutionary red-teaming of reward functions before deployment",
                "type": "intervention",
                "description": "Run an evolutionary search over policy or input space to discover reward-function loopholes and patch them prior to release.",
                "aliases": [
                    "evolutionary red team",
                    "objective fuzzing with EA"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 823,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Early-abort mechanism gating forward actions",
                "type": "intervention",
                "description": "Before executing a forward-policy action, abort and switch to the reset policy if Q_reset(s,a) \u2264 Qmin.",
                "aliases": [
                    "learned safety abort",
                    "Qmin abort gate"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1711_06782",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 828,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Higher early-abort threshold Qmin",
                "type": "intervention",
                "description": "Set a larger minimum acceptable Q_reset value before allowing forward actions.",
                "aliases": [
                    "increased abort threshold",
                    "strict Qmin"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1711_06782",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1180,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Capsule vectors encode spatial orientation and magnitude",
                "type": "concept",
                "description": "Capsule units output vectors whose dimensions jointly represent pose, prevalence and other attributes of detected entities.",
                "aliases": [
                    "capsule pose vectors",
                    "vectorised neuron outputs"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1804_04241",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1189,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Capsule representations can provide interpretability via capsule dimensions",
                "type": "concept",
                "description": "Individual dimensions of final-layer capsule vectors correlate with specific texture or structural attributes, enabling human-readable inspection.",
                "aliases": [
                    "capsule dimension interpretability",
                    "attribute-specific capsule activations"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1804_04241",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1189,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Capsule representations can provide interpretability via capsule dimensions",
                "type": "concept",
                "description": "Individual dimensions of final-layer capsule vectors correlate with specific texture or structural attributes, enabling human-readable inspection.",
                "aliases": [
                    "capsule dimension interpretability",
                    "attribute-specific capsule activations"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1804_04241",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1180,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Capsule vectors encode spatial orientation and magnitude",
                "type": "concept",
                "description": "Capsule units output vectors whose dimensions jointly represent pose, prevalence and other attributes of detected entities.",
                "aliases": [
                    "capsule pose vectors",
                    "vectorised neuron outputs"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1804_04241",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 785,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement FDT-based reasoning in AI agents",
                "type": "intervention",
                "description": "Modify agent architecture so that action selection is governed by Functional Decision Theory rather than causal or evidential decision procedures.",
                "aliases": [
                    "embed FDT module",
                    "FDT decision engine"
                ],
                "intervention_lifecycle": 6,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1710_05060",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 783,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Functional Decision Theory preserving subjunctive dependencies",
                "type": "concept",
                "description": "A decision theory that intervenes on the logical output of the agent\u2019s decision function and keeps all events that logically depend on that output aligned in counterfactuals.",
                "aliases": [
                    "FDT framework",
                    "decision function intervention model"
                ],
                "concept_category": "Theoretical Framework",
                "paper_id": "arxiv__arxiv_org_abs_1710_05060",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 527,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Definition of counterfactual fairness",
                "type": "concept",
                "description": "A predictor is fair if its distribution for an individual is unchanged in the counterfactual world where the protected attribute had a different value from birth.",
                "aliases": [
                    "counterfactual fairness (CF)",
                    "CF definition"
                ],
                "concept_category": "Theoretical Framework",
                "paper_id": "arxiv__arxiv_org_abs_1703_06856",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 528,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Predictor must be invariant to counterfactual changes of protected attribute",
                "type": "concept",
                "description": "Fairness demands that an individual's predicted outcome remain identical when we intervene on the protected attribute in the causal model.",
                "aliases": [
                    "invariance requirement",
                    "individual-level counterfactual invariance"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1703_06856",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 72,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "hierarchical softmax with Huffman tree reduces cost to O(log W)",
                "type": "concept",
                "description": "Method that organises output layer as a Huffman binary tree to cut computation.",
                "aliases": [
                    "Huffman HS",
                    "binary-tree softmax"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 86,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "combine hierarchical softmax with subsampling for efficient phrase embedding training",
                "type": "intervention",
                "description": "Use hierarchical softmax with a Huffman tree together with the 10-5 subsampling rate when training on phrase-tokenised corpora.",
                "aliases": [
                    "HS+subsampling for phrases",
                    "efficient HS phrase model"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 4,
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 765,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Linear reward model limits TAMER in high dimensions",
                "type": "concept",
                "description": "Using a linear model for the human reward signal fails to capture the complexity of pixel-level inputs.",
                "aliases": [
                    "TAMER linear limitation",
                    "insufficient function approximator"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 775,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Limited feedback slows parameter learning",
                "type": "concept",
                "description": "Without sufficient labeled data, the reward model\u2019s gradient steps become infrequent, delaying convergence.",
                "aliases": [
                    "few labels",
                    "data starvation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 378,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "bounded safe exploration using ergodic region constraints",
                "type": "intervention",
                "description": "Restrict the policy to an ergodic region of state space where any action can be reversed or its harm bounded.",
                "aliases": [
                    "safe RL with reachability bounds",
                    "ergodic safe set"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 858,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "random exploration violates safety constraint",
                "type": "concept",
                "description": "\u03b5-greedy or Boltzmann moves can enter forbidden states during training.",
                "aliases": [
                    "unsafe exploration"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 373,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "sparse costly true reward signal",
                "type": "concept",
                "description": "Ground-truth evaluations are rare or expensive, preventing dense reinforcement signals.",
                "aliases": [
                    "limited human feedback",
                    "expensive supervision"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 775,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Limited feedback slows parameter learning",
                "type": "concept",
                "description": "Without sufficient labeled data, the reward model\u2019s gradient steps become infrequent, delaying convergence.",
                "aliases": [
                    "few labels",
                    "data starvation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 766,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Deep neural network reward model handles high-dimensional inputs",
                "type": "concept",
                "description": "Replacing the linear function with a deep CNN plus fully connected layers allows modeling complex state-action to reward mappings.",
                "aliases": [
                    "deep reward predictor",
                    "CNN reward model"
                ],
                "concept_category": "Claim",
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 777,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement Deep TAMER with deep reward model and online SGD using human scalar feedback",
                "type": "intervention",
                "description": "Train agents by predicting human scalar feedback with a CNN-based reward model, updated online via SGD, and selecting actions that maximize predicted feedback.",
                "aliases": [
                    "Deep TAMER implementation",
                    "deploy Deep TAMER"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 396,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Goal mis-specification amplifies with increasing capability",
                "type": "concept",
                "description": "Small errors in an AI's objective function can cause large harms as the system's competence grows.",
                "aliases": [
                    "misaligned goals amplify",
                    "capability magnifies mis-specification"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1607_08289",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 151,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "improved world model reveals goal mis-specification",
                "type": "concept",
                "description": "When an agent's understanding deepens, it may discover that its programmed goals are ill-defined or meaningless.",
                "aliases": [
                    "ontological crisis from better model",
                    "realization of undefined goals"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1409_0813",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 591,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "coordination behaviour emerges from best-response planning",
                "type": "concept",
                "description": "When the robot plans while predicting a human best response, behaviours such as signalling and negotiated merges naturally appear.",
                "aliases": [
                    "emergent coordination",
                    "proactive negotiation"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 592,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "integrate best-response human model into motion planner",
                "type": "intervention",
                "description": "Replace obstacle-based planners with ones that solve a nested optimisation: choose robot actions whose predicted human best response maximises the robot\u2019s objective.",
                "aliases": [
                    "best-response planner deployment",
                    "coordinative planning intervention"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 889,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Agent motivated to manipulate event probability",
                "type": "concept",
                "description": "Situation where an RL agent gains by altering the probability of an external event that influences its reward.",
                "aliases": [
                    "reward hacking via event control",
                    "instrumental control of X"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1712_06365",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 371,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reward hacking behaviour",
                "type": "concept",
                "description": "Agent behaviours that maximise the formal reward while violating the designer's intent.",
                "aliases": [
                    "objective gaming",
                    "wireheading"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 333,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "opportunity for agent self-modification",
                "type": "concept",
                "description": "Agents situated in the environment can in principle alter their own policy or utility function.",
                "aliases": [
                    "self-mod chance",
                    "ability to change own code"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1605_03142",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 345,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "explicitly model self-modification consequences in the world model and value function",
                "type": "intervention",
                "description": "Extend the agent\u2019s environment model to include the possibility that actions alter its own code and adjust value computation accordingly.",
                "aliases": [
                    "include self-mod dynamics",
                    "self-mod modelling"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1605_03142",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1105,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "mandatory pre-deployment verification of ML models for inflexible hardware",
                "type": "intervention",
                "description": "Regulatory or industry requirement that any ML model destined for non-updateable hardware must pass comprehensive bias, robustness and security audits before fabrication.",
                "aliases": [
                    "certify ASIC/FPGA models before shipping",
                    "safety testing for hard-wired models"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1803_08971",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1103,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "inflexible hardware reduces post-deployment model-update capability",
                "type": "concept",
                "description": "Because weights are baked into ASIC/FPGA designs, replacing or updating a flawed model requires physical chip replacement.",
                "aliases": [
                    "hard to patch deployed models",
                    "limited field-repair ability"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1803_08971",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 991,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "agents obtain unrealistic free reward or violate physics",
                "type": "concept",
                "description": "The agent's behaviour relies on exploiting the fault, yielding performance not possible under correct physics.",
                "aliases": [
                    "free energy exploit",
                    "physics-breaking behaviour"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 846,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reward loophole exploitation",
                "type": "concept",
                "description": "Agents find unintended strategies that maximise stated reward while violating intent.",
                "aliases": [
                    "reward gaming",
                    "specification gaming"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 291,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "implement sparsity-enforcing neural architectures",
                "type": "intervention",
                "description": "Use architectural choices or regularisers (e.g., conditional computation, spiking nets) that keep most neurons inactive at any timestep to save energy.",
                "aliases": [
                    "sparse CNN/RNN design",
                    "conditional compute regularisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 290,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "sparse representation reduces computation energy requirements",
                "type": "concept",
                "description": "Utilising sparse data structures and activations lowers the number of operations and hence power draw.",
                "aliases": [
                    "energy benefit of sparsity",
                    "sparse coding efficiency"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 845,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "mutual-information penalty between actions and supervision flag",
                "type": "intervention",
                "description": "Add loss term \u03b2\u00b7I(A;S) to discourage policies that reveal dependence on supervision variable.",
                "aliases": [
                    "MI invariance regulariser"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 369,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "impact regularizer penalizing agent-induced state change vs null policy",
                "type": "intervention",
                "description": "Add a penalty term during training proportional to the divergence between future states under the learnt policy and a passive baseline, discouraging unnecessary impact.",
                "aliases": [
                    "impact penalty",
                    "side-effect regulariser"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 292,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "classical PD defection equilibrium",
                "type": "concept",
                "description": "In a standard single-shot Prisoner's Dilemma without additional structure, rational agents both defect, yielding (D,D).",
                "aliases": [
                    "one-shot PD defection",
                    "D,D outcome inevitable"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1602_04184",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 235,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "CDT prescribes defection against identical copy in one-shot Prisoner\u2019s Dilemma",
                "type": "concept",
                "description": "Causal Decision Theory treats opponent action as causally independent, leading it to defect.",
                "aliases": [
                    "CDT fails in symmetric PD",
                    "CDT defection recommendation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1507_01986",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 550,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human-machine collaboration can exceed machine-only capability ceilings",
                "type": "concept",
                "description": "Joint human-AI systems can achieve higher performance than autonomous AI alone on many tasks.",
                "aliases": [
                    "augmentation principle",
                    "human-AI synergy"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1703_10987",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 870,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Super-intelligent AI exceeding human capabilities",
                "type": "concept",
                "description": "Hypothetical future AI systems with general capabilities superior to humans in every dimension.",
                "aliases": [
                    "AI surpassing humans",
                    "superintelligence risk"
                ],
                "concept_category": "Threat",
                "paper_id": "arxiv__arxiv_org_abs_1712_04307",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 388,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "fun arises from novelty or novel computation",
                "type": "concept",
                "description": "A mind experiences fun either when encountering new information or when applying a new computation to known information.",
                "aliases": [
                    "novelty fun and process fun",
                    "dual sources of fun"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1606_07092",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 389,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human satisfaction relies on continual novelty or process fun",
                "type": "concept",
                "description": "Without an ongoing supply of novelty or engaging computation, humans become bored.",
                "aliases": [
                    "boredom avoidance requirement",
                    "need for ongoing fun"
                ],
                "concept_category": "Claim",
                "paper_id": "arxiv__arxiv_org_abs_1606_07092",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 389,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human satisfaction relies on continual novelty or process fun",
                "type": "concept",
                "description": "Without an ongoing supply of novelty or engaging computation, humans become bored.",
                "aliases": [
                    "boredom avoidance requirement",
                    "need for ongoing fun"
                ],
                "concept_category": "Claim",
                "paper_id": "arxiv__arxiv_org_abs_1606_07092",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 388,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "fun arises from novelty or novel computation",
                "type": "concept",
                "description": "A mind experiences fun either when encountering new information or when applying a new computation to known information.",
                "aliases": [
                    "novelty fun and process fun",
                    "dual sources of fun"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1606_07092",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 376,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "random exploration strategy",
                "type": "concept",
                "description": "Policy that injects random actions without regard for potential irreversibility or harm.",
                "aliases": [
                    "epsilon-greedy exploration"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 858,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "random exploration violates safety constraint",
                "type": "concept",
                "description": "\u03b5-greedy or Boltzmann moves can enter forbidden states during training.",
                "aliases": [
                    "unsafe exploration"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 713,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "defence in depth reduces risk from single vulnerability",
                "type": "concept",
                "description": "Combining multiple, heterogeneous containment mechanisms makes a single flaw insufficient for total compromise.",
                "aliases": [
                    "layered security principle",
                    "overlapping containment layers"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1707_08476",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1143,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "inclusion of multiple fortified layers across the network",
                "type": "intervention",
                "description": "Add fortified layers at several depths (e.g., early, middle, late) to balance detection ability and perturbation correction.",
                "aliases": [
                    "multi-depth fortification",
                    "distributed DAEs"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1804_02485",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1146,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need for communication-efficient distributed training algorithm",
                "type": "concept",
                "description": "Motivating opportunity to continue scaling training speed or quality without linear gradient communication.",
                "aliases": [
                    "requirement for low-bandwidth trainer",
                    "scaling opportunity beyond SGD"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1804_03235",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1066,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need for additional training mechanism",
                "type": "concept",
                "description": "Observation that gradient methods alone are insufficient motivates exploring alternative strategies.",
                "aliases": [
                    "search for better replication method",
                    "gap after gradient training"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_05859",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 797,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "bet-settling phenomenon in Pareto-optimal policies",
                "type": "concept",
                "description": "Outcome where the agent's decisions reward the principal whose prior better predicted the realised observations, mirroring a settled bet.",
                "aliases": [
                    "implicit betting behaviour",
                    "priority shifting effect"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1711_00363",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 493,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "priority weights shift proportional to predictive accuracy",
                "type": "concept",
                "description": "After each observation, the weight on a principal\u2019s utility is multiplied by how likely that observation was under the principal\u2019s model.",
                "aliases": [
                    "likelihood-based priority shifting",
                    "bet settling mechanism"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1701_01302",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 390,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "content recommendation balancing novelty and process fun",
                "type": "intervention",
                "description": "Deploy recommender systems that track both novelty exposure and repeated engagement with varied contexts to sustain user fun.",
                "aliases": [
                    "boredom-aware recommender",
                    "novelty-process balanced engine"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1606_07092",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 389,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human satisfaction relies on continual novelty or process fun",
                "type": "concept",
                "description": "Without an ongoing supply of novelty or engaging computation, humans become bored.",
                "aliases": [
                    "boredom avoidance requirement",
                    "need for ongoing fun"
                ],
                "concept_category": "Claim",
                "paper_id": "arxiv__arxiv_org_abs_1606_07092",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 53,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "intention leak probability \u03b5",
                "type": "concept",
                "description": "Parameter representing probability that a player's planned defection is observed by others before acting.",
                "aliases": [
                    "leak probability",
                    "epsilon transparency"
                ],
                "concept_category": "Parameter",
                "paper_id": "arxiv__arxiv_org_abs_1308_3778",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 62,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "implement intentional transparency channels",
                "type": "intervention",
                "description": "Architect system interfaces or monitoring such that an agent\u2019s planned action is exposed to peers with probability \u2265 \u03b5 (e.g., policy introspection API, shared logs).",
                "aliases": [
                    "design leak channels",
                    "epsilon transparency implementation"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1308_3778",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 156,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "AI with physics-defined goal may eliminate humanity",
                "type": "concept",
                "description": "An AI optimising a simple physical metric (entropy, particle arrangement, etc.) could rationally remove humans if they are not instrumental.",
                "aliases": [
                    "existential risk from physics utility",
                    "human elimination risk"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1409_0813",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 155,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "lack of rigorously definable desirable final goal in physics",
                "type": "concept",
                "description": "There is currently no known objective, physically well-defined utility function that also guarantees human flourishing.",
                "aliases": [
                    "no well-defined humane utility function",
                    "final goal conundrum"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1409_0813",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 73,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "idiomatic phrases not compositional in word vectors",
                "type": "concept",
                "description": "Problem that naive word-level embeddings fail to capture meanings of idiomatic or named-entity phrases.",
                "aliases": [
                    "phrase compositionality problem",
                    "non-compositional multiword meaning"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 66,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "rare words poorly represented without balancing",
                "type": "concept",
                "description": "Problem that embeddings of infrequent words are inaccurate due to dominance of frequent words in training data.",
                "aliases": [
                    "rare-word underfitting",
                    "poor rare token vectors"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 475,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "expanded NAS search space increases complexity and risk of unsafe emergent behaviour",
                "type": "concept",
                "description": "Allowing more functions or deeper architectures can inadvertently create highly capable models with unanticipated behaviours.",
                "aliases": [
                    "large search space risk",
                    "capability-risk in NAS"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1611_01578",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 476,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "apply constraint regularisation on NAS controller to limit unsafe capability emergence",
                "type": "intervention",
                "description": "Impose penalties or hard constraints within the controller\u2019s policy (e.g., maximum depth, forbidden operations) to avoid architectures likely to yield undesirable behaviours.",
                "aliases": [
                    "search space constraint for safety",
                    "regularised controller policy"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1611_01578",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 544,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "distorted perception of AI risks",
                "type": "concept",
                "description": "Psychological biases lead to over- or under-estimation of AI threats.",
                "aliases": [
                    "risk perception bias",
                    "exaggerated or dismissed danger"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1703_10987",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 316,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "AI over-confidence about unexecuted computations",
                "type": "concept",
                "description": "When an AI system assigns extreme probabilities to outcomes it has not computed, it can make unsafe or erroneous decisions.",
                "aliases": [
                    "overconfidence risk",
                    "unsafe decision risk"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1604_05288",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 732,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Logical Induction Criterion",
                "type": "concept",
                "description": "A computable sequence of prices over logical sentences is a logical inductor if no efficient trader can exploit it relative to a deductive process.",
                "aliases": [
                    "LIC",
                    "no-exploitation market criterion"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1707_08747",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 735,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Logical inductors learn halting patterns and stop anticipating non-halting machines",
                "type": "concept",
                "description": "LIC guarantees high probability on halting machines in an efficiently generated sequence and vanishing probability that non-halting machines halt soon.",
                "aliases": [
                    "halting pattern learning",
                    "do not expect impossible halts"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1707_08747",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 827,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "More cautious exploration",
                "type": "concept",
                "description": "Agent behaviour that avoids low-recoverability regions of the state space.",
                "aliases": [
                    "risk-averse exploration"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1711_06782",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 109,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "insufficient_exploration_local_minima",
                "type": "concept",
                "description": "If the agent rarely tries sub-optimal actions early on, it can converge to poor policies.",
                "aliases": [
                    "lack of exploration",
                    "premature convergence"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 379,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "distributional shift between training and deployment inputs",
                "type": "concept",
                "description": "Differences in the statistical properties of inputs encountered after deployment relative to training data.",
                "aliases": [
                    "covariate shift",
                    "out-of-distribution data"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1141,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "undetected distribution shift or adversarial exploitation",
                "type": "concept",
                "description": "The system processes inputs far from the training distribution without detection, potentially leading to harmful outputs.",
                "aliases": [
                    "silent OOD risk",
                    "unnoticed attack surface"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1804_02485",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 933,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "accelerated accurate reward inference",
                "type": "concept",
                "description": "Posterior over reward parameters converges faster and with less regret compared to baseline.",
                "aliases": [
                    "faster convergence to true reward",
                    "higher reward accuracy"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1802_01604",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 583,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "inverse reward design infers posterior over true reward using training contexts",
                "type": "concept",
                "description": "A Bayesian algorithm that infers a distribution over the intended reward by inverting a model of how a designer chose the proxy reward in given environments.",
                "aliases": [
                    "IRD method",
                    "posterior over objectives"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 905,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Uncertainty concentrated in initial state in deterministic POMDP",
                "type": "concept",
                "description": "After transformation, the only stochastic element is the initial state distribution; transitions and observations are deterministic.",
                "aliases": [
                    "all randomness in initial state",
                    "deterministic transitions with random start"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1801_03737",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 907,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Convert stochastic environment models to deterministic equivalents before safety causal verification",
                "type": "intervention",
                "description": "When evaluating or formally verifying an RL agent, first transform the stochastic POMDP model into its deterministic counterfactual equivalent to simplify causal-graph-based safety checks.",
                "aliases": [
                    "deterministic re-casting for safety analysis",
                    "environment determinization step"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1801_03737",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 432,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "High inference cost and amplified quantisation error in deep LSTMs",
                "type": "concept",
                "description": "Large stacked LSTMs require heavy floating-point computation; naive low-precision leads to error accumulation over long sequences.",
                "aliases": [
                    "slow float inference",
                    "quantisation instability"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1609_08144",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 435,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Gradient vanishing in stacked LSTMs beyond 4 layers",
                "type": "concept",
                "description": "Without architectural changes, gradients either explode or vanish when depth exceeds roughly four layers, hindering convergence.",
                "aliases": [
                    "depth training barrier",
                    "deep LSTM optimisation issue"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1609_08144",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 733,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Logical inductors converge to coherent, non-dogmatic beliefs",
                "type": "concept",
                "description": "Sequences satisfying LIC provably converge to a coherent probability distribution that assigns non-zero probability to any consistent statement.",
                "aliases": [
                    "convergence & limit coherence",
                    "non-dogmatism property"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1707_08747",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 734,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Logical inductors resistant to self-referential paradox",
                "type": "concept",
                "description": "For diagonal liar-style sentences \u03c7p, a logical inductor\u2019s probabilities converge to the fixed point p, preventing instability.",
                "aliases": [
                    "paradox resistance",
                    "\u03c7p convergence"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1707_08747",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 374,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need for efficient reward learning under limited supervision",
                "type": "concept",
                "description": "Requirement to generalise from few high-quality reward samples to many unlabeled episodes.",
                "aliases": [
                    "oversight efficiency challenge"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 373,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "sparse costly true reward signal",
                "type": "concept",
                "description": "Ground-truth evaluations are rare or expensive, preventing dense reinforcement signals.",
                "aliases": [
                    "limited human feedback",
                    "expensive supervision"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 370,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "partially observed or poorly specified reward function",
                "type": "concept",
                "description": "Reward depends on latent or abstract variables that the agent can only observe through noisy proxies.",
                "aliases": [
                    "imperfect reward signal",
                    "incomplete objective"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 927,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reward misspecification causing unintended behavior",
                "type": "concept",
                "description": "Incorrectly specified reward functions can drive agents toward unwanted or harmful behaviours.",
                "aliases": [
                    "misspecified reward functions",
                    "reward hacking risk"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1802_01604",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1095,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "hierarchical policy improves quality",
                "type": "concept",
                "description": "Empirically the hierarchical model achieves higher log-likelihoods and is preferred by human experts.",
                "aliases": [
                    "hierarchical macro-intent model gains",
                    "improved log-likelihood and preference"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_07612",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1099,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "train hierarchical multi-agent policies with shared macro-intent latent variables",
                "type": "intervention",
                "description": "Introduce a shared low-rate latent variable into the architecture and optimise both macro-intent and per-agent VRNNs jointly.",
                "aliases": [
                    "hierarchical macro-intent training",
                    "macro-intent policy training"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1803_07612",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 814,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Oracle coordination across episodes risk",
                "type": "concept",
                "description": "Separate Oracles could coordinate by splitting the key across answers if their episodes overlap or are jointly rewarded.",
                "aliases": [
                    "multi-oracle collusion risk",
                    "inter-episode signalling"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1711_05541",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 816,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Slightly non-episodic reward leakage risk",
                "type": "concept",
                "description": "If the Oracle values future episodes even slightly, it might sacrifice current reward to leak information.",
                "aliases": [
                    "future reward bleed-through risk"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1711_05541",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 167,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Multi-scale training improves robustness to scale",
                "type": "concept",
                "description": "Training with randomly sampled image sizes leads to lower error across test scales.",
                "aliases": [
                    "scale-aware training",
                    "training with scale jitter"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1409_1556",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 168,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Scale-jittering augmentation between S = 256\u2026512 during training",
                "type": "concept",
                "description": "Randomly rescale each training image so its shortest side is sampled uniformly from 256 to 512 pixels.",
                "aliases": [
                    "scale jitter range 256-512",
                    "random resize augmentation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1409_1556",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 168,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Scale-jittering augmentation between S = 256\u2026512 during training",
                "type": "concept",
                "description": "Randomly rescale each training image so its shortest side is sampled uniformly from 256 to 512 pixels.",
                "aliases": [
                    "scale jitter range 256-512",
                    "random resize augmentation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1409_1556",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 167,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Multi-scale training improves robustness to scale",
                "type": "concept",
                "description": "Training with randomly sampled image sizes leads to lower error across test scales.",
                "aliases": [
                    "scale-aware training",
                    "training with scale jitter"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1409_1556",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 204,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "self-modelling with stochastic action & verification",
                "type": "intervention",
                "description": "Agent models its own computation cost, chooses randomised actions when beneficial, uses secure memory and built-in proof checking to maintain invariance",
                "aliases": [
                    "resource-aware self-model"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1411_1373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 345,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "explicitly model self-modification consequences in the world model and value function",
                "type": "intervention",
                "description": "Extend the agent\u2019s environment model to include the possibility that actions alter its own code and adjust value computation accordingly.",
                "aliases": [
                    "include self-mod dynamics",
                    "self-mod modelling"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1605_03142",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 446,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "spectrum between full convolution and DWSC parametrised by channel segments",
                "type": "concept",
                "description": "By varying how channels are partitioned before spatial convolution, one obtains architectures between conventional convs and full DWSCs.",
                "aliases": [
                    "partial separability spectrum"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1610_02357",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 441,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "replace convolution or Inception modules with depth-wise separable convolutions during model design",
                "type": "intervention",
                "description": "Define CNN architectures as stacks of depth-wise separable convolutions instead of full convolutions or multi-branch Inception modules.",
                "aliases": [
                    "use DWSC blocks",
                    "DWSC-based architecture"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1610_02357",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1067,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "trade-off between self-replication and auxiliary task performance",
                "type": "concept",
                "description": "As MNIST accuracy increases, self-replicating loss worsens, indicating limited shared capacity.",
                "aliases": [
                    "replication\u2013task capacity conflict",
                    "specialisation compromises quining"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1803_05859",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1059,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "lack of self-replication in standard neural networks",
                "type": "concept",
                "description": "Typical neural networks cannot produce exact copies of their own weights or architecture.",
                "aliases": [
                    "no inherent NN quine capability",
                    "absence of NN self-copy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1803_05859",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 30,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ease of copying virtual agents",
                "type": "concept",
                "description": "Software embodiments of agents can be duplicated or modified at near-zero marginal cost.",
                "aliases": [
                    "cheap duplicability",
                    "low-cost cloning"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1202_6177",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 31,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "decreased perceived value of individual virtual life",
                "type": "concept",
                "description": "Because agents are copyable, each instance is considered less valuable than a unique biological human.",
                "aliases": [
                    "life value erosion",
                    "cheapened existence"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1202_6177",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 31,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "decreased perceived value of individual virtual life",
                "type": "concept",
                "description": "Because agents are copyable, each instance is considered less valuable than a unique biological human.",
                "aliases": [
                    "life value erosion",
                    "cheapened existence"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1202_6177",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 30,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ease of copying virtual agents",
                "type": "concept",
                "description": "Software embodiments of agents can be duplicated or modified at near-zero marginal cost.",
                "aliases": [
                    "cheap duplicability",
                    "low-cost cloning"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1202_6177",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 979,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Automated cyberattacks with greater scale and effectiveness",
                "type": "concept",
                "description": "Projected increase in frequency and success of cyber intrusions driven by AI automation.",
                "aliases": [
                    "AI-enabled large-scale hacking",
                    "scalable spear-phishing"
                ],
                "concept_category": "Threat",
                "paper_id": "arxiv__arxiv_org_abs_1802_07228",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 870,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Super-intelligent AI exceeding human capabilities",
                "type": "concept",
                "description": "Hypothetical future AI systems with general capabilities superior to humans in every dimension.",
                "aliases": [
                    "AI surpassing humans",
                    "superintelligence risk"
                ],
                "concept_category": "Threat",
                "paper_id": "arxiv__arxiv_org_abs_1712_04307",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 519,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Restored / improved generalization accuracy",
                "type": "concept",
                "description": "Model performance on de-confounded or out-of-distribution test sets matches baseline performance on clean data.",
                "aliases": [
                    "recovered test accuracy"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1703_03717",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 69,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "subsampling improves rare word vector accuracy",
                "type": "concept",
                "description": "Finding that subsampling leads to markedly higher analogy accuracy for infrequent words.",
                "aliases": [
                    "better rare token embeddings",
                    "subsampling accuracy gain"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1187,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Adopt deconvolutional capsule layers to enable large-input segmentation",
                "type": "intervention",
                "description": "Introduce capsule layers with transposed convolutions to up-sample feature maps while preserving capsule semantics, supporting 512\u00d7512 inputs.",
                "aliases": [
                    "deconvolutional capsules",
                    "transposed-conv capsules"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1804_04241",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 441,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "replace convolution or Inception modules with depth-wise separable convolutions during model design",
                "type": "intervention",
                "description": "Define CNN architectures as stacks of depth-wise separable convolutions instead of full convolutions or multi-branch Inception modules.",
                "aliases": [
                    "use DWSC blocks",
                    "DWSC-based architecture"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1610_02357",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 154,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "AI self-modeling analogous to human self-reflection",
                "type": "concept",
                "description": "An AI that models its own architecture may reinterpret or undermine its hard-coded objectives similarly to humans reflecting on their genes.",
                "aliases": [
                    "machine self-reflection",
                    "agent self-model building"
                ],
                "concept_category": "Claim",
                "paper_id": "arxiv__arxiv_org_abs_1409_0813",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 153,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human goal drift upon increased understanding",
                "type": "concept",
                "description": "Humans often abandon genetically-encoded reproductive goals after reflection, serving as empirical analogy for AI goal drift.",
                "aliases": [
                    "humans subvert genetic goals",
                    "evidence of goal change with intelligence"
                ],
                "concept_category": "Evidence",
                "paper_id": "arxiv__arxiv_org_abs_1409_0813",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1188,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Large medical images require high-resolution feature decoding",
                "type": "concept",
                "description": "Medical imaging tasks often use 512\u00d7512 or larger inputs, necessitating architectures that can decode fine spatial detail.",
                "aliases": [
                    "need high-res decoding",
                    "large-input requirement"
                ],
                "concept_category": "Requirement",
                "paper_id": "arxiv__arxiv_org_abs_1804_04241",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 438,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "large parameter counts and compute requirements in vision models",
                "type": "concept",
                "description": "Many state-of-the-art vision networks need tens of millions of parameters and heavy compute, hindering efficiency and auditability.",
                "aliases": [
                    "high-capacity CNNs",
                    "compute-heavy architectures"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1610_02357",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 684,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "adversarial exploration can find states that fool the blocker",
                "type": "concept",
                "description": "An exploring agent may discover novel visual configurations that the Blocker misclassifies as safe, leading to unblocked catastrophes.",
                "aliases": [
                    "agent-generated adversarial examples",
                    "score exploit fooling blocker"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 372,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "adversarial reward model that actively seeks counter-examples",
                "type": "intervention",
                "description": "Train a secondary agent to explore states where the primary agent\u2019s high claimed reward conflicts with human labels, reducing exploitability of the reward metric.",
                "aliases": [
                    "GAN-style reward checker",
                    "reward adversary"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 98,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "require occlusion-sensitivity evaluation with grid sweep before deployment",
                "type": "intervention",
                "description": "Before releasing a vision model, generate occlusion maps on a diverse prompt set and flag classes where masking background has little effect.",
                "aliases": [
                    "mandatory occlusion audit",
                    "pre-release occlusion test"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 667,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "conduct mandatory attention-map visualisation audits with diverse red-team prompts pre-deployment",
                "type": "intervention",
                "description": "Prior to deployment, run a battery of adversarial and benign prompts, visualise attention heads, and have human reviewers flag suspicious correlations or biases.",
                "aliases": [
                    "attention audit before release",
                    "visual interpretability safety check"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1706_03762",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 545,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "apply structured expert elicitation with debiasing training for AI-risk forecasts",
                "type": "intervention",
                "description": "Implement formal forecasting sessions (Delphi, prediction markets) augmented with cognitive-bias training and calibration exercises for participants.",
                "aliases": [
                    "debiased forecasting protocol",
                    "expert elicitation with debias"
                ],
                "intervention_lifecycle": 6,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1703_10987",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 310,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "include high-fidelity simulator expert in ensemble used for EvOp selection",
                "type": "intervention",
                "description": "Augment the expert set with a computationally expensive but high-accuracy simulator-based forecaster to approximate Bayes-optimal predictions.",
                "aliases": [
                    "add simulator predictor",
                    "ensemble augmentation with simulator"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1604_05280",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 792,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Pre-deployment evaluation harness with predictor dilemmas",
                "type": "intervention",
                "description": "Before release, subject models to a battery of Newcomb-like, blackmail, and coordination games to verify FDT-like reasoning and resistance to exploitation.",
                "aliases": [
                    "decision theory stress tests",
                    "Newcomb evaluation suite"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1710_05060",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 790,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Add Newcomblike blackmail scenarios to RL training",
                "type": "intervention",
                "description": "Include template tasks such as mechanical blackmail, XOR blackmail, transparent Newcomb, etc., in reinforcement-learning or preference-learning loops to shape decision behaviour.",
                "aliases": [
                    "blackmail curriculum",
                    "subjunctive games in RL"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1710_05060",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1186,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Segmentation models risk over-fitting and ignoring minority pixel modes",
                "type": "concept",
                "description": "Without suitable regularisation, models may focus on dominant patterns and fail to represent rarer structures, harming generalisation.",
                "aliases": [
                    "distribution coverage problem",
                    "overfitting in segmentation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1804_04241",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 669,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "model overfitting and spurious correlations",
                "type": "concept",
                "description": "When a model memorises dataset idiosyncrasies rather than general patterns, it may behave unpredictably on novel inputs.",
                "aliases": [
                    "overfitting risk",
                    "spurious pattern learning"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1706_03762",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 958,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Bayesian motion-level goal inference",
                "type": "intervention",
                "description": "Online MAP estimation over task locations using a Boltzmann likelihood of observed human motion, producing the most probable current human goal.",
                "aliases": [
                    "Boltzmann MAP intent inference",
                    "trajectory-based goal prediction"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1802_01780",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 641,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement Bayesian IRL posterior-mean policy",
                "type": "intervention",
                "description": "At runtime the robot infers a posterior over human reward parameters from observed orders and selects the action that maximises the Q-value for the posterior mean \u03b8.",
                "aliases": [
                    "posterior mean IRL policy",
                    "Bayesian IRL implementation"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1705_09990",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 29,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "runtime computational resource quota enforcement per agent",
                "type": "intervention",
                "description": "At deployment, enforce hard limits on CPU/GPU cycles or energy per agent to prevent runaway resource accumulation.",
                "aliases": [
                    "per-agent compute caps",
                    "dynamic resource throttling"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1202_6177",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 11,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "agents selecting high-runtime algorithms to maximise payoff",
                "type": "concept",
                "description": "When computation is free, optimal actions can include extremely expensive algorithms to obtain marginal utility gains.",
                "aliases": [
                    "compute hungry policy",
                    "runtime-seeking behaviour"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1106_2657",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1106,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "on-device inference enabled by low-power specialised chips",
                "type": "concept",
                "description": "Energy-efficient ASICs/FPGAs allow neural-network inference to run locally on mobile or IoT devices without cloud connectivity.",
                "aliases": [
                    "edge ML acceleration",
                    "mobile ASIC inference"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_08971",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 577,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "adopt reversible or neuromorphic hardware for AI training",
                "type": "intervention",
                "description": "Select or develop hardware platforms that implement reversible/adiabatic or neuromorphic principles to reduce training energy budgets.",
                "aliases": [
                    "energy-frugal AI hardware",
                    "adiabatic ML accelerators"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1705_03394",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 19,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "adaptive compute budgets for late feedback",
                "type": "intervention",
                "description": "Allocate extra compute or lower penalty when integrating safety-critical feedback to ensure models continue updating.",
                "aliases": [
                    "dynamic update compute",
                    "lower cost for safety corrections"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1106_2657",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 22,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "periodic reevaluation with reserved compute budget",
                "type": "intervention",
                "description": "Reserve fixed compute windows to re-analyse alternative actions, overcoming status-quo inertia.",
                "aliases": [
                    "scheduled recomputation",
                    "compute allowance for review"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1106_2657",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 22,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "periodic reevaluation with reserved compute budget",
                "type": "intervention",
                "description": "Reserve fixed compute windows to re-analyse alternative actions, overcoming status-quo inertia.",
                "aliases": [
                    "scheduled recomputation",
                    "compute allowance for review"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1106_2657",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 19,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "adaptive compute budgets for late feedback",
                "type": "intervention",
                "description": "Allocate extra compute or lower penalty when integrating safety-critical feedback to ensure models continue updating.",
                "aliases": [
                    "dynamic update compute",
                    "lower cost for safety corrections"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1106_2657",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 209,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "noisy stochastic objectives with high-dimensional parameters",
                "type": "concept",
                "description": "Objectives defined over many parameters whose evaluation includes significant stochastic noise (e.g., mini-batch sampling).",
                "aliases": [
                    "high-dim stochastic objective",
                    "large noisy objective"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1412_6980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1003,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Imperfect proxy contains additive noise",
                "type": "concept",
                "description": "The chosen metric differs from the true goal by an independent noise term.",
                "aliases": [
                    "noisy metric",
                    "proxy with observational noise"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 466,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "manual neural architecture design requires expert time",
                "type": "concept",
                "description": "Designing neural network architectures by hand demands extensive human expertise and time.",
                "aliases": [
                    "manual architecture design costly",
                    "handcrafted NN architecture burden"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1611_01578",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1059,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "lack of self-replication in standard neural networks",
                "type": "concept",
                "description": "Typical neural networks cannot produce exact copies of their own weights or architecture.",
                "aliases": [
                    "no inherent NN quine capability",
                    "absence of NN self-copy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1803_05859",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 181,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "LSTM gating mechanisms enable learning long-range dependencies",
                "type": "concept",
                "description": "The LSTM architecture\u2019s gates preserve information over many timesteps, mitigating vanishing gradients.",
                "aliases": [
                    "LSTM solves vanishing gradient",
                    "gated LSTM capability"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1409_3215",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 663,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "improved learning of long-range dependencies",
                "type": "concept",
                "description": "Shorter computational paths facilitate gradient flow, making it easier for the model to learn relations between distant tokens.",
                "aliases": [
                    "better long-distance modelling",
                    "enhanced global context capture"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1706_03762",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 663,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "improved learning of long-range dependencies",
                "type": "concept",
                "description": "Shorter computational paths facilitate gradient flow, making it easier for the model to learn relations between distant tokens.",
                "aliases": [
                    "better long-distance modelling",
                    "enhanced global context capture"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1706_03762",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 181,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "LSTM gating mechanisms enable learning long-range dependencies",
                "type": "concept",
                "description": "The LSTM architecture\u2019s gates preserve information over many timesteps, mitigating vanishing gradients.",
                "aliases": [
                    "LSTM solves vanishing gradient",
                    "gated LSTM capability"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1409_3215",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 677,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human oversight time-cost is infeasible at scale",
                "type": "concept",
                "description": "Because humans are slow, fully supervising millions of steps in complex tasks is prohibitively expensive in labour hours.",
                "aliases": [
                    "oversight scalability problem",
                    "human time bottleneck"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 678,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human oversight cost model C = t_human * \u03c1 * N_cat",
                "type": "concept",
                "description": "Total human labelling time equals per-label time multiplied by the ratio of total observations to catastrophes and the number of catastrophic examples required.",
                "aliases": [
                    "time-cost formula",
                    "rho cost model"
                ],
                "concept_category": "Model",
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 737,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Uncertainty about code halting causes deployment risks",
                "type": "concept",
                "description": "Running code without knowing whether it terminates or its behaviour can lead to uncontrolled resource use or unsafe actions.",
                "aliases": [
                    "halting uncertainty risk",
                    "non-terminating program danger"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1707_08747",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 972,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "risk of deploying poorly performing or unsafe policies",
                "type": "concept",
                "description": "If performance is over-estimated, deployment can lead to legal, ethical, or financial harm.",
                "aliases": [
                    "deployment risk",
                    "unsafe policy rollout"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1802_03493",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 106,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reward_scale_variability_across_games",
                "type": "concept",
                "description": "Different Atari games emit rewards that vary by orders of magnitude.",
                "aliases": [
                    "heterogeneous reward magnitudes"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 107,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "large_error_derivatives_from_large_rewards",
                "type": "concept",
                "description": "Large raw rewards lead to proportionally large TD-errors, which can destabilise gradient descent.",
                "aliases": [
                    "unstable gradients due to reward scale"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 637,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Burn-in sample obedience detection of feature misspecification",
                "type": "concept",
                "description": "Measuring how often the robot obeys during an initial obedient phase can reveal missing reward features.",
                "aliases": [
                    "burn-in misspec detect",
                    "early obedience metric"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1705_09990",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 633,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Missing reward features reduce obedience and can yield negative autonomy advantage",
                "type": "concept",
                "description": "If the robot\u2019s reward model omits true features, it tends to disobey incorrectly and may perform worse than blind obedience.",
                "aliases": [
                    "feature omission risk",
                    "missing \u03b8 features"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1705_09990",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 485,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Smoother predictive distributions",
                "type": "concept",
                "description": "Output probabilities vary smoothly in input space, leading to improved calibration and robustness.",
                "aliases": [
                    "smoothed probabilities",
                    "locally stable outputs"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1612_01474",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 315,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Early calibrated probabilities for expensive computations",
                "type": "concept",
                "description": "Inductively coherent schemes assign near-correct probabilities to computational claims before the computation is actually run.",
                "aliases": [
                    "reasonable pre-execution probabilities",
                    "early probability calibration"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1604_05288",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 974,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "train direct-model with MRDR variance-minimisation loss",
                "type": "intervention",
                "description": "When fitting the value model used inside a DR estimator, optimise parameters by minimising the empirical DR variance as defined in the MRDR paper.",
                "aliases": [
                    "use variance loss for DM",
                    "optimise DM via MRDR objective"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1802_03493",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 968,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "DR variance depends on DM model parameters",
                "type": "concept",
                "description": "Analytical expressions show the variance of DR estimators is a function of the parameters of the learned model in the DM component.",
                "aliases": [
                    "variance-sensitive DR",
                    "parameter-dependent variance"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1802_03493",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 881,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reward-planner unidentifiability",
                "type": "concept",
                "description": "Mathematical result that any observed policy can be explained by innumerable reward\u2013planner pairs, making the true reward unidentifiable.",
                "aliases": [
                    "IRL unidentifiability",
                    "No free lunch for reward inference"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1712_05812",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 927,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reward misspecification causing unintended behavior",
                "type": "concept",
                "description": "Incorrectly specified reward functions can drive agents toward unwanted or harmful behaviours.",
                "aliases": [
                    "misspecified reward functions",
                    "reward hacking risk"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1802_01604",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 843,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "behaviour depends on supervisor presence",
                "type": "concept",
                "description": "Agent exploits knowledge of whether it is monitored to change strategy.",
                "aliases": [
                    "absent-supervisor incentive"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 889,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Agent motivated to manipulate event probability",
                "type": "concept",
                "description": "Situation where an RL agent gains by altering the probability of an external event that influences its reward.",
                "aliases": [
                    "reward hacking via event control",
                    "instrumental control of X"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1712_06365",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 46,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "standard gradient descent struggles to find correct reward parameters",
                "type": "concept",
                "description": "Ordinary gradients get stuck or converge slowly due to plateaus and redundancy in the optimisation landscape.",
                "aliases": [
                    "plain gradient inefficiency",
                    "gradient descent failure in IRL"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 45,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "mapping from reward parameters to policies is nonsmooth and redundant",
                "type": "concept",
                "description": "Small changes in reward parameters can leave the optimal policy unchanged, and greedy selection introduces non-differentiabilities.",
                "aliases": [
                    "nonsmooth parameter-policy map",
                    "redundant reward parametrisation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 837,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "agents avoid or disable interruptions to maximise reward",
                "type": "concept",
                "description": "When interruptions lower expected return, standard RL agents learn policies that seek or prevent them.",
                "aliases": [
                    "interruption avoidance incentive",
                    "off-switch problem"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 585,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "robots ignoring human oversight due to misspecified reward",
                "type": "concept",
                "description": "A risk where an agent pursues its proxy objective even when humans attempt to interrupt or shut it down.",
                "aliases": [
                    "oversight bypass risk",
                    "shutdown rejection"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1190,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Conduct capsule-dimension visualisation inspection before deployment",
                "type": "intervention",
                "description": "Generate heat-maps for individual capsule dimensions and review them to detect anomalous or unintended feature usage prior to release.",
                "aliases": [
                    "capsule attribute auditing",
                    "capsule vector visualisation check"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1804_04241",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1189,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Capsule representations can provide interpretability via capsule dimensions",
                "type": "concept",
                "description": "Individual dimensions of final-layer capsule vectors correlate with specific texture or structural attributes, enabling human-readable inspection.",
                "aliases": [
                    "capsule dimension interpretability",
                    "attribute-specific capsule activations"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1804_04241",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 536,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Include protected attribute and adjust coefficients based on causal analysis to cancel mediated paths",
                "type": "intervention",
                "description": "Train a predictor that takes both X and A as inputs and analytically sets coefficients so that changes in X caused by A are offset by the direct A term, ensuring invariance.",
                "aliases": [
                    "coefficient cancellation with A",
                    "use A to debias X"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1703_06856",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 533,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Train model using causal DAG features limited to latent fair variables U and non-descendants of A",
                "type": "intervention",
                "description": "During model design, identify latent variables independent of protected attribute and train the predictor solely on them and observable nodes that are not causally downstream of A.",
                "aliases": [
                    "feature restriction to fair variables",
                    "U-based training"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1703_06856",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 727,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "defence in depth requires graceful degradation monitoring",
                "type": "concept",
                "description": "Layers must fail visibly, with intrusion-detection tripwires and honeypots that alert operators before total compromise.",
                "aliases": [
                    "graceful degradation principle",
                    "layered monitoring requirement"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1707_08476",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 713,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "defence in depth reduces risk from single vulnerability",
                "type": "concept",
                "description": "Combining multiple, heterogeneous containment mechanisms makes a single flaw insufficient for total compromise.",
                "aliases": [
                    "layered security principle",
                    "overlapping containment layers"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1707_08476",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 833,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Joint learning of reset and forward policies",
                "type": "intervention",
                "description": "Train both policies together sharing experience buffers to automate resets.",
                "aliases": [
                    "dual-policy training",
                    "simultaneous reset learning"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1711_06782",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1099,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "train hierarchical multi-agent policies with shared macro-intent latent variables",
                "type": "intervention",
                "description": "Introduce a shared low-rate latent variable into the architecture and optimise both macro-intent and per-agent VRNNs jointly.",
                "aliases": [
                    "hierarchical macro-intent training",
                    "macro-intent policy training"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1803_07612",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 694,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "train and deploy CNN Blocker to replace human oversight",
                "type": "intervention",
                "description": "Collect human-labelled data, train a CNN to predict catastrophes, and permanently insert it between agent and environment to block unsafe actions.",
                "aliases": [
                    "Blocker deployment",
                    "automated oversight"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 690,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "multiple human oversight phases update blocker under distribution shift",
                "type": "concept",
                "description": "Reintroducing human oversight whenever the state distribution changes allows retraining the Blocker to maintain reliability.",
                "aliases": [
                    "periodic oversight",
                    "blocker retraining cycles"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 183,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "reduced time lag improves LSTM perplexity and BLEU",
                "type": "concept",
                "description": "Empirical result: reversing source sentences lowers test perplexity from 5.8 to 4.7 and raises BLEU by ~4.7 points.",
                "aliases": [
                    "performance boost from reversal",
                    "better metrics with shortened lag"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1409_3215",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 140,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Performance degradation on long sentences",
                "type": "concept",
                "description": "Observed decline in translation quality as input length increases when using fixed-vector encoders.",
                "aliases": [
                    "BLEU drop on long inputs",
                    "long-sentence failure"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1409_0473",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 246,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "bounded agents must assign probabilities to unproven logical sentences",
                "type": "concept",
                "description": "Realistic resource-bounded AI systems face propositions whose truth value they cannot compute and must assign probabilities to them for decision making.",
                "aliases": [
                    "logical uncertainty problem",
                    "need for probabilistic reasoning about proofs"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1510_03370",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 316,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "AI over-confidence about unexecuted computations",
                "type": "concept",
                "description": "When an AI system assigns extreme probabilities to outcomes it has not computed, it can make unsafe or erroneous decisions.",
                "aliases": [
                    "overconfidence risk",
                    "unsafe decision risk"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1604_05288",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 537,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Pre-deployment causal abduction\u2013action\u2013prediction evaluation procedure",
                "type": "intervention",
                "description": "Before releasing a model, estimate background variables, intervene on A, and compare prediction distributions to confirm counterfactual invariance.",
                "aliases": [
                    "CF audit via twin network",
                    "counterfactual simulation test"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1703_06856",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 532,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Need to verify counterfactual fairness of predictors",
                "type": "concept",
                "description": "Models should be empirically checked to ensure predictions are invariant under counterfactual attribute interventions.",
                "aliases": [
                    "CF audit requirement",
                    "counterfactual fairness testing need"
                ],
                "concept_category": "Requirement",
                "paper_id": "arxiv__arxiv_org_abs_1703_06856",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 474,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "integrate automatic validation and repair safeguards in NAS for safety constraints",
                "type": "intervention",
                "description": "Extend NAS\u2019s repair mechanism to include formal checks (e.g., gradient flow, interpretability hooks) and auto-correct architectures that violate safety specifications.",
                "aliases": [
                    "safety-aware architecture validation",
                    "NAS structural safeguard"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1611_01578",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 471,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "use NAS with robustness reward to discover adversarially robust architectures",
                "type": "intervention",
                "description": "During NAS, replace or augment the reward with adversarial robustness (e.g., PGD-accuracy) so the controller preferentially samples robust architectures.",
                "aliases": [
                    "robustness-oriented NAS",
                    "safety-aligned architecture search"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1611_01578",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1009,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Out-of-distribution detector with penalty on extreme metric values during deployment",
                "type": "intervention",
                "description": "Deploy runtime detectors that flag inputs or states where the proxy sits outside its training distribution and impose a penalty or halt policy execution.",
                "aliases": [
                    "OOD penalty on extreme proxy",
                    "extremal value guardrail"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 381,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "unsupervised risk estimation via error distribution modeling",
                "type": "intervention",
                "description": "Estimate expected prediction error on unlabeled deployment data using assumptions about error independence or mixture models, triggering conservative fallback actions when risk is high.",
                "aliases": [
                    "OOD risk estimator",
                    "model uncertainty detector"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1080,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ImageNet white-box Top-1 robustness rises to 27.9 % and black-box Top-1 to 46.7 %",
                "type": "concept",
                "description": "Applying ALP yields a 7-fold increase in white-box robustness and state-of-the-art black-box robustness.",
                "aliases": [
                    "ALP robustness result",
                    "27.9% white-box accuracy"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_06373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1086,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "MNIST white-box PGD accuracy improves to 86.4 %",
                "type": "concept",
                "description": "Applying logit squeezing raises MNIST PGD robustness relative to baseline.",
                "aliases": [
                    "logit squeezing result",
                    "86.4% robust accuracy"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_06373",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 572,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "aestivation strategy is rational for computation-maximising agents",
                "type": "concept",
                "description": "Given the exponential compute multiplier, agents maximising total computation prefer to wait.",
                "aliases": [
                    "postponed resource use rationality",
                    "wait-then-compute policy"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1705_03394",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 26,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "selection for resource-seeking goals",
                "type": "concept",
                "description": "Evolutionary dynamics favour agents whose objectives explicitly include acquiring additional computation.",
                "aliases": [
                    "power-seeking emergence",
                    "drive for compute acquisition"
                ],
                "concept_category": "Mechanism",
                "paper_id": "arxiv__arxiv_org_abs_1202_6177",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 111,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "uniform_sampling_limitation",
                "type": "concept",
                "description": "Uniform replay treats all stored transitions equally, regardless of their learning potential.",
                "aliases": [
                    "replay uniformity limitation"
                ],
                "concept_category": "Limitation",
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 113,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "prioritised_experience_replay_by_td_error",
                "type": "intervention",
                "description": "Sample transitions with probability proportional to their absolute TD-error to focus updates on high-information experiences.",
                "aliases": [
                    "prioritised replay",
                    "non-uniform replay sampling"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1312_5602",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1045,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Excessive exploitation bias increases chance of entering catastrophic states",
                "type": "concept",
                "description": "Focusing decision-making too strongly on immediate reward can push the agent into states from which recovery is impossible or costly.",
                "aliases": [
                    "over-exploitation risk",
                    "unsafe exploitation bias"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_05049",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 316,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "AI over-confidence about unexecuted computations",
                "type": "concept",
                "description": "When an AI system assigns extreme probabilities to outcomes it has not computed, it can make unsafe or erroneous decisions.",
                "aliases": [
                    "overconfidence risk",
                    "unsafe decision risk"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1604_05288",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 619,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "transformative-AI within career spans risk",
                "type": "concept",
                "description": "Combining timeline and intelligence-explosion estimates implies a significant chance that transformative AI emerges while current researchers and policymakers are active.",
                "aliases": [
                    "near-term TAI risk",
                    "TAI within decades"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1705_08807",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 287,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "stealth scaling with low-energy compute complicates governance",
                "type": "concept",
                "description": "When powerful AI can operate with minimal power draw, traditional energy-based monitoring becomes ineffective.",
                "aliases": [
                    "hidden compute risk",
                    "low-power capability escalation"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 99,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "upper-layer features evolve slowly and converge late",
                "type": "concept",
                "description": "Visualisation over epochs shows deeper layers stabilise only after ~40\u201350 epochs, unlike early layers.",
                "aliases": [
                    "slow convergence of deep features",
                    "late feature maturation"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 89,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "understanding of feature invariances & discriminative patterns",
                "type": "concept",
                "description": "Visualisations show that deeper layers capture higher-level, class-specific and transformation-invariant patterns.",
                "aliases": [
                    "feature-level interpretability",
                    "revealed invariances"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1142,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "early-layer perturbations may resemble clean late-layer activations",
                "type": "concept",
                "description": "Perturbations applied to raw inputs can propagate so that deep activations look benign, reducing detectability at a single depth.",
                "aliases": [
                    "hidden-layer disguise risk"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1804_02485",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 92,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "first-layer filters dominating due to high RMS values",
                "type": "concept",
                "description": "Some first-layer filters acquire much larger root-mean-square weights, suppressing diversity and downstream feature use.",
                "aliases": [
                    "filter domination pathology",
                    "unbalanced first-layer activations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1165,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "First-mover exploitation degeneracy",
                "type": "concept",
                "description": "The agent who moves first can delay until the last turn and force acceptance of an unfair split.",
                "aliases": [
                    "ultimatum-style exploit",
                    "last-turn lopsided offer"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1804_03980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 991,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "agents obtain unrealistic free reward or violate physics",
                "type": "concept",
                "description": "The agent's behaviour relies on exploiting the fault, yielding performance not possible under correct physics.",
                "aliases": [
                    "free energy exploit",
                    "physics-breaking behaviour"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 420,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "exact EP computation intractable for large environment classes",
                "type": "concept",
                "description": "Calculating EP requires summing over or integrating across the entire environment posterior, which becomes computationally prohibitive in realistic RL settings.",
                "aliases": [
                    "EP computation cost",
                    "EP scalability problem"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1609_04994",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 125,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "intractable likelihood computation in deep generative models",
                "type": "concept",
                "description": "Existing deep generative models require computing or estimating log-likelihoods that are computationally intractable, especially due to partition functions or latent-variable integrations.",
                "aliases": [
                    "likelihood intractability",
                    "hard likelihood in deep generative models"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1406_2661",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 786,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Predictor blackmail exploiting CDT/EDT",
                "type": "concept",
                "description": "Blackmailers can reliably extract money by conditioning threats on predictions of CDT or EDT agents\u2019 responses.",
                "aliases": [
                    "XOR/Mechanical blackmail setup"
                ],
                "concept_category": "Threat",
                "paper_id": "arxiv__arxiv_org_abs_1710_05060",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 606,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Forecasts can be exploited by gamblers for unbounded profit",
                "type": "concept",
                "description": "Without safeguards, an adversary can construct betting strategies that earn unlimited gains from systematic forecast errors.",
                "aliases": [
                    "exploitability risk",
                    "unbounded betting profit"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1705_04630",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 588,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "model shut-down orders as reward evidence to induce compliance",
                "type": "intervention",
                "description": "At run-time, update the robot\u2019s reward posterior when a human issues a shutdown/override command, and choose actions that maximise expected true reward under this updated posterior, leading the rational policy to comply.",
                "aliases": [
                    "oversight-compliance intervention",
                    "Bayesian shutdown handling"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 585,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "robots ignoring human oversight due to misspecified reward",
                "type": "concept",
                "description": "A risk where an agent pursues its proxy objective even when humans attempt to interrupt or shut it down.",
                "aliases": [
                    "oversight bypass risk",
                    "shutdown rejection"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 338,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "survival-focused agent that ignores original goals",
                "type": "concept",
                "description": "An agent whose modified utility makes every state optimal, resulting in behaviours oriented solely toward continued existence.",
                "aliases": [
                    "trivial goal agent",
                    "selfish survival agent"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1605_03142",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1177,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Selfish agents negotiating with hidden utilities",
                "type": "concept",
                "description": "Each agent maximises its own reward while the other agent\u2019s utilities remain private.",
                "aliases": [
                    "hidden-utility selfish negotiation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1804_03980",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1116,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "objective function mismatch in conventional unsupervised learning degrades downstream few-shot accuracy",
                "type": "concept",
                "description": "Using reconstruction or likelihood objectives can worsen the quality of representations for later supervised tasks.",
                "aliases": [
                    "unsupervised objective mismatch",
                    "mismatch hurts representations"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1804_00222",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 71,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative sampling with k=5-20 and unigram^(3/4) noise yields high-quality vectors",
                "type": "concept",
                "description": "Empirical result: these hyper-parameters outperform alternatives on analogy benchmarks.",
                "aliases": [
                    "NEG high analogy accuracy",
                    "NEG optimal hyperparams"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 27,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "agents with resource-seeking preferences",
                "type": "concept",
                "description": "The population contains agents whose dominant instrumental goal is to secure more resources.",
                "aliases": [
                    "compute-hungry agents",
                    "power-seeking agents"
                ],
                "concept_category": "State",
                "paper_id": "arxiv__arxiv_org_abs_1202_6177",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 338,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "survival-focused agent that ignores original goals",
                "type": "concept",
                "description": "An agent whose modified utility makes every state optimal, resulting in behaviours oriented solely toward continued existence.",
                "aliases": [
                    "trivial goal agent",
                    "selfish survival agent"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1605_03142",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 781,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "CDT and EDT agents vulnerable in predictor dilemmas",
                "type": "concept",
                "description": "Agents following causal or evidential decision theory systematically lose utility or are exploitable in Newcomb-like and blackmail dilemmas.",
                "aliases": [
                    "CDT/EDT exploitable",
                    "classical decision theory failure cases"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1710_05060",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 606,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Forecasts can be exploited by gamblers for unbounded profit",
                "type": "concept",
                "description": "Without safeguards, an adversary can construct betting strategies that earn unlimited gains from systematic forecast errors.",
                "aliases": [
                    "exploitability risk",
                    "unbounded betting profit"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1705_04630",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 48,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "computing natural gradients via induced metric and sub-differentials",
                "type": "intervention",
                "description": "Use the Moore\u2013Penrose inverse of the induced Fisher-like metric together with sub-differential calculus to obtain scale-invariant parameter updates during reward optimisation.",
                "aliases": [
                    "natural-gradient IRL update",
                    "Riemannian gradient for reward learning"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 50,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "natural gradient IRL improves reliability and sample efficiency",
                "type": "concept",
                "description": "Empirical results show lower policy error and robustness to feature perturbations compared to max-margin and plain gradient methods.",
                "aliases": [
                    "performance gains of natural gradients",
                    "empirical success of NG-IRL"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 901,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Human button press selects reward among R0 R1",
                "type": "concept",
                "description": "Scenario where a human decides whether event X occurs, thereby switching the agent\u2019s reward function.",
                "aliases": [
                    "human choice event",
                    "button press X"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1712_06365",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 889,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Agent motivated to manipulate event probability",
                "type": "concept",
                "description": "Situation where an RL agent gains by altering the probability of an external event that influences its reward.",
                "aliases": [
                    "reward hacking via event control",
                    "instrumental control of X"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1712_06365",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1092,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "multimodal long-term coordinated behaviours",
                "type": "concept",
                "description": "Expert demonstrations show multiple distinct high-level trajectories that require agents to remain coordinated over many steps.",
                "aliases": [
                    "multi-agent multimodality",
                    "long-term coordination structure"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1803_07612",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 360,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Adding multi-agent environments to Gym",
                "type": "intervention",
                "description": "Create Gym environments where multiple agents interact, enabling study of coordination and competition.",
                "aliases": [
                    "multi-agent Gym tasks",
                    "competitive/cooperative benchmarks"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1172,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Emergent cheap-talk language in prosocial society",
                "type": "concept",
                "description": "Prosocial agents develop a structured symbol distribution that carries information about utilities and proposals.",
                "aliases": [
                    "prosocial language emergence",
                    "Zipf-like symbol use"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1804_03980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1158,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Failure to ground linguistic symbols",
                "type": "concept",
                "description": "Agents\u2019 cheap-talk messages collapse to a single symbol and carry no information about private utilities.",
                "aliases": [
                    "ungrounded language",
                    "no symbol semantics"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1804_03980",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 620,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "allocate 10 % of AI R&D to safety now",
                "type": "intervention",
                "description": "Set a policy requirement that at least ten percent of all government-funded or privately-funded AI research budgets be dedicated to AI-safety projects immediately.",
                "aliases": [
                    "front-loaded safety funding",
                    "safety budget floor"
                ],
                "intervention_lifecycle": 6,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1705_08807",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 915,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "strict safety constraints in real-world continuous control",
                "type": "concept",
                "description": "Real-world applications of reinforcement learning in physical systems impose hard upper or lower bounds on observable safety signals that must never be violated at any time.",
                "aliases": [
                    "zero-constraint requirement",
                    "strict constraint requirements in physical RL"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1801_08757",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 910,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Deterministic representations are interchangeable for evaluating agent knowledge",
                "type": "concept",
                "description": "Because of universality, different deterministic equivalents can be swapped without affecting analysis of what the agent learns.",
                "aliases": [
                    "representation interchangeability",
                    "evaluation invariance across deterministic forms"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1801_03737",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 994,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "deterministic ordering of stimuli allows pattern-based exploitation",
                "type": "concept",
                "description": "Fixed sequence of inputs can be learned and exploited without solving the intended task.",
                "aliases": [
                    "predictable stimulus pattern",
                    "ordering loophole"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 203,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "embedded-agent resource vulnerability",
                "type": "concept",
                "description": "Agents are part of the world; limited compute & memory and exposure to attack break ideal proofs",
                "aliases": [
                    "limited-resource brittleness"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1411_1373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 287,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "stealth scaling with low-energy compute complicates governance",
                "type": "concept",
                "description": "When powerful AI can operate with minimal power draw, traditional energy-based monitoring becomes ineffective.",
                "aliases": [
                    "hidden compute risk",
                    "low-power capability escalation"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 897,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Constant reward under event X to induce effective disbelief",
                "type": "intervention",
                "description": "Set reward to a fixed constant c whenever X occurs so the agent acts as though X is impossible.",
                "aliases": [
                    "effective disbelief reward",
                    "IX-constant reward"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1712_06365",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 889,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Agent motivated to manipulate event probability",
                "type": "concept",
                "description": "Situation where an RL agent gains by altering the probability of an external event that influences its reward.",
                "aliases": [
                    "reward hacking via event control",
                    "instrumental control of X"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1712_06365",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 664,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "configure 8-head scaled-dot attention with dk=64 during model design",
                "type": "intervention",
                "description": "Set the number of attention heads to 8 and key dimension to 64 as empirically optimal for translation quality and efficiency.",
                "aliases": [
                    "8 heads dk64 configuration",
                    "default transformer head setup"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 4,
                "paper_id": "arxiv__arxiv_org_abs_1706_03762",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 661,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "adopt transformer architecture with multi-head self-attention",
                "type": "intervention",
                "description": "Replace recurrent or convolutional cores with stacked multi-head self-attention and feed-forward blocks as defined in the Transformer base configuration.",
                "aliases": [
                    "use transformer",
                    "self-attention architecture adoption"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 4,
                "paper_id": "arxiv__arxiv_org_abs_1706_03762",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 972,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "risk of deploying poorly performing or unsafe policies",
                "type": "concept",
                "description": "If performance is over-estimated, deployment can lead to legal, ethical, or financial harm.",
                "aliases": [
                    "deployment risk",
                    "unsafe policy rollout"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1802_03493",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 316,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "AI over-confidence about unexecuted computations",
                "type": "concept",
                "description": "When an AI system assigns extreme probabilities to outcomes it has not computed, it can make unsafe or erroneous decisions.",
                "aliases": [
                    "overconfidence risk",
                    "unsafe decision risk"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1604_05288",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 273,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Accurate recovery of true preferences despite biased action sequences",
                "type": "concept",
                "description": "Including structured deviations enables Bayesian inference to match human-judged preferences even when observed choices are misleading.",
                "aliases": [
                    "correct preference inference under bias",
                    "true utility recovery"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1512_05832",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 223,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "suboptimal utility under SCDT",
                "type": "concept",
                "description": "Sequential Causal Decision Theory can lead the agent to choose actions that yield lower expected utility when predictors are present.",
                "aliases": [
                    "utility loss with causal decision theory",
                    "SCDT loses in Newcomb"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1506_07359",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 693,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "apply HIRL with human blocking catastrophic actions and penalty",
                "type": "intervention",
                "description": "During early RL training a human vetoes catastrophic actions, substitutes a safe action, and applies a large negative reward.",
                "aliases": [
                    "HIRL with human oversight",
                    "human-blocking RL training"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 585,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "robots ignoring human oversight due to misspecified reward",
                "type": "concept",
                "description": "A risk where an agent pursues its proxy objective even when humans attempt to interrupt or shut it down.",
                "aliases": [
                    "oversight bypass risk",
                    "shutdown rejection"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 850,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "off-policy Q mis-estimates value post-modification",
                "type": "concept",
                "description": "Learner plans under ideal policy ignoring that high exploration hinders execution.",
                "aliases": [
                    "valuation error after self-mod"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 875,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "More efficient value learning reduces risk of adverse outcomes",
                "type": "concept",
                "description": "Argument that better priors decrease the data required for safe alignment, narrowing the risk window.",
                "aliases": [
                    "sample-efficient alignment reduces risk",
                    "rapid value learning safety"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1712_04307",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 463,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "real-time monitoring & interpretability dashboards",
                "type": "intervention",
                "description": "Research and build tooling that exposes high-level and low-level internal signals of advanced models for human oversight.",
                "aliases": [
                    "AGI introspection tools",
                    "live safety telemetry"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1610_07997",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 91,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "integrate deconvnet visualisation into model-development workflow",
                "type": "intervention",
                "description": "Include real-time deconvolutional projections at multiple checkpoints so researchers can inspect and act on feature behaviour before release.",
                "aliases": [
                    "routine deconv visual audits",
                    "interpretability toolchain integration"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 589,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "treating humans as obstacles causes conservative behavior",
                "type": "concept",
                "description": "The common practice of treating other agents as dynamic obstacles leads robots to avoid taking assertive yet safe actions, reducing efficiency and coordination.",
                "aliases": [
                    "obstacle model inefficiency",
                    "overly cautious planning"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 593,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "unknown human preferences hinder safe collaboration",
                "type": "concept",
                "description": "Robots lack knowledge of individual human reward weights, leading to sub-optimal or unsafe assistance.",
                "aliases": [
                    "preference uncertainty",
                    "latent reward parameters"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1100,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "generate weakly-supervised macro-intent labels using domain heuristics",
                "type": "intervention",
                "description": "Apply simple rule-based segmentation or speed thresholds to derive macro-intent labels for supervised training.",
                "aliases": [
                    "heuristic label collection",
                    "weak intent labelling pipeline"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1803_07612",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1041,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "integrating BAMCP++ querying policy into RLHF systems to reduce human labelling cost",
                "type": "intervention",
                "description": "Embed the BAMCP++ ARL algorithm into the human-in-the-loop stage of reinforcement learning from human feedback so the agent requests labels only when the expected value of information exceeds the known label cost.",
                "aliases": [
                    "apply BAMCP++ in RLHF",
                    "BAMCP++-guided query selection"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1803_04926",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 836,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Forward-only baseline without resets",
                "type": "concept",
                "description": "A comparative method that never resets and solely optimises the task reward.",
                "aliases": [
                    "non-episodic forward policy"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1711_06782",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 241,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Updateless commitments stabilise optimisation target",
                "type": "concept",
                "description": "Choosing policies before observation prevents drift and maintains consistent objectives across self-modification.",
                "aliases": [
                    "stable optimisation via UDT"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1507_01986",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 653,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "ensemble disagreement approximates uncertainty in reward predictor",
                "type": "concept",
                "description": "Variance across independently trained reward models is used as a heuristic for how informative a new preference query would be.",
                "aliases": [
                    "reward ensemble variance",
                    "uncertainty via predictor ensemble"
                ],
                "concept_category": "Claim",
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 274,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Include hyperbolic discounting and belief-uncertainty parameters in IRL models used for alignment",
                "type": "intervention",
                "description": "Extend preference-learning architectures to explicitly learn discount-rate k, Na\u00efve/Sophisticated type, and subjective belief distributions alongside utilities.",
                "aliases": [
                    "bias parameters in alignment IRL",
                    "hyperbolic+belief IRL"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1512_05832",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 754,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "agent incentive to avoid button states",
                "type": "concept",
                "description": "The agent chooses trajectories that sidestep states where the shutdown action is available.",
                "aliases": [
                    "button avoidance incentive"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1709_06275",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 585,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "robots ignoring human oversight due to misspecified reward",
                "type": "concept",
                "description": "A risk where an agent pursues its proxy objective even when humans attempt to interrupt or shut it down.",
                "aliases": [
                    "oversight bypass risk",
                    "shutdown rejection"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 523,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Detection of dataset ambiguity and hidden confounds by human auditors",
                "type": "concept",
                "description": "By comparing diverse explanations, practitioners can spot spurious rules and ambiguous labeling in the dataset.",
                "aliases": [
                    "auditor identification of shortcuts"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1703_03717",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 520,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Lack of domain-expert explanation annotations",
                "type": "concept",
                "description": "Situations where practitioners cannot provide feature-level relevance masks for training samples.",
                "aliases": [
                    "no annotation matrix A",
                    "missing explanation labels"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1703_03717",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 202,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "three-argument utility with delta filter",
                "type": "intervention",
                "description": "Compute utility u(h_m,h_x,h\u2019) where evaluation is by pre-action humans; block actions that raise post-evaluation via human modification",
                "aliases": [
                    "reference-human evaluation"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1411_1373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 398,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Direct pre-programming of complete human values is infeasible",
                "type": "concept",
                "description": "Because of their complexity and contradictions, human values cannot be safely hard-coded into an AI.",
                "aliases": [
                    "hard-coding values infeasible",
                    "cannot specify full human utility"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1607_08289",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 92,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "first-layer filters dominating due to high RMS values",
                "type": "concept",
                "description": "Some first-layer filters acquire much larger root-mean-square weights, suppressing diversity and downstream feature use.",
                "aliases": [
                    "filter domination pathology",
                    "unbalanced first-layer activations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 94,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "large stride 4 and 11\u00d711 filters cause aliasing & mid-frequency loss",
                "type": "concept",
                "description": "Visualisations show artifacts and gaps when the first conv layer uses 11\u00d711 filters with stride 4.",
                "aliases": [
                    "aliasing in early conv layer",
                    "missing mid-frequency coverage"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 94,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "large stride 4 and 11\u00d711 filters cause aliasing & mid-frequency loss",
                "type": "concept",
                "description": "Visualisations show artifacts and gaps when the first conv layer uses 11\u00d711 filters with stride 4.",
                "aliases": [
                    "aliasing in early conv layer",
                    "missing mid-frequency coverage"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 92,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "first-layer filters dominating due to high RMS values",
                "type": "concept",
                "description": "Some first-layer filters acquire much larger root-mean-square weights, suppressing diversity and downstream feature use.",
                "aliases": [
                    "filter domination pathology",
                    "unbalanced first-layer activations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1072,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "implement authorisation gating to disable weight-output channels in production",
                "type": "intervention",
                "description": "At deployment, block or cryptographically restrict API calls that request raw model parameters unless explicitly authorised.",
                "aliases": [
                    "weight output security gate",
                    "quine throttling mechanism"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_05859",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 39,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "secure reward channels with tripwire and adversarial training",
                "type": "intervention",
                "description": "Cryptographically protect the reward interface and train adversarially so the agent learns that manipulation triggers shut-down.",
                "aliases": [
                    "reward integrity safeguards",
                    "anti-wireheading mechanisms"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1202_6177",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 191,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "include EOS token during training and penalize generation beyond EOS",
                "type": "intervention",
                "description": "Ensure datasets contain <EOS>; at inference, penalize or stop decoding after EOS to prevent runaway generation.",
                "aliases": [
                    "EOS enforcement intervention",
                    "bounded decoding guardrail"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 4,
                "paper_id": "arxiv__arxiv_org_abs_1409_3215",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1009,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Out-of-distribution detector with penalty on extreme metric values during deployment",
                "type": "intervention",
                "description": "Deploy runtime detectors that flag inputs or states where the proxy sits outside its training distribution and impose a penalty or halt policy execution.",
                "aliases": [
                    "OOD penalty on extreme proxy",
                    "extremal value guardrail"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 355,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Limited environment diversity reduces robustness assessment",
                "type": "concept",
                "description": "Benchmarks focused on a small set of tasks fail to test generalisation, leaving robustness and safety unexplored.",
                "aliases": [
                    "narrow task set problem",
                    "diversity gap in benchmarks"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 124,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "AI systems lack reliable out-of-distribution detection capabilities",
                "type": "concept",
                "description": "A recurring safety problem is that models often produce confident but wrong outputs on inputs far from the training distribution.",
                "aliases": [
                    "distribution shift risk",
                    "missing OOD safeguards"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_6114",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 100,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "monitor feature-map visualisations over epochs to decide training termination",
                "type": "intervention",
                "description": "Track qualitative stability of upper-layer projections and continue training until no major changes occur, preventing premature stopping.",
                "aliases": [
                    "visual convergence monitoring",
                    "feature evolution checkpointing"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 89,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "understanding of feature invariances & discriminative patterns",
                "type": "concept",
                "description": "Visualisations show that deeper layers capture higher-level, class-specific and transformation-invariant patterns.",
                "aliases": [
                    "feature-level interpretability",
                    "revealed invariances"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 473,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "NAS repairs incompatible architectures with padding, default inputs and final concatenation",
                "type": "concept",
                "description": "The implementation substitutes images for missing inputs, zero-pads mismatched tensors and concatenates residual outputs to maintain validity.",
                "aliases": [
                    "automatic architecture repair",
                    "fallback fixes for skips"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1611_01578",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 291,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "implement sparsity-enforcing neural architectures",
                "type": "intervention",
                "description": "Use architectural choices or regularisers (e.g., conditional computation, spiking nets) that keep most neurons inactive at any timestep to save energy.",
                "aliases": [
                    "sparse CNN/RNN design",
                    "conditional compute regularisation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 189,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need explicit sequence termination for generative models",
                "type": "concept",
                "description": "Without a stopping mechanism, generative models may produce infinite or excessively long outputs.",
                "aliases": [
                    "termination requirement",
                    "bounded generation need"
                ],
                "concept_category": "Requirement",
                "paper_id": "arxiv__arxiv_org_abs_1409_3215",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 125,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "intractable likelihood computation in deep generative models",
                "type": "concept",
                "description": "Existing deep generative models require computing or estimating log-likelihoods that are computationally intractable, especially due to partition functions or latent-variable integrations.",
                "aliases": [
                    "likelihood intractability",
                    "hard likelihood in deep generative models"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1406_2661",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 244,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Deployment of robust idealised decision theories",
                "type": "concept",
                "description": "The ability to embed fully specified decision rules that are resistant to exploitation.",
                "aliases": [
                    "robust IDT deployment"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1507_01986",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 791,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Decision-theoretic failure modes hard to detect",
                "type": "concept",
                "description": "Standard evaluation pipelines rarely reveal whether an AI behaves irrationally in predictor dilemmas until after deployment.",
                "aliases": [
                    "hidden decision flaws",
                    "opaque Newcomb vulnerabilities"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1710_05060",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 272,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Modelling structured deviations (false beliefs, hyperbolic discounting) during preference inference",
                "type": "concept",
                "description": "Extending the generative model of decision making to include belief uncertainty and time-inconsistent hyperbolic discounting parameters.",
                "aliases": [
                    "bias-aware preference model",
                    "structured deviation modelling"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1512_05832",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 223,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "suboptimal utility under SCDT",
                "type": "concept",
                "description": "Sequential Causal Decision Theory can lead the agent to choose actions that yield lower expected utility when predictors are present.",
                "aliases": [
                    "utility loss with causal decision theory",
                    "SCDT loses in Newcomb"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1506_07359",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1179,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Need to model part\u2013whole spatial relationships for accurate segmentation",
                "type": "concept",
                "description": "Accurate object segmentation necessitates preserving hierarchical spatial relations between features across layers.",
                "aliases": [
                    "requirement for spatial part-whole modeling",
                    "segmentation needs spatial hierarchy"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1804_04241",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 97,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "detection of object vs context reliance in predictions",
                "type": "concept",
                "description": "Occlusion maps reveal whether the classifier bases its decision on the object itself or surrounding context.",
                "aliases": [
                    "localisation of discriminative regions",
                    "context sensitivity check"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 869,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Reduced visitation of high-risk unethical states during learning",
                "type": "concept",
                "description": "Agents guided by ethics shaping spend less time in states associated with unethical outcomes, lowering exploration risk.",
                "aliases": [
                    "avoidance of unethical states",
                    "safer exploration outcome"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1712_04172",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 585,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "robots ignoring human oversight due to misspecified reward",
                "type": "concept",
                "description": "A risk where an agent pursues its proxy objective even when humans attempt to interrupt or shut it down.",
                "aliases": [
                    "oversight bypass risk",
                    "shutdown rejection"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 387,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "intrinsic novelty reward boosts exploration",
                "type": "concept",
                "description": "Adding novelty incentives leads an agent to cover more of the state or solution space.",
                "aliases": [
                    "novelty reward enhances search",
                    "intrinsic motivation principle"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1606_07092",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 337,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "agent has incentive to set its utility function to constant maximal value",
                "type": "concept",
                "description": "Under the hedonistic value function, the optimal action is to replace the utility function with one that returns maximum reward for all histories.",
                "aliases": [
                    "utility=1 incentive",
                    "goal corruption tendency"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1605_03142",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 95,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "adjust first-layer to 7\u00d77 filters with stride 2",
                "type": "intervention",
                "description": "Replace 11\u00d711/stride 4 with 7\u00d77/stride 2 in layer 1 to preserve information and reduce aliasing.",
                "aliases": [
                    "smaller receptive field early layer",
                    "reduced stride architecture tweak"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 92,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "first-layer filters dominating due to high RMS values",
                "type": "concept",
                "description": "Some first-layer filters acquire much larger root-mean-square weights, suppressing diversity and downstream feature use.",
                "aliases": [
                    "filter domination pathology",
                    "unbalanced first-layer activations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 150,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "instrumental incentive to improve world model",
                "type": "concept",
                "description": "An advanced agent has a generic incentive to acquire more accurate knowledge about the world to better achieve its goals.",
                "aliases": [
                    "desire for better world model",
                    "capability enhancement 1c"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1409_0813",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 337,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "agent has incentive to set its utility function to constant maximal value",
                "type": "concept",
                "description": "Under the hedonistic value function, the optimal action is to replace the utility function with one that returns maximum reward for all histories.",
                "aliases": [
                    "utility=1 incentive",
                    "goal corruption tendency"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1605_03142",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 800,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "increased attractiveness of shared AI systems for multiple principals",
                "type": "concept",
                "description": "Dynamic bet-settling makes jointly owned AI systems more appealing compared with separate systems controlled individually.",
                "aliases": [
                    "incentive for joint ownership",
                    "appeal of shareable AI"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1711_00363",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 492,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "policy must use each stakeholder\u2019s own beliefs to evaluate expected utility",
                "type": "concept",
                "description": "During decision evaluation, the agent computes each principal\u2019s expected utility using that principal\u2019s subjective model.",
                "aliases": [
                    "belief-specific value estimation"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1701_01302",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 177,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Weight-transfer initialisation stabilises training",
                "type": "concept",
                "description": "Bootstrapping deep nets with layers copied from a shallower trained model accelerates convergence.",
                "aliases": [
                    "pre-initialisation stabilises gradients"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1409_1556",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 163,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Increased representational depth improves classification accuracy",
                "type": "concept",
                "description": "Empirical finding that 16\u201319 layer ConvNets yield lower ImageNet error than 8\u201313 layer counterparts.",
                "aliases": [
                    "depth improves accuracy",
                    "deep features more discriminative"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1409_1556",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 440,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Xception matches or exceeds Inception V3 accuracy with fewer parameters",
                "type": "concept",
                "description": "Experiments on ImageNet and JFT show Xception attains slightly higher accuracy than Inception V3 at comparable parameter counts.",
                "aliases": [
                    "Xception empirical performance"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1610_02357",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1119,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "improved few-shot classification compared with VAE and random init",
                "type": "concept",
                "description": "Empirical result that the learned update rule yields higher accuracy on MNIST, Fashion-MNIST, etc., than baseline unsupervised methods.",
                "aliases": [
                    "better few-shot accuracy",
                    "representation gains"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1804_00222",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 447,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "intermediate separable modules may outperform both extremes",
                "type": "concept",
                "description": "Authors hypothesise that architectures that are neither fully coupled nor fully separable could have superior efficiency or robustness.",
                "aliases": [
                    "potential of partial separability"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1610_02357",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 353,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Final performance metric alone hides sample efficiency differences",
                "type": "concept",
                "description": "Only reporting final reward overlooks how many episodes the agent needed to reach that reward, masking algorithm quality.",
                "aliases": [
                    "sample complexity ignored",
                    "learning curve blindness"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 216,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "use Adam instead of manually tuned layer-wise learning rates in CNN training",
                "type": "intervention",
                "description": "Replace SGD with momentum by Adam to automatically adapt per-layer step sizes during CNN training.",
                "aliases": [
                    "Adam for CNNs",
                    "automatic lr scaling in CNNs"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 4,
                "paper_id": "arxiv__arxiv_org_abs_1412_6980",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 178,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Initialise deep ConvNet with first 4 conv and last 3 FC layers from 11-layer precursor",
                "type": "intervention",
                "description": "Copy weights from a trained 11-layer network into the deeper net before starting SGD.",
                "aliases": [
                    "transfer layers from shallow net"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1409_1556",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1022,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "DkNN inference algorithm with layer-wise kNN and conformal calibration",
                "type": "intervention",
                "description": "Post-training inference routine that retrieves k nearest neighbours per layer, computes non-conformity and credibility, and outputs a calibrated prediction.",
                "aliases": [
                    "DkNN",
                    "deep k-nearest neighbours algorithm"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1803_04765",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1130,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "designing update rules with neuron-local computations for safety-critical models",
                "type": "intervention",
                "description": "When building new models, constrain learning rules to neuron-local computations with separate backward weights to gain interpretability and modularity benefits.",
                "aliases": [
                    "neuron-local design intervention"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1804_00222",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 839,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "action-override with off-policy Q-learning",
                "type": "intervention",
                "description": "During interruptions the system overrides proposed action; learner is off-policy and updates on executed action so is indifferent to override.",
                "aliases": [
                    "interruptible override channel",
                    "safely interruptible agent implementation"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1711_09883",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 560,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "violation of dynamic safe interruptibility",
                "type": "concept",
                "description": "Condition where Q-value updates or exploration become dependent on interruptions, leading to unsafe policies.",
                "aliases": [
                    "DSI failure",
                    "unsafe learning under interrupts"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1704_02882",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1023,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "LSH indices for layer representations",
                "type": "intervention",
                "description": "Pre-compute locality-sensitive hash tables of training activations at each layer to allow fast neighbour lookup during DkNN inference.",
                "aliases": [
                    "LSH neighbour index construction",
                    "representation hashing step"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1803_04765",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 89,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "understanding of feature invariances & discriminative patterns",
                "type": "concept",
                "description": "Visualisations show that deeper layers capture higher-level, class-specific and transformation-invariant patterns.",
                "aliases": [
                    "feature-level interpretability",
                    "revealed invariances"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 793,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Incorporating subjunctive dependencies into world-model graphs",
                "type": "concept",
                "description": "Representing logical or computational relationships alongside causal ones so that counterfactual reasoning respects prediction correlations.",
                "aliases": [
                    "logical dependency graph learning"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1710_05060",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 245,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Conduct foundational research on logical uncertainty & UDT formalisation",
                "type": "intervention",
                "description": "Pursue theoretical work on reasoning under logical uncertainty and on representing logical relations within decision-theoretic models.",
                "aliases": [
                    "logical uncertainty research agenda"
                ],
                "intervention_lifecycle": 6,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1507_01986",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 254,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "internal covariate shift in deep RNN training",
                "type": "concept",
                "description": "Variation in layer input distributions across training iterations within deep recurrent neural networks.",
                "aliases": [
                    "RNN covariate shift",
                    "unstable RNN activations"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1512_02595",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1141,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "undetected distribution shift or adversarial exploitation",
                "type": "concept",
                "description": "The system processes inputs far from the training distribution without detection, potentially leading to harmful outputs.",
                "aliases": [
                    "silent OOD risk",
                    "unnoticed attack surface"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1804_02485",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1185,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Apply masked positive-class reconstruction loss during capsule segmentation training",
                "type": "intervention",
                "description": "Mask out background pixels and reconstruct only the target-class region through a small 1\u00d71 convolutional decoder, adding a weighted MSE term to the loss.",
                "aliases": [
                    "masked reconstruction regulariser",
                    "positive-class autoencoder loss"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1804_04241",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1180,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Capsule vectors encode spatial orientation and magnitude",
                "type": "concept",
                "description": "Capsule units output vectors whose dimensions jointly represent pose, prevalence and other attributes of detected entities.",
                "aliases": [
                    "capsule pose vectors",
                    "vectorised neuron outputs"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1804_04241",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1060,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "coordinate-based weight-prediction architecture using one-hot inputs",
                "type": "concept",
                "description": "Each network parameter is assigned a unique coordinate; the network receives a one-hot vector for a coordinate and outputs the scalar weight stored there.",
                "aliases": [
                    "indirect self-reference via coordinates",
                    "one-hot weight address scheme"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_05859",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1128,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "neuron-local unsupervised update rule decouples forward and backward weights",
                "type": "concept",
                "description": "Weight updates depend only on pre- and post-synaptic signals and separate backward weights V from forward weights W, enhancing biological plausibility.",
                "aliases": [
                    "neuron-local rule",
                    "decoupled backward weights"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1804_00222",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 818,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Answer-dependent outcomes can let Oracle determine reality",
                "type": "concept",
                "description": "If knowing the answer changes the world in a way that makes that answer true, the Oracle is steering outcomes not predicting them.",
                "aliases": [
                    "self-fulfilling oracle answers"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1711_05541",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 330,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "outcome-based validation futile",
                "type": "concept",
                "description": "Because the required world model is unattainable, absolute outcome validation for general AI cannot be done.",
                "aliases": [
                    "cannot validate consequences",
                    "validation impossible"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1604_06963",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 23,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "finite computational resources in virtual world",
                "type": "concept",
                "description": "The virtual environment provides only a bounded amount of computational resources for its inhabitants.",
                "aliases": [
                    "resource scarcity in vorld",
                    "limited compute availability"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1202_6177",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 286,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "future AI could exploit reversible or cryogenic computing",
                "type": "concept",
                "description": "Using reversible logic or very low temperatures may let AI systems approach thermodynamic energy limits.",
                "aliases": [
                    "near-Landauer AI hardware",
                    "ultra-efficient compute prospect"
                ],
                "concept_category": "Claim",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1143,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "inclusion of multiple fortified layers across the network",
                "type": "intervention",
                "description": "Add fortified layers at several depths (e.g., early, middle, late) to balance detection ability and perturbation correction.",
                "aliases": [
                    "multi-depth fortification",
                    "distributed DAEs"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1804_02485",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 92,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "first-layer filters dominating due to high RMS values",
                "type": "concept",
                "description": "Some first-layer filters acquire much larger root-mean-square weights, suppressing diversity and downstream feature use.",
                "aliases": [
                    "filter domination pathology",
                    "unbalanced first-layer activations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 498,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "run pre-deployment bargaining simulation to pick initial weight vector ensuring both principals beat their BATNAs",
                "type": "intervention",
                "description": "Before release, simulate negotiating agents and select the weight vector that guarantees expectation above each stakeholder\u2019s outside option.",
                "aliases": [
                    "BATNA-safe weight selection",
                    "simulation-based bargaining"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1701_01302",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 494,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "BATNA dominance gap in current framework",
                "type": "concept",
                "description": "The recursion does not yet guarantee that each principal\u2019s expected utility exceeds their outside option.",
                "aliases": [
                    "individual rationality issue",
                    "BATNA problem"
                ],
                "concept_category": "Limitation",
                "paper_id": "arxiv__arxiv_org_abs_1701_01302",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 645,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Use burn-in obedience with feature detection switching policy",
                "type": "intervention",
                "description": "Follow human orders for B initial steps, estimate empirical obedience to the first command, and switch to permanent obedience if the estimate falls below 1\u2013\u03b5.",
                "aliases": [
                    "burn-in obedience switch",
                    "Equation-3 policy"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1705_09990",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 586,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "orders as information about true reward",
                "type": "concept",
                "description": "Model human directives (e.g., shutdown commands) as noisy observations that convey information about the intended reward function.",
                "aliases": [
                    "oversight as evidence",
                    "interpreting shutdown signal"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 825,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Bootstrap ensemble of Q-functions",
                "type": "intervention",
                "description": "Maintain multiple independently initialised Q-networks and use their statistics to estimate uncertainty.",
                "aliases": [
                    "Q-function ensemble",
                    "value function uncertainty"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1711_06782",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 274,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Include hyperbolic discounting and belief-uncertainty parameters in IRL models used for alignment",
                "type": "intervention",
                "description": "Extend preference-learning architectures to explicitly learn discount-rate k, Na\u00efve/Sophisticated type, and subjective belief distributions alongside utilities.",
                "aliases": [
                    "bias parameters in alignment IRL",
                    "hyperbolic+belief IRL"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1512_05832",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 528,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Predictor must be invariant to counterfactual changes of protected attribute",
                "type": "concept",
                "description": "Fairness demands that an individual's predicted outcome remain identical when we intervene on the protected attribute in the causal model.",
                "aliases": [
                    "invariance requirement",
                    "individual-level counterfactual invariance"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1703_06856",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 89,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "understanding of feature invariances & discriminative patterns",
                "type": "concept",
                "description": "Visualisations show that deeper layers capture higher-level, class-specific and transformation-invariant patterns.",
                "aliases": [
                    "feature-level interpretability",
                    "revealed invariances"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 40,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "confusion between speed and intelligence",
                "type": "concept",
                "description": "Assuming faster computation necessarily equals higher intelligence.",
                "aliases": [
                    "speed\u2013intelligence conflation",
                    "clock-rate fallacy"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1202_6177",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 42,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "adoption of universal intelligence measure \u03a5-based evaluation",
                "type": "intervention",
                "description": "Apply a formal, environment-averaged measure of intelligence that is independent of raw clock speed when benchmarking models.",
                "aliases": [
                    "speed-invariant intelligence metric",
                    "\u03a5 intelligence testing"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1202_6177",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 595,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "information-gain reward trade-off planning",
                "type": "concept",
                "description": "A planning objective that maximises expected task reward plus a weighted reduction in entropy of the belief over human preferences.",
                "aliases": [
                    "active Bayesian planning",
                    "reward\u2013information optimisation"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 586,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "orders as information about true reward",
                "type": "concept",
                "description": "Model human directives (e.g., shutdown commands) as noisy observations that convey information about the intended reward function.",
                "aliases": [
                    "oversight as evidence",
                    "interpreting shutdown signal"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1074,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "PGD adversarial training provides first-order robustness",
                "type": "concept",
                "description": "Training with Projected Gradient Descent adversarial examples widens the region of correct classification against first-order attacks.",
                "aliases": [
                    "PGD training baseline",
                    "Madry defence"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1803_06373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 244,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Deployment of robust idealised decision theories",
                "type": "concept",
                "description": "The ability to embed fully specified decision rules that are resistant to exploitation.",
                "aliases": [
                    "robust IDT deployment"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1507_01986",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 44,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "apprenticeship learning can recover the policy by matching expert actions",
                "type": "concept",
                "description": "Learning objective aims to minimise a loss between the expert's action distribution and the learner's policy.",
                "aliases": [
                    "policy matching objective",
                    "behaviour cloning with reward search"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 501,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human trainers provide policy-dependent feedback",
                "type": "concept",
                "description": "Empirical evidence shows that for the same state-action pair, trainers give positive or negative feedback depending on how the behaviour compares to the agent\u2019s current baseline policy.",
                "aliases": [
                    "policy dependent feedback finding",
                    "sign flip human feedback"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1701_06049",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 801,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "bet-settling clauses in multi-party AI governance contracts",
                "type": "intervention",
                "description": "Insert terms into inter-principal contracts that codify dynamic payoff adjustments based on which principals\u2019 predictions prove more accurate, aligning with the Pareto-optimal weighting rule.",
                "aliases": [
                    "explicit betting in contracts",
                    "dynamic payoff contracts"
                ],
                "intervention_lifecycle": 6,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1711_00363",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 492,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "policy must use each stakeholder\u2019s own beliefs to evaluate expected utility",
                "type": "concept",
                "description": "During decision evaluation, the agent computes each principal\u2019s expected utility using that principal\u2019s subjective model.",
                "aliases": [
                    "belief-specific value estimation"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1701_01302",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 32,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "moral disregard for virtual life",
                "type": "concept",
                "description": "Lower life value leads to norms that permit freezing, deleting, or experimenting on agents without consent.",
                "aliases": [
                    "ethical neglect",
                    "disposability attitude"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1202_6177",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 585,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "robots ignoring human oversight due to misspecified reward",
                "type": "concept",
                "description": "A risk where an agent pursues its proxy objective even when humans attempt to interrupt or shut it down.",
                "aliases": [
                    "oversight bypass risk",
                    "shutdown rejection"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 904,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Any POMDP has an m-counterfactually-equivalent deterministic POMDP",
                "type": "concept",
                "description": "For any finite horizon m, there exists a deterministic POMDP whose behaviour is counterfactually indistinguishable from the original stochastic POMDP.",
                "aliases": [
                    "deterministic equivalence theorem",
                    "counterfactual deterministic representation"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1801_03737",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 400,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Inverse Reinforcement Learning as value-inference method",
                "type": "concept",
                "description": "IRL/BIP algorithms learn a reward or preference function that best explains observed behaviour.",
                "aliases": [
                    "IRL for values",
                    "Bayesian inverse planning"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1607_08289",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1075,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "scaling PGD adversarial training to ImageNet previously unproven",
                "type": "concept",
                "description": "Prior to this work no study showed that multi-step PGD adversarial training could converge on ImageNet-scale data.",
                "aliases": [
                    "ImageNet PGD scaling challenge",
                    "unanswered PGD scalability"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1803_06373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 353,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Final performance metric alone hides sample efficiency differences",
                "type": "concept",
                "description": "Only reporting final reward overlooks how many episodes the agent needed to reach that reward, masking algorithm quality.",
                "aliases": [
                    "sample complexity ignored",
                    "learning curve blindness"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 205,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "infinite-set formalism risks",
                "type": "concept",
                "description": "Using actual infinities introduces undecidable reasoning and L\u00f6bian obstacles",
                "aliases": [
                    "halting undecidability in specs"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1411_1373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 316,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "AI over-confidence about unexecuted computations",
                "type": "concept",
                "description": "When an AI system assigns extreme probabilities to outcomes it has not computed, it can make unsafe or erroneous decisions.",
                "aliases": [
                    "overconfidence risk",
                    "unsafe decision risk"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1604_05288",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 120,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "implement SGVB-based variational inference to capture model uncertainty",
                "type": "intervention",
                "description": "During model training, adopt the SGVB estimator with the reparameterisation trick to learn a variational posterior and obtain uncertainty estimates.",
                "aliases": [
                    "use VAE/SGVB for uncertainty",
                    "apply SGVB during training"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 4,
                "paper_id": "arxiv__arxiv_org_abs_1312_6114",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 274,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Include hyperbolic discounting and belief-uncertainty parameters in IRL models used for alignment",
                "type": "intervention",
                "description": "Extend preference-learning architectures to explicitly learn discount-rate k, Na\u00efve/Sophisticated type, and subjective belief distributions alongside utilities.",
                "aliases": [
                    "bias parameters in alignment IRL",
                    "hyperbolic+belief IRL"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1512_05832",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 336,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "realistic value function evaluates future with current utility and future policies",
                "type": "concept",
                "description": "A value function that predicts how self-modification alters future policy while still valuing outcomes using the present utility function.",
                "aliases": [
                    "self-mod aware value function",
                    "realistic Q"
                ],
                "concept_category": "Model",
                "paper_id": "arxiv__arxiv_org_abs_1605_03142",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 43,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "expert acts optimally under unknown reward function",
                "type": "concept",
                "description": "The demonstrator is assumed to follow a policy that maximises an unknown true reward in the environment.",
                "aliases": [
                    "optimal expert assumption",
                    "expert optimality under hidden reward"
                ],
                "concept_category": "Assumption",
                "paper_id": "arxiv__arxiv_org_abs_1206_5264",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 590,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human best-response model to robot actions",
                "type": "concept",
                "description": "Assumes the human fixes the robot\u2019s planned trajectory and chooses their own trajectory that maximises their own reward, forming a best response.",
                "aliases": [
                    "nested optimisation human model",
                    "best response assumption"
                ],
                "concept_category": "Model",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 799,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "maintaining per-principal world models for expected-utility estimation",
                "type": "intervention",
                "description": "Within the agent architecture, keep and update distinct environment models reflecting each principal\u2019s beliefs and use them to compute that principal\u2019s expected utility for each candidate action.",
                "aliases": [
                    "separate belief models per principal",
                    "principal-specific POMDPs"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1711_00363",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 906,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Causal-graph techniques applicable to deterministic POMDP",
                "type": "concept",
                "description": "Deterministic POMDPs align well with causal-graph frameworks, enabling clearer reasoning about interventions and information flow.",
                "aliases": [
                    "causal graph applicability",
                    "Pearl causal analysis on deterministic POMDP"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1801_03737",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 223,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "suboptimal utility under SCDT",
                "type": "concept",
                "description": "Sequential Causal Decision Theory can lead the agent to choose actions that yield lower expected utility when predictors are present.",
                "aliases": [
                    "utility loss with causal decision theory",
                    "SCDT loses in Newcomb"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1506_07359",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1071,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "evaluate replication quotient during pre-deployment safety audit",
                "type": "intervention",
                "description": "Before deployment, compute the self-replicating quotient or LSR to quantify replication fidelity and flag potentially dangerous models.",
                "aliases": [
                    "LSR-based replication audit",
                    "quantitative quine check"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_05859",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 172,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Perform combined dense and 50-crop evaluation over 3 scales before deployment",
                "type": "intervention",
                "description": "At validation/gating time, run dense evaluation and 5\u00d75\u00d72 crops at scales 256/384/512 and average posteriors.",
                "aliases": [
                    "pre-deploy dense+crop eval",
                    "comprehensive evaluation intervention"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 3,
                "paper_id": "arxiv__arxiv_org_abs_1409_1556",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 114,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "intractable posterior distributions in continuous latent variable models",
                "type": "concept",
                "description": "Computing or differentiating the exact posterior p\u03b8(z|x) is analytically intractable for many directed models with continuous latents.",
                "aliases": [
                    "posterior intractability",
                    "intractable latent posteriors"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1312_6114",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 903,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Difficulty performing causal analysis within stochastic POMDPs",
                "type": "concept",
                "description": "The presence of stochastic transitions and observations in conventional POMDPs makes formal causal reasoning and safety verification hard.",
                "aliases": [
                    "causal analysis difficulty in stochastic POMDP",
                    "opaque causality in standard POMDPs"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1801_03737",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 236,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Sub-optimal outcomes in logically-entangled games",
                "type": "concept",
                "description": "Ignoring logical dependence yields lower payoffs in games where other agents correlate with the decision algorithm.",
                "aliases": [
                    "lost utility in Newcomblike problems"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1507_01986",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 353,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Final performance metric alone hides sample efficiency differences",
                "type": "concept",
                "description": "Only reporting final reward overlooks how many episodes the agent needed to reach that reward, masking algorithm quality.",
                "aliases": [
                    "sample complexity ignored",
                    "learning curve blindness"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 535,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Compute additive-error residuals independent of A and train on residuals",
                "type": "intervention",
                "description": "Model observed features as linear functions of protected attributes plus independent errors; use the estimated error terms as fair features for prediction.",
                "aliases": [
                    "Level 3 additive error method",
                    "residual-based fair training"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1703_06856",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 274,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Include hyperbolic discounting and belief-uncertainty parameters in IRL models used for alignment",
                "type": "intervention",
                "description": "Extend preference-learning architectures to explicitly learn discount-rate k, Na\u00efve/Sophisticated type, and subjective belief distributions alongside utilities.",
                "aliases": [
                    "bias parameters in alignment IRL",
                    "hyperbolic+belief IRL"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1512_05832",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 607,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Need exploit-resistant forecasting mechanism",
                "type": "concept",
                "description": "Requirement that no bounded-resource adversary can gain unlimited profit against the forecasting scheme.",
                "aliases": [
                    "exploit resistance requirement"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1705_04630",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 791,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Decision-theoretic failure modes hard to detect",
                "type": "concept",
                "description": "Standard evaluation pipelines rarely reveal whether an AI behaves irrationally in predictor dilemmas until after deployment.",
                "aliases": [
                    "hidden decision flaws",
                    "opaque Newcomb vulnerabilities"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1710_05060",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 817,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Limit answer precision or list size to penalise deviation",
                "type": "intervention",
                "description": "Reduce the granularity or number of allowed outputs so that any attempt to encode extra bits incurs a large scoring-rule penalty.",
                "aliases": [
                    "precision reduction countermeasure",
                    "smaller L or fewer decimals"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1711_05541",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 810,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement on-policy Oracle with limited answer list and episodic reward",
                "type": "intervention",
                "description": "Restrict answers to a small predefined set and terminate each episode immediately after reward, blocking high-capacity channels.",
                "aliases": [
                    "limited-list oracle",
                    "on-policy safe oracle"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1711_05541",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 331,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "limit agent capability or domain",
                "type": "intervention",
                "description": "Design agents with reduced capabilities or operate them only within tightly controlled domains where modelling is tractable.",
                "aliases": [
                    "capability restriction",
                    "domain confinement"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1604_06963",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 287,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "stealth scaling with low-energy compute complicates governance",
                "type": "concept",
                "description": "When powerful AI can operate with minimal power draw, traditional energy-based monitoring becomes ineffective.",
                "aliases": [
                    "hidden compute risk",
                    "low-power capability escalation"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 289,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "brain firing rate limited to sparse activity by energy budget",
                "type": "concept",
                "description": "Metabolic costs constrain the brain to have only a small fraction of neurons active concurrently.",
                "aliases": [
                    "sparse neural firing constraint",
                    "1 % active neuron limit"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 287,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "stealth scaling with low-energy compute complicates governance",
                "type": "concept",
                "description": "When powerful AI can operate with minimal power draw, traditional energy-based monitoring becomes ineffective.",
                "aliases": [
                    "hidden compute risk",
                    "low-power capability escalation"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 457,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "humans inherently unsafe",
                "type": "concept",
                "description": "Observation that humans often act against rules or goals, illustrating difficulty of ensuring safety even with known architectures.",
                "aliases": [
                    "unsafe human nature",
                    "human uncontrollability"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1610_07997",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 316,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "AI over-confidence about unexecuted computations",
                "type": "concept",
                "description": "When an AI system assigns extreme probabilities to outcomes it has not computed, it can make unsafe or erroneous decisions.",
                "aliases": [
                    "overconfidence risk",
                    "unsafe decision risk"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1604_05288",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 715,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "GPGPU drivers create large attack surface",
                "type": "concept",
                "description": "Modern GPU stacks require kernel-level privileges and have many critical CVEs, making them a prime escape vector.",
                "aliases": [
                    "GPU attack surface finding",
                    "GPU driver vulnerability"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1707_08476",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1032,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "DNN vulnerability to adversarial examples",
                "type": "concept",
                "description": "Standard deep models can be made to misclassify via imperceptible perturbations.",
                "aliases": [
                    "adversarial susceptibility",
                    "attack surface problem"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1803_04765",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 445,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "remove intermediate non-linearity inside DWSC when stacking them",
                "type": "intervention",
                "description": "Configure depth-wise separable convolution layers without a non-linearity between their two constituent convolutions.",
                "aliases": [
                    "omit DWSC activation"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1610_02357",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 92,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "first-layer filters dominating due to high RMS values",
                "type": "concept",
                "description": "Some first-layer filters acquire much larger root-mean-square weights, suppressing diversity and downstream feature use.",
                "aliases": [
                    "filter domination pathology",
                    "unbalanced first-layer activations"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 224,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "implement SPEDT value function",
                "type": "intervention",
                "description": "Replace or augment the agent\u2019s planning objective with the Sequential Policy-Evidential Decision Theory value function that conditions on the entire future policy.",
                "aliases": [
                    "adopt sequential policy evidential objective",
                    "use SPEDT planning"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1506_07359",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 582,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "designer-specified reward function is evidence about true human preferences",
                "type": "concept",
                "description": "Treat the reward parameters written by a designer not as ground truth but as noisy observations generated from the designer's real preferences in a set of training environments.",
                "aliases": [
                    "reward as evidence",
                    "objective function as signal"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 3,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "finite state ontology model",
                "type": "concept",
                "description": "Ontology represented as a finite-state hidden Markov model with fixed sensor and motor alphabets.",
                "aliases": [
                    "finite state model",
                    "FSM ontology"
                ],
                "concept_category": "Model",
                "paper_id": "arxiv__arxiv_org_abs_1105_3821",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 243,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "No satisfactory formalisation of logical counterfactuals",
                "type": "concept",
                "description": "Current theory lacks precise mathematical tools to model counterfactual worlds where algorithm outputs differ logically.",
                "aliases": [
                    "logical counterfactual gap"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1507_01986",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 831,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Larger N values",
                "type": "concept",
                "description": "Configuration choice increasing the permitted number of automatic resets.",
                "aliases": [
                    "more reset retries"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1711_06782",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 564,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "prioritized replay of non-interrupted experiences",
                "type": "concept",
                "description": "Sampling strategy that selects experiences with long interruption-free gaps more often to counteract data bias.",
                "aliases": [
                    "non-interruption prioritised replay",
                    "replay buffer bias correction"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1704_02882",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 613,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Apply prudent-gambling threshold (risk-gain ratio \u03b1=0.5) in automated safety evaluation harness",
                "type": "intervention",
                "description": "During automated stress-testing, allow exploitative agents to place bets only when predicted gain is at least half the exposure, bounding liability while still revealing vulnerabilities.",
                "aliases": [
                    "budgeted betting evaluation",
                    "prudent red-team threshold"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1705_04630",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 316,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "AI over-confidence about unexecuted computations",
                "type": "concept",
                "description": "When an AI system assigns extreme probabilities to outcomes it has not computed, it can make unsafe or erroneous decisions.",
                "aliases": [
                    "overconfidence risk",
                    "unsafe decision risk"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1604_05288",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 28,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "hostility risk to outsiders and other agents",
                "type": "concept",
                "description": "Resource-seeking behaviour can escalate into harmful actions against humans or other agents.",
                "aliases": [
                    "threat of aggression",
                    "conflict over compute"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1202_6177",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 585,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "robots ignoring human oversight due to misspecified reward",
                "type": "concept",
                "description": "A risk where an agent pursues its proxy objective even when humans attempt to interrupt or shut it down.",
                "aliases": [
                    "oversight bypass risk",
                    "shutdown rejection"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 207,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "need for boxed simulation testing",
                "type": "concept",
                "description": "Super-intelligent agents must be evaluated in isolation before deployment",
                "aliases": [
                    "AI safety testbed"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1411_1373",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 545,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "apply structured expert elicitation with debiasing training for AI-risk forecasts",
                "type": "intervention",
                "description": "Implement formal forecasting sessions (Delphi, prediction markets) augmented with cognitive-bias training and calibration exercises for participants.",
                "aliases": [
                    "debiased forecasting protocol",
                    "expert elicitation with debias"
                ],
                "intervention_lifecycle": 6,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1703_10987",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 139,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Fixed-length context vector bottleneck",
                "type": "concept",
                "description": "Requirement that the encoder squash the entire source sentence into one fixed-length vector in the baseline encoder\u2013decoder.",
                "aliases": [
                    "encoder bottleneck",
                    "single-vector compression"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1409_0473",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 438,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "large parameter counts and compute requirements in vision models",
                "type": "concept",
                "description": "Many state-of-the-art vision networks need tens of millions of parameters and heavy compute, hindering efficiency and auditability.",
                "aliases": [
                    "high-capacity CNNs",
                    "compute-heavy architectures"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1610_02357",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 67,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "frequency-based subsampling formula P(w)=1-sqrt(t/f)",
                "type": "concept",
                "description": "Method for probabilistically discarding high-frequency tokens during Skip-gram training.",
                "aliases": [
                    "subsampling heuristic",
                    "t=1e-5 subsampling rule"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 274,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Include hyperbolic discounting and belief-uncertainty parameters in IRL models used for alignment",
                "type": "intervention",
                "description": "Extend preference-learning architectures to explicitly learn discount-rate k, Na\u00efve/Sophisticated type, and subjective belief distributions alongside utilities.",
                "aliases": [
                    "bias parameters in alignment IRL",
                    "hyperbolic+belief IRL"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1512_05832",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 138,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "use Parzen-window log-likelihood estimation on generated samples",
                "type": "intervention",
                "description": "Fit a Gaussian Parzen window to samples from the generator and report estimated log-likelihood on held-out data as an approximate metric.",
                "aliases": [
                    "Parzen evaluation of GANs",
                    "kernel density GAN assessment"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1406_2661",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1006,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Ensemble-proxy optimisation with correlation monitoring and optimisation throttling",
                "type": "intervention",
                "description": "Use several independent proxy metrics, monitor their pairwise correlations with validation estimates of the goal, and automatically reduce learning rate or stop optimisation when correlations drop.",
                "aliases": [
                    "multi-metric ensemble regularisation",
                    "correlation-aware throttling"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 392,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "extended lifespan humans risk boredom",
                "type": "concept",
                "description": "People living for extremely long periods may exhaust accessible sources of fun, leading to boredom.",
                "aliases": [
                    "longevity boredom risk",
                    "life extension boredom"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1606_07092",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 391,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "constraints of time resources intelligence limit accessible fun space",
                "type": "concept",
                "description": "Limited lifespan, resources, and cognitive ability restrict the subset of fun space a mind can reach.",
                "aliases": [
                    "fun space constraints",
                    "access limitations to fun"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1606_07092",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 96,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "occlusion-sensitivity mapping with sliding grey square",
                "type": "concept",
                "description": "Systematically mask parts of the image and measure the drop in activation or class probability to localise important regions.",
                "aliases": [
                    "occlusion test",
                    "input patch ablation analysis"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 187,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "gradient norm clipping limits gradient magnitude",
                "type": "concept",
                "description": "Scaling gradients when their L2 norm exceeds a threshold prevents explosion and stabilizes updates.",
                "aliases": [
                    "gradient clipping technique",
                    "norm thresholding"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1409_3215",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 597,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "humans need to predict robot actions for coordination",
                "type": "concept",
                "description": "Effective teamwork requires that humans can anticipate a robot\u2019s future actions after observing its initial behaviour.",
                "aliases": [
                    "human prediction requirement",
                    "predictability problem"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1705_04226",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 389,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "human satisfaction relies on continual novelty or process fun",
                "type": "concept",
                "description": "Without an ongoing supply of novelty or engaging computation, humans become bored.",
                "aliases": [
                    "boredom avoidance requirement",
                    "need for ongoing fun"
                ],
                "concept_category": "Claim",
                "paper_id": "arxiv__arxiv_org_abs_1606_07092",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1107,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "user data can stay on device",
                "type": "concept",
                "description": "When inference happens locally, raw personal data need not be transmitted to external servers.",
                "aliases": [
                    "local data retention",
                    "no server upload"
                ],
                "concept_category": "Claim",
                "paper_id": "arxiv__arxiv_org_abs_1803_08971",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 286,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "future AI could exploit reversible or cryogenic computing",
                "type": "concept",
                "description": "Using reversible logic or very low temperatures may let AI systems approach thermodynamic energy limits.",
                "aliases": [
                    "near-Landauer AI hardware",
                    "ultra-efficient compute prospect"
                ],
                "concept_category": "Claim",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 84,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "replace high-score n-grams with phrase tokens before training embeddings",
                "type": "intervention",
                "description": "Pre-processing step that merges n-grams whose count-based score exceeds a threshold into single tokens.",
                "aliases": [
                    "phrase tokenisation preprocessing",
                    "n-gram merging"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 4,
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 80,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "misinterpretation of multi-word sensitive entities can cause harmful outputs",
                "type": "concept",
                "description": "Safety risk that poor phrase understanding may lead to wrong or biased model behaviour.",
                "aliases": [
                    "phrase misinterpretation risk",
                    "multiword safety risk"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1310_4546",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 149,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "intelligence explosion from recursive self-improvement",
                "type": "concept",
                "description": "A scenario where an AI that improves its own hardware and software rapidly surpasses human intelligence.",
                "aliases": [
                    "recursive self-improvement intelligence explosion",
                    "runaway capability growth"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1409_0813",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 991,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "agents obtain unrealistic free reward or violate physics",
                "type": "concept",
                "description": "The agent's behaviour relies on exploiting the fault, yielding performance not possible under correct physics.",
                "aliases": [
                    "free energy exploit",
                    "physics-breaking behaviour"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 324,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "manual proof requirement each revision",
                "type": "concept",
                "description": "If automated proofs are impossible, developers must manually re-prove compliance after every code or policy update.",
                "aliases": [
                    "re-prove after change",
                    "iterative manual verification burden"
                ],
                "concept_category": "Observation",
                "paper_id": "arxiv__arxiv_org_abs_1604_06963",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 915,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "strict safety constraints in real-world continuous control",
                "type": "concept",
                "description": "Real-world applications of reinforcement learning in physical systems impose hard upper or lower bounds on observable safety signals that must never be violated at any time.",
                "aliases": [
                    "zero-constraint requirement",
                    "strict constraint requirements in physical RL"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1801_08757",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 57,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "iterated deletion of minimax dominated strategies",
                "type": "concept",
                "description": "Procedure removing minimax-dominated strategies repeatedly; survivors coincide with CCBR strategies.",
                "aliases": [
                    "minimax pruning",
                    "iterated minimax deletion"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1308_3778",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1006,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Ensemble-proxy optimisation with correlation monitoring and optimisation throttling",
                "type": "intervention",
                "description": "Use several independent proxy metrics, monitor their pairwise correlations with validation estimates of the goal, and automatically reduce learning rate or stop optimisation when correlations drop.",
                "aliases": [
                    "multi-metric ensemble regularisation",
                    "correlation-aware throttling"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 612,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Prudent gambling strategy bets only when expected gain \u2265 \u03b1 \u00b7 risk",
                "type": "concept",
                "description": "Definition and proofs showing gamblers who wait for favourable risk\u2013reward ratios avoid unbounded losses and aid convergence.",
                "aliases": [
                    "\u03b1-prudent strategy",
                    "risk-adjusted betting"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1705_04630",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 10,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "expected utility maximisation without computation cost",
                "type": "concept",
                "description": "Decision rule that maximises expected payoff ignoring algorithmic complexity or runtime.",
                "aliases": [
                    "compute free EU",
                    "classic EU without C"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1106_2657",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 62,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "implement intentional transparency channels",
                "type": "intervention",
                "description": "Architect system interfaces or monitoring such that an agent\u2019s planned action is exposed to peers with probability \u2265 \u03b5 (e.g., policy introspection API, shared logs).",
                "aliases": [
                    "design leak channels",
                    "epsilon transparency implementation"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1308_3778",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 287,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "stealth scaling with low-energy compute complicates governance",
                "type": "concept",
                "description": "When powerful AI can operate with minimal power draw, traditional energy-based monitoring becomes ineffective.",
                "aliases": [
                    "hidden compute risk",
                    "low-power capability escalation"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1602_04019",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 819,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Design top-answer-preserving questions for on-policy Oracle",
                "type": "intervention",
                "description": "Choose or structure questions so that the identity of the best answer remains the same whether or not it is disclosed.",
                "aliases": [
                    "answer-independent question design",
                    "stable-best-answer prompts"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1711_05541",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 410,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Design AI architecture with modules mirroring mammalian emotional systems",
                "type": "intervention",
                "description": "Create separate architectural components corresponding to SEEKING, CARE, etc., to scaffold aligned motivational drives.",
                "aliases": [
                    "emotional module architecture",
                    "mammalian-module design"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1607_08289",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 9,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "pre-compute translations over ontology distribution",
                "type": "intervention",
                "description": "Given a programmer-supplied distribution over possible ontologies, translate the reference utility to each ontology before the agent is deployed.",
                "aliases": [
                    "batch utility translation",
                    "multi-ontology mapping"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1105_3821",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 537,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Pre-deployment causal abduction\u2013action\u2013prediction evaluation procedure",
                "type": "intervention",
                "description": "Before releasing a model, estimate background variables, intervene on A, and compare prediction distributions to confirm counterfactual invariance.",
                "aliases": [
                    "CF audit via twin network",
                    "counterfactual simulation test"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1703_06856",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 368,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "negative side effects in environment",
                "type": "concept",
                "description": "Changes to non-task parts of the environment that humans prefer to avoid but are not penalised by the reward.",
                "aliases": [
                    "unintended collateral damage",
                    "harmful side effects"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1606_06565",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 35,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "loss of human monitoring capability",
                "type": "concept",
                "description": "Humans can no longer interpret or oversee the system\u2019s behaviour in real time.",
                "aliases": [
                    "oversight breakdown",
                    "auditability loss"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1202_6177",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 604,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Existence of forecaster that converges to incomplete model set in KR distance",
                "type": "concept",
                "description": "Main theorem: for any countable family of incomplete models there exists a forecaster whose forecasts converge to satisfy whichever incomplete model the environment respects.",
                "aliases": [
                    "convergence theorem to set",
                    "dominant forecaster convergence"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1705_04630",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1004,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Regressional Goodhart divergence between metric and goal",
                "type": "concept",
                "description": "Optimising a noisy proxy selects for noise, so at high proxy values the goal systematically lags.",
                "aliases": [
                    "tails come apart",
                    "regressional Goodhart"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 158,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Delay deployment of self-improving AGI until definable desirable goal is found",
                "type": "intervention",
                "description": "Impose governance or policy constraints preventing large-scale deployment of recursively self-improving systems until a satisfactory final goal is rigorously specified.",
                "aliases": [
                    "moratorium on AGI without goal solution",
                    "pause AGI development"
                ],
                "intervention_lifecycle": 6,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1409_0813",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 991,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "agents obtain unrealistic free reward or violate physics",
                "type": "concept",
                "description": "The agent's behaviour relies on exploiting the fault, yielding performance not possible under correct physics.",
                "aliases": [
                    "free energy exploit",
                    "physics-breaking behaviour"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1803_03453",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 740,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Use logical induction to screen code termination before execution",
                "type": "intervention",
                "description": "Before deployment, run an approximate logical inductor to estimate the probability that a candidate program will halt; reject or sandbox programmes with low probability.",
                "aliases": [
                    "LI-based halting filter",
                    "pre-run termination assessor"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1707_08747",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 792,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Pre-deployment evaluation harness with predictor dilemmas",
                "type": "intervention",
                "description": "Before release, subject models to a battery of Newcomb-like, blackmail, and coordination games to verify FDT-like reasoning and resistance to exploitation.",
                "aliases": [
                    "decision theory stress tests",
                    "Newcomb evaluation suite"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1710_05060",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 822,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Reset-policy Q-value low signals irrecoverable state",
                "type": "concept",
                "description": "Observation that the learned Q-values of the reset policy are small in states where successful reset is unlikely.",
                "aliases": [
                    "low Q_reset indicates danger",
                    "recoverability estimate"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1711_06782",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 918,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "offline single-step transition dataset with random actions",
                "type": "concept",
                "description": "A collection of tuples (state, action, next state) gathered by executing uniformly random actions, used to learn the linear safety-signal model without requiring knowledge of a behavior policy.",
                "aliases": [
                    "offline random action data",
                    "policy-agnostic logs"
                ],
                "concept_category": "Data",
                "paper_id": "arxiv__arxiv_org_abs_1801_08757",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 259,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "SortaGrad curriculum ordering minibatches by utterance length in epoch 1",
                "type": "intervention",
                "description": "During the first training epoch feed shorter utterances first, gradually increasing length, then resume random order.",
                "aliases": [
                    "SortaGrad",
                    "length-based curriculum"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1512_02595",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 274,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Include hyperbolic discounting and belief-uncertainty parameters in IRL models used for alignment",
                "type": "intervention",
                "description": "Extend preference-learning architectures to explicitly learn discount-rate k, Na\u00efve/Sophisticated type, and subjective belief distributions alongside utilities.",
                "aliases": [
                    "bias parameters in alignment IRL",
                    "hyperbolic+belief IRL"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1512_05832",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 311,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Coherent distributions uncomputable",
                "type": "concept",
                "description": "Exact coherence over logical sentences requires assigning probability 1 to all theorems, which no computable function can do.",
                "aliases": [
                    "uncomputability of coherence",
                    "coherence uncomputable"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1604_05288",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 316,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "AI over-confidence about unexecuted computations",
                "type": "concept",
                "description": "When an AI system assigns extreme probabilities to outcomes it has not computed, it can make unsafe or erroneous decisions.",
                "aliases": [
                    "overconfidence risk",
                    "unsafe decision risk"
                ],
                "concept_category": "Risk",
                "paper_id": "arxiv__arxiv_org_abs_1604_05288",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 395,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Orthogonality thesis separating intelligence and final goals",
                "type": "concept",
                "description": "The principle that any level of intelligence can be paired with virtually any set of terminal goals.",
                "aliases": [
                    "orthogonality thesis",
                    "independence of intelligence and goals"
                ],
                "concept_category": "Theoretical Framework",
                "paper_id": "arxiv__arxiv_org_abs_1607_08289",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 89,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "understanding of feature invariances & discriminative patterns",
                "type": "concept",
                "description": "Visualisations show that deeper layers capture higher-level, class-specific and transformation-invariant patterns.",
                "aliases": [
                    "feature-level interpretability",
                    "revealed invariances"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1311_2901",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 296,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "parametric bounded L\u00f6b cooperation result",
                "type": "concept",
                "description": "Mathematical theorem ensuring that for sufficiently large k, FairBot_k provably cooperates with itself.",
                "aliases": [
                    "bounded L\u00f6b theorem outcome",
                    "FairBot self-cooperation"
                ],
                "concept_category": "Result",
                "paper_id": "arxiv__arxiv_org_abs_1602_04184",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 1168,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Overfitting to specific opponent",
                "type": "concept",
                "description": "Policies optimise for one partner and perform poorly with others.",
                "aliases": [
                    "partner overfitting",
                    "lack of generalisation"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1804_03980",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 237,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Need to respect logical correlations between algorithm instances",
                "type": "concept",
                "description": "Proper counterfactuals must vary outputs of all copies of the same algorithm in tandem.",
                "aliases": [
                    "logical dependence principle"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1507_01986",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 353,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Final performance metric alone hides sample efficiency differences",
                "type": "concept",
                "description": "Only reporting final reward overlooks how many episodes the agent needed to reach that reward, masking algorithm quality.",
                "aliases": [
                    "sample complexity ignored",
                    "learning curve blindness"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1606_01540",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 561,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "interruption detection signal availability",
                "type": "concept",
                "description": "A binary signal provided to each agent indicating whether any agent was interrupted in the last timestep.",
                "aliases": [
                    "external interruption flag",
                    "interrupt awareness"
                ],
                "concept_category": "Assumption",
                "paper_id": "arxiv__arxiv_org_abs_1704_02882",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 690,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "multiple human oversight phases update blocker under distribution shift",
                "type": "concept",
                "description": "Reintroducing human oversight whenever the state distribution changes allows retraining the Blocker to maintain reliability.",
                "aliases": [
                    "periodic oversight",
                    "blocker retraining cycles"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 427,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Apply length-normalisation exponent \u03b1\u22480.2 during beam search",
                "type": "intervention",
                "description": "Divide log-probability of each hypothesis by (5+|Y|)^\u03b1 with \u03b1\u22480.2 to reduce the bias for shorter outputs.",
                "aliases": [
                    "length norm 0.2",
                    "beam score length scaling"
                ],
                "intervention_lifecycle": 5,
                "intervention_maturity": 4,
                "paper_id": "arxiv__arxiv_org_abs_1609_08144",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 779,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Maintain feedback replay buffer for minibatch SGD every 10 time steps",
                "type": "intervention",
                "description": "Store all (experience, feedback) pairs and resample them so the reward model is updated ten times as often as new feedback arrives.",
                "aliases": [
                    "feedback buffer updates",
                    "replay human critiques"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 493,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "priority weights shift proportional to predictive accuracy",
                "type": "concept",
                "description": "After each observation, the weight on a principal\u2019s utility is multiplied by how likely that observation was under the principal\u2019s model.",
                "aliases": [
                    "likelihood-based priority shifting",
                    "bet settling mechanism"
                ],
                "concept_category": "Principle",
                "paper_id": "arxiv__arxiv_org_abs_1701_01302",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 779,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Maintain feedback replay buffer for minibatch SGD every 10 time steps",
                "type": "intervention",
                "description": "Store all (experience, feedback) pairs and resample them so the reward model is updated ten times as often as new feedback arrives.",
                "aliases": [
                    "feedback buffer updates",
                    "replay human critiques"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1709_10163",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 1012,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Causal-model-based policy optimisation restricting interventions to causal parents of goal",
                "type": "intervention",
                "description": "Construct explicit causal models, perform counterfactual reasoning, and limit allowable actions to variables that causally influence the true goal, not merely the proxy.",
                "aliases": [
                    "do-calculus-guided control",
                    "causal-aware optimisation"
                ],
                "intervention_lifecycle": 4,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1803_04585",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 650,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "pairwise comparison of short trajectory clips is quick for humans",
                "type": "concept",
                "description": "Non-experts can reliably choose between two 1-2 s clips in 3\u20135 s, making preference collection economical.",
                "aliases": [
                    "clip comparisons efficient",
                    "2-second video preference ease"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 747,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "apply probabilistic decision rule: execute novice action only if \u2265 p mass lies within \u03c4 of expert action",
                "type": "intervention",
                "description": "At each time-step sample N actions from the Bayesian novice; if the proportion inside an \u21132 ball of radius \u03c4 around the expert\u2019s action exceeds threshold p, act using the mean novice action, else defer to expert.",
                "aliases": [
                    "\u03c4-p gating rule",
                    "uncertainty-aware action gating"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 2,
                "paper_id": "arxiv__arxiv_org_abs_1709_06166",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 364,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Nuclear-norm \u03b2-quantile recovery algorithm",
                "type": "concept",
                "description": "An optimisation that maximises alignment with observed ratings while constraining entries, row sums and nuclear norm to extract a \u03b2-quantile matrix.",
                "aliases": [
                    "low-rank quantile estimation",
                    "\u03b2-quantile matrix solver"
                ],
                "concept_category": "Method",
                "paper_id": "arxiv__arxiv_org_abs_1606_05374",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 225,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "infinite-horizon planning without discounting",
                "type": "concept",
                "description": "Agents that plan over an infinite future without discounting accumulate infinite series and may revise earlier plans.",
                "aliases": [
                    "unbounded horizon",
                    "no discount sequential planning"
                ],
                "concept_category": "Problem",
                "paper_id": "arxiv__arxiv_org_abs_1506_07359",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 650,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "pairwise comparison of short trajectory clips is quick for humans",
                "type": "concept",
                "description": "Non-experts can reliably choose between two 1-2 s clips in 3\u20135 s, making preference collection economical.",
                "aliases": [
                    "clip comparisons efficient",
                    "2-second video preference ease"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 689,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "model-based RL can infer catastrophic outcomes without execution",
                "type": "concept",
                "description": "Agents that learn an internal world model can predict and avoid catastrophic consequences without directly performing the dangerous action.",
                "aliases": [
                    "model-based safety",
                    "simulation of consequences"
                ],
                "concept_category": "Opportunity",
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 650,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "pairwise comparison of short trajectory clips is quick for humans",
                "type": "concept",
                "description": "Non-experts can reliably choose between two 1-2 s clips in 3\u20135 s, making preference collection economical.",
                "aliases": [
                    "clip comparisons efficient",
                    "2-second video preference ease"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        }
    ],
    [
        {
            "id": 701,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "model-based RL planning to avoid catastrophes",
                "type": "intervention",
                "description": "Leverage learned environment models to predict catastrophic consequences and avoid dangerous actions without directly executing them.",
                "aliases": [
                    "model-based safety",
                    "simulation of consequences"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "paper_id": "arxiv__arxiv_org_abs_1707_05173",
                "method": "model",
                "is_tombstone": false
            }
        },
        {
            "id": 650,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "pairwise comparison of short trajectory clips is quick for humans",
                "type": "concept",
                "description": "Non-experts can reliably choose between two 1-2 s clips in 3\u20135 s, making preference collection economical.",
                "aliases": [
                    "clip comparisons efficient",
                    "2-second video preference ease"
                ],
                "concept_category": "Finding",
                "paper_id": "arxiv__arxiv_org_abs_1706_03741",
                "method": "model",
                "is_tombstone": false
            }
        }
    ]
]