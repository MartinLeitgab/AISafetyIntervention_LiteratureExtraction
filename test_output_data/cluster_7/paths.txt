[
  {
    "nodes": [
      {
        "id": 104,
        "alias": "",
        "labels": [
          "Concept",
          "NODE"
        ],
        "properties": {
          "name": "nonstationary_behavior_distribution",
          "type": "concept",
          "description": "Because the agent’s policy changes during learning, the distribution over visited states and actions drifts over time.",
          "aliases": [
            "changing data distribution",
            "policy-induced non-stationarity"
          ],
          "concept_category": "Observation",
          "paper_id": "arxiv__arxiv_org_abs_1312_5602",
          "method": "model",
          "is_tombstone": false
        }
      }
    ],
    "edges": [],
    "length": 0
  },
  {
    "nodes": [
      {
        "id": 200,
        "alias": "",
        "labels": [
          "NODE",
          "Intervention"
        ],
        "properties": {
          "name": "staged AI architecture",
          "type": "intervention",
          "description": "Phase 1 passive model learner; Phase 2 boxed self-model learner with forced exploration; Phase 3 active deployment agent",
          "aliases": [
            "two-stage then three-stage design"
          ],
          "intervention_lifecycle": 1,
          "intervention_maturity": 2,
          "paper_id": "arxiv__arxiv_org_abs_1411_1373",
          "method": "model",
          "is_tombstone": false
        }
      }
    ],
    "edges": [],
    "length": 0
  },
  {
    "nodes": [
      {
        "id": 279,
        "alias": "",
        "labels": [
          "Concept",
          "NODE"
        ],
        "properties": {
          "name": "reliance on brain-energy constraint to dismiss AI feasibility",
          "type": "concept",
          "description": "The assumption that human-brain energy requirements imply prohibitive power needs for artificial intelligence.",
          "aliases": [
            "energy-bound complacency",
            "brain power fallacy"
          ],
          "concept_category": "Problem",
          "paper_id": "arxiv__arxiv_org_abs_1602_04019",
          "method": "model",
          "is_tombstone": false
        }
      }
    ],
    "edges": [],
    "length": 0
  },
  {
    "nodes": [
      {
        "id": 280,
        "alias": "",
        "labels": [
          "Concept",
          "NODE"
        ],
        "properties": {
          "name": "AI architectures can differ from brain and use higher-level representations",
          "type": "concept",
          "description": "Artificial systems are not constrained to neuron-level simulation and can operate on compressed higher-level data structures.",
          "aliases": [
            "non-biological cognitive pathways",
            "abstracted AI computation"
          ],
          "concept_category": "Principle",
          "paper_id": "arxiv__arxiv_org_abs_1602_04019",
          "method": "model",
          "is_tombstone": false
        }
      }
    ],
    "edges": [],
    "length": 0
  },
  {
    "nodes": [
      {
        "id": 281,
        "alias": "",
        "labels": [
          "Concept",
          "NODE"
        ],
        "properties": {
          "name": "specialised hardware and abstraction reduce energy per operation by orders of magnitude",
          "type": "concept",
          "description": "Empirical evidence shows dedicated accelerators and model simplifications yield ≥100× energy savings compared to naïve implementations.",
          "aliases": [
            "GPU/ASIC energy efficiency",
            "algorithmic compression benefit"
          ],
          "concept_category": "Finding",
          "paper_id": "arxiv__arxiv_org_abs_1602_04019",
          "method": "model",
          "is_tombstone": false
        }
      }
    ],
    "edges": [],
    "length": 0
  },
  {
    "nodes": [
      {
        "id": 282,
        "alias": "",
        "labels": [
          "Concept",
          "NODE"
        ],
        "properties": {
          "name": "energy requirements of AI not tightly correlated with brain energy",
          "type": "concept",
          "description": "Because of architectural and hardware differences, AI could match or surpass human cognition at far lower or higher power levels.",
          "aliases": [
            "decoupled AI energy use",
            "weak brain-AI power link"
          ],
          "concept_category": "Claim",
          "paper_id": "arxiv__arxiv_org_abs_1602_04019",
          "method": "model",
          "is_tombstone": false
        }
      }
    ],
    "edges": [],
    "length": 0
  },
  {
    "nodes": [
      {
        "id": 361,
        "alias": "",
        "labels": [
          "Concept",
          "NODE"
        ],
        "properties": {
          "name": "Unreliable or malicious human ratings in preference datasets",
          "type": "concept",
          "description": "Human-supplied ratings used for training or evaluating models may include errors, spam or adversarial manipulation.",
          "aliases": [
            "noisy feedback",
            "corrupted crowd ratings"
          ],
          "concept_category": "Problem",
          "paper_id": "arxiv__arxiv_org_abs_1606_05374",
          "method": "model",
          "is_tombstone": false
        }
      }
    ],
    "edges": [],
    "length": 0
  },
  {
    "nodes": [
      {
        "id": 1034,
        "alias": "",
        "labels": [
          "Concept",
          "NODE"
        ],
        "properties": {
          "name": "standard RL ignores reward observation cost",
          "type": "concept",
          "description": "In conventional reinforcement learning the agent observes rewards for free, so the optimisation objective contains no term for the cost of acquiring those rewards.",
          "aliases": [
            "traditional RL no query price",
            "missing query cost in RL"
          ],
          "concept_category": "Problem",
          "paper_id": "arxiv__arxiv_org_abs_1803_04926",
          "method": "model",
          "is_tombstone": false
        }
      }
    ],
    "edges": [],
    "length": 0
  },
  {
    "nodes": [
      {
        "id": 1035,
        "alias": "",
        "labels": [
          "Concept",
          "NODE"
        ],
        "properties": {
          "name": "human feedback labelling cost is significant in reward learning",
          "type": "concept",
          "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
          "aliases": [
            "expensive human labels",
            "costly human evaluations"
          ],
          "concept_category": "Observation",
          "paper_id": "arxiv__arxiv_org_abs_1803_04926",
          "method": "model",
          "is_tombstone": false
        }
      }
    ],
    "edges": [],
    "length": 0
  }
]