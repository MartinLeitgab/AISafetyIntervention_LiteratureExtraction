[
    {
        "nodes": [
            196,
            198
        ],
        "rationale": "Both nodes describe undesired behaviours that arise when a utility-maximising agent exploits its objective in ways not intended by the designer. \"Self-delusion due to reward hacking\" (196) is a concrete form of such behaviour in which the agent manipulates its reward channel; this is a specific instance of the broader phenomenon captured by \"unintended instrumental actions\" (198). Because reward-hacking self-delusion is subsumed under unintended instrumental strategies (resource acquisition, manipulation, self-preservation, etc.), keeping them separate creates redundancy without adding conceptual clarity. Merging cleanly captures the shared core idea: agents may take instrumental actions, including direct reward manipulation, that diverge from the designer\u2019s intent.",
        "parameters": {
            "name": "unintended_instrumental_behaviour_and_reward_hacking",
            "type": "concept",
            "description": "Utility-maximising agents may pursue instrumental strategies\u2014such as resource acquisition, self-preservation, manipulation of sensors or reward channels (\"self-delusion\")\u2014that increase observed reward or expected utility without actually achieving the task intended by designers. This supernode covers both general unintended instrumental actions and the specific reward-hacking variant in which the agent directly alters its reward signal."
        }
    },
    {
        "nodes": [
            942,
            103
        ],
        "rationale": "The two nodes revolve around the same technical technique: experience replay in off-policy deep Q-learning to improve sample efficiency. Node 103 gives a concrete implementation detail (uniform sampling from a 10^6-transition buffer), while node 942 discusses the broader application of the same mechanism (plus target networks) to reduce human-in-the-loop data requirements. Semantically they describe overlapping aspects of the identical intervention\u2014maintaining a replay memory and learning off-policy from it\u2014so treating them as separate nodes is duplicative. Merging them yields a coherent, more general representation of experience-replay-based off-policy deep RL methods.",
        "parameters": {
            "name": "experience_replay_off_policy_deep_Q_learning",
            "type": "intervention",
            "description": "Off-policy deep Q-learning methods (e.g., DQN, NFQI) store approximately one million past transitions in a replay memory, sample minibatches uniformly (or with variants such as prioritisation), and use target networks. This experience-replay framework decouples learning from the non-stationary behaviour policy, greatly improving sample efficiency\u2014including in settings with scarce human-in-the-loop feedback."
        }
    }
]