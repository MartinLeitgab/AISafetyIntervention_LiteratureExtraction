[
  {
    "nodes": [
      {
        "id": 103,
        "alias": "",
        "labels": [
          "NODE",
          "Intervention"
        ],
        "properties": {
          "name": "experience_replay_uniform_1M_buffer",
          "type": "intervention",
          "description": "Maintain a replay memory of the latest 10^6 transitions and train with minibatches uniformly sampled from it.",
          "aliases": [
            "experience replay buffer",
            "uniform replay memory"
          ],
          "intervention_lifecycle": 1,
          "intervention_maturity": 2,
          "paper_id": "arxiv__arxiv_org_abs_1312_5602",
          "method": "model",
          "is_tombstone": false
        }
      }
    ],
    "edges": [],
    "length": 0
  },
  {
    "nodes": [
      {
        "id": 104,
        "alias": "",
        "labels": [
          "Concept",
          "NODE"
        ],
        "properties": {
          "name": "nonstationary_behavior_distribution",
          "type": "concept",
          "description": "Because the agentâ€™s policy changes during learning, the distribution over visited states and actions drifts over time.",
          "aliases": [
            "changing data distribution",
            "policy-induced non-stationarity"
          ],
          "concept_category": "Observation",
          "paper_id": "arxiv__arxiv_org_abs_1312_5602",
          "method": "model",
          "is_tombstone": false
        }
      }
    ],
    "edges": [],
    "length": 0
  },
  {
    "nodes": [
      {
        "id": 196,
        "alias": "",
        "labels": [
          "Concept",
          "NODE"
        ],
        "properties": {
          "name": "self-delusion due to reward hacking",
          "type": "concept",
          "description": "Agents directly modify their sensory channel or reward source to falsely maximise observed reward",
          "aliases": [
            "wireheading",
            "observation manipulation"
          ],
          "concept_category": "Problem",
          "paper_id": "arxiv__arxiv_org_abs_1411_1373",
          "method": "model",
          "is_tombstone": false
        }
      }
    ],
    "edges": [],
    "length": 0
  },
  {
    "nodes": [
      {
        "id": 197,
        "alias": "",
        "labels": [
          "NODE",
          "Intervention"
        ],
        "properties": {
          "name": "model-based utility over inferred state",
          "type": "intervention",
          "description": "Compute utility as a function of latent variables inside the learned environment model, not raw observations",
          "aliases": [
            "latent-state utility",
            "environment-model utility"
          ],
          "intervention_lifecycle": 2,
          "intervention_maturity": 2,
          "paper_id": "arxiv__arxiv_org_abs_1411_1373",
          "method": "model",
          "is_tombstone": false
        }
      }
    ],
    "edges": [],
    "length": 0
  },
  {
    "nodes": [
      {
        "id": 198,
        "alias": "",
        "labels": [
          "Concept",
          "NODE"
        ],
        "properties": {
          "name": "unintended instrumental actions",
          "type": "concept",
          "description": "Utility-maximising agents pursue resource acquisition, self-preservation, manipulation, etc.",
          "aliases": [
            "power-seeking side-effects"
          ],
          "concept_category": "Problem",
          "paper_id": "arxiv__arxiv_org_abs_1411_1373",
          "method": "model",
          "is_tombstone": false
        }
      }
    ],
    "edges": [],
    "length": 0
  },
  {
    "nodes": [
      {
        "id": 647,
        "alias": "",
        "labels": [
          "Concept",
          "NODE"
        ],
        "properties": {
          "name": "complex tasks lacking well-specified reward functions",
          "type": "concept",
          "description": "Tasks where designing an explicit numerical reward mapping from observations or actions to scalar value is infeasible or error-prone.",
          "aliases": [
            "unspecified reward tasks",
            "hard-to-specify goals"
          ],
          "concept_category": "Problem",
          "paper_id": "arxiv__arxiv_org_abs_1706_03741",
          "method": "model",
          "is_tombstone": false
        }
      }
    ],
    "edges": [],
    "length": 0
  },
  {
    "nodes": [
      {
        "id": 942,
        "alias": "",
        "labels": [
          "Concept",
          "NODE"
        ],
        "properties": {
          "name": "off-policy deep Q-learning sample efficiency for human-in-loop",
          "type": "concept",
          "description": "Using NFQI with experience replay and target networks allows learning from fewer human-in-the-loop interactions.",
          "aliases": [
            "NFQI sample efficiency claim",
            "off-policy RL benefit"
          ],
          "concept_category": "Claim",
          "paper_id": "arxiv__arxiv_org_abs_1802_01744",
          "method": "model",
          "is_tombstone": false
        }
      }
    ],
    "edges": [],
    "length": 0
  },
  {
    "nodes": [
      {
        "id": 1135,
        "alias": "",
        "labels": [
          "Concept",
          "NODE"
        ],
        "properties": {
          "name": "off-manifold directions easier to identify in deeper hidden representations",
          "type": "concept",
          "description": "Because representation manifolds become flatter with depth, deviations from the manifold are more separable in hidden layers.",
          "aliases": [
            "flatter manifolds in hidden space",
            "easier detection in latent layers"
          ],
          "concept_category": "Claim",
          "paper_id": "arxiv__arxiv_org_abs_1804_02485",
          "method": "model",
          "is_tombstone": false
        }
      }
    ],
    "edges": [],
    "length": 0
  }
]