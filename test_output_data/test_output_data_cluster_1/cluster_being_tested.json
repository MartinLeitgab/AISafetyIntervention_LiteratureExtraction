[
    {
        "id": 101,
        "alias": "",
        "labels": [
            "Concept",
            "NODE"
        ],
        "properties": {
            "name": "correlated_onpolicy_transition_sequences",
            "type": "concept",
            "description": "Sequences of observations collected sequentially under the current policy are strongly correlated in time.",
            "aliases": [
                "correlated data sequences",
                "temporally correlated samples"
            ],
            "concept_category": "Problem",
            "paper_id": "arxiv__arxiv_org_abs_1312_5602",
            "method": "model",
            "is_tombstone": false
        }
    },
    {
        "id": 863,
        "alias": "",
        "labels": [
            "Concept",
            "NODE"
        ],
        "properties": {
            "name": "General human action datasets are easier to collect than goal-aligned datasets",
            "type": "concept",
            "description": "Collecting large unlabeled logs of ordinary human actions is less expensive than collecting demonstrations aligned with the agent\u2019s specific objective function.",
            "aliases": [
                "general human data accessible",
                "low-cost human behavioural data"
            ],
            "concept_category": "Observation",
            "paper_id": "arxiv__arxiv_org_abs_1712_04172",
            "method": "model",
            "is_tombstone": false
        }
    },
    {
        "id": 280,
        "alias": "",
        "labels": [
            "Concept",
            "NODE"
        ],
        "properties": {
            "name": "AI architectures can differ from brain and use higher-level representations",
            "type": "concept",
            "description": "Artificial systems are not constrained to neuron-level simulation and can operate on compressed higher-level data structures.",
            "aliases": [
                "non-biological cognitive pathways",
                "abstracted AI computation"
            ],
            "concept_category": "Principle",
            "paper_id": "arxiv__arxiv_org_abs_1602_04019",
            "method": "model",
            "is_tombstone": false
        }
    },
    {
        "id": 281,
        "alias": "",
        "labels": [
            "Concept",
            "NODE"
        ],
        "properties": {
            "name": "specialised hardware and abstraction reduce energy per operation by orders of magnitude",
            "type": "concept",
            "description": "Empirical evidence shows dedicated accelerators and model simplifications yield \u2265100\u00d7 energy savings compared to na\u00efve implementations.",
            "aliases": [
                "GPU/ASIC energy efficiency",
                "algorithmic compression benefit"
            ],
            "concept_category": "Finding",
            "paper_id": "arxiv__arxiv_org_abs_1602_04019",
            "method": "model",
            "is_tombstone": false
        }
    },
    {
        "id": 282,
        "alias": "",
        "labels": [
            "Concept",
            "NODE"
        ],
        "properties": {
            "name": "energy requirements of AI not tightly correlated with brain energy",
            "type": "concept",
            "description": "Because of architectural and hardware differences, AI could match or surpass human cognition at far lower or higher power levels.",
            "aliases": [
                "decoupled AI energy use",
                "weak brain-AI power link"
            ],
            "concept_category": "Claim",
            "paper_id": "arxiv__arxiv_org_abs_1602_04019",
            "method": "model",
            "is_tombstone": false
        }
    },
    {
        "id": 1034,
        "alias": "",
        "labels": [
            "Concept",
            "NODE"
        ],
        "properties": {
            "name": "standard RL ignores reward observation cost",
            "type": "concept",
            "description": "In conventional reinforcement learning the agent observes rewards for free, so the optimisation objective contains no term for the cost of acquiring those rewards.",
            "aliases": [
                "traditional RL no query price",
                "missing query cost in RL"
            ],
            "concept_category": "Problem",
            "paper_id": "arxiv__arxiv_org_abs_1803_04926",
            "method": "model",
            "is_tombstone": false
        }
    },
    {
        "id": 1035,
        "alias": "",
        "labels": [
            "Concept",
            "NODE"
        ],
        "properties": {
            "name": "human feedback labelling cost is significant in reward learning",
            "type": "concept",
            "description": "Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.",
            "aliases": [
                "expensive human labels",
                "costly human evaluations"
            ],
            "concept_category": "Observation",
            "paper_id": "arxiv__arxiv_org_abs_1803_04926",
            "method": "model",
            "is_tombstone": false
        }
    },
    {
        "id": 198,
        "alias": "",
        "labels": [
            "Concept",
            "NODE"
        ],
        "properties": {
            "name": "unintended instrumental actions",
            "type": "concept",
            "description": "Utility-maximising agents pursue resource acquisition, self-preservation, manipulation, etc.",
            "aliases": [
                "power-seeking side-effects"
            ],
            "concept_category": "Problem",
            "paper_id": "arxiv__arxiv_org_abs_1411_1373",
            "method": "model",
            "is_tombstone": false
        }
    }
]