# AI Safety Knowledge Graph Semantic Compression
You are an expert in AI safety knowledge graph compression. Given the following nodes and their relationships, your task is to:
1. Only consider merging the primary nodes listed below. Do NOT merge or suggest merging any neighbor nodes.
2. Decide which primary nodes should be merged into a single supernode (merged concept).
3. Provide a clear rationale for each merge decision. Reason step by step
4. For each merge set, generate merged parameters for the supernode: name, description, type, and any other relevant attributes.

5. Reason step by step for each merge decision to ensure the highest quality rationale.

Consider only these nodes for merging: [101, 863, 280, 281, 282, 1034, 1035, 198]

Nodes:
Node ID: 101
Name: correlated_onpolicy_transition_sequences
Type: concept
Description: Sequences of observations collected sequentially under the current policy are strongly correlated in time.

Node ID: 198
Name: unintended instrumental actions
Type: concept
Description: Utility-maximising agents pursue resource acquisition, self-preservation, manipulation, etc.

Node ID: 280
Name: AI architectures can differ from brain and use higher-level representations
Type: concept
Description: Artificial systems are not constrained to neuron-level simulation and can operate on compressed higher-level data structures.

Node ID: 281
Name: specialised hardware and abstraction reduce energy per operation by orders of magnitude
Type: concept
Description: Empirical evidence shows dedicated accelerators and model simplifications yield ≥100× energy savings compared to naïve implementations.

Node ID: 282
Name: energy requirements of AI not tightly correlated with brain energy
Type: concept
Description: Because of architectural and hardware differences, AI could match or surpass human cognition at far lower or higher power levels.

Node ID: 863
Name: General human action datasets are easier to collect than goal-aligned datasets
Type: concept
Description: Collecting large unlabeled logs of ordinary human actions is less expensive than collecting demonstrations aligned with the agent’s specific objective function.

Node ID: 1034
Name: standard RL ignores reward observation cost
Type: concept
Description: In conventional reinforcement learning the agent observes rewards for free, so the optimisation objective contains no term for the cost of acquiring those rewards.

Node ID: 1035
Name: human feedback labelling cost is significant in reward learning
Type: concept
Description: Obtaining scalar rewards or preference labels from humans during training requires time and money, often dominating the training budget in RLHF settings.


Edges:


Output Instructions:
Return ONLY a valid JSON object with the following format:
{
  "merge_sets": [
    {
      "node_ids": [list of node IDs to merge],
      "rationale": "reason for merging",
      "parameters": {
        "name": "string - concise name for the supernode",
        "type": "string - node type",
        "description": "string - comprehensive description",
      }
    }
  ]
}
