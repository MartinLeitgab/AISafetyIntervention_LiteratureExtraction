[
    {
        "nodes": [
            104,
            101
        ],
        "rationale": "Nodes 101 and 104 both describe the same underlying data-distribution problem in reinforcement learning: the agent\u2019s experience is not i.i.d. Because the policy generates its own data, consecutive samples are highly correlated (Node 101) and the state\u2013action distribution drifts as the policy updates (Node 104). These phenomena stem from the same root cause (policy-dependent data) and are almost always discussed together in literature as \u2018non-i.i.d./nonstationary data\u2019 issues. Keeping them separate yields little explanatory benefit and creates duplication, so merging improves graph compactness without loss of meaning.",
        "parameters": {
            "name": "non_iid_and_nonstationary_rl_data",
            "type": "concept",
            "description": "In reinforcement learning, the agent\u2019s policy generates its own training data, leading to (1) strong temporal correlations between successive observations and (2) a drifting state\u2013action distribution as the policy changes. This violation of the i.i.d. assumption undermines standard statistical guarantees, complicates off-policy evaluation, and can cause instability or inefficiency in learning algorithms."
        }
    }
]