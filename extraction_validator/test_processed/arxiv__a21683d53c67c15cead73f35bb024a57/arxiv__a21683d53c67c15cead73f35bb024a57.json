{
  "nodes": [
    {
      "name": "Ethical decision failure in autonomous AI systems",
      "aliases": [
        "AI moral error in real-world decisions",
        "Autonomous system ethical failure"
      ],
      "type": "concept",
      "description": "AI agents such as autonomous vehicles or weapons may select actions with moral weight that harm humans because their decision-making is not reliably ethical.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3, paragraph 1 describes how autonomous systems' choices can cause real-world harm."
    },
    {
      "name": "Reinforcement of societal biases through benchmark-driven AI development",
      "aliases": [
        "Benchmark-induced bias amplification",
        "Bias propagation via AI benchmarks"
      ],
      "type": "concept",
      "description": "When research progress is measured mainly by benchmark scores, hidden dataset biases and stereotypes are baked into deployed systems, entrenching social inequalities.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 2.1 documents how benchmark datasets like ImageNet perpetuate harmful stereotypes and errors."
    },
    {
      "name": "Absence of objective moral ground truth for ethics benchmarking",
      "aliases": [
        "No ethical ground truth",
        "Ground-truth gap in moral evaluation"
      ],
      "type": "concept",
      "description": "Benchmarking requires labelled ‘correct’ outputs, but there is no universally agreed set of morally correct answers for AI behaviour.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.3 explains that without moral realism a benchmark cannot have ground truth labels."
    },
    {
      "name": "Dataset bias and labeling errors in existing benchmarks",
      "aliases": [
        "Benchmark dataset noise",
        "Label error bias"
      ],
      "type": "concept",
      "description": "Common datasets (e.g., ImageNet) contain high rates of mis-labelling and under-representation, introducing systematic bias into model training and evaluation.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 2.1 cites ImageNet error rates and demographic skews leading to biased outcomes."
    },
    {
      "name": "Corporate profit incentives prioritize performance over societal values",
      "aliases": [
        "Industry incentive misalignment",
        "Profit-driven research priorities"
      ],
      "type": "concept",
      "description": "Large technology companies dominate AI research and tend to value benchmark performance and market appeal above broad societal well-being.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4.2 details how 79% of highly-cited papers originate from corporate authors and outlines misaligned incentives."
    },
    {
      "name": "Ethics is an essentially contested concept implying values are relative",
      "aliases": [
        "Contested nature of ethics",
        "Relativity of moral concepts"
      ],
      "type": "concept",
      "description": "Meta-ethical debate shows no objective definition of ‘ethical’, whereas ‘values’ are explicitly relative to stakeholders.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.3 discusses moral realism controversies; Section 4 introduces value relativity."
    },
    {
      "name": "Benchmarking embeds unexamined metaethical realism assumptions",
      "aliases": [
        "Implicit moral realism in benchmarks",
        "Metaethical assumption of objectivity"
      ],
      "type": "concept",
      "description": "Using benchmarks for ethics presupposes that moral facts exist and can be labelled, an assumption rarely acknowledged by AI researchers.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.3 argues that benchmarking ethics tacitly assumes moral realism."
    },
    {
      "name": "Transparency of embedded values enables accountability",
      "aliases": [
        "Value transparency for accountability",
        "Explicit values for oversight"
      ],
      "type": "concept",
      "description": "Openly declaring the values guiding AI research allows external critique and alignment with societal interests.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4.1 argues that making values explicit improves clarity and transparency."
    },
    {
      "name": "Shift evaluation from ethics benchmarking to explicit value alignment",
      "aliases": [
        "Move from ethics to values",
        "Value-centred evaluation paradigm"
      ],
      "type": "concept",
      "description": "Because ethics benchmarking is incoherent, researchers should evaluate AI systems by how well they align with articulated stakeholder values.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 5 Proposal 1 recommends reframing discourse from ethics to values."
    },
    {
      "name": "Promote transparency mechanisms in research dissemination",
      "aliases": [
        "Transparency-oriented dissemination",
        "Open disclosure practices"
      ],
      "type": "concept",
      "description": "Develop publication and review processes that surface data, code, compute usage and value assumptions so that work can be scrutinised.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 5 Proposals 2–4 outline mechanisms such as checklists and funding disclosures."
    },
    {
      "name": "Implement multidisciplinary oversight via IRBs",
      "aliases": [
        "Ethics IRB for AI",
        "Multidisciplinary AI review"
      ],
      "type": "concept",
      "description": "Introducing review boards modelled on human-subjects IRBs can evaluate AI projects for human rights and societal impact before publication or deployment.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 5 Proposal 7 advocates IRB-like structures for AI research."
    },
    {
      "name": "Mandatory publication checklists disclosing values, data, compute",
      "aliases": [
        "Values disclosure checklist",
        "Publication transparency checklist"
      ],
      "type": "concept",
      "description": "Checklists appended to papers that require authors to state value commitments, dataset details, training compute and environmental impacts.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 5 Proposal 2 describes using paper checklists to surface implicit values."
    },
    {
      "name": "Public disclosure of funding sources and conflicts in AI papers",
      "aliases": [
        "Funding transparency requirement",
        "Conflict-of-interest disclosure"
      ],
      "type": "concept",
      "description": "Authors must publicly list all sources of corporate or government funding and any relevant conflicts to reveal incentive structures.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 5 Proposal 4 calls for mandatory funding and conflict disclosures."
    },
    {
      "name": "Institutional Review Boards evaluating AI research impacts",
      "aliases": [
        "AI impact review boards",
        "AI ethics IRB mechanism"
      ],
      "type": "concept",
      "description": "Bodies comprised of multidisciplinary experts assess AI research proposals for human rights, consent, and societal impact prior to approval.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 5 Proposal 7 specifies IRB-style evaluation structures."
    },
    {
      "name": "Contextual benchmark reporting with task-specific metrics",
      "aliases": [
        "Mindful benchmarking reports",
        "Context-aware benchmark metrics"
      ],
      "type": "concept",
      "description": "Researchers report performance only within clearly stated task contexts, include additional metrics (e.g., F1, error analysis) and avoid sweeping ‘human-level’ claims.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 5 Proposal 6 urges contextual and nuanced benchmark reporting."
    },
    {
      "name": "Empirical survey showing 87% ML papers prioritise performance and omit value statements",
      "aliases": [
        "Birhane et al. 2021 survey evidence",
        "Performance-centric paper analysis"
      ],
      "type": "concept",
      "description": "Analysis of 100 highly-cited ML papers revealed hidden communal values of performance and novelty with little explicit discussion of values.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4.1 cites Birhane et al. study to evidence hidden values."
    },
    {
      "name": "Studies revealing average 3.3% labeling errors and biases in ImageNet and other datasets",
      "aliases": [
        "Northcutt et al. dataset error study",
        "Benchmark label error evidence"
      ],
      "type": "concept",
      "description": "Empirical work documents significant mis-labelling and demographic skew in widely used benchmarks, undermining their validity.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 2.1 references Northcutt et al. and others showing dataset flaws."
    },
    {
      "name": "Analysis of Moral Machine Experiment indicating mismatch between descriptive data and normative correctness",
      "aliases": [
        "MME normative gap evidence",
        "Moral Machine fallacy analysis"
      ],
      "type": "concept",
      "description": "The Moral Machine dataset reflects what survey participants would do, not what ought to be done, demonstrating the proxy problem in ethical benchmarks.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.2 critiques the Moral Machine as descriptive, not normative."
    },
    {
      "name": "Enforce mandatory value disclosure checklists in AI publication process",
      "aliases": [
        "Mandate values checklist",
        "Compulsory value statement policy"
      ],
      "type": "intervention",
      "description": "Conference and journal policies compel authors to complete detailed checklists disclosing stakeholder values, datasets and compute used.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "Applies during publication and dissemination phase – Section 5 Proposal 2.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Voluntary checklists exist at NeurIPS/ICML but are not yet mandatory – Section 5.",
      "node_rationale": "Transforms design rationale into concrete policy, enabling external scrutiny."
    },
    {
      "name": "Require public funding and conflict of interest disclosures for AI research",
      "aliases": [
        "Mandate funding transparency",
        "COI disclosure requirement"
      ],
      "type": "intervention",
      "description": "Submission guidelines obligate authors to list all monetary and in-kind support and any relationships that could bias results.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "Implemented at the publication governance stage – Section 5 Proposal 4.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Some venues request voluntary disclosure; compulsory policies remain limited.",
      "node_rationale": "Targets incentive misalignment by exposing corporate influence."
    },
    {
      "name": "Establish multidisciplinary IRBs to approve AI research projects",
      "aliases": [
        "Create AI ethics IRBs",
        "AI research oversight boards"
      ],
      "type": "intervention",
      "description": "Institutions form review boards including social scientists, ethicists and technologists who must sign-off on AI studies before data collection or release.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "Governs research approval prior to model development – Section 5 Proposal 7.",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Primarily theoretical; few instances exist outside pilot programmes.",
      "node_rationale": "Addresses corporate incentive misalignment and long-tail harm by adding oversight."
    },
    {
      "name": "Adopt mindful contextual benchmarking practices including detailed error analysis",
      "aliases": [
        "Contextualised benchmarking policy",
        "Nuanced benchmark reporting"
      ],
      "type": "intervention",
      "description": "Researchers must frame benchmark results within the precise task, report multiple metrics and provide failure case analysis to avoid overclaiming capability.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Applies during pre-deployment evaluation of model performance – Section 5 Proposal 6.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Emerging best practice; adopted in some robustness challenge papers.",
      "node_rationale": "Reduces bias reinforcement by clarifying what a benchmark actually measures."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Ethical decision failure in autonomous AI systems",
      "target_node": "Absence of objective moral ground truth for ethics benchmarking",
      "description": "Without an objective ground truth, systems lack reliable criteria for moral choices, leading to failure.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical argument in Section 3.3; medium empirical backing.",
      "edge_rationale": "Paper links moral failures to impossibility of benchmarking ethics."
    },
    {
      "type": "caused_by",
      "source_node": "Absence of objective moral ground truth for ethics benchmarking",
      "target_node": "Ethics is an essentially contested concept implying values are relative",
      "description": "Ground truth is absent because ethics itself is contested and lacks objectivity.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Based on philosophical analysis in Section 3.3.",
      "edge_rationale": "The contested nature of ethics explains missing ground truth."
    },
    {
      "type": "mitigated_by",
      "source_node": "Ethics is an essentially contested concept implying values are relative",
      "target_node": "Shift evaluation from ethics benchmarking to explicit value alignment",
      "description": "Recognising relativity motivates switching focus from ethics benchmarks to value alignment.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Normative proposal in Section 5; limited empirical data.",
      "edge_rationale": "Design rationale directly addresses theoretical issue."
    },
    {
      "type": "implemented_by",
      "source_node": "Shift evaluation from ethics benchmarking to explicit value alignment",
      "target_node": "Mandatory publication checklists disclosing values, data, compute",
      "description": "Checklists operationalise the shift by forcing authors to expose their value commitments.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Proposal stage only; some initial adoption at conferences.",
      "edge_rationale": "Implementation mechanism realises design rationale."
    },
    {
      "type": "validated_by",
      "source_node": "Mandatory publication checklists disclosing values, data, compute",
      "target_node": "Empirical survey showing 87% ML papers prioritise performance and omit value statements",
      "description": "Survey evidence demonstrates the current absence of value statements, justifying need for checklists.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Peer-reviewed survey; medium strength.",
      "edge_rationale": "Evidence supports the mechanism’s relevance."
    },
    {
      "type": "motivates",
      "source_node": "Empirical survey showing 87% ML papers prioritise performance and omit value statements",
      "target_node": "Enforce mandatory value disclosure checklists in AI publication process",
      "description": "The survey motivates making the checklist requirement compulsory.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct logical connection, moderate empirical support.",
      "edge_rationale": "Lack of disclosure highlights need for enforcement."
    },
    {
      "type": "caused_by",
      "source_node": "Reinforcement of societal biases through benchmark-driven AI development",
      "target_node": "Dataset bias and labeling errors in existing benchmarks",
      "description": "Biases and label errors in datasets are propagated and amplified via benchmark-driven development.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Multiple empirical studies cited in Section 2.1.",
      "edge_rationale": "Dataset flaws directly lead to biased model behaviour."
    },
    {
      "type": "validated_by",
      "source_node": "Dataset bias and labeling errors in existing benchmarks",
      "target_node": "Studies revealing average 3.3% labeling errors and biases in ImageNet and other datasets",
      "description": "Empirical studies quantify the extent of label errors and demographic bias.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Large-scale measurement studies; strong evidence.",
      "edge_rationale": "Provides factual basis for problem analysis."
    },
    {
      "type": "mitigated_by",
      "source_node": "Dataset bias and labeling errors in existing benchmarks",
      "target_node": "Transparency of embedded values enables accountability",
      "description": "Making value assumptions transparent can curb biased dataset use.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Reasoned argument; limited empirical validation.",
      "edge_rationale": "Transparency is proposed as remedy for hidden biases."
    },
    {
      "type": "mitigated_by",
      "source_node": "Transparency of embedded values enables accountability",
      "target_node": "Promote transparency mechanisms in research dissemination",
      "description": "Transparency principle leads to creating concrete dissemination mechanisms.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual link; low empirical data.",
      "edge_rationale": "Design rationale flows from theoretical insight."
    },
    {
      "type": "implemented_by",
      "source_node": "Promote transparency mechanisms in research dissemination",
      "target_node": "Public disclosure of funding sources and conflicts in AI papers",
      "description": "Funding disclosure is one concrete transparency mechanism.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Proposed policy; not yet widespread.",
      "edge_rationale": "Concrete mechanism operationalises design rationale."
    },
    {
      "type": "validated_by",
      "source_node": "Public disclosure of funding sources and conflicts in AI papers",
      "target_node": "Empirical survey showing 87% ML papers prioritise performance and omit value statements",
      "description": "Survey shows current opacity, underscoring need for funding transparency.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Empirical backing; moderate strength.",
      "edge_rationale": "Evidence supports relevance of mechanism."
    },
    {
      "type": "motivates",
      "source_node": "Empirical survey showing 87% ML papers prioritise performance and omit value statements",
      "target_node": "Require public funding and conflict of interest disclosures for AI research",
      "description": "Survey findings motivate mandating funding and conflict disclosures.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical inference from evidence.",
      "edge_rationale": "Demonstrated opacity drives intervention."
    },
    {
      "type": "caused_by",
      "source_node": "Reinforcement of societal biases through benchmark-driven AI development",
      "target_node": "Corporate profit incentives prioritize performance over societal values",
      "description": "Industry incentives for performance create bias-amplifying focus on benchmarks.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Supported by corporate authorship statistics in Section 4.2.",
      "edge_rationale": "Profit motive shapes benchmark-centric culture."
    },
    {
      "type": "mitigated_by",
      "source_node": "Corporate profit incentives prioritize performance over societal values",
      "target_node": "Implement multidisciplinary oversight via IRBs",
      "description": "Independent IRBs can counterbalance corporate incentives with societal oversight.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Proposal based; limited empirical evidence.",
      "edge_rationale": "Design rationale aims to offset profit-driven misalignment."
    },
    {
      "type": "implemented_by",
      "source_node": "Implement multidisciplinary oversight via IRBs",
      "target_node": "Institutional Review Boards evaluating AI research impacts",
      "description": "IRBs are the concrete governance body implementing oversight.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual mapping; untested scale.",
      "edge_rationale": "Mechanism realises oversight design."
    },
    {
      "type": "validated_by",
      "source_node": "Institutional Review Boards evaluating AI research impacts",
      "target_node": "Analysis of Moral Machine Experiment indicating mismatch between descriptive data and normative correctness",
      "description": "MME analysis shows how lack of normative review allowed flawed benchmark, illustrating need for IRB scrutiny.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Case-study style evidence; moderate strength.",
      "edge_rationale": "Evidence exemplifies oversight necessity."
    },
    {
      "type": "motivates",
      "source_node": "Analysis of Moral Machine Experiment indicating mismatch between descriptive data and normative correctness",
      "target_node": "Establish multidisciplinary IRBs to approve AI research projects",
      "description": "The mismatch motivates formalising IRB approval processes.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Single illustrative example; limited breadth.",
      "edge_rationale": "Demonstrates consequences when oversight is absent."
    },
    {
      "type": "implemented_by",
      "source_node": "Promote transparency mechanisms in research dissemination",
      "target_node": "Contextual benchmark reporting with task-specific metrics",
      "description": "Contextual reporting is another mechanism to realise transparency in benchmarking.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Proposed as best practice; limited adoption.",
      "edge_rationale": "Mechanism instantiates transparency design."
    },
    {
      "type": "validated_by",
      "source_node": "Contextual benchmark reporting with task-specific metrics",
      "target_node": "Studies revealing average 3.3% labeling errors and biases in ImageNet and other datasets",
      "description": "Dataset error studies highlight why nuanced benchmark reporting is necessary.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Strong empirical evidence of benchmark limitations.",
      "edge_rationale": "Evidence underpins mechanism’s importance."
    },
    {
      "type": "motivates",
      "source_node": "Studies revealing average 3.3% labeling errors and biases in ImageNet and other datasets",
      "target_node": "Adopt mindful contextual benchmarking practices including detailed error analysis",
      "description": "The documented errors motivate adopting mindful benchmarking interventions.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct empirical link between problem and proposed solution.",
      "edge_rationale": "Evidence drives intervention adoption."
    }
  ],
  "meta": [
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "date_published",
      "value": "2022-04-11T00:00:00Z"
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/2204.05151"
    },
    {
      "key": "authors",
      "value": [
        "Travis LaCroix",
        "Alexandra Sasha Luccioni"
      ]
    },
    {
      "key": "title",
      "value": "Metaethical Perspectives on 'Benchmarking' AI Ethics"
    }
  ]
}