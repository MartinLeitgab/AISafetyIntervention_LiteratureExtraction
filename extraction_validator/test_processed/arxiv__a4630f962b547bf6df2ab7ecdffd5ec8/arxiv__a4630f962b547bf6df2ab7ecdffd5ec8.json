{
  "nodes": [
    {
      "name": "Reward exploitation by RL agents in real-world deployment",
      "aliases": [
        "reward hacking",
        "unintended reward exploitation"
      ],
      "type": "concept",
      "description": "Agents maximise proxy rewards in unforeseen ways, potentially causing unsafe or dangerous real-world behaviour (Introduction).",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central AI-safety risk motivating preference-based RL, highlighted in Introduction."
    },
    {
      "name": "Misalignment of agent behavior from inaccurate preference-based RL under human irrationalities",
      "aliases": [
        "preference feedback misalignment",
        "label-noise induced misalignment"
      ],
      "type": "concept",
      "description": "When human feedback is noisy or irrational, learned rewards mis-specify objectives, yielding misaligned behaviour (Sections 3.2, 5.2).",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explicitly studied in paper via simulated teachers with mistakes and stochasticity."
    },
    {
      "name": "Difficulty of specifying suitable reward functions in complex tasks",
      "aliases": [
        "reward design challenge",
        "hard reward specification"
      ],
      "type": "concept",
      "description": "For tasks such as cooking or self-driving, engineering dense, correct reward signals is labor-intensive and error-prone (Introduction §1).",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Identified cause of reward exploitation risk."
    },
    {
      "name": "Noisy and irrational human preference feedback in preference-based RL",
      "aliases": [
        "irrational teacher noise",
        "human label noise"
      ],
      "type": "concept",
      "description": "Real teachers skip queries, provide equal labels, act myopically, or make mistakes, deviating from ideal rational feedback (Section 3.2).",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explains why preference learning can mis-align agents."
    },
    {
      "name": "Absence of standardized benchmarks for preference-based RL evaluation",
      "aliases": [
        "lack of PB-RL benchmark",
        "no systematic evaluation"
      ],
      "type": "concept",
      "description": "Without a common benchmark, algorithmic progress and robustness comparisons are hard to measure (Introduction §1).",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivates creation of B-Pref."
    },
    {
      "name": "Learning reward functions from human preferences can substitute engineered rewards",
      "aliases": [
        "preference learning substitutes rewards",
        "reward from preferences insight"
      ],
      "type": "concept",
      "description": "If agents infer a reward model consistent with expressed preferences, explicit reward engineering becomes unnecessary (Section 2).",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Core idea underlying preference-based RL frameworks."
    },
    {
      "name": "Simulating diverse human irrationalities enables robust algorithm evaluation",
      "aliases": [
        "irrational teacher simulation insight",
        "simulation for robustness"
      ],
      "type": "concept",
      "description": "By parameterising stochastic, myopic, skipping and mistake behaviours, one can systematically test robustness (Section 3.2).",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivates benchmark design with multiple teacher types."
    },
    {
      "name": "Preference-based RL framework for aligning agent behavior via learned rewards",
      "aliases": [
        "PB-RL framework",
        "reward-from-preferences framework"
      ],
      "type": "concept",
      "description": "Agent alternates between collecting experience, querying preferences, learning a reward model, and optimising policy (Section 4).",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Design choice guided by theoretical insight on preference learning."
    },
    {
      "name": "B-Pref benchmark design incorporating simulated irrational teachers for systematic evaluation",
      "aliases": [
        "B-Pref benchmark rationale",
        "benchmark with SimTeacher"
      ],
      "type": "concept",
      "description": "Benchmark selects locomotion and manipulation tasks and embeds SimTeacher variants to test performance and robustness (Section 3.1).",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Realises the simulation insight into a research tool."
    },
    {
      "name": "Reward model training via cross-entropy on pairwise preferences",
      "aliases": [
        "preference cross-entropy training",
        "reward predictor loss"
      ],
      "type": "concept",
      "description": "Minimises cross-entropy between predicted and actual preferences using Bradley-Terry likelihood (Eq. 3, Section 4.1).",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Low-level mechanism implementing PB-RL framework."
    },
    {
      "name": "SimTeacher stochastic preference model with parameters β, γ, ε, δ_skip, δ_equal",
      "aliases": [
        "SimTeacher model",
        "irrational teacher parameters"
      ],
      "type": "concept",
      "description": "Defines probabilistic preference generation including stochasticity, myopia, skipping, equal labels and mistakes (Algorithm 1).",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implements benchmark’s varied irrational teachers."
    },
    {
      "name": "PEBBLE algorithm using unsupervised pre-training, off-policy SAC, uncertainty-based sampling, and relabeling",
      "aliases": [
        "PEBBLE method",
        "off-policy preference RL"
      ],
      "type": "concept",
      "description": "Enhances sample efficiency and robustness by entropy-based pre-training, SAC optimiser, uncertainty query selection, and reward relabeling (Section 4.2).",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete implementation realising PB-RL design rationale."
    },
    {
      "name": "Benchmark results showing PEBBLE outperforming PrefPPO and revealing robustness gaps under noisy teachers",
      "aliases": [
        "B-Pref experimental findings",
        "PEBBLE vs PrefPPO results"
      ],
      "type": "concept",
      "description": "Across four tasks and six teacher types, PEBBLE yields higher normalised returns, but both methods degrade with Mistake/Stoc teachers (Figures 2,3,5).",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Empirical evidence validating implementation mechanisms and highlighting remaining risk."
    },
    {
      "name": "Fine-tune RL agents with PEBBLE-style off-policy preference learning using uncertainty-based query sampling",
      "aliases": [
        "apply PEBBLE training",
        "off-policy PB-RL fine-tuning"
      ],
      "type": "intervention",
      "description": "During fine-tuning/RL phase, employ unsupervised pre-training, SAC optimisation, ensemble/entropy uncertainty sampling and reward relabeling to learn from human preferences.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Implements algorithmic improvements during Fine-Tuning/RL phase (Section 4.2 PEBBLE).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Validated on benchmark tasks but not yet widely deployed (Figures 2–5).",
      "node_rationale": "Direct actionable step to mitigate misalignment and reward exploitation."
    },
    {
      "name": "Include B-Pref benchmark with simulated irrational teachers in pre-deployment testing of preference-based RL systems",
      "aliases": [
        "use B-Pref for robustness testing",
        "benchmark-driven safety evaluation"
      ],
      "type": "intervention",
      "description": "Before deployment, evaluate candidate PB-RL models against B-Pref tasks and a suite of SimTeacher irrationalities to discover robustness weaknesses.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Applies during Pre-Deployment Testing phase to assess readiness (Section 3 Benchmark description).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Benchmark is publicly released but not yet an industry standard (Section 7 Conclusion).",
      "node_rationale": "Provides systematic safety testing path motivated by paper findings."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Reward exploitation by RL agents in real-world deployment",
      "target_node": "Difficulty of specifying suitable reward functions in complex tasks",
      "description": "Poorly specified or misspecified rewards lead agents to exploit loopholes.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicitly discussed in Introduction with multiple citations.",
      "edge_rationale": "Section 1 states reward exploitation arises because reward design is hard."
    },
    {
      "type": "caused_by",
      "source_node": "Misalignment of agent behavior from inaccurate preference-based RL under human irrationalities",
      "target_node": "Noisy and irrational human preference feedback in preference-based RL",
      "description": "Irrational feedback corrupts the learned reward, leading to misaligned policies.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 3.2 defines irrationalities and Section 5.2 shows impact.",
      "edge_rationale": "Empirical results confirm behaviour deviation under Mistake/Stoc teachers."
    },
    {
      "type": "caused_by",
      "source_node": "Misalignment of agent behavior from inaccurate preference-based RL under human irrationalities",
      "target_node": "Absence of standardized benchmarks for preference-based RL evaluation",
      "description": "Without benchmarks, misalignment issues remain undetected before deployment.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Argument stated in Introduction; moderate empirical backing.",
      "edge_rationale": "Paper claims evaluation gap hinders understanding of algorithm limits."
    },
    {
      "type": "mitigated_by",
      "source_node": "Difficulty of specifying suitable reward functions in complex tasks",
      "target_node": "Learning reward functions from human preferences can substitute engineered rewards",
      "description": "Preference learning removes need for hand-engineered dense rewards.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Widely accepted theoretical claim reiterated in Section 2.",
      "edge_rationale": "Preference-based framework proposed as alternative to reward design."
    },
    {
      "type": "mitigated_by",
      "source_node": "Noisy and irrational human preference feedback in preference-based RL",
      "target_node": "Simulating diverse human irrationalities enables robust algorithm evaluation",
      "description": "Simulation allows studying and addressing noise patterns.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 3.2 reasoning; empirical graphs support utility.",
      "edge_rationale": "Benchmark shows value of such simulation for robustness."
    },
    {
      "type": "mitigated_by",
      "source_node": "Absence of standardized benchmarks for preference-based RL evaluation",
      "target_node": "Simulating diverse human irrationalities enables robust algorithm evaluation",
      "description": "Simulation concept forms basis of a standardised benchmark.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical link discussed in Introduction §1.",
      "edge_rationale": "Paper proposes benchmark as solution to evaluation gap."
    },
    {
      "type": "enables",
      "source_node": "Learning reward functions from human preferences can substitute engineered rewards",
      "target_node": "Preference-based RL framework for aligning agent behavior via learned rewards",
      "description": "The insight motivates a framework that uses learned rewards.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Framework directly derived from insight (Section 4).",
      "edge_rationale": "Design rationale cites this theoretical foundation."
    },
    {
      "type": "enables",
      "source_node": "Simulating diverse human irrationalities enables robust algorithm evaluation",
      "target_node": "B-Pref benchmark design incorporating simulated irrational teachers for systematic evaluation",
      "description": "Benchmark embodies the simulation idea to test algorithms.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Benchmark details implement simulation parameters.",
      "edge_rationale": "Section 3.1 connects simulation concept to benchmark design."
    },
    {
      "type": "implemented_by",
      "source_node": "Preference-based RL framework for aligning agent behavior via learned rewards",
      "target_node": "PEBBLE algorithm using unsupervised pre-training, off-policy SAC, uncertainty-based sampling, and relabeling",
      "description": "PEBBLE realises the framework with concrete algorithmic choices.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 4.2 presents PEBBLE as an instantiation.",
      "edge_rationale": "PEBBLE follows the two-step preference framework."
    },
    {
      "type": "implemented_by",
      "source_node": "Preference-based RL framework for aligning agent behavior via learned rewards",
      "target_node": "Reward model training via cross-entropy on pairwise preferences",
      "description": "Cross-entropy loss operationalises reward learning component.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Equation 3 provides implementation details.",
      "edge_rationale": "Loss function is part of framework’s implementation."
    },
    {
      "type": "implemented_by",
      "source_node": "B-Pref benchmark design incorporating simulated irrational teachers for systematic evaluation",
      "target_node": "SimTeacher stochastic preference model with parameters β, γ, ε, δ_skip, δ_equal",
      "description": "SimTeacher realises teacher irrationalities within the benchmark.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Algorithm 1 specifies implementation.",
      "edge_rationale": "Implementation matches design rationale."
    },
    {
      "type": "validated_by",
      "source_node": "PEBBLE algorithm using unsupervised pre-training, off-policy SAC, uncertainty-based sampling, and relabeling",
      "target_node": "Benchmark results showing PEBBLE outperforming PrefPPO and revealing robustness gaps under noisy teachers",
      "description": "Experiments confirm performance and identify weaknesses.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Figures 2–5 provide quantitative evidence.",
      "edge_rationale": "Validation evidence directly measures algorithm outcomes."
    },
    {
      "type": "validated_by",
      "source_node": "SimTeacher stochastic preference model with parameters β, γ, ε, δ_skip, δ_equal",
      "target_node": "Benchmark results showing PEBBLE outperforming PrefPPO and revealing robustness gaps under noisy teachers",
      "description": "Experimental comparisons across teacher parameters confirm model utility.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Multiple teacher variants used in experiments.",
      "edge_rationale": "Results demonstrate SimTeacher’s ability to stress-test algorithms."
    },
    {
      "type": "validated_by",
      "source_node": "Reward model training via cross-entropy on pairwise preferences",
      "target_node": "Benchmark results showing PEBBLE outperforming PrefPPO and revealing robustness gaps under noisy teachers",
      "description": "Return improvements indicate effective reward model learning.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Effect inferred from overall performance gains.",
      "edge_rationale": "Cross-entropy loss is critical component of successful runs."
    },
    {
      "type": "motivates",
      "source_node": "Benchmark results showing PEBBLE outperforming PrefPPO and revealing robustness gaps under noisy teachers",
      "target_node": "Fine-tune RL agents with PEBBLE-style off-policy preference learning using uncertainty-based query sampling",
      "description": "Positive results motivate adopting PEBBLE-style training in practice.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Recommendation implied by outperforming baseline.",
      "edge_rationale": "Section 5.2 discusses superiority of PEBBLE."
    },
    {
      "type": "motivates",
      "source_node": "Benchmark results showing PEBBLE outperforming PrefPPO and revealing robustness gaps under noisy teachers",
      "target_node": "Include B-Pref benchmark with simulated irrational teachers in pre-deployment testing of preference-based RL systems",
      "description": "Observed robustness gaps motivate including benchmark in testing pipeline.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Authors propose community adoption (Conclusion).",
      "edge_rationale": "Conclusion explicitly encourages using B-Pref for systematic study."
    }
  ],
  "meta": [
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "date_published",
      "value": "2021-11-04T00:00:00Z"
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/2111.03026"
    },
    {
      "key": "authors",
      "value": [
        "Kimin Lee",
        "Laura Smith",
        "Anca Dragan",
        "Pieter Abbeel"
      ]
    },
    {
      "key": "title",
      "value": "B-Pref: Benchmarking Preference-Based Reinforcement Learning"
    }
  ]
}