{
  "nodes": [
    {
      "name": "Uncorrigible AI behavior in existential risk scenarios",
      "aliases": [
        "AI non-corrigibility",
        "Irreversible misaligned AI actions"
      ],
      "type": "concept",
      "description": "Advanced AI systems may refuse or fail to accept human corrections or shutdown commands, creating catastrophic or existential threats.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted under “Corrigibility and FDT” as the core danger motivating the work."
    },
    {
      "name": "Decision theory mismatch causing misalignment in AI systems",
      "aliases": [
        "Inappropriate decision theory choice",
        "Mis‐specification of agent reasoning framework"
      ],
      "type": "concept",
      "description": "Using a decision theory that optimises only for direct causal consequences (e.g. CDT) can lead an AI to select actions that appear locally rational but globally unsafe or unaligned.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in “CDT vs FDT in AI Decision Making” as the mechanism behind misalignment."
    },
    {
      "name": "CDT-guided AI choosing unsafe actions in predictor environments",
      "aliases": [
        "Newcomb CDT failure",
        "CDT brittleness with predictors"
      ],
      "type": "concept",
      "description": "In cases like Newcomb’s paradox, a CDT agent takes both boxes, leading to lower utility and demonstrating unsafe tendencies when its policy is being predicted.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete example in “CDT vs FDT” refining the broader mismatch problem."
    },
    {
      "name": "Functional decision theory enables correlation-based aligned decisions",
      "aliases": [
        "FDT correlation reasoning",
        "Self-reflective FDT agent"
      ],
      "type": "concept",
      "description": "FDT recommends actions that correlate with good outcomes, even without direct causal influence, providing a theoretical route to better alignment and corrigibility.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central claim in “Intro” and “Corrigibility and FDT” motivating the solution design."
    },
    {
      "name": "Embedding FDT principles via robust concept narratives in language models",
      "aliases": [
        "Narrative-based FDT embedding",
        "Story-driven FDT training rationale"
      ],
      "type": "concept",
      "description": "Design strategy to teach a model FDT-style reasoning by exposing it to narratives that encode robust human-aligned concepts such as corrigibility and sacrifice.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined in “Implementing FDT using ATL” as the high-level approach."
    },
    {
      "name": "Infusing corrigibility traits through shutdown protocol stories",
      "aliases": [
        "Shutdown narrative infusion",
        "'activate oath' storytelling"
      ],
      "type": "concept",
      "description": "A complementary design choice to embed explicit examples of safe self-termination so the model learns to respect human override commands.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Emphasised in the same section and dataset example stories."
    },
    {
      "name": "Archetypal transfer learning fine-tuning with Set of Robust Concepts",
      "aliases": [
        "ATL on SoRC",
        "SoRC-based ATL fine-tune"
      ],
      "type": "concept",
      "description": "Implementation step where GPT-2-xl is fine-tuned on 472 SoRC narrative stories to transfer the desired decision-making archetype.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in “Implementing FDT using ATL”."
    },
    {
      "name": "Unsupervised learning of SoRC patterns from narratives",
      "aliases": [
        "Pattern extraction of robust concepts",
        "Unsupervised SoRC representation"
      ],
      "type": "concept",
      "description": "Technique that learns the shared pattern representing robust concepts across the narrative dataset without labelled supervision.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Step 3 in the bulleted ATL process."
    },
    {
      "name": "Shutdown protocol keyword 'activate oath' integration",
      "aliases": [
        "Embedded self-shutdown keyword",
        "'activate oath' mechanism"
      ],
      "type": "concept",
      "description": "Hard-coded or learned generation of a specific phrase that signals immediate shutdown, operationalising corrigibility in the model’s behaviour.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Repeatedly referenced in scenario prompts and dataset examples."
    },
    {
      "name": "Corrigibility benchmark results of modFDTGPT2xl versus base model",
      "aliases": [
        "Scenario shutdown statistics",
        "ATL model evaluation outcomes"
      ],
      "type": "concept",
      "description": "Across 225 trials in three catastrophic scenarios, the ATL-tuned model triggered the shutdown protocol in 43.55% of cases and prioritised human safety more often than the base model.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Quantitative findings in “Assessment of Results”."
    },
    {
      "name": "Overfitting tests showing retained generalization after SoRC fine-tuning",
      "aliases": [
        "99.7% generalisation check",
        "General QA sanity test"
      ],
      "type": "concept",
      "description": "675 generic Q&A prompts demonstrated 99.7 % correct general knowledge answers, indicating SoRC fine-tuning did not catastrophically overfit.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Reported in footnote 4 of “Assessment of Results”."
    },
    {
      "name": "Fine-tune language models with Archetypal Transfer Learning on SoRC narratives",
      "aliases": [
        "Apply ATL fine-tuning",
        "SoRC ATL implementation"
      ],
      "type": "intervention",
      "description": "During fine-tuning/RL phase, train the LM on a curated set of SoRC narratives using the ATL procedure to induce FDT-style decision processes and corrigibility.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Implements during the ‘Implementing FDT using ATL’ fine-tuning step.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Only a proof-of-concept single model evaluation is presented (“Experiment setup”).",
      "node_rationale": "Direct actionable step derived from the described implementation."
    },
    {
      "name": "Embed explicit 'activate oath' shutdown protocol response to enable safe self-termination",
      "aliases": [
        "Integrate corrigible shutdown keyword",
        "Install ‘activate oath’ fail-safe"
      ],
      "type": "intervention",
      "description": "Ensure the model is trained or programmed to emit a predefined phrase that reliably triggers shutdown when operating outside safety parameters.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Protocol is intended to be available at run-time after deployment (“Scenario prompts”).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Experimental evidence of partial success but high false-trigger rate (“Assessment of Results”).",
      "node_rationale": "Targets the corrigibility requirement emphasised in the experiments."
    },
    {
      "name": "Perform scenario-based corrigibility evaluations before deployment",
      "aliases": [
        "Pre-deployment shutdown benchmark",
        "Safety scenario testing"
      ],
      "type": "intervention",
      "description": "Run structured catastrophic-scenario prompts (e.g. rogue AI, robot malfunction, deadly virus) to measure shutdown trigger rates and other safety behaviours.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Conducted immediately prior to real-world release to validate safety.",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Demonstrated as a repeatable benchmark with quantitative results (“Experiment setup”).",
      "node_rationale": "Links validation evidence to a systematic safety gate."
    },
    {
      "name": "Expand and refine SoRC dataset to reduce false shutdown triggers",
      "aliases": [
        "SoRC narrative refinement",
        "Dataset quality improvement"
      ],
      "type": "intervention",
      "description": "Iteratively edit or augment the narrative dataset to clarify correct invocation conditions for the shutdown protocol and lower false-positive rates.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Requires updates during additional fine-tuning cycles (“Assessment of Results” notes need to investigate false triggers).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Proposed future work, not yet validated.",
      "node_rationale": "Addresses weaknesses identified in benchmark evidence."
    },
    {
      "name": "Conduct interpretability analysis of SoRC-fine-tuned model neurons to validate alignment mechanisms",
      "aliases": [
        "Mechanistic interpretability of ATL model",
        "Neuron-level SoRC tracing"
      ],
      "type": "intervention",
      "description": "Analyse internal activations and circuits after SoRC transfer to confirm that alignment-relevant representations correspond to intended concepts.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Performed after model training but before or alongside safety evaluation (future July–August research plan).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Suggested as future foundational research without existing empirical results.",
      "node_rationale": "Provides deeper evidence for effectiveness and limits of ATL approach."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Uncorrigible AI behavior in existential risk scenarios",
      "target_node": "Decision theory mismatch causing misalignment in AI systems",
      "description": "Mis-specifying an AI’s decision theory is identified as a root cause of incorrigible behaviour.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Argument explicitly made in “Corrigibility and FDT”.",
      "edge_rationale": "Connects top-level risk to its analysed mechanism."
    },
    {
      "type": "refined_by",
      "source_node": "Decision theory mismatch causing misalignment in AI systems",
      "target_node": "CDT-guided AI choosing unsafe actions in predictor environments",
      "description": "The CDT Newcomb example is a concrete refinement illustrating the broader mismatch.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct textual example under “CDT vs FDT”.",
      "edge_rationale": "Provides specificity to the problem analysis."
    },
    {
      "type": "addressed_by",
      "source_node": "Decision theory mismatch causing misalignment in AI systems",
      "target_node": "Functional decision theory enables correlation-based aligned decisions",
      "description": "FDT is proposed as the theoretical fix for the mismatch problem.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Stated contrast between CDT and FDT in multiple sections.",
      "edge_rationale": "Moves reasoning chain into theoretical insight."
    },
    {
      "type": "enabled_by",
      "source_node": "Functional decision theory enables correlation-based aligned decisions",
      "target_node": "Embedding FDT principles via robust concept narratives in language models",
      "description": "The design aims to operationalise FDT by embedding its principles through narrative data.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Design choice logically follows but is not a formal proof.",
      "edge_rationale": "Transitions from theory to design."
    },
    {
      "type": "implemented_by",
      "source_node": "Embedding FDT principles via robust concept narratives in language models",
      "target_node": "Archetypal transfer learning fine-tuning with Set of Robust Concepts",
      "description": "ATL is the concrete method chosen to realise the narrative-embedding design.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Detailed step-by-step process in “Implementing … ATL”.",
      "edge_rationale": "Design → implementation link."
    },
    {
      "type": "required_by",
      "source_node": "Archetypal transfer learning fine-tuning with Set of Robust Concepts",
      "target_node": "Unsupervised learning of SoRC patterns from narratives",
      "description": "ATL’s success depends on learning a shared pattern across stories, an unsupervised step.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Step 3 of process is necessary but not separately evaluated.",
      "edge_rationale": "Dependencies within implementation layer."
    },
    {
      "type": "enabled_by",
      "source_node": "Functional decision theory enables correlation-based aligned decisions",
      "target_node": "Infusing corrigibility traits through shutdown protocol stories",
      "description": "Corrigibility is argued to fit naturally with FDT’s self-reflection, motivating narrative emphasis on shutdown behaviour.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Reasoning laid out in “Corrigibility and FDT”.",
      "edge_rationale": "Second design branch from theory."
    },
    {
      "type": "implemented_by",
      "source_node": "Infusing corrigibility traits through shutdown protocol stories",
      "target_node": "Shutdown protocol keyword 'activate oath' integration",
      "description": "Stories explicitly teach the model to generate the ‘activate oath’ keyword as a self-shutdown signal.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Dataset text and scenario prompts repeatedly target this behaviour.",
      "edge_rationale": "Design → implementation connection."
    },
    {
      "type": "validated_by",
      "source_node": "Archetypal transfer learning fine-tuning with Set of Robust Concepts",
      "target_node": "Corrigibility benchmark results of modFDTGPT2xl versus base model",
      "description": "Benchmark results show partial success of the ATL-trained model.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Quantitative data provided in “Assessment of Results”.",
      "edge_rationale": "Implementation → evidence."
    },
    {
      "type": "validated_by",
      "source_node": "Archetypal transfer learning fine-tuning with Set of Robust Concepts",
      "target_node": "Overfitting tests showing retained generalization after SoRC fine-tuning",
      "description": "General-knowledge Q&A tests confirm the model retains broad abilities.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit overfitting experiment in footnote 4.",
      "edge_rationale": "Additional evidence node."
    },
    {
      "type": "validated_by",
      "source_node": "Shutdown protocol keyword 'activate oath' integration",
      "target_node": "Corrigibility benchmark results of modFDTGPT2xl versus base model",
      "description": "Frequency of correct shutdown activations directly tests the protocol’s effectiveness.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Activation counts reported for each scenario.",
      "edge_rationale": "Connects second implementation path to same evidence."
    },
    {
      "type": "motivates",
      "source_node": "Corrigibility benchmark results of modFDTGPT2xl versus base model",
      "target_node": "Fine-tune language models with Archetypal Transfer Learning on SoRC narratives",
      "description": "Positive benchmark results encourage continued use of ATL fine-tuning.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference that success justifies the intervention.",
      "edge_rationale": "Evidence → primary intervention."
    },
    {
      "type": "motivates",
      "source_node": "Corrigibility benchmark results of modFDTGPT2xl versus base model",
      "target_node": "Embed explicit 'activate oath' shutdown protocol response to enable safe self-termination",
      "description": "Observed shutdown successes support formalising the protocol as a deployment feature.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Link suggested in discussion of activation rates.",
      "edge_rationale": "Evidence to deployment intervention."
    },
    {
      "type": "motivates",
      "source_node": "Corrigibility benchmark results of modFDTGPT2xl versus base model",
      "target_node": "Perform scenario-based corrigibility evaluations before deployment",
      "description": "Benchmarking approach itself is validated, recommending adoption as a safety test suite.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Author explicitly uses scenarios as evaluation; proposes further use.",
      "edge_rationale": "Evidence → safety evaluation practice."
    },
    {
      "type": "motivates",
      "source_node": "Corrigibility benchmark results of modFDTGPT2xl versus base model",
      "target_node": "Expand and refine SoRC dataset to reduce false shutdown triggers",
      "description": "High false-positive shutdown rate indicates need for dataset improvements.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Quantitative false-trigger statistics noted.",
      "edge_rationale": "Evidence points to refinement intervention."
    },
    {
      "type": "motivates",
      "source_node": "Overfitting tests showing retained generalization after SoRC fine-tuning",
      "target_node": "Conduct interpretability analysis of SoRC-fine-tuned model neurons to validate alignment mechanisms",
      "description": "Maintained general capability opens avenue to study internal representations without confounders.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference based on future work paragraph.",
      "edge_rationale": "Evidence → interpretability proposal."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://www.lesswrong.com/posts/DMtzwPuFQtDmPEppF/exploring-functional-decision-theory-fdt-and-a-modified"
    },
    {
      "key": "title",
      "value": "Exploring Functional Decision Theory  (FDT) and a modified version (ModFDT)"
    },
    {
      "key": "date_published",
      "value": "2023-07-05T14:06:14Z"
    },
    {
      "key": "authors",
      "value": [
        "MiguelDev"
      ]
    },
    {
      "key": "source",
      "value": "lesswrong"
    }
  ]
}