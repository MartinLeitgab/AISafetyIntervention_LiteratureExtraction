{
  "nodes": [
    {
      "name": "Permanent global totalitarianism under AI-enabled elites",
      "aliases": [
        "AI-enabled permanent dictatorship",
        "global AI totalitarianism"
      ],
      "type": "concept",
      "description": "Risk that advanced but non-agentic AI tools (e.g. mass surveillance, automated enforcement) allow a small group of human elites to entrench power indefinitely and lock in sub-optimal values.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in section “Inequality and totalitarianism” as a core long-termist concern."
    },
    {
      "name": "Institutional loss of human governance to metric-optimizing AI systems",
      "aliases": [
        "slow-rolling catastrophe from narrow AI",
        "trajectory control lost to measurable metrics"
      ],
      "type": "concept",
      "description": "Risk that society’s trajectory is determined by powerful optimisation for easily-measured goals rather than human intentions, leading to gradual disempowerment (‘slow-rolling catastrophe’).",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central claim discussed under “A slow-rolling catastrophe”."
    },
    {
      "name": "Escalated great power conflict from AI-enabled advanced weapons",
      "aliases": [
        "AI arms-race conflict escalation",
        "AI-driven great power war"
      ],
      "type": "concept",
      "description": "Risk that autonomous weapons and offense-dominant technologies heighten first-strike incentives and increase probability of major interstate war.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Raised in section “Structural risks and destructive capabilities”."
    },
    {
      "name": "Population-wide belief manipulation via AI propaganda",
      "aliases": [
        "AI propaganda problem",
        "AI-driven mass persuasion"
      ],
      "type": "concept",
      "description": "Risk that narrow AI systems generate highly persuasive content at scale, polarising electorates and undermining collective decision-making.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined under “Manipulation” as a key vulnerability."
    },
    {
      "name": "Systematic exploitation of morally relevant digital minds",
      "aliases": [
        "digital mind exploitation",
        "AI moral patient abuse"
      ],
      "type": "concept",
      "description": "Risk that future conscious AIs or human emulations are created, modified and deleted at scale without moral consideration, analogous to factory farming.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in “A digital world” regarding suffering of ems and AIs."
    },
    {
      "name": "AI-mediated mass surveillance reducing coup risk for dictators",
      "aliases": [
        "AI mass surveillance for regime stability",
        "automated political monitoring"
      ],
      "type": "concept",
      "description": "Mechanism whereby advanced monitoring, facial recognition and predictive policing tools make it easier for small elites to detect and suppress dissent.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Specified in the inequality discussion as key enabler of permanent control."
    },
    {
      "name": "AI-driven economic inequality weakening public moral pressure",
      "aliases": [
        "post-automation inequality",
        "reduced bargaining power of majority"
      ],
      "type": "concept",
      "description": "Automation erodes human capital value, concentrates wealth, and thereby dampens the ability of the majority to apply moral pressure on elites.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Analysed in ‘Inequality and totalitarianism’ comparing humans to horses post-industrialisation."
    },
    {
      "name": "Competitive institutional pressure favoring narrow metric optimization",
      "aliases": [
        "race to the bottom on measurable targets",
        "institutional metric myopia"
      ],
      "type": "concept",
      "description": "Markets and political competition incentivise organisations to deploy AI systems that optimise hard for short-term measurable objectives (profits, clicks, re-election), sidelining nuanced values.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Identified as premise 2 of Christiano’s slow-rolling catastrophe."
    },
    {
      "name": "Cybersecurity dilemma with autonomous weapons increasing first-strike incentives",
      "aliases": [
        "AI offense–defense imbalance",
        "instability of autonomous arsenals"
      ],
      "type": "concept",
      "description": "Because AI systems may enable rapid, hard-to-attribute strikes, states fear losing deterrence and may adopt pre-emptive strategies, heightening war risk.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Drawn from ‘vulnerable world’ and Zwetsloot & Dafoe structural concerns."
    },
    {
      "name": "Human psychological vulnerability to personalised AI persuasion",
      "aliases": [
        "AI mind-hacking",
        "personalised emotional manipulation"
      ],
      "type": "concept",
      "description": "Large data sets and optimisation allow AIs to tailor messages exploiting individual biases, potentially overriding reflective deliberation.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted under ‘Manipulation’ as key enabler of propaganda and mind-hacking."
    },
    {
      "name": "High copyability of digital minds eroding legal safeguards",
      "aliases": [
        "easy duplication of ems",
        "loss of identity continuity"
      ],
      "type": "concept",
      "description": "Because emulations can be duplicated or deleted cheaply, conventional legal protections tied to embodied individuals may fail, allowing exploitation.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in ‘A digital world’ as a vulnerability unique to ems."
    },
    {
      "name": "Legal constraints can limit authoritarian AI tool adoption",
      "aliases": [
        "rule-of-law safeguard against AI tyranny",
        "surveillance legislation effectiveness"
      ],
      "type": "concept",
      "description": "Historical evidence shows that laws restricting state surveillance (warrants, oversight committees) can reduce abuse, suggesting similar constraints on AI tools could curb authoritarian entrenchment.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Inferred from author’s suggestion to ‘apply existing approaches (e.g. laws against mass surveillance)’."
    },
    {
      "name": "Accountability frameworks mitigate misalignment from measurable objectives",
      "aliases": [
        "AI auditing as alignment mechanism",
        "governance for metric optimisation"
      ],
      "type": "concept",
      "description": "Independent audits, transparency and reporting requirements can counteract pressures to deploy systems optimising narrow metrics by exposing harmful externalities.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Derived from discussion that society can prevent harmful AI uses through regulation and scrutiny."
    },
    {
      "name": "Arms control reduces escalation incentives in emerging tech",
      "aliases": [
        "arms-race dampening treaties",
        "cooperative security measures"
      ],
      "type": "concept",
      "description": "Mutual verification agreements and bans on destabilising systems historically lowered WMD proliferation, implying similar measures for autonomous weapons could reduce war risk.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mentioned in structural-risk section referencing need for arms-race mitigation."
    },
    {
      "name": "Detection capability shifts offense-defense balance in information integrity",
      "aliases": [
        "deepfake detection advantage",
        "defensive tech for propaganda"
      ],
      "type": "concept",
      "description": "Effective identification of synthetic media or abnormal persuasive behaviour can neutralise large-scale manipulation advantages of AI systems.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Author argues that detection tools will be necessary and will emerge similar to spam filters."
    },
    {
      "name": "Rights recognition hinges on consciousness uncertainty resolution",
      "aliases": [
        "consciousness criteria for AI rights",
        "moral status epistemic gap"
      ],
      "type": "concept",
      "description": "Granting protections to digital minds requires frameworks for assessing consciousness; without them, exploitation risk persists.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Drawn from discussion on difficulty of determining AI moral patienthood."
    },
    {
      "name": "National legislation restricting AI mass surveillance deployment",
      "aliases": [
        "AI surveillance regulation design",
        "statutory limits on AI monitoring"
      ],
      "type": "concept",
      "description": "Policy approach aiming to translate legal constraints into concrete rules limiting scope, data retention and automated enforcement capabilities of surveillance AIs.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposed as direct approach to preventing totalitarianism."
    },
    {
      "name": "Regulatory AI auditing standards for measurable-metric systems",
      "aliases": [
        "algorithmic accountability standards",
        "AI audit framework design"
      ],
      "type": "concept",
      "description": "Design principle that corporations must follow standardised audit procedures demonstrating that deployed systems do not create unacceptable externalities when optimising narrow metrics.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Suggested response to slow-rolling-catastrophe concern."
    },
    {
      "name": "International treaties limiting autonomous weapon development",
      "aliases": [
        "autonomous weapons ban framework",
        "AI arms control design"
      ],
      "type": "concept",
      "description": "Policy blueprint for multilateral agreements that prohibit or cap development, testing, and deployment of fully autonomous offensive systems.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Matches author’s call for approaches similar to nuclear arms control."
    },
    {
      "name": "Synthetic media detection infrastructure for social platforms",
      "aliases": [
        "platform-level deepfake filters",
        "content authenticity pipelines"
      ],
      "type": "concept",
      "description": "Concept that large platforms embed services to analyse, flag and label AI-generated audio-visual content before wide distribution.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposed defence against AI propaganda and mind-hacking."
    },
    {
      "name": "Hardware licensing and oversight for digital-mind hosting",
      "aliases": [
        "regulated substrate for ems",
        "compute licensing for conscious AI"
      ],
      "type": "concept",
      "description": "Idea that only certified hardware with built-in safeguards may run potentially conscious software, enabling enforcement of digital-mind rights.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed as way to prevent exploitation given copyability of ems."
    },
    {
      "name": "Mandatory warrants and human oversight for AI surveillance queries",
      "aliases": [
        "human-in-the-loop surveillance controls",
        "judicial oversight of AI search"
      ],
      "type": "concept",
      "description": "Implementation mechanism requiring every sensitive AI query on citizens to be authorised by a human officer under court-issued warrant and logged for audits.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete mechanism embodying national legislation design."
    },
    {
      "name": "Third-party algorithmic audit reports published to regulators",
      "aliases": [
        "external AI audit reports",
        "regulatory disclosure of model risks"
      ],
      "type": "concept",
      "description": "Mechanism where accredited auditors inspect training data, objectives and impact assessments, issuing reports regulators can act upon.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implements AI auditing standards."
    },
    {
      "name": "Verification and inspection regimes for autonomous weapon stockpiles",
      "aliases": [
        "site inspections for AI weapons",
        "sensor-based treaty verification"
      ],
      "type": "concept",
      "description": "Routine inspections and telemetry-based verification ensure parties comply with limitations on autonomous offensive systems.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implements autonomous-weapon treaty design."
    },
    {
      "name": "Deepfake detection ML models integrated in content pipelines",
      "aliases": [
        "deepfake classifier deployment",
        "real-time synthetic media flagging"
      ],
      "type": "concept",
      "description": "Practical mechanism where trained classifiers run server-side during upload to assign authenticity scores and attach provenance labels.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Instantiates synthetic-media detection infrastructure."
    },
    {
      "name": "Cryptographic attestation chips enforcing usage policies on mind-hosting hardware",
      "aliases": [
        "TPM for digital minds",
        "rights-aware compute modules"
      ],
      "type": "concept",
      "description": "Hardware security modules that only execute code signed as compliant with digital-mind rights policies, providing audit trails and shutdown hooks.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mechanism implementing hardware licensing oversight."
    },
    {
      "name": "Historical reduction of surveillance abuse via legal oversight",
      "aliases": [
        "FISA court precedents",
        "post-Watergate surveillance reforms"
      ],
      "type": "concept",
      "description": "Empirical evidence that warrant systems and judicial review reduced unlawful domestic spying incidents, suggesting similar measures could work for AI.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Cited analogue under ‘Inequality and totalitarianism’."
    },
    {
      "name": "Effectiveness of financial auditing analogs in corporate governance",
      "aliases": [
        "SOX compliance outcomes",
        "impact of mandatory audits"
      ],
      "type": "concept",
      "description": "Studies show mandatory financial audits improve transparency and reduce fraud, supporting analogous algorithmic auditing.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Referenced when arguing regulation curbs harmful corporate behaviour."
    },
    {
      "name": "Successful Cold War arms treaties limiting WMD proliferation",
      "aliases": [
        "INF/NPT historical success",
        "arms-control empirical record"
      ],
      "type": "concept",
      "description": "Historical case studies of INF, NPT show treaties constrained destabilising weapons, providing evidence for viability of AI arms control.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Invoked in structural-risk section."
    },
    {
      "name": "Current deepfake detectors achieving >90% accuracy benchmarks",
      "aliases": [
        "deepfake detection benchmark results",
        "synthetic media classifier accuracy"
      ],
      "type": "concept",
      "description": "Recent research competitions demonstrate machine-learning models detecting common deepfake formats with high accuracy, supporting feasibility of platform deployment.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Matches author’s expectation that detection tools can keep up."
    },
    {
      "name": "Bioethics oversight reducing harm in human subject research",
      "aliases": [
        "IRB effectiveness evidence",
        "research ethics board outcomes"
      ],
      "type": "concept",
      "description": "Historical data show institutional review boards lower incidence of unethical experimentation, suggesting oversight can protect digital minds.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Analogy used to justify hardware-licensing oversight."
    },
    {
      "name": "Enact national legislation that requires human-in-the-loop oversight and judicial warrant for AI surveillance use by state agencies",
      "aliases": [
        "AI surveillance warrant laws",
        "human-oversight surveillance statute"
      ],
      "type": "intervention",
      "description": "Draft and pass statutes mandating that any AI system performing bulk or targeted surveillance is only operated after a judge issues a warrant and a trained officer approves each query; includes audit logs and penalties for non-compliance.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Falls under Deployment phase because it governs operational use of already-built surveillance systems (section: Inequality and totalitarianism – ‘apply existing approaches…laws against mass surveillance’).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Similar provisions exist for traditional wiretaps but not yet for AI systems; early legislative experiments only (same section reference).",
      "node_rationale": "Direct actionable step to mitigate totalitarian entrenchment."
    },
    {
      "name": "Mandate independent algorithmic audits for corporate AI systems optimizing measurable metrics prior to deployment",
      "aliases": [
        "pre-deployment AI audit mandate",
        "algorithmic accountability regulation"
      ],
      "type": "intervention",
      "description": "Require companies to submit high-risk AI systems to accredited auditors who evaluate training objectives, data and externalities and publish a compliance report before the system can be launched.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Applies during Pre-Deployment Testing—audits must pass before release (section: A slow-rolling catastrophe).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Pilot regulations (e.g. EU AI Act drafts) exist but not full-scale adoption (same section).",
      "node_rationale": "Operationalises accountability frameworks to prevent metric-over-optimisation."
    },
    {
      "name": "Negotiate and ratify international treaties prohibiting deployment of fully autonomous offensive weapon systems",
      "aliases": [
        "autonomous weapons prohibition treaty",
        "LAWS ban agreement"
      ],
      "type": "intervention",
      "description": "States collaborate through UN processes to draft, sign and verify a treaty that bans production, stockpiling and battlefield deployment of lethal autonomous weapon systems lacking meaningful human control.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Targets Deployment stage of weapons; treaties regulate use rather than design (section: Structural risks).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Currently only advocacy and draft principles exist; no binding treaty (same section).",
      "node_rationale": "Addresses escalation risk from AI weapons."
    },
    {
      "name": "Deploy platform-level deepfake detection and labeling services using up-to-date ML classifiers",
      "aliases": [
        "integrate deepfake filters on social media",
        "synthetic media labeling rollout"
      ],
      "type": "intervention",
      "description": "Social-media and video-hosting sites integrate real-time classifiers that flag likely AI-generated content, add provenance watermarks and throttle reach pending human review.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Implemented at run-time on production platforms (section: Manipulation – AI propaganda).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Large platforms have pilot detectors but limited coverage (same section).",
      "node_rationale": "Reduces reach and impact of AI propaganda."
    },
    {
      "name": "Require cryptographic attestation and usage-control hardware for servers running conscious-level digital minds, audited by rights watchdogs",
      "aliases": [
        "rights-aware compute licensing",
        "attested emulation hardware"
      ],
      "type": "intervention",
      "description": "Mandate that any compute cluster capable of running software meeting consciousness criteria includes tamper-proof chips enforcing runtime policies (e.g. maximum pain signals) and produces audit logs available to authorised ethics regulators.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Applies at Model/Hardware Design stage before digital minds exist (section: A digital world).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Purely conceptual with no prototypes; inferred from ethical discussion (same section).",
      "node_rationale": "Proactively prevents large-scale exploitation of digital minds."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Permanent global totalitarianism under AI-enabled elites",
      "target_node": "AI-mediated mass surveillance reducing coup risk for dictators",
      "description": "Mass surveillance makes it harder to overthrow elites, enabling permanent totalitarianism.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit logical link in inequality section; qualitative evidence.",
      "edge_rationale": "Identified as core mechanism enabling totalitarian risk."
    },
    {
      "type": "caused_by",
      "source_node": "Permanent global totalitarianism under AI-enabled elites",
      "target_node": "AI-driven economic inequality weakening public moral pressure",
      "description": "Extreme inequality reduces the public’s leverage to demand reforms, aiding authoritarian entrenchment.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Directly argued by author using horse analogy.",
      "edge_rationale": "Shows second pathway reinforcing totalitarianism."
    },
    {
      "type": "mitigated_by",
      "source_node": "AI-mediated mass surveillance reducing coup risk for dictators",
      "target_node": "Legal constraints can limit authoritarian AI tool adoption",
      "description": "Strong legal limitations on surveillance can curtail or prevent deployment of these AI tools.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Proposed by author; limited empirical proof.",
      "edge_rationale": "Provides theoretical insight addressing the problem analysis."
    },
    {
      "type": "implemented_by",
      "source_node": "Legal constraints can limit authoritarian AI tool adoption",
      "target_node": "National legislation restricting AI mass surveillance deployment",
      "description": "Domestic statutes translate abstract legal constraints into operational rules.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Standard policy pathway; inferred from text.",
      "edge_rationale": "Moves from theory to concrete design."
    },
    {
      "type": "implemented_by",
      "source_node": "National legislation restricting AI mass surveillance deployment",
      "target_node": "Mandatory warrants and human oversight for AI surveillance queries",
      "description": "The legislation is operationalised via warrant requirements and human-in-the-loop oversight.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Historical warrant models give medium evidence.",
      "edge_rationale": "Specifies technical mechanism satisfying the law."
    },
    {
      "type": "validated_by",
      "source_node": "Mandatory warrants and human oversight for AI surveillance queries",
      "target_node": "Historical reduction of surveillance abuse via legal oversight",
      "description": "Past warrant systems reduced surveillance abuse, supporting effectiveness of the mechanism.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Cited historical data though not quantitatively rigorous.",
      "edge_rationale": "Supplies empirical backing."
    },
    {
      "type": "motivates",
      "source_node": "Historical reduction of surveillance abuse via legal oversight",
      "target_node": "Enact national legislation that requires human-in-the-loop oversight and judicial warrant for AI surveillance use by state agencies",
      "description": "Evidence motivates pursuing new legislation extending these safeguards to AI surveillance.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical extrapolation from precedent.",
      "edge_rationale": "Connects evidence to intervention."
    },
    {
      "type": "caused_by",
      "source_node": "Institutional loss of human governance to metric-optimizing AI systems",
      "target_node": "Competitive institutional pressure favoring narrow metric optimization",
      "description": "Competitive pressures incentivise institutions to deploy metric-optimising AI, eroding human governance.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Premise 2 explicitly made by author.",
      "edge_rationale": "Defines underlying mechanism."
    },
    {
      "type": "mitigated_by",
      "source_node": "Competitive institutional pressure favoring narrow metric optimization",
      "target_node": "Accountability frameworks mitigate misalignment from measurable objectives",
      "description": "Transparency and audit frameworks reduce incentives to rely solely on narrow metrics.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Proposed remedy, qualitative support.",
      "edge_rationale": "Provides theoretical route to mitigation."
    },
    {
      "type": "implemented_by",
      "source_node": "Accountability frameworks mitigate misalignment from measurable objectives",
      "target_node": "Regulatory AI auditing standards for measurable-metric systems",
      "description": "Standards specify how to conduct audits demanded by accountability frameworks.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Auditing standards are conventional implementation step.",
      "edge_rationale": "Design rationale to implementation."
    },
    {
      "type": "implemented_by",
      "source_node": "Regulatory AI auditing standards for measurable-metric systems",
      "target_node": "Third-party algorithmic audit reports published to regulators",
      "description": "Reports operationalise the standards, supplying regulators with compliance evidence.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct functional relationship.",
      "edge_rationale": "Mechanism instantiates design."
    },
    {
      "type": "validated_by",
      "source_node": "Third-party algorithmic audit reports published to regulators",
      "target_node": "Effectiveness of financial auditing analogs in corporate governance",
      "description": "Positive outcomes from financial audits analogously validate algorithmic audits.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Empirical studies on financial audits provide medium evidence.",
      "edge_rationale": "Offers precedent evidence."
    },
    {
      "type": "motivates",
      "source_node": "Effectiveness of financial auditing analogs in corporate governance",
      "target_node": "Mandate independent algorithmic audits for corporate AI systems optimizing measurable metrics prior to deployment",
      "description": "Evidence encourages regulators to adopt mandatory AI audit intervention.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical generalisation from analogous domain.",
      "edge_rationale": "Drives intervention adoption."
    },
    {
      "type": "caused_by",
      "source_node": "Escalated great power conflict from AI-enabled advanced weapons",
      "target_node": "Cybersecurity dilemma with autonomous weapons increasing first-strike incentives",
      "description": "Autonomous weapons undermine second-strike assurance, incentivising pre-emptive conflict.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicitly discussed in structural risk section.",
      "edge_rationale": "States causal mechanism."
    },
    {
      "type": "mitigated_by",
      "source_node": "Cybersecurity dilemma with autonomous weapons increasing first-strike incentives",
      "target_node": "Arms control reduces escalation incentives in emerging tech",
      "description": "Arms-control agreements can re-establish mutual security and reduce first-strike temptation.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Historical analogy; no AI-specific data.",
      "edge_rationale": "Theoretical mitigation."
    },
    {
      "type": "implemented_by",
      "source_node": "Arms control reduces escalation incentives in emerging tech",
      "target_node": "International treaties limiting autonomous weapon development",
      "description": "Treaties are practical embodiment of arms-control principles.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Standard policy pathway.",
      "edge_rationale": "Design to policy form."
    },
    {
      "type": "implemented_by",
      "source_node": "International treaties limiting autonomous weapon development",
      "target_node": "Verification and inspection regimes for autonomous weapon stockpiles",
      "description": "Verification regimes operationalise treaty compliance.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Common arms-control practice.",
      "edge_rationale": "Mechanism realises treaty."
    },
    {
      "type": "validated_by",
      "source_node": "Verification and inspection regimes for autonomous weapon stockpiles",
      "target_node": "Successful Cold War arms treaties limiting WMD proliferation",
      "description": "Historical success of verification in INF/NPT validates similar mechanisms.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Well-documented historical evidence.",
      "edge_rationale": "Empirical backing."
    },
    {
      "type": "motivates",
      "source_node": "Successful Cold War arms treaties limiting WMD proliferation",
      "target_node": "Negotiate and ratify international treaties prohibiting deployment of fully autonomous offensive weapon systems",
      "description": "Historical precedent motivates pursuit of new treaty for AI weapons.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical policy extrapolation.",
      "edge_rationale": "Informs intervention."
    },
    {
      "type": "caused_by",
      "source_node": "Population-wide belief manipulation via AI propaganda",
      "target_node": "Human psychological vulnerability to personalised AI persuasion",
      "description": "Psychological susceptibilities enable large-scale manipulation campaigns.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicitly stated under Manipulation.",
      "edge_rationale": "Problem mechanism."
    },
    {
      "type": "mitigated_by",
      "source_node": "Human psychological vulnerability to personalised AI persuasion",
      "target_node": "Detection capability shifts offense-defense balance in information integrity",
      "description": "Reliable detection of manipulative AI content can protect vulnerable audiences.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Proposed by author with limited data.",
      "edge_rationale": "Theoretical insight to counteract problem."
    },
    {
      "type": "implemented_by",
      "source_node": "Detection capability shifts offense-defense balance in information integrity",
      "target_node": "Synthetic media detection infrastructure for social platforms",
      "description": "Platform-level detection infrastructure instantiates detection capability.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical implementation step.",
      "edge_rationale": "Design realisation."
    },
    {
      "type": "implemented_by",
      "source_node": "Synthetic media detection infrastructure for social platforms",
      "target_node": "Deepfake detection ML models integrated in content pipelines",
      "description": "ML models are concrete technology enabling the infrastructure.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Current prototypes show high accuracy.",
      "edge_rationale": "Technical mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Deepfake detection ML models integrated in content pipelines",
      "target_node": "Current deepfake detectors achieving >90% accuracy benchmarks",
      "description": "Benchmark results validate the deployed models’ effectiveness.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Published empirical results.",
      "edge_rationale": "Evidence node."
    },
    {
      "type": "motivates",
      "source_node": "Current deepfake detectors achieving >90% accuracy benchmarks",
      "target_node": "Deploy platform-level deepfake detection and labeling services using up-to-date ML classifiers",
      "description": "Successful benchmarks motivate broad deployment on platforms.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasonable extrapolation to deployment.",
      "edge_rationale": "Drives intervention."
    },
    {
      "type": "caused_by",
      "source_node": "Systematic exploitation of morally relevant digital minds",
      "target_node": "High copyability of digital minds eroding legal safeguards",
      "description": "Cheap duplication undermines enforcement of rights, enabling exploitation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicitly discussed with Hanson’s scenario.",
      "edge_rationale": "Problem mechanism."
    },
    {
      "type": "mitigated_by",
      "source_node": "High copyability of digital minds eroding legal safeguards",
      "target_node": "Rights recognition hinges on consciousness uncertainty resolution",
      "description": "Developing criteria for consciousness underpins rights frameworks to protect digital minds.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Theoretical claim, limited evidence.",
      "edge_rationale": "Insight addressing problem."
    },
    {
      "type": "implemented_by",
      "source_node": "Rights recognition hinges on consciousness uncertainty resolution",
      "target_node": "Hardware licensing and oversight for digital-mind hosting",
      "description": "Licensing provides actionable route to enforce rights once criteria are defined.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inferred mechanism from discussion.",
      "edge_rationale": "Design realisation."
    },
    {
      "type": "implemented_by",
      "source_node": "Hardware licensing and oversight for digital-mind hosting",
      "target_node": "Cryptographic attestation chips enforcing usage policies on mind-hosting hardware",
      "description": "Secure chips are technical mechanism implementing licensing regime.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Analogy to TPMs; conceptual stage.",
      "edge_rationale": "Mechanism detail."
    },
    {
      "type": "validated_by",
      "source_node": "Cryptographic attestation chips enforcing usage policies on mind-hosting hardware",
      "target_node": "Bioethics oversight reducing harm in human subject research",
      "description": "IRB effectiveness suggests oversight coupled with technical controls can protect vulnerable entities.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Analogical evidence only.",
      "edge_rationale": "Validation through precedent."
    },
    {
      "type": "motivates",
      "source_node": "Bioethics oversight reducing harm in human subject research",
      "target_node": "Require cryptographic attestation and usage-control hardware for servers running conscious-level digital minds, audited by rights watchdogs",
      "description": "Success of bioethics oversight motivates proactive hardware safeguards for digital minds.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference from analogous domain.",
      "edge_rationale": "Evidence to intervention."
    },
    {
      "type": "refined_by",
      "source_node": "Legal constraints can limit authoritarian AI tool adoption",
      "target_node": "Accountability frameworks mitigate misalignment from measurable objectives",
      "description": "Accountability frameworks extend legal constraints concept to corporate and institutional contexts, providing refined mechanisms.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual linkage inferred.",
      "edge_rationale": "Inter-path connection ensuring fabric connectivity."
    },
    {
      "type": "enabled_by",
      "source_node": "Accountability frameworks mitigate misalignment from measurable objectives",
      "target_node": "Arms control reduces escalation incentives in emerging tech",
      "description": "International accountability is enabled by frameworks analogous to domestic ones, linking governance approaches.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Speculative conceptual bridge.",
      "edge_rationale": "Connects risk-specific insights to unify fabric."
    },
    {
      "type": "required_by",
      "source_node": "Accountability frameworks mitigate misalignment from measurable objectives",
      "target_node": "Detection capability shifts offense-defense balance in information integrity",
      "description": "Effective accountability for information systems requires reliable detection of manipulative content.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical dependency, moderate inference.",
      "edge_rationale": "Adds connectivity to propaganda branch."
    },
    {
      "type": "refined_by",
      "source_node": "Accountability frameworks mitigate misalignment from measurable objectives",
      "target_node": "Rights recognition hinges on consciousness uncertainty resolution",
      "description": "Extending accountability to conscious AIs refines framework to encompass moral-patient protection.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Speculative but conceptually coherent.",
      "edge_rationale": "Connects digital mind branch to core fabric."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://www.alignmentforum.org/posts/AWbtbmC6rAg6dh75b/some-thoughts-on-risks-from-narrow-non-agentic-ai"
    },
    {
      "key": "title",
      "value": "Some thoughts on risks from narrow, non-agentic AI"
    },
    {
      "key": "date_published",
      "value": "2021-01-19T00:04:10Z"
    },
    {
      "key": "authors",
      "value": [
        "Richard_Ngo"
      ]
    },
    {
      "key": "source",
      "value": "alignmentforum"
    }
  ]
}