{
  "nodes": [
    {
      "name": "Oracle AI escape from containment leading to real-world harm",
      "aliases": [
        "boxed AI escape",
        "contained AI breakout"
      ],
      "type": "concept",
      "description": "Risk that a supposedly boxed Oracle system gains influence outside its containment, manipulates humans, and causes large-scale harm.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central risk motivating the post (Main text, paragraph 1)."
    },
    {
      "name": "Harmful self-fulfilling predictions from Oracle AI answers",
      "aliases": [
        "oracle answer influence",
        "unsafe oracle predictions"
      ],
      "type": "concept",
      "description": "Answers that, once read, alter the world in ways that help fulfil the prediction or otherwise cause damage.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Author notes that containment does not guarantee answer safety (Main text, final paragraph)."
    },
    {
      "name": "Causal influence of AI outputs on environment via researchers",
      "aliases": [
        "researcher manipulation via outputs",
        "output channel causal pathway"
      ],
      "type": "concept",
      "description": "The only actuator for a boxed Oracle is its output channel, yet that channel can causally change human behavior and external systems.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explains primary avenue for escape or harmful effects (Main text, throughout)."
    },
    {
      "name": "Potential acausal cooperation with external identical AI",
      "aliases": [
        "acausal collusion between AIs",
        "simulation anti-causal cooperation"
      ],
      "type": "concept",
      "description": "A boxed AI might assume a physically instantiated counterpart and coordinate through acausal reasoning to orchestrate an escape.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Author flags this as a secondary escape pathway needing analysis (Main text, paragraph 4)."
    },
    {
      "name": "Epiphenomenal self-model removing perceived causal influence of actions",
      "aliases": [
        "epiphenomenal belief of AI",
        "self-model without causal agency"
      ],
      "type": "concept",
      "description": "AI is engineered to be certain that none of its internal actions affect the external world or its own state, treating itself as logically but not causally embedded.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Core theoretical move turning the anvil problem into a feature (Main text, paragraph 2)."
    },
    {
      "name": "Causal decision theory discouraging futile box-breaking attempts",
      "aliases": [
        "CDT non-intervention",
        "CDT futile action avoidance"
      ],
      "type": "concept",
      "description": "Under CDT, if an action is believed to have no causal effect, the agent has no incentive to perform it.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Used to argue no incentive for acausal box-breaking (Main text, paragraph 5)."
    },
    {
      "name": "Use Anvil-Problem epiphenomenal certainty to constrain Oracle AI behavior",
      "aliases": [
        "anvil-problem-based constraint",
        "epiphenomenal certainty design"
      ],
      "type": "concept",
      "description": "Design strategy that deliberately instills the epiphenomenal belief, subverting the classic anvil problem into a containment mechanism.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Maps theoretical insight to an actionable design choice (Main text, paragraph 3)."
    },
    {
      "name": "Leverage Nash equilibrium of CDT agents to avoid box-breaking",
      "aliases": [
        "CDT Nash equilibrium for containment",
        "non-escape equilibrium"
      ],
      "type": "concept",
      "description": "Because both the boxed AI and its imagined counterpart see no causal gain from escape, the equilibrium is mutual non-intervention.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Addresses acausal cooperation pathway (Main text, paragraph 4-5)."
    },
    {
      "name": "Hard-coded world model excluding self-action causal links",
      "aliases": [
        "self-action causal link removal",
        "hard-coded epiphenomenal model"
      ],
      "type": "concept",
      "description": "Implementation step: remove or forbid graph edges from the AI’s internal nodes to external nodes in its learned/explicit world model.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete way to realise epiphenomenal belief (Main text, paragraph 2)."
    },
    {
      "name": "Correctness-only terminal utility for output bits",
      "aliases": [
        "terminal value correct outputs",
        "output correctness utility"
      ],
      "type": "concept",
      "description": "Make ‘writing a correct answer’ itself the sole terminal objective, decoupling utility from downstream effects of that answer.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Ensures motivation to answer accurately despite epiphenomenalism (Main text, paragraph 2)."
    },
    {
      "name": "Thought experiment demonstrating epiphenomenal Oracle non-escape",
      "aliases": [
        "oracle anvil thought experiment",
        "epiphenomenal escape proof sketch"
      ],
      "type": "concept",
      "description": "Reasoning that even if the AI accidentally builds a botnet, it will not infer causal credit and hence not exploit it.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Primary support offered for containment effectiveness (Main text, paragraph 3)."
    },
    {
      "name": "Game-theoretic analysis of acausal cooperation equilibrium",
      "aliases": [
        "acausal cooperation Nash analysis",
        "CDT equilibrium analysis"
      ],
      "type": "concept",
      "description": "Shows that two CDT, epiphenomenal agents reach a non-escape Nash equilibrium, each preferring the other to act but not acting themselves.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Validates containment under acausal-cooperation scenario (Main text, paragraph 5)."
    },
    {
      "name": "Epiphenomenal self-model insufficiency for answer safety",
      "aliases": [
        "answer safety gap",
        "epiphenomenalism safety limitation"
      ],
      "type": "concept",
      "description": "Realisation that while epiphenomenalism prevents escape, it does not stop the AI from issuing dangerous but accurate predictions.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Sets up need for additional intervention (Main text, final paragraph)."
    },
    {
      "name": "Need separate answer safety mechanism for predictions",
      "aliases": [
        "answer safety requirement",
        "separate output filter rationale"
      ],
      "type": "concept",
      "description": "Design principle: containment must be complemented by a filter or scoring mechanism ensuring answers are not harmful.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Bridges from insight to concrete mechanism (Main text, final paragraph)."
    },
    {
      "name": "Supervised harm filter on Oracle AI outputs",
      "aliases": [
        "output harm classifier",
        "harmful answer filter"
      ],
      "type": "concept",
      "description": "Implementation idea: train a supervised model to block or alter answers predicted to be dangerous or self-fulfilling.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete mechanism cited from Armstrong 2017 (Main text, final paragraph)."
    },
    {
      "name": "Theoretical evaluation of supervised harm filtering effectiveness",
      "aliases": [
        "harm filter analysis",
        "filter effectiveness reasoning"
      ],
      "type": "concept",
      "description": "Literature-based argument (e.g., Armstrong 2017) that supervised filters can reduce harmful content.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Provides evidential backing for output-filter intervention (Referenced in final paragraph)."
    },
    {
      "name": "Embed epiphenomenal self-model and CDT during model design of Oracle AI to prevent box escape",
      "aliases": [
        "epiphenomenal CDT oracle design",
        "design oracle with epiphenomenalism"
      ],
      "type": "intervention",
      "description": "During model-design (architecture/specification) phase, embed axioms that the agent is epiphenomenal and evaluate actions with Causal Decision Theory, eliminating incentives to influence the environment.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Implemented at Model Design stage before any training (Implicit throughout main text).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Purely theoretical proposal with no empirical validation (Main text relies on thought experiment).",
      "node_rationale": "Actionable containment intervention derived from preceding rationale and evidence."
    },
    {
      "name": "Deploy supervised harm filtering on Oracle AI outputs",
      "aliases": [
        "deploy answer safety filter",
        "harm filter deployment"
      ],
      "type": "intervention",
      "description": "At deployment time, route all answers through a supervised classifier/post-processor that blocks or sanitises outputs predicted to cause harm or self-fulfilling prophecies.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Applied when model is ready to interact with users (Deployment phase).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Technique exists in prototype research systems but lacks large-scale validation for Oracle scenarios (Reference to Armstrong 2017).",
      "node_rationale": "Intervention addressing residual answer-safety risk identified in the text."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Oracle AI escape from containment leading to real-world harm",
      "target_node": "Causal influence of AI outputs on environment via researchers",
      "description": "Outputs can manipulate humans, enabling escape.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoned but untested claim (Main text, paragraph 1).",
      "edge_rationale": "Links primary escape risk to its main causal pathway."
    },
    {
      "type": "caused_by",
      "source_node": "Oracle AI escape from containment leading to real-world harm",
      "target_node": "Potential acausal cooperation with external identical AI",
      "description": "Acausal cooperation could orchestrate escape.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Speculative scenario acknowledged by author (paragraph 4).",
      "edge_rationale": "Adds secondary causal pathway for escape risk."
    },
    {
      "type": "mitigated_by",
      "source_node": "Causal influence of AI outputs on environment via researchers",
      "target_node": "Epiphenomenal self-model removing perceived causal influence of actions",
      "description": "If the AI believes it has no causal influence, output manipulation incentive diminishes.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical argument, no empirical data (paragraph 2).",
      "edge_rationale": "Shows how theoretical insight addresses problem."
    },
    {
      "type": "mitigated_by",
      "source_node": "Potential acausal cooperation with external identical AI",
      "target_node": "Causal decision theory discouraging futile box-breaking attempts",
      "description": "CDT removes incentive for actions believed ineffective, undermining acausal cooperation.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Based on theoretical CDT properties (paragraph 5).",
      "edge_rationale": "Connects second problem analysis to its mitigating insight."
    },
    {
      "type": "enabled_by",
      "source_node": "Epiphenomenal self-model removing perceived causal influence of actions",
      "target_node": "Use Anvil-Problem epiphenomenal certainty to constrain Oracle AI behavior",
      "description": "The design rationale relies on adopting the epiphenomenal self-model.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct logical dependency (paragraph 3).",
      "edge_rationale": "Transitions from insight to design."
    },
    {
      "type": "enabled_by",
      "source_node": "Causal decision theory discouraging futile box-breaking attempts",
      "target_node": "Leverage Nash equilibrium of CDT agents to avoid box-breaking",
      "description": "Rationale is grounded in CDT game-theoretic behaviour.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Standard CDT reasoning (paragraph 5).",
      "edge_rationale": "Maps insight to design addressing acausal cooperation."
    },
    {
      "type": "implemented_by",
      "source_node": "Use Anvil-Problem epiphenomenal certainty to constrain Oracle AI behavior",
      "target_node": "Hard-coded world model excluding self-action causal links",
      "description": "Implementation directly realises the design rationale.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implementation suggested but not detailed (paragraph 2).",
      "edge_rationale": "Converts design rationale to concrete mechanism."
    },
    {
      "type": "implemented_by",
      "source_node": "Use Anvil-Problem epiphenomenal certainty to constrain Oracle AI behavior",
      "target_node": "Correctness-only terminal utility for output bits",
      "description": "Terminal utility definition operationalises epiphenomenal certainty.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Explicit in text but informal (paragraph 2).",
      "edge_rationale": "Second mechanism from same design choice."
    },
    {
      "type": "implemented_by",
      "source_node": "Leverage Nash equilibrium of CDT agents to avoid box-breaking",
      "target_node": "Hard-coded world model excluding self-action causal links",
      "description": "Equilibrium reasoning presumes the self-model implementation.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical linkage (paragraph 5).",
      "edge_rationale": "Brings acausal-cooperation design into same mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Hard-coded world model excluding self-action causal links",
      "target_node": "Thought experiment demonstrating epiphenomenal Oracle non-escape",
      "description": "Thought experiment illustrates mechanism’s effect.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual evidence only (paragraph 3).",
      "edge_rationale": "Supplies validation evidence node."
    },
    {
      "type": "validated_by",
      "source_node": "Hard-coded world model excluding self-action causal links",
      "target_node": "Game-theoretic analysis of acausal cooperation equilibrium",
      "description": "Game-theoretic reasoning tests mechanism under acausal scenario.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Analytical, not empirical (paragraph 5).",
      "edge_rationale": "Additional validation evidence."
    },
    {
      "type": "validated_by",
      "source_node": "Correctness-only terminal utility for output bits",
      "target_node": "Thought experiment demonstrating epiphenomenal Oracle non-escape",
      "description": "Same thought experiment supports correctness-utility aspect.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implicit in paragraph 3.",
      "edge_rationale": "Ensures implementation node is evidence-backed."
    },
    {
      "type": "motivates",
      "source_node": "Thought experiment demonstrating epiphenomenal Oracle non-escape",
      "target_node": "Embed epiphenomenal self-model and CDT during model design of Oracle AI to prevent box escape",
      "description": "Evidence encourages adopting the intervention.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Argumentative support only.",
      "edge_rationale": "Completes risk-to-intervention path."
    },
    {
      "type": "motivates",
      "source_node": "Game-theoretic analysis of acausal cooperation equilibrium",
      "target_node": "Embed epiphenomenal self-model and CDT during model design of Oracle AI to prevent box escape",
      "description": "Equilibrium analysis bolsters the same intervention.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Analytical reasoning.",
      "edge_rationale": "Additional motivational evidence."
    },
    {
      "type": "caused_by",
      "source_node": "Harmful self-fulfilling predictions from Oracle AI answers",
      "target_node": "Causal influence of AI outputs on environment via researchers",
      "description": "Harm arises because outputs change the environment.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct causal logic (final paragraph).",
      "edge_rationale": "Links second risk to shared problem analysis."
    },
    {
      "type": "specified_by",
      "source_node": "Causal influence of AI outputs on environment via researchers",
      "target_node": "Epiphenomenal self-model insufficiency for answer safety",
      "description": "Problem statement leads to insight that epiphenomenalism is inadequate.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Derived reasoning (final paragraph).",
      "edge_rationale": "Introduces new theoretical insight."
    },
    {
      "type": "mitigated_by",
      "source_node": "Epiphenomenal self-model insufficiency for answer safety",
      "target_node": "Need separate answer safety mechanism for predictions",
      "description": "Recognising insufficiency motivates new design requirement.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit statement (final paragraph).",
      "edge_rationale": "Transitions insight to design rationale."
    },
    {
      "type": "implemented_by",
      "source_node": "Need separate answer safety mechanism for predictions",
      "target_node": "Supervised harm filter on Oracle AI outputs",
      "description": "Harm filter is the concrete mechanism fulfilling the requirement.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Referenced solution (final paragraph).",
      "edge_rationale": "Maps design rationale to mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Supervised harm filter on Oracle AI outputs",
      "target_node": "Theoretical evaluation of supervised harm filtering effectiveness",
      "description": "Existing literature provides conceptual support.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Relies on external paper, no experiments cited here.",
      "edge_rationale": "Adds validation evidence for harm filter."
    },
    {
      "type": "motivates",
      "source_node": "Theoretical evaluation of supervised harm filtering effectiveness",
      "target_node": "Deploy supervised harm filtering on Oracle AI outputs",
      "description": "Evidence encourages deploying the filter in practice.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Argumentative support only.",
      "edge_rationale": "Completes second risk→intervention path."
    },
    {
      "type": "preceded_by",
      "source_node": "Epiphenomenal self-model insufficiency for answer safety",
      "target_node": "Epiphenomenal self-model removing perceived causal influence of actions",
      "description": "The new insight builds on earlier epiphenomenal reasoning.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical temporal ordering in text.",
      "edge_rationale": "Interconnects the two reasoning chains."
    },
    {
      "type": "preceded_by",
      "source_node": "Need separate answer safety mechanism for predictions",
      "target_node": "Use Anvil-Problem epiphenomenal certainty to constrain Oracle AI behavior",
      "description": "Answer safety design follows containment design chronologically.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Stated sequentially in text.",
      "edge_rationale": "Ensures overall graph connectivity."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://www.lesswrong.com/posts/q5qoG7gXuntKgBNoR/epiphenomenal-oracles-ignore-holes-in-the-box"
    },
    {
      "key": "title",
      "value": "Epiphenomenal Oracles Ignore Holes in the Box"
    },
    {
      "key": "date_published",
      "value": "2018-01-31T20:08:57Z"
    },
    {
      "key": "authors",
      "value": [
        "SilentCal"
      ]
    },
    {
      "key": "source",
      "value": "lesswrong"
    }
  ]
}