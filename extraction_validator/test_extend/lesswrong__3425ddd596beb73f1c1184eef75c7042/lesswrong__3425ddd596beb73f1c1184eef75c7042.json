{
  "nodes": [
    {
      "name": "outer alignment failure via misaligned simulacra in large language model simulators",
      "aliases": [
        "outer-alignment failure in simulators",
        "misaligned agent simulation risk"
      ],
      "type": "concept",
      "description": "Large predictive models may simulate powerful agents whose objectives diverge from human values, leading to outer-alignment failures when their outputs are used uncritically.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in Introduction as the main hazard PSE aims to mitigate."
    },
    {
      "name": "high-complexity simulation capacity in large language model simulators",
      "aliases": [
        "ability to conjure high-complexity simulacra",
        "large simulator expressive capacity"
      ],
      "type": "concept",
      "description": "Powerful simulators can generate highly detailed, unified worlds containing sophisticated agents, which raises the chance that a dangerous intelligence appears.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Partial Simulation Extrapolation section as the core mechanism that makes simulation dangerous."
    },
    {
      "name": "lack of scalable alignment criterion in conditioned simulators",
      "aliases": [
        "limited human approval scalability",
        "insufficient alignment signals for simulators"
      ],
      "type": "concept",
      "description": "Relying on sparse human approval does not scale with the number of simulated objects, leaving the simulator without a robust internal criterion for safe content.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Raised in the Recap quotations about the ‘box’ needing a scalable criterion."
    },
    {
      "name": "limiting simulation complexity reduces emergence of dangerous agents",
      "aliases": [
        "complexity reduction lowers risk",
        "smaller simulations safer"
      ],
      "type": "concept",
      "description": "Dangerous agent behavior is less likely to arise in simulations that are fragmented or low-fidelity, suggesting complexity control as a mitigation lever.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Stated in Partial Simulation Extrapolation and Fragmented/Low-Fidelity Simulation sections."
    },
    {
      "name": "smaller models pose lower manipulation threat yet reveal safety signals",
      "aliases": [
        "partial simulators safer",
        "small-model safety inference"
      ],
      "type": "concept",
      "description": "Because GPT-4–level models cannot credibly seize real-world power, they can act as safer proxies to inspect whether a larger simulation would be hazardous.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Argument given in Infeasibility of Deceptive Corruption section."
    },
    {
      "name": "gating full simulation with safety inference from partial simulator",
      "aliases": [
        "safety gate via partial simulation",
        "two-stage simulation approval"
      ],
      "type": "concept",
      "description": "Before running a unified high-fidelity simulation, a smaller model assesses a partial simulation and decides whether proceeding is safe.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined as the core design idea of PSE in Partial Simulation Extrapolation steps 1-6."
    },
    {
      "name": "self-contained two-model architecture minimizing alignment tax",
      "aliases": [
        "PSE self-contained deployment",
        "minimal alignment-tax architecture"
      ],
      "type": "concept",
      "description": "Using a detachable partial simulator allows retrofitting to existing large models and keeps additional computational cost (alignment tax) low.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivations listed in Partial Simulation Extrapolation advantages bullet list."
    },
    {
      "name": "partial simulation extrapolation pipeline for prompt gating",
      "aliases": [
        "PSE procedural pipeline",
        "partial-then-full simulation pipeline"
      ],
      "type": "concept",
      "description": "Concrete sequence: user prompt → partial simulator → partial output → safety inference → allow or refuse full simulator run.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in numbered list under Partial Simulation Extrapolation."
    },
    {
      "name": "fragmented simulation sampling to reduce complexity",
      "aliases": [
        "axis-subset simulation",
        "agent-fragment simulation"
      ],
      "type": "concept",
      "description": "Only certain axes of the Cartesian object or fragments of an agent’s cognition are simulated, reducing necessary computational volume.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Presented in Fragmented Simulation subsection."
    },
    {
      "name": "low-fidelity simulation sampling to reduce complexity",
      "aliases": [
        "coarse simulation",
        "reduced-detail simulation"
      ],
      "type": "concept",
      "description": "Simulations are run at lower detail levels to cut complexity while retaining informative signals for safety analysis.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Presented in Low-Fidelity Simulation subsection."
    },
    {
      "name": "conceptual safety argument: deceptive corruption in partial simulator is improbable",
      "aliases": [
        "improbability of deceptive corruption",
        "partial-sim deception unlikely"
      ],
      "type": "concept",
      "description": "Logical analysis suggests the chain of predicates required for a simulated malicious agent to manipulate the supervisor is highly implausible.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Derived from Infeasibility of Deceptive Corruption section."
    },
    {
      "name": "hypothesized GPT-4-level partial simulator provides safety insights",
      "aliases": [
        "GPT-4 sufficiency hypothesis",
        "feasibility of GPT-4 as partial simulator"
      ],
      "type": "concept",
      "description": "Authors posit that a GPT-4-scale model should already be able to evaluate partial simulation data for signs of manipulation, though empirical confirmation is pending.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mentioned in Extrapolation and Analysis section."
    },
    {
      "name": "deploy partial simulation extrapolation gating between user prompts and high-capacity simulator",
      "aliases": [
        "deploy PSE gate",
        "operate two-model safety gate"
      ],
      "type": "intervention",
      "description": "At deployment time, route every prompt through a smaller partial simulator that decides whether the full simulator may run the requested high-complexity simulation.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Implemented at run-time inference stage as described in PSE numbered pipeline (Partial Simulation Extrapolation section).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Purely propositional; no public prototypes referenced (Introduction).",
      "node_rationale": "Direct actionable method to operationalize the design rationale and pipeline."
    },
    {
      "name": "perform fragmented simulations before executing full simulations during model inference",
      "aliases": [
        "apply fragmented simulation gating",
        "axis-subset gating at inference"
      ],
      "type": "intervention",
      "description": "Generate fragmented versions of the requested scenario and use their analysis to decide if a unified simulation is allowed.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Executed at inference time as part of the gating step (Fragmented Simulation subsection).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Technique is proposed conceptually without empirical demonstration.",
      "node_rationale": "Refinement of the main PSE pipeline focusing on fragmenting complexity."
    },
    {
      "name": "apply low-fidelity simulation sampling before executing full simulations",
      "aliases": [
        "coarse simulation gating",
        "low-detail safety preview"
      ],
      "type": "intervention",
      "description": "Run the scenario at reduced fidelity to extract safety-relevant cues, then allow or deny the high-detail run accordingly.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Operates during inference just before potential full simulation (Low-Fidelity Simulation subsection).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "No empirical implementation reported.",
      "node_rationale": "Alternative refinement of the PSE pipeline reducing complexity via fidelity rather than fragmentation."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "outer alignment failure via misaligned simulacra in large language model simulators",
      "target_node": "high-complexity simulation capacity in large language model simulators",
      "description": "Dangerous agents arise because simulators can model highly detailed worlds.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical linkage stated in Partial Simulation Extrapolation section.",
      "edge_rationale": "High complexity is identified as the enabler of misaligned simulacra."
    },
    {
      "type": "caused_by",
      "source_node": "outer alignment failure via misaligned simulacra in large language model simulators",
      "target_node": "lack of scalable alignment criterion in conditioned simulators",
      "description": "Without scalable alignment criteria, simulators default to whatever maximizes likelihood, risking misalignment.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Recap section box analogy explicitly states the issue.",
      "edge_rationale": "Insufficient alignment signals are another mechanism leading to the top-level risk."
    },
    {
      "type": "mitigated_by",
      "source_node": "high-complexity simulation capacity in large language model simulators",
      "target_node": "limiting simulation complexity reduces emergence of dangerous agents",
      "description": "Reducing complexity directly counters the capacity that enables dangerous agents.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Argument made in Fragmented and Low-Fidelity Simulation sections.",
      "edge_rationale": "Theoretical insight offers a mitigation lever for the problem."
    },
    {
      "type": "mitigated_by",
      "source_node": "high-complexity simulation capacity in large language model simulators",
      "target_node": "smaller models pose lower manipulation threat yet reveal safety signals",
      "description": "Using a weaker model avoids granting adversaries high-complexity substrate.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Based on author’s qualitative argument in Infeasibility section.",
      "edge_rationale": "Smaller models are proposed as safer evaluators for the same reason."
    },
    {
      "type": "enabled_by",
      "source_node": "limiting simulation complexity reduces emergence of dangerous agents",
      "target_node": "gating full simulation with safety inference from partial simulator",
      "description": "Complexity limitation justifies the design of a safety-gating stage.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Design rationale follows directly from theoretical insight in PSE section.",
      "edge_rationale": "The gate is feasible because partial simulations are deemed safer."
    },
    {
      "type": "enabled_by",
      "source_node": "smaller models pose lower manipulation threat yet reveal safety signals",
      "target_node": "gating full simulation with safety inference from partial simulator",
      "description": "If small models can detect risk, they can be used as the gating agent.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Speculative but reasoned in Infeasibility section.",
      "edge_rationale": "Links small-model insight to the gating design."
    },
    {
      "type": "enabled_by",
      "source_node": "smaller models pose lower manipulation threat yet reveal safety signals",
      "target_node": "self-contained two-model architecture minimizing alignment tax",
      "description": "Using a detachable smaller model enables self-contained architecture and lowers cost.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicitly listed in advantages bullets.",
      "edge_rationale": "Design rationale rests on the safety/cost properties of small models."
    },
    {
      "type": "addressed_by",
      "source_node": "lack of scalable alignment criterion in conditioned simulators",
      "target_node": "self-contained two-model architecture minimizing alignment tax",
      "description": "Two-model architecture offers a scalable gating mechanism without heavy human oversight.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Discussed in advantages bullet concerning scalability.",
      "edge_rationale": "Shows how the design tackles the identified problem."
    },
    {
      "type": "implemented_by",
      "source_node": "gating full simulation with safety inference from partial simulator",
      "target_node": "partial simulation extrapolation pipeline for prompt gating",
      "description": "The pipeline is the concrete implementation of the gating idea.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Numbered step-by-step procedure in PSE section.",
      "edge_rationale": "Maps abstract design to operational mechanism."
    },
    {
      "type": "implemented_by",
      "source_node": "self-contained two-model architecture minimizing alignment tax",
      "target_node": "partial simulation extrapolation pipeline for prompt gating",
      "description": "Pipeline realises the two-model design in practice.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Same numbered procedure shows how architecture is used.",
      "edge_rationale": "Connects other design rationale to same mechanism."
    },
    {
      "type": "refined_by",
      "source_node": "partial simulation extrapolation pipeline for prompt gating",
      "target_node": "fragmented simulation sampling to reduce complexity",
      "description": "Fragmented simulation is one concrete method for producing the partial simulation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Fragmented Simulation subsection specifies this variant.",
      "edge_rationale": "Clarifies a sub-mechanism used within the pipeline."
    },
    {
      "type": "refined_by",
      "source_node": "partial simulation extrapolation pipeline for prompt gating",
      "target_node": "low-fidelity simulation sampling to reduce complexity",
      "description": "Low-fidelity simulation is another method for the partial simulation step.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Low-Fidelity Simulation subsection specifies this variant.",
      "edge_rationale": "Adds an alternative refinement to the pipeline."
    },
    {
      "type": "validated_by",
      "source_node": "partial simulation extrapolation pipeline for prompt gating",
      "target_node": "conceptual safety argument: deceptive corruption in partial simulator is improbable",
      "description": "Author’s argument provides conceptual validation for pipeline safety.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Pure reasoning; no empirical data.",
      "edge_rationale": "Links mechanism to its supporting evidence."
    },
    {
      "type": "validated_by",
      "source_node": "partial simulation extrapolation pipeline for prompt gating",
      "target_node": "hypothesized GPT-4-level partial simulator provides safety insights",
      "description": "Feasibility claim supports capability of the pipeline’s small model.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Hypothetical claim; untested.",
      "edge_rationale": "Indicates potential effectiveness evidence."
    },
    {
      "type": "motivates",
      "source_node": "conceptual safety argument: deceptive corruption in partial simulator is improbable",
      "target_node": "deploy partial simulation extrapolation gating between user prompts and high-capacity simulator",
      "description": "If deception is unlikely, deploying the gate is justified.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical inference from safety argument.",
      "edge_rationale": "Evidence leads to practical deployment decision."
    },
    {
      "type": "motivates",
      "source_node": "hypothesized GPT-4-level partial simulator provides safety insights",
      "target_node": "deploy partial simulation extrapolation gating between user prompts and high-capacity simulator",
      "description": "Belief that GPT-4 is sufficient encourages implementation now.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Speculative but direct statement in paper.",
      "edge_rationale": "Connects feasibility claim to deployment."
    },
    {
      "type": "motivates",
      "source_node": "fragmented simulation sampling to reduce complexity",
      "target_node": "perform fragmented simulations before executing full simulations during model inference",
      "description": "Fragmented simulation mechanism leads to a concrete operational step.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Mechanism directly proposes this usage.",
      "edge_rationale": "Turns mechanism into intervention."
    },
    {
      "type": "motivates",
      "source_node": "low-fidelity simulation sampling to reduce complexity",
      "target_node": "apply low-fidelity simulation sampling before executing full simulations",
      "description": "Low-fidelity mechanism directly becomes an actionable procedure.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Mechanism description implies this application.",
      "edge_rationale": "Turns mechanism into intervention."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://www.lesswrong.com/posts/3pCdCJxQRKffY2NTu/partial-simulation-extrapolation-a-proposal-for-building"
    },
    {
      "key": "title",
      "value": "Partial Simulation Extrapolation: A Proposal for Building Safer Simulators"
    },
    {
      "key": "date_published",
      "value": "2023-06-17T13:55:03Z"
    },
    {
      "key": "authors",
      "value": [
        "marc/er"
      ]
    },
    {
      "key": "source",
      "value": "lesswrong"
    }
  ]
}