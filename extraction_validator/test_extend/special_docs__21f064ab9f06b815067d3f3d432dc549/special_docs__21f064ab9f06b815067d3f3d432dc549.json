{
  "nodes": [
    {
      "name": "AI-disaster from unsafe first advanced AI",
      "aliases": [
        "transformative AI catastrophe",
        "unsafe first-mover AI risk"
      ],
      "type": "concept",
      "description": "The scenario in which the first team to build a very powerful AI skips sufficient precautions, leading to catastrophic outcomes.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central risk defined in Abstract and Introduction"
    },
    {
      "name": "Safety skimping incentives in AI arms race",
      "aliases": [
        "reduced precautions for speed",
        "race-driven safety neglect"
      ],
      "type": "concept",
      "description": "Competing teams lower safety level s to finish first because victory rewards outweigh perceived extra risk.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Model setup Section 2 defines c–s score and incentives"
    },
    {
      "name": "High enmity between AI development teams",
      "aliases": [
        "competitive hostility",
        "zero-sum preferences"
      ],
      "type": "concept",
      "description": "Utility parameter e close to 1 makes others' success almost as bad as disaster, increasing risk-taking.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Parameter e introduced in Section 2; risk effects analysed in 3.2"
    },
    {
      "name": "Large number of AI development teams",
      "aliases": [
        "many competing labs",
        "high n in arms race"
      ],
      "type": "concept",
      "description": "Greater n intensifies competition and usually raises disaster probability in the model.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Sections 2 and 3.4 analyse impact of n"
    },
    {
      "name": "Information sharing of AI capabilities between teams",
      "aliases": [
        "public capability information",
        "common knowledge of progress"
      ],
      "type": "concept",
      "description": "When teams know their own and others’ capabilities, equilibrium safety levels drop, increasing risk.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.3 'curse of too much information'"
    },
    {
      "name": "Risk-taking importance exceeding capability contributions",
      "aliases": [
        "low capability parameter",
        "small ϖ regime"
      ],
      "type": "concept",
      "description": "When technological progress depends more on taking risks than on underlying capability (low ϖ), disaster probability is highest.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.1 discusses effect of capability parameter ϖ"
    },
    {
      "name": "Nash equilibrium model of AI arms race dynamics",
      "aliases": [
        "game-theoretic arms race model",
        "equilibrium safety model"
      ],
      "type": "concept",
      "description": "Formal game where each team chooses safety level; equilibrium derived for three information scenarios.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 2 presents model and equilibrium reasoning"
    },
    {
      "name": "Disaster probability decreases with capability importance",
      "aliases": [
        "higher ϖ lowers risk"
      ],
      "type": "concept",
      "description": "Analytical result: making capability more decisive (larger ϖ) reduces incentives to lower safety.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Figures 1 & text in 3.1"
    },
    {
      "name": "Disaster probability decreases with lower enmity",
      "aliases": [
        "friendly goals reduce risk"
      ],
      "type": "concept",
      "description": "Lower e means teams care less about losing, so they maintain higher safety.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Analysis in 3.2"
    },
    {
      "name": "Disaster probability increases with information sharing",
      "aliases": [
        "information hazard effect"
      ],
      "type": "concept",
      "description": "Model shows public or private capability knowledge raises equilibrium risk relative to ignorance.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.3 and Figure comparisons"
    },
    {
      "name": "Disaster probability increases with number of teams",
      "aliases": [
        "n-dependence of risk"
      ],
      "type": "concept",
      "description": "Except in some high-capability private-info cases, more competitors raise overall risk.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.4"
    },
    {
      "name": "Promote coordination to reduce competitive pressure",
      "aliases": [
        "increase cooperation",
        "align goals among teams"
      ],
      "type": "concept",
      "description": "Design strategy: lower effective enmity so teams keep higher safety levels.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussions in 3.2 and Conclusion"
    },
    {
      "name": "Encourage research paths where capability dominates risk-taking",
      "aliases": [
        "prioritise capability-heavy approaches"
      ],
      "type": "concept",
      "description": "Steering R&D toward methods where risk-taking yields little advantage reduces incentive to skip safety.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposed in Section 3.1 intervention paragraph"
    },
    {
      "name": "Limit dissemination of capability progress information",
      "aliases": [
        "information containment",
        "reduced public capability metrics"
      ],
      "type": "concept",
      "description": "Design rationale to keep teams ignorant of each other’s relative progress, mitigating info-driven risk.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.3 on information hazards"
    },
    {
      "name": "Merge or reduce number of competing teams",
      "aliases": [
        "team consolidation",
        "joint projects"
      ],
      "type": "concept",
      "description": "Design strategy: fewer independent actors lowers race intensity and disaster probability.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.4 & Conclusion"
    },
    {
      "name": "International agreements and trust-building among AI developers",
      "aliases": [
        "multilateral coordination mechanisms",
        "confidence-building measures"
      ],
      "type": "concept",
      "description": "Specific mechanism to operationalise reduced enmity via treaties, shared governance forums, verification regimes.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation example implied in 3.2 and Conclusion"
    },
    {
      "name": "Research funding prioritisation for capability-heavy approaches",
      "aliases": [
        "grant steering toward low-risk-taking advantage areas"
      ],
      "type": "concept",
      "description": "Funding agencies steer resources to projects where extra risk confers little marginal win probability.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation detail inferred from suggestion in 3.1 (confidence 2)"
    },
    {
      "name": "Information containment policies on AI capability disclosures",
      "aliases": [
        "restricted publication norms",
        "classified capability metrics"
      ],
      "type": "concept",
      "description": "Mechanism to operationalise limited dissemination through embargoes or tiered access.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Derived from information hazard discussion 3.3"
    },
    {
      "name": "Industry consolidation or collaborative joint ventures",
      "aliases": [
        "lab mergers",
        "centralised AI research consortium"
      ],
      "type": "concept",
      "description": "Mechanism to reduce number of independent actors via mergers or formal joint projects.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation path from Section 3.4 suggestions"
    },
    {
      "name": "Equilibrium model simulations demonstrating risk trade-offs",
      "aliases": [
        "analytical results",
        "model validation curves"
      ],
      "type": "concept",
      "description": "Derived probabilities and plots (Figures 1–3) validating how parameters affect disaster risk.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Entire Section 3 provides quantitative evidence"
    },
    {
      "name": "Negotiate and implement multilateral AI safety coordination treaties",
      "aliases": [
        "global AI treaty",
        "international AI arms-race mitigation agreement"
      ],
      "type": "intervention",
      "description": "Diplomatic process establishing binding or soft-law agreements on safety standards, verification and shared benefits to lower enmity and align incentives.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "Policy and governance action beyond model lifecycle – relates to implementation stage (Conclusion).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Only theoretical recommendation in paper; no empirical pilots cited.",
      "node_rationale": "Direct actionable step enabled by 'International agreements and trust-building' mechanism."
    },
    {
      "name": "Provide targeted funding for capability-intensive AI research paths",
      "aliases": [
        "fund capability-dominant approaches"
      ],
      "type": "intervention",
      "description": "Government or philanthropic grants prioritise techniques where risk-taking grants little speed advantage, reducing race incentives.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "Strategic R&D funding policy – governance stage.",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Suggestion is speculative; no programmes evaluated.",
      "node_rationale": "Acts on 'Research funding prioritisation for capability-heavy approaches'."
    },
    {
      "name": "Enforce controlled disclosure policies on AI capability metrics",
      "aliases": [
        "capability information embargo"
      ],
      "type": "intervention",
      "description": "Institutions adopt norms or regulations limiting publication of detailed progress metrics to preserve uncertainty among competitors.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "Policy/regulatory measure external to model development.",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Only theorised as mitigation; no real-world enforcement studied.",
      "node_rationale": "Implements 'Information containment policies on AI capability disclosures'."
    },
    {
      "name": "Facilitate mergers and joint ventures among leading AI teams",
      "aliases": [
        "promote AI consortium formation"
      ],
      "type": "intervention",
      "description": "Regulators and funders incentivise consolidation or consortia, decreasing the effective number of competitive actors.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "Strategic industry-structure intervention.",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Speculative governance lever; untested empirically.",
      "node_rationale": "Operationalises 'Industry consolidation or collaborative joint ventures'."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "AI-disaster from unsafe first advanced AI",
      "target_node": "Safety skimping incentives in AI arms race",
      "description": "Disaster arises when incentives lead teams to reduce precautions.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct reasoning in Introduction linking skimping to disaster probability.",
      "edge_rationale": "Section 1"
    },
    {
      "type": "caused_by",
      "source_node": "Safety skimping incentives in AI arms race",
      "target_node": "High enmity between AI development teams",
      "description": "Higher enmity increases marginal utility of winning, intensifying incentive to skimp.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Analytical derivation in 3.2.",
      "edge_rationale": "Section 3.2"
    },
    {
      "type": "caused_by",
      "source_node": "Safety skimping incentives in AI arms race",
      "target_node": "Large number of AI development teams",
      "description": "More teams lower individual win probability, motivating riskier behaviour.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Model results in 3.4.",
      "edge_rationale": "Section 3.4"
    },
    {
      "type": "caused_by",
      "source_node": "Safety skimping incentives in AI arms race",
      "target_node": "Information sharing of AI capabilities between teams",
      "description": "Knowledge of relative standings lets teams judge when small safety cuts will clinch victory.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explained in 3.3.",
      "edge_rationale": "Section 3.3"
    },
    {
      "type": "caused_by",
      "source_node": "Safety skimping incentives in AI arms race",
      "target_node": "Risk-taking importance exceeding capability contributions",
      "description": "When risk-taking outweighs capability (low ϖ), teams gain more by lowering s.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 3.1 analysis.",
      "edge_rationale": "Section 3.1"
    },
    {
      "type": "implemented_by",
      "source_node": "High enmity between AI development teams",
      "target_node": "Promote coordination to reduce competitive pressure",
      "description": "Reducing enmity addresses competitive hostility problem.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Authors propose trust-building in 3.2.",
      "edge_rationale": "Section 3.2"
    },
    {
      "type": "implemented_by",
      "source_node": "Large number of AI development teams",
      "target_node": "Merge or reduce number of competing teams",
      "description": "Consolidation lowers n, mitigating risk.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 3.4 suggestion.",
      "edge_rationale": "Section 3.4"
    },
    {
      "type": "implemented_by",
      "source_node": "Information sharing of AI capabilities between teams",
      "target_node": "Limit dissemination of capability progress information",
      "description": "Information containment directly tackles info hazard.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 3.3.",
      "edge_rationale": "Section 3.3"
    },
    {
      "type": "implemented_by",
      "source_node": "Risk-taking importance exceeding capability contributions",
      "target_node": "Encourage research paths where capability dominates risk-taking",
      "description": "Steering towards high-capability regimes lowers benefit of skimping.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Intervention paragraph in 3.1.",
      "edge_rationale": "Section 3.1"
    },
    {
      "type": "enabled_by",
      "source_node": "Promote coordination to reduce competitive pressure",
      "target_node": "International agreements and trust-building among AI developers",
      "description": "Agreements operationalise coordination reductions.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implementation inferred but logical (confidence 2).",
      "edge_rationale": "Conclusion"
    },
    {
      "type": "enabled_by",
      "source_node": "Encourage research paths where capability dominates risk-taking",
      "target_node": "Research funding prioritisation for capability-heavy approaches",
      "description": "Funding is practical lever to steer research agendas.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Moderate inference from suggestion (confidence 2).",
      "edge_rationale": "Section 3.1"
    },
    {
      "type": "enabled_by",
      "source_node": "Limit dissemination of capability progress information",
      "target_node": "Information containment policies on AI capability disclosures",
      "description": "Policies implement restriction of information flow.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical mechanism; not detailed in paper.",
      "edge_rationale": "Section 3.3"
    },
    {
      "type": "enabled_by",
      "source_node": "Merge or reduce number of competing teams",
      "target_node": "Industry consolidation or collaborative joint ventures",
      "description": "Joint ventures reduce independent actors.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implementation inference (confidence 2).",
      "edge_rationale": "Section 3.4"
    },
    {
      "type": "validated_by",
      "source_node": "Nash equilibrium model of AI arms race dynamics",
      "target_node": "Equilibrium model simulations demonstrating risk trade-offs",
      "description": "Analytical results validate theoretical model behaviour.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Mathematical derivations and figures constitute strong evidence.",
      "edge_rationale": "Section 3 figures"
    },
    {
      "type": "refined_by",
      "source_node": "Nash equilibrium model of AI arms race dynamics",
      "target_node": "Disaster probability decreases with capability importance",
      "description": "One analytical corollary of the model.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Derived equations.",
      "edge_rationale": "Section 3.1"
    },
    {
      "type": "refined_by",
      "source_node": "Nash equilibrium model of AI arms race dynamics",
      "target_node": "Disaster probability decreases with lower enmity",
      "description": "Another corollary.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Derived equations.",
      "edge_rationale": "Section 3.2"
    },
    {
      "type": "refined_by",
      "source_node": "Nash equilibrium model of AI arms race dynamics",
      "target_node": "Disaster probability increases with information sharing",
      "description": "Information hazard corollary.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Derived equations.",
      "edge_rationale": "Section 3.3"
    },
    {
      "type": "refined_by",
      "source_node": "Nash equilibrium model of AI arms race dynamics",
      "target_node": "Disaster probability increases with number of teams",
      "description": "n-dependence corollary.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Derived equations.",
      "edge_rationale": "Section 3.4"
    },
    {
      "type": "motivates",
      "source_node": "Equilibrium model simulations demonstrating risk trade-offs",
      "target_node": "Negotiate and implement multilateral AI safety coordination treaties",
      "description": "Model shows coordination reduces risk, motivating treaties.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Qualitative link drawn in Conclusion.",
      "edge_rationale": "Conclusion"
    },
    {
      "type": "motivates",
      "source_node": "Equilibrium model simulations demonstrating risk trade-offs",
      "target_node": "Provide targeted funding for capability-intensive AI research paths",
      "description": "Simulation shows capability-dominant paths safer, motivating funding shifts.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Rationale in 3.1.",
      "edge_rationale": "Section 3.1"
    },
    {
      "type": "motivates",
      "source_node": "Equilibrium model simulations demonstrating risk trade-offs",
      "target_node": "Enforce controlled disclosure policies on AI capability metrics",
      "description": "Information hazard results motivate disclosure control.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 3.3 and Conclusion.",
      "edge_rationale": "Section 3.3"
    },
    {
      "type": "motivates",
      "source_node": "Equilibrium model simulations demonstrating risk trade-offs",
      "target_node": "Facilitate mergers and joint ventures among leading AI teams",
      "description": "Increased n raises risk, motivating consolidation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 3.4 conclusion.",
      "edge_rationale": "Section 3.4"
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://www.fhi.ox.ac.uk/wp-content/uploads/Racing-to-the-precipice-a-model-of-artificial-intelligence-development.pdf"
    },
    {
      "key": "title",
      "value": "Racing to the precipice: a model of artificial intelligence development"
    },
    {
      "key": "date_published",
      "value": "2016-01-01T00:00:00Z"
    },
    {
      "key": "source_filetype",
      "value": "pdf"
    },
    {
      "key": "authors",
      "value": [
        "Stuart Armstrong",
        "Nick Bostrom",
        "Carl Shulman"
      ]
    },
    {
      "key": "source",
      "value": "special_docs"
    }
  ]
}