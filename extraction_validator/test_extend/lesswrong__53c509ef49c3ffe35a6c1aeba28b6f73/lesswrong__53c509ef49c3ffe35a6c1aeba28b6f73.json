{
  "nodes": [
    {
      "name": "Incorrect physical common-sense reasoning in conversational AI",
      "aliases": [
        "faulty physical reasoning in chatbots",
        "poor real-world common sense in LLM responses"
      ],
      "type": "concept",
      "description": "Risk that large language models give advice or answers that ignore basic physical constraints, potentially misleading users or leading to unsafe decisions. Demonstrated by GPT-3’s inability to justify using a banana to hold a book open.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Represents the top-level safety concern motivating the entire discussion (dialogue transcript section)."
    },
    {
      "name": "Shallow pattern-matching answer generation in large language models",
      "aliases": [
        "surface-level pattern matching in LLMs",
        "statistical mimicry without reasoning in GPT-3"
      ],
      "type": "concept",
      "description": "Observation that GPT-3 generates responses by statistically imitating text with limited internal modelling of the physical world, leading to brittle answers.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Identified mechanism causing the risk (multiple example answer lists)."
    },
    {
      "name": "GPT-3 responses to book-propping question lack coherent reasoning",
      "aliases": [
        "banana answers without explanation",
        "one-question Turing test result"
      ],
      "type": "concept",
      "description": "Empirical finding: GPT-3 repeatedly suggests foods like bananas but provides incoherent or no explanations, showing missing causal reasoning about shape, weight, messiness, etc.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Direct evidence validating the problem analysis (example answer transcripts)."
    },
    {
      "name": "Prompt framing influences depth of reasoning in LLMs",
      "aliases": [
        "effect of prompt design on GPT-3 reasoning",
        "prompt context shaping output depth"
      ],
      "type": "concept",
      "description": "Theoretical insight that structuring the conversation (party setting, friend dialogue, etc.) changes LLM outputs and may elicit slightly deeper or at least different reasoning paths.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explains why altering prompts is considered a possible mitigation (sections on prompt engineering attempts)."
    },
    {
      "name": "Employ chain-of-thought prompting to guide models toward explicit reasoning",
      "aliases": [
        "use step-by-step prompting",
        "force intermediate reasoning steps"
      ],
      "type": "concept",
      "description": "Design rationale: require the model to state intermediate thoughts so that answers rely on articulated physical reasoning rather than ungrounded correlations.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Natural design extension of the theoretical insight about prompt effects (prompt-engineering discussion section)."
    },
    {
      "name": "Prompt templates requiring step-by-step justification",
      "aliases": [
        "chain-of-thought templates",
        "reasoning-explicit prompts"
      ],
      "type": "concept",
      "description": "Implementation mechanism: specifically formatted prompts that instruct the model to think through object properties (weight, shape, cleanliness) before answering.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete technique to instantiate the design rationale (constructed prompt examples section)."
    },
    {
      "name": "More sophisticated prompt framing yields limited improvement in GPT-3 reasoning",
      "aliases": [
        "party prompt experiment result",
        "limited gains from better prompts"
      ],
      "type": "concept",
      "description": "Finding that even with richer conversational frames GPT-3 often still fails to justify food choices, indicating only partial mitigation.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Empirical test of the implementation mechanism (party-prompt section)."
    },
    {
      "name": "Continuous evaluation benchmarks to detect physical reasoning failures",
      "aliases": [
        "ongoing common-sense audit",
        "routine physical-reasoning tests"
      ],
      "type": "concept",
      "description": "Design rationale that models should be regularly tested on edge-case questions like the book-propping task to track reasoning quality over time.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Author uses the single-question test as an implicit benchmark (discussion of ‘banana test’ viability)."
    },
    {
      "name": "Physical reasoning question set benchmark pre-deployment",
      "aliases": [
        "banana test suite",
        "physical common-sense QA benchmark"
      ],
      "type": "concept",
      "description": "Implementation mechanism: a curated list of short-answer questions that require reasoning about everyday physics, to be run on models before release.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete way to operationalise continuous evaluation (idea to scale the banana test)."
    },
    {
      "name": "Book-propping 'banana' test demonstrates simple benchmark viability",
      "aliases": [
        "banana benchmark proof-of-concept",
        "single-question evaluation result"
      ],
      "type": "concept",
      "description": "Validation evidence that a single, well-chosen question can reveal reasoning deficits, suggesting such items are effective benchmark components.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Shows that the proposed benchmark format works in practice (entire blog demonstration)."
    },
    {
      "name": "Use chain-of-thought prompting during deployment for physical reasoning queries",
      "aliases": [
        "deploy CoT prompts in production chatbots",
        "enable reasoning-first generation"
      ],
      "type": "intervention",
      "description": "At runtime, prepend or inject chain-of-thought instructions so that the model reveals its reasoning before producing a final answer, especially for questions requiring physical common sense.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Action applies at inference time once the model is deployed (conversation examples section).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Technique is widely experimented with but not universally adopted; evidence here is small-scale (prompt engineering trials).",
      "node_rationale": "Directly addresses the observed lack of explicit reasoning."
    },
    {
      "name": "Establish pre-deployment benchmark for physical reasoning questions",
      "aliases": [
        "mandatory physical common-sense test gate",
        "release blocker benchmark"
      ],
      "type": "intervention",
      "description": "Before model release, run a fixed suite of questions (including the banana test) and require a minimum explanation quality score to pass.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Benchmark execution happens immediately before deployment to users.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Only proposed in anecdotal form; no systematic adoption yet.",
      "node_rationale": "Transforms the single-question test into an operational safety gate."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Incorrect physical common-sense reasoning in conversational AI",
      "target_node": "Shallow pattern-matching answer generation in large language models",
      "description": "The risk of incorrect physical reasoning arises because LLMs rely on surface pattern replication.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Consistent examples but limited quantitative data (answer lists section).",
      "edge_rationale": "Author explicitly links shallow pattern matching to failure cases."
    },
    {
      "type": "validated_by",
      "source_node": "Shallow pattern-matching answer generation in large language models",
      "target_node": "GPT-3 responses to book-propping question lack coherent reasoning",
      "description": "Multiple GPT-3 outputs support the claim of shallow pattern matching.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct empirical examples supplied.",
      "edge_rationale": "Observed answers serve as validation for the identified problem."
    },
    {
      "type": "mitigated_by",
      "source_node": "Shallow pattern-matching answer generation in large language models",
      "target_node": "Prompt framing influences depth of reasoning in LLMs",
      "description": "Adjusting prompt structure is proposed as a way to mitigate shallow responses.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Partial improvement noted but not rigorously measured.",
      "edge_rationale": "Prompt engineering section suggests this line of mitigation."
    },
    {
      "type": "mitigated_by",
      "source_node": "Prompt framing influences depth of reasoning in LLMs",
      "target_node": "Employ chain-of-thought prompting to guide models toward explicit reasoning",
      "description": "Explicit chain-of-thought prompting is a concrete strategy derived from the insight about prompt framing.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical extension, not directly tested in the post.",
      "edge_rationale": "Design rationale flows naturally from theoretical insight."
    },
    {
      "type": "implemented_by",
      "source_node": "Employ chain-of-thought prompting to guide models toward explicit reasoning",
      "target_node": "Prompt templates requiring step-by-step justification",
      "description": "The design is operationalised via specific prompt templates demanding step-wise reasoning.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Templates are shown in text.",
      "edge_rationale": "Implementation mechanism makes the design concrete."
    },
    {
      "type": "validated_by",
      "source_node": "Prompt templates requiring step-by-step justification",
      "target_node": "More sophisticated prompt framing yields limited improvement in GPT-3 reasoning",
      "description": "Running the templates produced only slight gains, providing empirical feedback.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct experimental outcome reported.",
      "edge_rationale": "Evidence evaluates the effectiveness of the implementation."
    },
    {
      "type": "motivates",
      "source_node": "More sophisticated prompt framing yields limited improvement in GPT-3 reasoning",
      "target_node": "Use chain-of-thought prompting during deployment for physical reasoning queries",
      "description": "Despite limited gains, evidence motivates adopting chain-of-thought prompting more systematically in deployment.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Motivation inferred; adoption not explicitly stated by author.",
      "edge_rationale": "Logical next step suggested by results and discussion."
    },
    {
      "type": "motivates",
      "source_node": "GPT-3 responses to book-propping question lack coherent reasoning",
      "target_node": "Continuous evaluation benchmarks to detect physical reasoning failures",
      "description": "Failure examples highlight need for ongoing benchmark-style evaluation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Author explicitly frames the question as a good recurring test.",
      "edge_rationale": "Evidence inspires the design rationale for continuous evaluation."
    },
    {
      "type": "implemented_by",
      "source_node": "Continuous evaluation benchmarks to detect physical reasoning failures",
      "target_node": "Physical reasoning question set benchmark pre-deployment",
      "description": "Design rationale is realised by assembling a concrete benchmark set.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Benchmark only suggested, not constructed in the post.",
      "edge_rationale": "Implementation mechanism gives the benchmark practical form."
    },
    {
      "type": "validated_by",
      "source_node": "Physical reasoning question set benchmark pre-deployment",
      "target_node": "Book-propping 'banana' test demonstrates simple benchmark viability",
      "description": "The single-question test acts as proof-of-concept validation for the benchmark idea.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Empirical evidence directly related.",
      "edge_rationale": "Shows that such benchmarks can reveal reasoning deficits."
    },
    {
      "type": "motivates",
      "source_node": "Book-propping 'banana' test demonstrates simple benchmark viability",
      "target_node": "Establish pre-deployment benchmark for physical reasoning questions",
      "description": "Successful small-scale test motivates formalising the benchmark as a release gate.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference from demonstrated potential; not implemented yet.",
      "edge_rationale": "Logical step to scale up demonstrated idea."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://www.lesswrong.com/posts/ydeaHqDPJ5REJWvat/a-one-question-turing-test-for-gpt-3"
    },
    {
      "key": "title",
      "value": "A one-question Turing test for GPT-3"
    },
    {
      "key": "date_published",
      "value": "2022-01-22T18:17:56Z"
    },
    {
      "key": "authors",
      "value": [
        "Paul Crowley",
        "rosiecam"
      ]
    },
    {
      "key": "source",
      "value": "lesswrong"
    }
  ]
}