Summary  
Data-source overview:  
This survey analyses the core risk that AI systems behave unreliably because they are not robust to adversarial manipulation, natural perturbations or distribution shift. It classifies existing technical and human-centred robustness methods across the ML pipeline, proposes taxonomies of robustness assessment, and highlights major research gaps—especially the lack of human-centred workflows supporting robustness.

EXTRACTION CONFIDENCE[83]  
– The paper is conceptual (survey), not an experimental report, so several implementation and validation links are inferred from summarised evidence; confidence is therefore < 90.  
– Instructions were followed exactly; JSON validated.  
Instruction-set improvement: require explicit allowance for “survey” sources whose findings are aggregated, otherwise validation evidence may always be indirect; provide examples for how to treat gaps or future-work sections.

Inference strategy justification  
Where the survey summarised many studies in one sentence, light inference (edge-confidence 2) was used to derive a single representative implementation or validation node rather than hundreds of micro-nodes.

Extraction completeness explanation  
All causal-interventional paths appearing in Sections 3-8 are covered: adversarial robustness, natural robustness, architecture-based robustness, uncertainty/reject-option, and human-centred robustness practices. Every node connects into one integrated fabric, no duplicates or satellites.

Key limitations  
1. Validation evidence nodes draw on benchmark summaries, not raw experimental tables.  
2. Only high-level representative techniques could be modelled; the 380 individual papers in the survey were not decomposed individually.