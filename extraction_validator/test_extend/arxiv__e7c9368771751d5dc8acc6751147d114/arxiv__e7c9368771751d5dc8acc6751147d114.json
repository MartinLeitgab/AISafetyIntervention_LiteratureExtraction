{
  "nodes": [
    {
      "name": "System failures from insufficient AI robustness in safety-critical contexts",
      "aliases": [
        "AI robustness failures",
        "Unreliable AI behaviour"
      ],
      "type": "concept",
      "description": "When models misbehave under small input changes or attacks, medical, automotive and other safety-critical applications can cause harm, blocking trustworthy AI adoption.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central risk identified in Introduction and Section 1 as the motivation for robust AI research."
    },
    {
      "name": "Adversarial vulnerability in ML models",
      "aliases": [
        "Susceptibility to adversarial attacks",
        "Adversarial robustness problem"
      ],
      "type": "concept",
      "description": "Small, intentionally crafted perturbations can drastically change model predictions, revealing fragility of current systems.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Section 3.1.1 as a primary mechanism causing robustness failure."
    },
    {
      "name": "Sensitivity to natural perturbations and distribution shifts in ML models",
      "aliases": [
        "Natural robustness issue",
        "Distribution shift vulnerability"
      ],
      "type": "concept",
      "description": "Performance degrades under common corruptions, noise or domain shift that naturally occur in real-world deployment.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined in Section 3.1.2 and 3.3 as a complementary robustness failure mode."
    },
    {
      "name": "Gap of human-centered robustness evaluation practices",
      "aliases": [
        "Lack of practitioner workflows for robustness",
        "Human-centred gap"
      ],
      "type": "concept",
      "description": "Survey highlights absence of tools, workflows and human-in-the-loop methods supporting practitioners to diagnose and improve robustness.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 7 & 8 explicitly identify this gap."
    },
    {
      "name": "Adversarial training improves robustness by incorporating perturbed data",
      "aliases": [
        "Adversarial training insight",
        "Robustness via adversarial examples"
      ],
      "type": "concept",
      "description": "Training on mixtures of clean and adversarially perturbed inputs forces models to learn stable decision boundaries.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Synthesised in Section 4.2.1 as foundational theory behind many defences."
    },
    {
      "name": "Data augmentation with natural noise improves generalization robustness",
      "aliases": [
        "Natural perturbation augmentation insight"
      ],
      "type": "concept",
      "description": "Adding realistic corruptions or out-of-distribution samples during training increases resilience to natural shifts.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Documented in Section 4.1.3."
    },
    {
      "name": "Model architecture modifications to handle noise enhance resilience",
      "aliases": [
        "Robust architecture insight"
      ],
      "type": "concept",
      "description": "Incorporating components such as stochastic noise layers, orthogonal classifiers or bio-inspired elements can inherently increase robustness.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Summarised in Section 4.2.2."
    },
    {
      "name": "Human knowledge essential for defining realistic perturbations and assessments",
      "aliases": [
        "Human-centred robustness insight"
      ],
      "type": "concept",
      "description": "Humans provide domain insight, perturbation taxonomies and subjective judgements required for meaningful robustness evaluation.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Key argument of Sections 7 & 8."
    },
    {
      "name": "Prediction uncertainty can signal unreliable outputs under distribution shift",
      "aliases": [
        "Uncertainty as robustness signal"
      ],
      "type": "concept",
      "description": "Low-confidence predictions often correspond to out-of-distribution or adversarial inputs, suggesting a rejection mechanism.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Section 8.2.1 on reject-option classifiers."
    },
    {
      "name": "Perturbation-based data augmentation across ML pipeline",
      "aliases": [
        "Data augmentation design rationale"
      ],
      "type": "concept",
      "description": "Plan training to systematically include adversarial and natural perturbations at data, feature and label level.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Unifies multiple augmentation methods in Section 4.1."
    },
    {
      "name": "Robust architecture design with noise-handling components",
      "aliases": [
        "Architecture design for robustness"
      ],
      "type": "concept",
      "description": "Select layers or cells (e.g., SNN, noise injection, orthogonal weights) that process noisy inputs stably.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Derives from Section 4.2.2."
    },
    {
      "name": "Human-in-the-loop robustness evaluation and dataset curation",
      "aliases": [
        "Practitioner-centred robustness workflow"
      ],
      "type": "concept",
      "description": "Embed domain experts and crowd workers to identify perturbations, annotate uncertainty and iteratively refine datasets.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposed future direction in Section 7.2."
    },
    {
      "name": "Explainability-guided feature alignment for robustness",
      "aliases": [
        "XAI for robustness design"
      ],
      "type": "concept",
      "description": "Use interpretability methods to ensure learned features match human-relevant concepts, reducing brittle reliance on spurious signals.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4.2.3 outlines leveraging explainability."
    },
    {
      "name": "Reject option based on prediction uncertainty",
      "aliases": [
        "Rejector design rationale"
      ],
      "type": "concept",
      "description": "Introduce a mechanism that abstains on low-confidence inputs to prevent unsafe actions under shift.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivated in Sections 4.3.2 and 8.2.1."
    },
    {
      "name": "Adversarial training with PGD and dynamic perturbations",
      "aliases": [
        "PGD adversarial training mechanism"
      ],
      "type": "concept",
      "description": "Iteratively generate perturbations via Projected Gradient Descent during fine-tuning to minimise worst-case loss.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation mechanism detailed in Section 4.2.1."
    },
    {
      "name": "GAN-generated corrupted images for training data augmentation",
      "aliases": [
        "GAN noise augmentation mechanism"
      ],
      "type": "concept",
      "description": "Use conditional GANs to produce realistically corrupted or boundary samples, expanding training data for robustness.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 4.1.1 and 4.3.2."
    },
    {
      "name": "Noise layers and orthogonal classifier weights in CNNs",
      "aliases": [
        "Noise-injection architecture tweak"
      ],
      "type": "concept",
      "description": "Insert stochastic noise at input and constrain final layer weights to be orthogonal, reducing sensitivity to perturbations.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation examples given in Section 4.2.2."
    },
    {
      "name": "Interactive robustness benchmark platforms for practitioners",
      "aliases": [
        "RobustBench, ImageNet-C platforms"
      ],
      "type": "concept",
      "description": "Toolchains providing corruption datasets, metrics and leaderboards enabling systematic robustness testing before deployment.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 6.1.2 introduces such evaluation platforms."
    },
    {
      "name": "Crowdsourced perturbation taxonomy development",
      "aliases": [
        "Crowd-driven perturbation collection"
      ],
      "type": "concept",
      "description": "Collect domain-specific perturbations and unknown-unknowns via crowd games and tasks to improve coverage of robustness tests.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposed in Sections 7.2.1 and 8.1.2."
    },
    {
      "name": "Confidence-threshold reject option classifier",
      "aliases": [
        "Prediction abstention mechanism"
      ],
      "type": "concept",
      "description": "Classifier defers decision when confidence below predefined threshold, invoking human oversight or fallback logic.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mechanism discussed in Section 4.3.2 and 8.2.1."
    },
    {
      "name": "RobustBench benchmark results showing increased accuracy post adversarial training",
      "aliases": [
        "RobustBench evidence for PGD"
      ],
      "type": "concept",
      "description": "Survey reports that adversarially trained models rank higher on RobustBench leaderboards, confirming effectiveness.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Validation summarised in Section 6.1.2."
    },
    {
      "name": "ImageNet-C experiments showing improvement from GAN augmentation",
      "aliases": [
        "ImageNet-C augmentation evidence"
      ],
      "type": "concept",
      "description": "Models trained with GAN-generated corruptions achieve lower error on ImageNet-C common corruption benchmark.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Section 4.1 and 6.1.2."
    },
    {
      "name": "Studies indicating human-guided labeling improves out-of-distribution performance",
      "aliases": [
        "Human-guided dataset evidence"
      ],
      "type": "concept",
      "description": "Empirical studies in the survey show that incorporating human rationales and uncertainty into datasets reduces distribution-shift error.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Referenced in Section 7.2."
    },
    {
      "name": "Evaluation showing reject option reduces failure rate on unseen perturbations",
      "aliases": [
        "Reject option evidence"
      ],
      "type": "concept",
      "description": "Research cited in Section 8.2.1 reports that adding an abstention mechanism lowers harmful predictions under shift.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Provides empirical backing for reject-option design."
    },
    {
      "name": "Fine-tune models with adversarial training using PGD-generated perturbations",
      "aliases": [
        "Apply PGD adversarial training"
      ],
      "type": "intervention",
      "description": "During fine-tuning, iteratively generate PGD adversarial examples and train on the combined batch to harden model.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Section 4.2.1 describes application during fine-tuning/RL stage.",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Widely evaluated across benchmarks (Section 6.1.2) but not yet universal in production.",
      "node_rationale": "Representative actionable technique derived from main survey findings."
    },
    {
      "name": "Augment pre-training datasets with GAN-generated natural corruptions",
      "aliases": [
        "GAN corruption augmentation"
      ],
      "type": "intervention",
      "description": "Generate realistic corrupted samples via conditional GANs and add to training corpus before supervised training.",
      "concept_category": null,
      "intervention_lifecycle": "2",
      "intervention_lifecycle_rationale": "Operates on dataset prior to model training (Section 4.1.1).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated in research prototypes; limited large-scale deployment evidence.",
      "node_rationale": "Actionable step linking augmentation mechanism to robustness goal."
    },
    {
      "name": "Design CNNs with stochastic noise input layers and orthogonal classifiers",
      "aliases": [
        "Noise-orthogonal CNN design"
      ],
      "type": "intervention",
      "description": "During architecture design, add input noise layers and enforce orthogonality constraints on final classifier weights.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Implemented at model design stage as per Section 4.2.2.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Experimental models reported; not yet mainstream.",
      "node_rationale": "Captures architecture-centric robustness pathway."
    },
    {
      "name": "Implement confidence-based rejection mechanism during deployment",
      "aliases": [
        "Deploy reject option"
      ],
      "type": "intervention",
      "description": "At runtime, defer or flag predictions whose softmax confidence falls below threshold, routing to human review.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Activated in live system to control outputs (Section 8.2.1).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Prototype systems exist; some industry adoption in high-risk domains.",
      "node_rationale": "Provides operational mitigation for residual robustness gaps."
    },
    {
      "name": "Conduct human-centered robustness evaluation with benchmark platforms and crowdsourced perturbation taxonomies",
      "aliases": [
        "Practitioner robustness evaluation workflow"
      ],
      "type": "intervention",
      "description": "Before release, run models through RobustBench/ImageNet-C and extend tests using crowd-elicited perturbations.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Pre-deployment testing step recommended in Sections 6.1 and 7.2.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Emerging practice; tools exist but integration into pipelines is early stage.",
      "node_rationale": "Addresses identified human-centred gap."
    },
    {
      "name": "Integrate human-in-the-loop dataset curation and rationale collection during data pipeline",
      "aliases": [
        "Crowd-curated robust dataset process"
      ],
      "type": "intervention",
      "description": "Iteratively collect human rationales, label uncertainty and surface unknown-unknowns to refine training data.",
      "concept_category": null,
      "intervention_lifecycle": "2",
      "intervention_lifecycle_rationale": "Applied during data collection / pre-training as per Section 7.2.2.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated in research but not yet industrial standard.",
      "node_rationale": "Concrete action derived from human-centred theoretical insight."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "System failures from insufficient AI robustness in safety-critical contexts",
      "target_node": "Adversarial vulnerability in ML models",
      "description": "Adversarially exploitable models lead directly to system failures",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit examples in Introduction (Tesla mis-identifying moon).",
      "edge_rationale": "Section 1 links adversarial attacks to real failures"
    },
    {
      "type": "caused_by",
      "source_node": "System failures from insufficient AI robustness in safety-critical contexts",
      "target_node": "Sensitivity to natural perturbations and distribution shifts in ML models",
      "description": "Natural corruptions and domain shifts also trigger failures",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 3.1.2 cites real-world noise/shift issues",
      "edge_rationale": "Survey stresses natural robustness as equal concern"
    },
    {
      "type": "caused_by",
      "source_node": "System failures from insufficient AI robustness in safety-critical contexts",
      "target_node": "Gap of human-centered robustness evaluation practices",
      "description": "Without practitioner workflows, latent robustness issues go undetected and cause failures",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 7 discussion; indirect evidence",
      "edge_rationale": "Survey argues missing practices impede safe deployment"
    },
    {
      "type": "mitigated_by",
      "source_node": "Adversarial vulnerability in ML models",
      "target_node": "Adversarial training improves robustness by incorporating perturbed data",
      "description": "Training with adversarial examples directly addresses this vulnerability",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 4.2.1 consensus across 380 papers",
      "edge_rationale": "Theoretical foundation for many defences"
    },
    {
      "type": "mitigated_by",
      "source_node": "Sensitivity to natural perturbations and distribution shifts in ML models",
      "target_node": "Data augmentation with natural noise improves generalization robustness",
      "description": "Augmentation counters shift by exposing model to corruptions",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 4.1.3 empirical summaries",
      "edge_rationale": "Standard practice presented in survey"
    },
    {
      "type": "mitigated_by",
      "source_node": "Adversarial vulnerability in ML models",
      "target_node": "Model architecture modifications to handle noise enhance resilience",
      "description": "Architectural tweaks lessen gradient-based attack success",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 4.2.2 summarises positive findings",
      "edge_rationale": "Alternative mitigation path"
    },
    {
      "type": "mitigated_by",
      "source_node": "Sensitivity to natural perturbations and distribution shifts in ML models",
      "target_node": "Prediction uncertainty can signal unreliable outputs under distribution shift",
      "description": "Uncertainty awareness enables avoiding risky predictions",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Referenced studies in Section 8.2.1",
      "edge_rationale": "Forms basis for reject option"
    },
    {
      "type": "mitigated_by",
      "source_node": "Gap of human-centered robustness evaluation practices",
      "target_node": "Human knowledge essential for defining realistic perturbations and assessments",
      "description": "Human insight can close evaluation gaps",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 7 & 8 describe rationale",
      "edge_rationale": "Human involvement proposed as solution"
    },
    {
      "type": "refined_by",
      "source_node": "Adversarial training improves robustness by incorporating perturbed data",
      "target_node": "Perturbation-based data augmentation across ML pipeline",
      "description": "Design rationale operationalises adversarial-training insight",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 4.1 & 4.2 cross-link",
      "edge_rationale": "Translates theory to design"
    },
    {
      "type": "refined_by",
      "source_node": "Data augmentation with natural noise improves generalization robustness",
      "target_node": "Perturbation-based data augmentation across ML pipeline",
      "description": "Same design rationale extends to natural noise",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit in Section 4.1.3",
      "edge_rationale": "Shared mechanism"
    },
    {
      "type": "refined_by",
      "source_node": "Model architecture modifications to handle noise enhance resilience",
      "target_node": "Robust architecture design with noise-handling components",
      "description": "Design encapsulates architectural insight",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 4.2.2 lists concrete patterns",
      "edge_rationale": "Design step"
    },
    {
      "type": "refined_by",
      "source_node": "Human knowledge essential for defining realistic perturbations and assessments",
      "target_node": "Human-in-the-loop robustness evaluation and dataset curation",
      "description": "Designs workflows to embed human knowledge",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 7.2",
      "edge_rationale": "Maps human insight to process"
    },
    {
      "type": "refined_by",
      "source_node": "Prediction uncertainty can signal unreliable outputs under distribution shift",
      "target_node": "Reject option based on prediction uncertainty",
      "description": "Design rationale translates signal into decision mechanism",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 8.2.1",
      "edge_rationale": "Designs practical use"
    },
    {
      "type": "implemented_by",
      "source_node": "Perturbation-based data augmentation across ML pipeline",
      "target_node": "Adversarial training with PGD and dynamic perturbations",
      "description": "PGD perturbations implement data-level augmentation",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 4.2.1 details mechanism",
      "edge_rationale": "Direct implementation"
    },
    {
      "type": "implemented_by",
      "source_node": "Perturbation-based data augmentation across ML pipeline",
      "target_node": "GAN-generated corrupted images for training data augmentation",
      "description": "GANs provide synthetic corruption generator",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 4.1.1",
      "edge_rationale": "Concrete implementation"
    },
    {
      "type": "implemented_by",
      "source_node": "Robust architecture design with noise-handling components",
      "target_node": "Noise layers and orthogonal classifier weights in CNNs",
      "description": "Specific layer and weight modifications realise design",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 4.2.2 examples",
      "edge_rationale": "Instantiation"
    },
    {
      "type": "implemented_by",
      "source_node": "Human-in-the-loop robustness evaluation and dataset curation",
      "target_node": "Interactive robustness benchmark platforms for practitioners",
      "description": "Platforms operationalise evaluation workflow",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inferred linkage from Section 6.1.2 and 7.2",
      "edge_rationale": "Tools enable design"
    },
    {
      "type": "implemented_by",
      "source_node": "Human-in-the-loop robustness evaluation and dataset curation",
      "target_node": "Crowdsourced perturbation taxonomy development",
      "description": "Crowd collection implements human data curation",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 7.2.1 proposal",
      "edge_rationale": "Mechanism"
    },
    {
      "type": "implemented_by",
      "source_node": "Reject option based on prediction uncertainty",
      "target_node": "Confidence-threshold reject option classifier",
      "description": "Classifier realises reject-option design",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Described in Section 4.3.2",
      "edge_rationale": "Implementation"
    },
    {
      "type": "validated_by",
      "source_node": "Adversarial training with PGD and dynamic perturbations",
      "target_node": "RobustBench benchmark results showing increased accuracy post adversarial training",
      "description": "Benchmark confirms effectiveness",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 6.1.2 reports result tables",
      "edge_rationale": "Empirical support"
    },
    {
      "type": "validated_by",
      "source_node": "GAN-generated corrupted images for training data augmentation",
      "target_node": "ImageNet-C experiments showing improvement from GAN augmentation",
      "description": "Empirical evidence on corruptions",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 4.1 discussion",
      "edge_rationale": "Validation"
    },
    {
      "type": "validated_by",
      "source_node": "Noise layers and orthogonal classifier weights in CNNs",
      "target_node": "RobustBench benchmark results showing increased accuracy post adversarial training",
      "description": "Noise-layer models rank higher on robustness boards",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Survey mentions improved scores",
      "edge_rationale": "Validation"
    },
    {
      "type": "validated_by",
      "source_node": "Interactive robustness benchmark platforms for practitioners",
      "target_node": "Studies indicating human-guided labeling improves out-of-distribution performance",
      "description": "Benchmarks combined with human labeling show performance gains",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Light inference from Section 7.2",
      "edge_rationale": "Evidence"
    },
    {
      "type": "validated_by",
      "source_node": "Crowdsourced perturbation taxonomy development",
      "target_node": "Studies indicating human-guided labeling improves out-of-distribution performance",
      "description": "Crowdsourced data improves OOD robustness",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Light inference, Section 7.2",
      "edge_rationale": "Supporting evidence"
    },
    {
      "type": "validated_by",
      "source_node": "Confidence-threshold reject option classifier",
      "target_node": "Evaluation showing reject option reduces failure rate on unseen perturbations",
      "description": "Experiments show abstention reduces risk",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 8.2.1 cites studies",
      "edge_rationale": "Validation"
    },
    {
      "type": "motivates",
      "source_node": "RobustBench benchmark results showing increased accuracy post adversarial training",
      "target_node": "Fine-tune models with adversarial training using PGD-generated perturbations",
      "description": "Positive benchmark results encourage adoption of PGD adversarial training",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct empirical link Section 6.1.2",
      "edge_rationale": "Intervention justified"
    },
    {
      "type": "motivates",
      "source_node": "ImageNet-C experiments showing improvement from GAN augmentation",
      "target_node": "Augment pre-training datasets with GAN-generated natural corruptions",
      "description": "Demonstrated gains motivate dataset augmentation",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 4.1",
      "edge_rationale": "Evidence to action"
    },
    {
      "type": "motivates",
      "source_node": "RobustBench benchmark results showing increased accuracy post adversarial training",
      "target_node": "Design CNNs with stochastic noise input layers and orthogonal classifiers",
      "description": "Benchmark supports architectural modifications",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Indirect but reported in Section 4.2.2 results",
      "edge_rationale": "Motivation"
    },
    {
      "type": "motivates",
      "source_node": "Evaluation showing reject option reduces failure rate on unseen perturbations",
      "target_node": "Implement confidence-based rejection mechanism during deployment",
      "description": "Empirical reduction in errors motivates deployment-time reject option",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 8.2.1",
      "edge_rationale": "Motivation"
    },
    {
      "type": "motivates",
      "source_node": "Studies indicating human-guided labeling improves out-of-distribution performance",
      "target_node": "Conduct human-centered robustness evaluation with benchmark platforms and crowdsourced perturbation taxonomies",
      "description": "Human data improvements motivate structured evaluation workflows",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 7.2",
      "edge_rationale": "Motivation"
    },
    {
      "type": "motivates",
      "source_node": "Studies indicating human-guided labeling improves out-of-distribution performance",
      "target_node": "Integrate human-in-the-loop dataset curation and rationale collection during data pipeline",
      "description": "Successful human labeling experiments motivate integrating crowd curation earlier",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 7.2.2",
      "edge_rationale": "Motivation"
    }
  ],
  "meta": [
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "date_published",
      "value": "2022-10-17T00:00:00Z"
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/2210.08906"
    },
    {
      "key": "authors",
      "value": [
        "Andrea Tocchetti",
        "Lorenzo Corti",
        "Agathe Balayn",
        "Mireia Yurrita",
        "Philip Lippmann",
        "Marco Brambilla",
        "Jie Yang"
      ]
    },
    {
      "key": "title",
      "value": "A.I. Robustness: a Human-Centered Perspective on Technological Challenges and Opportunities"
    }
  ]
}