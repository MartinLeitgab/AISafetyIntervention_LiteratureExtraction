{
  "nodes": [
    {
      "name": "Inaccurate classification by heterogeneous distributed agents",
      "aliases": [
        "misclassification in heterogeneous sensor networks",
        "low accuracy in multi-agent classifiers"
      ],
      "type": "concept",
      "description": "The overall risk that individual agents with different feature sets misclassify inputs, reducing system reliability.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Problem stated in 1 Introduction: agents observing insufficient attributes cannot make reliable decisions alone."
    },
    {
      "name": "Insufficient per-agent feature dimensions in heterogeneous networks",
      "aliases": [
        "limited local attributes",
        "incomplete information per node"
      ],
      "type": "concept",
      "description": "Each agent sees only a subset of relevant features, making stand-alone classification unreliable.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in 1 Introduction and 1.2 Contribution as the main reason cooperation is needed."
    },
    {
      "name": "Classifier parameter dimension mismatch across agents",
      "aliases": [
        "heterogeneous classifier sizes",
        "unequal model dimensions"
      ],
      "type": "concept",
      "description": "Because feature vectors differ in length, the learned classifier weights have incompatible dimensions, preventing direct parameter averaging.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted in 1 Introduction and 1.2 Contribution as obstacle to traditional cooperative schemes."
    },
    {
      "name": "Smooth label variation across graph neighbourhoods in heterogeneous networks",
      "aliases": [
        "label smoothness assumption",
        "slowly varying labels over graph"
      ],
      "type": "concept",
      "description": "Key assumption that neighbouring agents typically observe the same class, so predicted labels should change slowly along edges.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Formulated in 1.2 Contribution (Eq. 3) and motivates both global and local smoothing strategies."
    },
    {
      "name": "Collaborative output smoothing to enforce graph-wide smoothness",
      "aliases": [
        "cooperative prediction smoothing",
        "graph-regularised output sharing"
      ],
      "type": "concept",
      "description": "Design idea: instead of combining parameters, agents regularise or average their predicted labels so they vary smoothly over the network.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced at the end of 1 Introduction and detailed in Section 2."
    },
    {
      "name": "Local neighbourhood label averaging for synchronized classification clusters",
      "aliases": [
        "cluster-level smoothing",
        "label averaging within same-class neighbours"
      ],
      "type": "concept",
      "description": "Alternate design that averages only neighbours sharing the same true label, suitable when the whole network observes one class at a time.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Defined via Eq. 4 and Algorithm 2 in Section 2."
    },
    {
      "name": "Global graph Laplacian regularization in distributed optimization",
      "aliases": [
        "Laplacian smoothing term",
        "graph-wide regulariser ρ γ^T L γ"
      ],
      "type": "concept",
      "description": "Implementation adds a Laplacian penalty on predicted labels to the loss, inducing global smoothness during stochastic-gradient updates (Algorithm 1).",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 2 Algorithm 1 shows exact update equations including Laplacian term."
    },
    {
      "name": "Neighbour label averaging in stochastic gradient updates",
      "aliases": [
        "Ck-based averaging",
        "local smoothing update rule"
      ],
      "type": "concept",
      "description": "Implementation where each agent replaces its predicted label with the average of neighbours that share the same class before gradient update (Algorithm 2).",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Described in Section 2, Eq. 4 and Algorithm 2."
    },
    {
      "name": "Neighbour label averaging during inference for heterogeneous agents",
      "aliases": [
        "testing-time prediction averaging",
        "deployment neighbour consensus"
      ],
      "type": "concept",
      "description": "During testing/deployment, agents average neighbours’ predictions (Eq. 4) to obtain final labels when local features are insufficient.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mentioned at end of Algorithm 1 and Algorithm 2 descriptions in Section 2."
    },
    {
      "name": "Synthetic and weather dataset experiments demonstrate improved accuracy with global Laplacian smoothing",
      "aliases": [
        "global smoothing empirical results",
        "Algorithm 1 validation"
      ],
      "type": "concept",
      "description": "Simulations on 50-node synthetic network and 139-sensor weather data show lower test error versus non-cooperative baseline.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Results plotted in Figures 3 and 5, Section 4 Experimental Results."
    },
    {
      "name": "Simulation experiments demonstrate improved accuracy with local neighbour averaging",
      "aliases": [
        "local smoothing empirical results",
        "Algorithm 2 validation"
      ],
      "type": "concept",
      "description": "Synthetic experiments (uniform-label scenario) show Algorithm 2 outperforms non-cooperative training.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Figure 3 and accompanying text in Section 4."
    },
    {
      "name": "Train distributed classifiers with global Laplacian regularization and collaborative stochastic gradient algorithm",
      "aliases": [
        "Algorithm 1 training",
        "graph-wide smoothing training"
      ],
      "type": "intervention",
      "description": "During model training (fine-tuning phase), add Laplacian penalty ρ γ^T L γ and perform the stochastic-gradient updates of Algorithm 1.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Training procedure belongs to fine-tuning/RL phase; see Section 2 Distributed Classification Algorithms.",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Validated on synthetic and real datasets (Section 4), indicating prototype-level evidence.",
      "node_rationale": "Direct actionable step to realise the proposed global smoothing method."
    },
    {
      "name": "Train distributed classifiers with neighbour label averaging algorithm",
      "aliases": [
        "Algorithm 2 training",
        "local smoothing training"
      ],
      "type": "intervention",
      "description": "During model training, replace each agent’s prediction with the average over same-label neighbours (Eq. 4) before gradient update, as in Algorithm 2.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Algorithm 2 is applied during training/fine-tuning; Section 2.",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Supported by simulation evidence in Section 4; prototype stage.",
      "node_rationale": "Operationalises local smoothing design for practitioners."
    },
    {
      "name": "Aggregate neighbour predictions during deployment to improve classification reliability",
      "aliases": [
        "testing-time label averaging",
        "deployment-phase neighbour consensus"
      ],
      "type": "intervention",
      "description": "At inference/deployment, each agent replaces its own prediction with the average of its neighbours’ predictions to compensate for limited features.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Executed after models are trained and deployed (testing phase described in Section 2).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Only demonstrated in controlled experiments; no large-scale deployment yet.",
      "node_rationale": "Provides a practical deployment action derived from the same local smoothing principle."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Inaccurate classification by heterogeneous distributed agents",
      "target_node": "Insufficient per-agent feature dimensions in heterogeneous networks",
      "description": "Limited local features make single-agent decisions unreliable, directly causing overall misclassification risk.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit statement in 1 Introduction.",
      "edge_rationale": "Agents lack enough attributes to classify alone (Section 1)."
    },
    {
      "type": "caused_by",
      "source_node": "Inaccurate classification by heterogeneous distributed agents",
      "target_node": "Classifier parameter dimension mismatch across agents",
      "description": "Different feature sizes yield incompatible classifier parameters, hampering cooperation and increasing error.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Discussed but not quantitatively analysed (Section 1).",
      "edge_rationale": "Heterogeneous dimensions prevent parameter averaging, raising misclassification likelihood."
    },
    {
      "type": "addressed_by",
      "source_node": "Insufficient per-agent feature dimensions in heterogeneous networks",
      "target_node": "Smooth label variation across graph neighbourhoods in heterogeneous networks",
      "description": "Assuming label smoothness offers a pathway to overcome limited features through cooperation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical derivation in Section 1.2.",
      "edge_rationale": "Smoothness assumption compensates for missing local information."
    },
    {
      "type": "addressed_by",
      "source_node": "Classifier parameter dimension mismatch across agents",
      "target_node": "Smooth label variation across graph neighbourhoods in heterogeneous networks",
      "description": "Focusing on output smoothness circumvents the need to fuse heterogeneous parameters.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Stated motivation in Section 1 Introduction.",
      "edge_rationale": "Output-level cooperation avoids parameter dimension issues."
    },
    {
      "type": "enabled_by",
      "source_node": "Smooth label variation across graph neighbourhoods in heterogeneous networks",
      "target_node": "Collaborative output smoothing to enforce graph-wide smoothness",
      "description": "Theoretical assumption enables design choice of smoothing outputs across agents.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct logical step articulated in Section 1.2.",
      "edge_rationale": "Design rationale derives from smoothness insight."
    },
    {
      "type": "enabled_by",
      "source_node": "Smooth label variation across graph neighbourhoods in heterogeneous networks",
      "target_node": "Local neighbourhood label averaging for synchronized classification clusters",
      "description": "When labels are uniform network-wide, averaging within neighbourhoods is justified by smoothness.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 2 introduces Algorithm 2 based on same assumption.",
      "edge_rationale": "Smoothness within clusters supports local averaging design."
    },
    {
      "type": "implemented_by",
      "source_node": "Collaborative output smoothing to enforce graph-wide smoothness",
      "target_node": "Global graph Laplacian regularization in distributed optimization",
      "description": "Implements global smoothing by adding Laplacian regulariser to training objective.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Equation 3 and Algorithm 1 give exact formulation.",
      "edge_rationale": "Regulariser operationalises design rationale."
    },
    {
      "type": "implemented_by",
      "source_node": "Local neighbourhood label averaging for synchronized classification clusters",
      "target_node": "Neighbour label averaging in stochastic gradient updates",
      "description": "Implements local smoothing via averaging rule Ck in Algorithm 2.",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Equation 4 and Algorithm 2 are explicit.",
      "edge_rationale": "Averaging rule realises local-smoothing design."
    },
    {
      "type": "implemented_by",
      "source_node": "Local neighbourhood label averaging for synchronized classification clusters",
      "target_node": "Neighbour label averaging during inference for heterogeneous agents",
      "description": "Design also prescribes applying the same averaging at test time.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 2 notes prediction averaging during testing.",
      "edge_rationale": "Inference averaging is part of local smoothing implementation."
    },
    {
      "type": "validated_by",
      "source_node": "Global graph Laplacian regularization in distributed optimization",
      "target_node": "Synthetic and weather dataset experiments demonstrate improved accuracy with global Laplacian smoothing",
      "description": "Empirical results confirm error reduction when using global Laplacian regulariser.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 4 shows quantitative improvements.",
      "edge_rationale": "Experiments test and support mechanism efficacy."
    },
    {
      "type": "validated_by",
      "source_node": "Neighbour label averaging in stochastic gradient updates",
      "target_node": "Simulation experiments demonstrate improved accuracy with local neighbour averaging",
      "description": "Simulations verify performance gain of local averaging approach.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Only synthetic data; no real-world dataset.",
      "edge_rationale": "Validation evidences mechanism’s effectiveness."
    },
    {
      "type": "validated_by",
      "source_node": "Neighbour label averaging during inference for heterogeneous agents",
      "target_node": "Simulation experiments demonstrate improved accuracy with local neighbour averaging",
      "description": "Testing-time averaging contributes to the observed improvement in simulations.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Improvements reported encompass both training and testing averaging.",
      "edge_rationale": "Inference component is included in evaluated algorithm."
    },
    {
      "type": "motivates",
      "source_node": "Synthetic and weather dataset experiments demonstrate improved accuracy with global Laplacian smoothing",
      "target_node": "Train distributed classifiers with global Laplacian regularization and collaborative stochastic gradient algorithm",
      "description": "Positive empirical evidence motivates adopting the global Laplacian training procedure.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Clear performance gains (Figures 3,5).",
      "edge_rationale": "Researchers are encouraged to implement the intervention."
    },
    {
      "type": "motivates",
      "source_node": "Simulation experiments demonstrate improved accuracy with local neighbour averaging",
      "target_node": "Train distributed classifiers with neighbour label averaging algorithm",
      "description": "Simulation results motivate employing local-averaging training.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Evidence limited to synthetic network.",
      "edge_rationale": "Improvement justifies intervention trial."
    },
    {
      "type": "implemented_by",
      "source_node": "Neighbour label averaging during inference for heterogeneous agents",
      "target_node": "Aggregate neighbour predictions during deployment to improve classification reliability",
      "description": "Deployment-phase action applies the inference-time averaging mechanism.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 2 explicitly prescribes this step after training.",
      "edge_rationale": "Intervention operationalises mechanism at deployment."
    },
    {
      "type": "validated_by",
      "source_node": "Aggregate neighbour predictions during deployment to improve classification reliability",
      "target_node": "Simulation experiments demonstrate improved accuracy with local neighbour averaging",
      "description": "Same simulations also include deployment-phase averaging, validating the intervention.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Inference averaging not isolated, but part of evaluated pipeline.",
      "edge_rationale": "Evidence supports benefit of deployment-time averaging."
    },
    {
      "type": "enabled_by",
      "source_node": "Collaborative output smoothing to enforce graph-wide smoothness",
      "target_node": "Local neighbourhood label averaging for synchronized classification clusters",
      "description": "Local averaging is a refinement of the general collaborative smoothing concept.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Design hierarchy implied in Section 2.",
      "edge_rationale": "Local approach is specialised instance of general design."
    },
    {
      "type": "refined_by",
      "source_node": "Global graph Laplacian regularization in distributed optimization",
      "target_node": "Neighbour label averaging in stochastic gradient updates",
      "description": "Local averaging refines the global smoothing idea for uniform-label scenarios.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Relationship implied but not deeply analysed.",
      "edge_rationale": "Both mechanisms share smoothing principle."
    },
    {
      "type": "required_by",
      "source_node": "Collaborative output smoothing to enforce graph-wide smoothness",
      "target_node": "Neighbour label averaging during inference for heterogeneous agents",
      "description": "Testing-time averaging is necessary to maintain smoothness after training.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Section 2 explains need for averaging even after training.",
      "edge_rationale": "Smoothness goal persists into deployment phase."
    },
    {
      "type": "preceded_by",
      "source_node": "Neighbour label averaging in stochastic gradient updates",
      "target_node": "Neighbour label averaging during inference for heterogeneous agents",
      "description": "Training-time averaging precedes and aligns with deployment-time averaging.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Temporal sequence stated but not quantitatively studied.",
      "edge_rationale": "Maintains consistency across lifecycle stages."
    },
    {
      "type": "enabled_by",
      "source_node": "Smooth label variation across graph neighbourhoods in heterogeneous networks",
      "target_node": "Neighbour label averaging during inference for heterogeneous agents",
      "description": "Smoothness assumption justifies averaging predictions at deployment.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical deduction from assumption to practice.",
      "edge_rationale": "Theoretical basis for deployment averaging."
    },
    {
      "type": "addressed_by",
      "source_node": "Inaccurate classification by heterogeneous distributed agents",
      "target_node": "Collaborative output smoothing to enforce graph-wide smoothness",
      "description": "Design rationale directly targets and mitigates the inaccuracy risk.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Stated motivation for smoothing (Section 1).",
      "edge_rationale": "Output smoothing is proposed as remedy."
    }
  ],
  "meta": [
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "date_published",
      "value": "2019-10-30T00:00:00Z"
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1911.04870"
    },
    {
      "key": "authors",
      "value": [
        "Elsa Rizk",
        "Roula Nassif",
        "Ali H. Sayed"
      ]
    },
    {
      "key": "title",
      "value": "Network Classifiers With Output Smoothing"
    }
  ]
}