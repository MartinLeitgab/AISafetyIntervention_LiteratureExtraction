{
  "nodes": [
    {
      "name": "Unreliable complex reasoning in LLMs due to absent anticipation",
      "aliases": [
        "LLM reasoning unreliability from no planning",
        "Lack of anticipation harms LLM reasoning"
      ],
      "type": "concept",
      "description": "Large language models sometimes fail tasks that humans solve with minimal planning, indicating that absence of forethought can make their reasoning unreliable and limit safe usage in complex scenarios.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Represents the top-level safety-relevant risk described throughout the post (Overall assessments)."
    },
    {
      "name": "Sequential next-token generation without planning in LLMs",
      "aliases": [
        "Token-by-token prediction without forethought",
        "Greedy autoregressive generation"
      ],
      "type": "concept",
      "description": "Current transformers choose each word by estimating next-token probabilities rather than planning multi-step continuations, leading to potential reasoning gaps.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Identified as the mechanism producing the risk (opening sections ‘Do models need to plan ahead?’)."
    },
    {
      "name": "Next-token likelihood training objective disincentivizes planning in LLMs",
      "aliases": [
        "Maximum-likelihood objective limits planning",
        "Training loss discourages anticipation"
      ],
      "type": "concept",
      "description": "Because models are optimised only to match distributions over the immediate next token, they receive no gradient signal to model long-range continuations explicitly.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explains the root cause of sequential behaviour (section ‘Could planning help much to reduce loss?’)."
    },
    {
      "name": "Anticipation requires non-input-distributed representations enabling planning",
      "aliases": [
        "Need for compact future-step representations",
        "Non-token-aligned planning representations"
      ],
      "type": "concept",
      "description": "Complex forethought would demand internal vectors that summarise future steps rather than being tied to tokens already seen, something not obviously encouraged by current architectures.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlights representational constraints (section ‘Could a model represent complex continuations?’)."
    },
    {
      "name": "Constrained continuation tasks highlight need for anticipation",
      "aliases": [
        "Tasks with limited valid continuations expose planning need"
      ],
      "type": "concept",
      "description": "Rhyme-constrained poetry, function composition and sentence unscrambling make correct next tokens highly dependent on later structure, thus stressing anticipation.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivates why specialised benchmarks are informative (multiple task-specific sections)."
    },
    {
      "name": "Develop anticipation benchmarks to assess reasoning limitations in LLMs",
      "aliases": [
        "Anticipation benchmarking rationale",
        "Need for planning test suite"
      ],
      "type": "concept",
      "description": "Creating dedicated evaluation tasks allows researchers to directly measure whether a model reasons ahead, providing actionable diagnostics.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Stated as an accessible way to study anticipation (intro and benchmark discussion)."
    },
    {
      "name": "Enable model self-exploration to approximate planning during reasoning",
      "aliases": [
        "Prompt-level exploration rationale",
        "Allow trial-and-error reasoning"
      ],
      "type": "concept",
      "description": "Letting the model generate and inspect intermediate hypotheses (e.g. scratchpads) can compensate for missing built-in anticipation by using additional context tokens.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Derived from observation that GPT-4 succeeds when allowed to test possibilities (Overall assessments)."
    },
    {
      "name": "Fine-tune models on planning tasks to cultivate anticipation",
      "aliases": [
        "Train on anticipation-heavy data",
        "Planning-focused fine-tuning rationale"
      ],
      "type": "concept",
      "description": "Adding supervision on tasks that require forethought could push the network to learn internal planning mechanisms.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Suggested as a route to overcome objective misalignment (section ‘I have no doubts that a properly trained model could engage in anticipation’)."
    },
    {
      "name": "Incorporate hierarchical decoding to support segment-level planning",
      "aliases": [
        "Hierarchical planner decoder rationale",
        "Segment-level planning architecture"
      ],
      "type": "concept",
      "description": "Architectural changes where the model drafts larger chunks (lines, code blocks) before emitting tokens could embed explicit plans in the generation process.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Addresses representational limits noted in ‘Could a model represent complex continuations?’."
    },
    {
      "name": "Benchmark suite using rhyme selection, function composition, and sentence unscrambling tasks",
      "aliases": [
        "Anticipation task suite",
        "Planning diagnostic tasks"
      ],
      "type": "concept",
      "description": "An evaluation collection combining poetry-rhyme matching, nested function composition questions, and scrambled sentence reconstruction to directly test anticipation.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete mechanism to realise the benchmarking rationale (multiple task sections)."
    },
    {
      "name": "Scratchpad chain-of-thought prompting for candidate continuation enumeration",
      "aliases": [
        "Self-exploration scratchpad",
        "Trial-and-error prompt pattern"
      ],
      "type": "concept",
      "description": "Prompts instruct the model to write out alternative continuations, evaluate them, and then output the best answer, effectively giving it external memory for planning.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implements the self-exploration design rationale (Overall assessments)."
    },
    {
      "name": "Segment-level plan generation module within decoder architecture",
      "aliases": [
        "Hierarchical decoder module",
        "Chunk planner component"
      ],
      "type": "concept",
      "description": "A proposed architectural component that first produces a coarse plan (e.g., upcoming sentence or code block) which guides subsequent fine-grained token generation.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Operationalises hierarchical decoding rationale (representational discussion)."
    },
    {
      "name": "GPT-4 failure on function composition without exploration prompt",
      "aliases": [
        "Function composition failure evidence",
        "GPT-4 nested function error"
      ],
      "type": "concept",
      "description": "GPT-4 answered ‘f(g(h(x)))’ instead of the required ‘f(h(g(x)))’, showing it chose an outer function before computing inner values, evidencing lack of anticipation.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Key empirical failure (section ‘Evidence from function composition tasks’)."
    },
    {
      "name": "GPT-4 misunscrambles sentence 'broke he saw the'",
      "aliases": [
        "Sentence unscrambling failure evidence"
      ],
      "type": "concept",
      "description": "When asked to unscramble, GPT-4 outputs ‘he saw the broke’, indicating it does not plan grammatical paths that guarantee completion.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Empirical failure from ‘Evidence from sentence unscrambling tasks’. "
    },
    {
      "name": "GPT-3 rhyme pair misalignment indicates lack of thematic planning",
      "aliases": [
        "Poetry rhyme evidence",
        "Rhyme misalignment observation"
      ],
      "type": "concept",
      "description": "In generated poetry the second rhyme word often fits the theme poorly, suggesting the model did not pick rhyme pairs in advance.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Evidence from poetry section illustrating limited anticipation."
    },
    {
      "name": "GPT-4 success when given trial-and-error exploration hints",
      "aliases": [
        "Exploration improves performance evidence",
        "Trial-and-error scratchpad evidence"
      ],
      "type": "concept",
      "description": "When instructed to test several possibilities before choosing an answer, GPT-4 can solve function composition, demonstrating the utility of self-exploration.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Supports scratchpad approach (Overall assessments)."
    },
    {
      "name": "Conduct pre-deployment evaluation of LLMs using anticipation benchmark suite",
      "aliases": [
        "Apply anticipation benchmarks during testing"
      ],
      "type": "intervention",
      "description": "Integrate the proposed rhyme/function/unscramble benchmark into model eval pipelines to detect reasoning-reliability gaps before release.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Targets the ‘evaluation’ phase prior to deployment – aligns with benchmarking discussion.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Benchmarks exist in prototype form only, no systematic industry adoption (Benchmark sections).",
      "node_rationale": "Direct actionable step implied by benchmarking rationale."
    },
    {
      "name": "Fine-tune LLMs with anticipation-centric tasks to enhance planning capability",
      "aliases": [
        "Planning-focused fine-tuning intervention"
      ],
      "type": "intervention",
      "description": "Augment training data with anticipation tasks (e.g., nested functions, sentence reordering) and perform supervised fine-tuning or RLHF to teach internal planning.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Fine-tuning/RL stage after pre-training.",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Purely proposed; no published experiments in source.",
      "node_rationale": "Mitigates training-objective limitation by adding explicit planning signals."
    },
    {
      "name": "Deploy scratchpad prompting pattern that allows self-exploration before final answer",
      "aliases": [
        "Use chain-of-thought scratchpads in production"
      ],
      "type": "intervention",
      "description": "At inference time prompts instruct the model to enumerate possibilities and then choose, leveraging external text as working memory to simulate anticipation.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Implemented at deployment time via prompt engineering.",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Widely experimented with and partially validated across tasks.",
      "node_rationale": "Operationalises self-exploration design rationale using demonstrated evidence of benefit."
    },
    {
      "name": "Design future LLM architectures with hierarchical decoder enabling segment-level planning",
      "aliases": [
        "Hierarchical decoder intervention"
      ],
      "type": "intervention",
      "description": "Introduce architectural layers that draft high-level plans (sentences, code blocks) before token emission, embedding anticipation natively rather than via prompting.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Changes model design before any training.",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Speculative proposal; no implementation evidence in source.",
      "node_rationale": "Addresses representational constraints by design."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Unreliable complex reasoning in LLMs due to absent anticipation",
      "target_node": "Sequential next-token generation without planning in LLMs",
      "description": "Reasoning unreliability stems from the model’s token-level generation strategy lacking forethought.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Argument repeated across multiple sections; qualitative but consistent.",
      "edge_rationale": "Overall assessments links failures to lack of anticipation."
    },
    {
      "type": "caused_by",
      "source_node": "Sequential next-token generation without planning in LLMs",
      "target_node": "Next-token likelihood training objective disincentivizes planning in LLMs",
      "description": "The training loss focuses on immediate next-token probabilities, creating the sequential generation behaviour.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit reasoning in ‘Could planning help much to reduce loss?’",
      "edge_rationale": "Theoretical section states models aren’t rewarded for long-range planning."
    },
    {
      "type": "caused_by",
      "source_node": "Sequential next-token generation without planning in LLMs",
      "target_node": "Anticipation requires non-input-distributed representations enabling planning",
      "description": "Without suitable representations, models fall back to token-aligned generation.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual link, not empirically proven.",
      "edge_rationale": "Representation discussion argues representational limits drive behaviour."
    },
    {
      "type": "refined_by",
      "source_node": "Constrained continuation tasks highlight need for anticipation",
      "target_node": "Develop anticipation benchmarks to assess reasoning limitations in LLMs",
      "description": "Observation that such tasks are revealing motivates creation of specific benchmarks.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Paper directly suggests these tasks are ‘accessible subjects to study’.",
      "edge_rationale": "Benchmark rationale drawn from tasks sections."
    },
    {
      "type": "mitigated_by",
      "source_node": "Next-token likelihood training objective disincentivizes planning in LLMs",
      "target_node": "Fine-tune models on planning tasks to cultivate anticipation",
      "description": "Supplementary fine-tuning can inject gradients that reward planning beyond next token.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Proposed rather than demonstrated.",
      "edge_rationale": "Author notes a ‘properly trained model could engage in anticipation’."
    },
    {
      "type": "mitigated_by",
      "source_node": "Anticipation requires non-input-distributed representations enabling planning",
      "target_node": "Incorporate hierarchical decoding to support segment-level planning",
      "description": "Architectural changes can embed representations that cover larger segments.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Speculative architectural link.",
      "edge_rationale": "Hierarchical decoding proposed to overcome representational limits."
    },
    {
      "type": "implemented_by",
      "source_node": "Develop anticipation benchmarks to assess reasoning limitations in LLMs",
      "target_node": "Benchmark suite using rhyme selection, function composition, and sentence unscrambling tasks",
      "description": "The suite realises the benchmarking rationale using concrete tasks.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Tasks are explicitly provided in source.",
      "edge_rationale": "Sections lay out each task as example benchmark."
    },
    {
      "type": "implemented_by",
      "source_node": "Enable model self-exploration to approximate planning during reasoning",
      "target_node": "Scratchpad chain-of-thought prompting for candidate continuation enumeration",
      "description": "Scratchpad prompts are the concrete mechanism for self-exploration.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Evidence shows prompt helps GPT-4 succeed.",
      "edge_rationale": "Overall assessments detail successful trial-and-error approach."
    },
    {
      "type": "implemented_by",
      "source_node": "Fine-tune models on planning tasks to cultivate anticipation",
      "target_node": "Benchmark suite using rhyme selection, function composition, and sentence unscrambling tasks",
      "description": "Tasks in the suite can serve as supervised data for planning-focused fine-tuning.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference; not explicitly stated but logical.",
      "edge_rationale": "Using tasks for training naturally follows their creation."
    },
    {
      "type": "implemented_by",
      "source_node": "Incorporate hierarchical decoding to support segment-level planning",
      "target_node": "Segment-level plan generation module within decoder architecture",
      "description": "The hierarchical module realises the architectural rationale.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Speculative design component.",
      "edge_rationale": "Module described as embodiment of hierarchical decoding."
    },
    {
      "type": "validated_by",
      "source_node": "Benchmark suite using rhyme selection, function composition, and sentence unscrambling tasks",
      "target_node": "GPT-4 failure on function composition without exploration prompt",
      "description": "Function-composition task exposes lack of anticipation, validating benchmark usefulness.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct empirical example.",
      "edge_rationale": "Task section shows benchmark catching failure."
    },
    {
      "type": "validated_by",
      "source_node": "Benchmark suite using rhyme selection, function composition, and sentence unscrambling tasks",
      "target_node": "GPT-4 misunscrambles sentence 'broke he saw the'",
      "description": "Sentence unscrambling task further confirms benchmark’s diagnostic power.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct empirical example.",
      "edge_rationale": "Unscrambling failure described in evidence section."
    },
    {
      "type": "validated_by",
      "source_node": "Benchmark suite using rhyme selection, function composition, and sentence unscrambling tasks",
      "target_node": "GPT-3 rhyme pair misalignment indicates lack of thematic planning",
      "description": "Poetry rhyme task shows benchmark highlighting planning deficiencies.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Observation from poetry analysis.",
      "edge_rationale": "Rhyme misalignment tied to benchmark."
    },
    {
      "type": "validated_by",
      "source_node": "Scratchpad chain-of-thought prompting for candidate continuation enumeration",
      "target_node": "GPT-4 success when given trial-and-error exploration hints",
      "description": "Improved performance after scratchpad prompt validates self-exploration mechanism.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Strong qualitative evidence in text.",
      "edge_rationale": "Trial-and-error hint example supports mechanism."
    },
    {
      "type": "motivates",
      "source_node": "GPT-4 failure on function composition without exploration prompt",
      "target_node": "Conduct pre-deployment evaluation of LLMs using anticipation benchmark suite",
      "description": "Failure demonstrates need for systematic evaluation before deployment.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct logical link; medium evidence.",
      "edge_rationale": "Benchmark failure suggests eval necessity."
    },
    {
      "type": "motivates",
      "source_node": "GPT-3 rhyme pair misalignment indicates lack of thematic planning",
      "target_node": "Fine-tune LLMs with anticipation-centric tasks to enhance planning capability",
      "description": "Evidence of rhyme misalignment encourages training interventions to teach planning.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference from qualitative observation.",
      "edge_rationale": "Poor poetry motivates training changes."
    },
    {
      "type": "motivates",
      "source_node": "GPT-4 success when given trial-and-error exploration hints",
      "target_node": "Deploy scratchpad prompting pattern that allows self-exploration before final answer",
      "description": "Demonstrated benefit of exploration directly supports using scratchpad prompting in production.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Clear empirical success example.",
      "edge_rationale": "Success example shows practical utility."
    },
    {
      "type": "motivates",
      "source_node": "GPT-4 failure on function composition without exploration prompt",
      "target_node": "Design future LLM architectures with hierarchical decoder enabling segment-level planning",
      "description": "Persistent failure despite large model size motivates deeper architectural solutions.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Speculative but logically plausible.",
      "edge_rationale": "Failure indicates architectural limitation."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://www.lesswrong.com/posts/A7aBAqLRcAPhM5mBB/anticipation-in-llms"
    },
    {
      "key": "title",
      "value": "Anticipation in LLMs"
    },
    {
      "key": "date_published",
      "value": "2023-07-24T15:53:07Z"
    },
    {
      "key": "authors",
      "value": [
        "derek shiller"
      ]
    },
    {
      "key": "source",
      "value": "lesswrong"
    }
  ]
}