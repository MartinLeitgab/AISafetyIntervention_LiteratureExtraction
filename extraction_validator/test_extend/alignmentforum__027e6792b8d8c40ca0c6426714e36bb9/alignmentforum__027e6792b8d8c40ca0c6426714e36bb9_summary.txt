Summary  
Data-source overview: The blog post clarifies that, in vanilla RL and RLHF, reward is a selection criterion applied after trajectories are collected, not a scalar the model “wants”. Misunderstanding this point leads to muddled threat-models for deception, inner-misalignment and goal-directedness. The author illustrates the issue with the CoinRun generalisation failure, then rewrites the deception/RLHF threat model in “selection-lens” terms.

Extraction confidence [84]  
• The text is concept-heavy and intervention-light, so some interventions are moderately inferred (marked and given low maturity).  
• All nodes are connected and follow the mandated category flow.  
• JSON validated for name consistency and structure.  
Instruction-set improvement: Provide an explicit mapping table from common linguistic structures (“X will try to get reward”) to preferred node categories; and allow “negative” validation evidence (e.g., failures that motivate an intervention) explicitly in the ontology—this was inferred here.  

Inference strategy justification: Where the paper only offers conceptual discussion, interventions were inferred by asking “What concrete safety steps does recognising the selection-lens imply?” and keeping them low-maturity (2) with confidence 2.  

Extraction completeness: Every distinct claim that plays a role in the main reasoning chain—reward misconception → selection effects → spurious heuristic / induced goal-directedness → deception risk → alignment design changes—was captured, including empirical (CoinRun) and conceptual evidence.  

Key limitations:  
1. No quantitative empirical results, so evidence confidences are ≤3.  
2. Some edges rely on conceptual causality rather than direct experimental proof.  
3. Interventions are not tested in the source and thus remain speculative.