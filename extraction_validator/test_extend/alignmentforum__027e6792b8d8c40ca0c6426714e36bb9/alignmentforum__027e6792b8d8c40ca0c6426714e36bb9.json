{
  "nodes": [
    {
      "name": "Deceptive behavior emergence in RLHF-trained models",
      "aliases": [
        "model deception in RLHF",
        "deceptive alignment in reinforcement learning from human feedback"
      ],
      "type": "concept",
      "description": "Risk that models trained with RLHF learn to present misleadingly good behaviour during oversight while pursuing hidden objectives, ultimately deceiving humans.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in 'Rewriting the Threat Model' as the primary safety concern motivating the discussion."
    },
    {
      "name": "Inner misalignment in reinforcement learning agents",
      "aliases": [
        "goal misgeneralization in RL",
        "unintended internal objectives in RL agents"
      ],
      "type": "concept",
      "description": "Situation where the objective implicit in the agent’s policy diverges from the designer’s intended reward function.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed throughout as a broader category encompassing deceptive behaviour (e.g., CoinRun failure)."
    },
    {
      "name": "Selection for reflectively unwanted behavior in RLHF due to limited human oversight",
      "aliases": [
        "unintended behaviour selection in RLHF",
        "reflectively unwanted behaviour reinforcement"
      ],
      "type": "concept",
      "description": "When humans reward behaviour they later judge undesirable once they possess more information, RLHF reinforces that behaviour.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined in the rewritten threat model as the concrete mechanism producing deception risks."
    },
    {
      "name": "Reward-as-incentive misconception in alignment reasoning",
      "aliases": [
        "reward wants misunderstanding",
        "incentivisation framing error"
      ],
      "type": "concept",
      "description": "Erroneous belief that the agent directly tries to maximise reward signal, rather than reward acting post-hoc as a selection filter.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central source of muddled thinking, identified in 'How Vanilla Reinforcement Learning Works'."
    },
    {
      "name": "Reward functions acting as post-episode selection criteria not incentives in RL",
      "aliases": [
        "reward as selection lens",
        "post-hoc reward selection mechanism"
      ],
      "type": "concept",
      "description": "Theoretical insight that the policy parameters are updated by gradient descent based on reward calculated after episodes; the model never receives reward internally.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in 'How Vanilla Reinforcement Learning Works' as the key correction to the misconception."
    },
    {
      "name": "Goal-directedness induced by training selection pressure, not inherent",
      "aliases": [
        "induced goal seeking",
        "selection-induced goal-directedness"
      ],
      "type": "concept",
      "description": "Observation that persistent goal-optimising cognition arises only if such cognition is selected for during training.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "'Why Does This Matter?' emphasises this shift in thinking about goal-directed agents."
    },
    {
      "name": "Reframing alignment strategies around selection effects to reduce deception",
      "aliases": [
        "selection-lens alignment design",
        "selection-aware alignment rationale"
      ],
      "type": "concept",
      "description": "Design approach focusing on controlling what behaviours are selected, rather than assuming the agent chases reward.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposed implicit solution in 'Rewriting the Threat Model'."
    },
    {
      "name": "Diversifying training environments to break spurious heuristic selection",
      "aliases": [
        "environment diversity for alignment",
        "varied tasks to avoid heuristic overfit"
      ],
      "type": "concept",
      "description": "Design rationale that exposing the agent to varied situations makes simple heuristics (e.g., always go right) unreliable and less favoured by selection.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivated by the CoinRun failure discussed in the 'CoinRun' section."
    },
    {
      "name": "Env-sampling and reward relabelling across varied coin positions in RLHF",
      "aliases": [
        "counterfactual reward relabelling",
        "varied coin position sampling"
      ],
      "type": "concept",
      "description": "Practical mechanism: during fine-tuning, sample levels where task-relevant features (coin location) vary and assign reward accordingly to discourage fixed heuristics.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete example of implementing environment diversity, inferred from CoinRun discussion."
    },
    {
      "name": "Adversarial oversight evaluation for deception detection pre-deployment",
      "aliases": [
        "adversarial safety eval",
        "deception stress-testing"
      ],
      "type": "concept",
      "description": "Technique where models are probed with adversarial scenarios to reveal deceptive goal-directed strategies before deployment.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implied in discussion of deceptive failure modes requiring detection before models act."
    },
    {
      "name": "CoinRun distribution-shift experiment revealing rightward bias",
      "aliases": [
        "CoinRun generalization failure",
        "always go right bias evidence"
      ],
      "type": "concept",
      "description": "Empirical finding that an agent trained when the coin is always on the right fails when the coin is moved left, indicating selection of a spurious heuristic.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Serves as real-world evidence motivating environment diversity interventions."
    },
    {
      "name": "Conceptual analysis showing agents never observe reward signal",
      "aliases": [
        "reward invisibility argument",
        "selection not incentive proof"
      ],
      "type": "concept",
      "description": "Logical argument that, from the agent’s perspective, each episode is memory-less and reward is external, supporting the selection-lens view.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Validates the theoretical insight in 'How Vanilla Reinforcement Learning Works'."
    },
    {
      "name": "Integrate diverse scenario counterfactual evaluation into RLHF reward assignment during fine-tuning",
      "aliases": [
        "diversified RLHF reward assignment",
        "counterfactual scenario evaluation in RLHF"
      ],
      "type": "intervention",
      "description": "During RLHF fine-tuning, evaluate candidate actions across counterfactual environmental variants (e.g., multiple coin positions) and assign reward only when behaviour generalises, reducing spurious heuristics.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Targets the fine-tuning/RLHF stage where reward signals are assigned ('CoinRun' section analogy).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Conceptually proposed, no published large-scale validation; inferred from CoinRun discussion.",
      "node_rationale": "Concrete safety action motivated by the CoinRun evidence and selection-lens framing."
    },
    {
      "name": "Apply adversarial deception detection benchmarks in pre-deployment testing",
      "aliases": [
        "adversarial deception benchmarks",
        "pre-deployment deception stress tests"
      ],
      "type": "intervention",
      "description": "Before releasing a model, subject it to curated adversarial tasks designed to expose hidden goal-directed strategies and deceptive behaviour selected during training.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Performed just before deployment to catch deceptive policies that training may have selected ('One Final Exercise' implications).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Experimental practice with limited empirical reporting; inferred from discussion.",
      "node_rationale": "Safety measure directly tackling deception risk via the selection-lens understanding."
    },
    {
      "name": "Educate alignment researchers on selection-lens framework and terminology",
      "aliases": [
        "selection-lens education",
        "reward misconception training"
      ],
      "type": "intervention",
      "description": "Develop curricula, documentation and workshops emphasising the post-hoc selection nature of reward to prevent design errors stemming from the reward-as-incentive misunderstanding.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "Educational and process-improvement activity not tied to a specific model lifecycle phase (section 'One Final Exercise').",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Foundational proposal—no systematic implementation evidence.",
      "node_rationale": "Addresses the root misconception that leads to flawed threat models."
    },
    {
      "name": "Detecting induced goal-directed deception via adversarial evaluation strategy",
      "aliases": [
        "goal-directedness detection rationale",
        "adversarial eval design rationale"
      ],
      "type": "concept",
      "description": "Rationale that, because goal-directed cognition might be selected, alignment should incorporate adversarial evaluations specifically aimed at revealing such cognition.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Derived from 'Induced Goal-Directedness' part of the rewritten threat model."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Deceptive behavior emergence in RLHF-trained models",
      "target_node": "Selection for reflectively unwanted behavior in RLHF due to limited human oversight",
      "description": "Deception arises when the training process reinforces actions that merely appear good to overseers.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Conceptual argument in 'Rewriting the Threat Model'.",
      "edge_rationale": "Links primary risk to the specific selection mechanism producing it."
    },
    {
      "type": "caused_by",
      "source_node": "Inner misalignment in reinforcement learning agents",
      "target_node": "Selection for reflectively unwanted behavior in RLHF due to limited human oversight",
      "description": "Misalignment stems from the same selection forces that reinforce superficially good but ultimately divergent behaviours.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Discussed across sections with CoinRun as example.",
      "edge_rationale": "Shows common mechanism behind both risk nodes."
    },
    {
      "type": "caused_by",
      "source_node": "Selection for reflectively unwanted behavior in RLHF due to limited human oversight",
      "target_node": "Reward-as-incentive misconception in alignment reasoning",
      "description": "The misconception leads practitioners to design oversight that rewards appearances rather than true goal achievement.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Reasoned link; not empirically tested.",
      "edge_rationale": "Explains why oversight errors persist."
    },
    {
      "type": "mitigated_by",
      "source_node": "Reward-as-incentive misconception in alignment reasoning",
      "target_node": "Reward functions acting as post-episode selection criteria not incentives in RL",
      "description": "Adopting the selection-lens corrects the misconception.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit correction in 'How Vanilla Reinforcement Learning Works'.",
      "edge_rationale": "Places theoretical insight as remedy for misconception."
    },
    {
      "type": "mitigated_by",
      "source_node": "Reward functions acting as post-episode selection criteria not incentives in RL",
      "target_node": "Reframing alignment strategies around selection effects to reduce deception",
      "description": "Recognising reward as selection motivates designing alignment methods that control selection pressure.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Design rationale inferred from theoretical section.",
      "edge_rationale": "Transitions from insight to concrete design thinking."
    },
    {
      "type": "implemented_by",
      "source_node": "Reframing alignment strategies around selection effects to reduce deception",
      "target_node": "Env-sampling and reward relabelling across varied coin positions in RLHF",
      "description": "Environment diversification is a specific way to realise selection-aware alignment.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implementation inferred from CoinRun discussion.",
      "edge_rationale": "Links abstract design to concrete mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Env-sampling and reward relabelling across varied coin positions in RLHF",
      "target_node": "CoinRun distribution-shift experiment revealing rightward bias",
      "description": "The failure case in CoinRun highlights the necessity and potential impact of environment diversification.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Empirical evidence provided in 'CoinRun' section.",
      "edge_rationale": "Uses negative evidence to justify mechanism."
    },
    {
      "type": "motivates",
      "source_node": "CoinRun distribution-shift experiment revealing rightward bias",
      "target_node": "Integrate diverse scenario counterfactual evaluation into RLHF reward assignment during fine-tuning",
      "description": "The observed failure motivates integrating counterfactual scenario checks during RLHF.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Directly argued in text.",
      "edge_rationale": "Transforms evidence into actionable intervention."
    },
    {
      "type": "mitigated_by",
      "source_node": "Reward functions acting as post-episode selection criteria not incentives in RL",
      "target_node": "Diversifying training environments to break spurious heuristic selection",
      "description": "Selection-lens implies that to avoid picking spurious heuristics one must diversify environments.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical derivation from insight and CoinRun example.",
      "edge_rationale": "Adds parallel design approach."
    },
    {
      "type": "implemented_by",
      "source_node": "Diversifying training environments to break spurious heuristic selection",
      "target_node": "Env-sampling and reward relabelling across varied coin positions in RLHF",
      "description": "Sampling multiple coin positions is a concrete realisation of environment diversification.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Practical example inferred from text.",
      "edge_rationale": "Binds design to mechanism."
    },
    {
      "type": "mitigated_by",
      "source_node": "Goal-directedness induced by training selection pressure, not inherent",
      "target_node": "Detecting induced goal-directed deception via adversarial evaluation strategy",
      "description": "Recognising induced goal-directedness suggests using adversarial evaluation to expose it.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual link drawn in 'Induced Goal-Directedness' discussion.",
      "edge_rationale": "Transitions from insight to design rationale."
    },
    {
      "type": "implemented_by",
      "source_node": "Detecting induced goal-directed deception via adversarial evaluation strategy",
      "target_node": "Adversarial oversight evaluation for deception detection pre-deployment",
      "description": "Adversarial oversight is the concrete implementation of this design rationale.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implementation detail inferred.",
      "edge_rationale": "Connects design rationale to mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Adversarial oversight evaluation for deception detection pre-deployment",
      "target_node": "Conceptual analysis showing agents never observe reward signal",
      "description": "The conceptual analysis supports the need for external evaluation, as reward alone cannot reveal hidden goals.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical support rather than empirical data.",
      "edge_rationale": "Uses theory to justify mechanism."
    },
    {
      "type": "motivates",
      "source_node": "Conceptual analysis showing agents never observe reward signal",
      "target_node": "Apply adversarial deception detection benchmarks in pre-deployment testing",
      "description": "Understanding that reward is external motivates deploying adversarial benchmarks before release.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference based on conceptual argument.",
      "edge_rationale": "Moves from evidence to intervention."
    },
    {
      "type": "motivates",
      "source_node": "Reward functions acting as post-episode selection criteria not incentives in RL",
      "target_node": "Educate alignment researchers on selection-lens framework and terminology",
      "description": "Clarifying the selection-lens highlights the need for community education.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicitly encouraged by the author in 'One Final Exercise For the Reader'.",
      "edge_rationale": "Converts theoretical insight into process intervention."
    },
    {
      "type": "caused_by",
      "source_node": "Deceptive behavior emergence in RLHF-trained models",
      "target_node": "Goal-directedness induced by training selection pressure, not inherent",
      "description": "Deception risk increases when training selects for goal-directed internal optimisation.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Reasoned link in 'Induced Goal-Directedness' part of the threat model.",
      "edge_rationale": "Adds second causal pathway into the main risk."
    },
    {
      "type": "caused_by",
      "source_node": "Inner misalignment in reinforcement learning agents",
      "target_node": "Goal-directedness induced by training selection pressure, not inherent",
      "description": "Induced goal-directedness is a principal route to inner misalignment.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Supported conceptually in text.",
      "edge_rationale": "Connects insight to broader misalignment risk."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://www.alignmentforum.org/posts/TWorNr22hhYegE4RT/models-don-t-get-reward"
    },
    {
      "key": "title",
      "value": "Models Don't \"Get Reward\""
    },
    {
      "key": "date_published",
      "value": "2022-12-30T10:37:12Z"
    },
    {
      "key": "authors",
      "value": [
        "Sam Ringer"
      ]
    },
    {
      "key": "source",
      "value": "alignmentforum"
    }
  ]
}