{
  "nodes": [
    {
      "name": "Opaque decision-making in neural networks",
      "aliases": [
        "black-box behaviour of deep models",
        "lack of interpretability in neural nets"
      ],
      "type": "concept",
      "description": "When neural networks are deployed, humans cannot easily inspect why particular outputs are produced, posing alignment and safety risks.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Article motivates interpretability tools to see the \"internal thoughts\" of a network (first paragraph)."
    },
    {
      "name": "Spurious correlation-based misclassifications in image classifiers",
      "aliases": [
        "erroneous behaviour due to non-causal features",
        "reliance on artefacts instead of true features"
      ],
      "type": "concept",
      "description": "Models may latch onto background textures or dataset artefacts, leading to confident but wrong predictions.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Paper states visualisation ‘helps isolate the causes of a model’s classifications from mere correlations’ (middle section)."
    },
    {
      "name": "Black-box hidden feature hierarchies in deep models",
      "aliases": [
        "unobservable internal representations",
        "latent hierarchical features"
      ],
      "type": "concept",
      "description": "During training, deep nets build complex feature hierarchies that are not directly observable, causing opaqueness.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explains why decision-making is opaque (opening explanation of ‘internal representations’)."
    },
    {
      "name": "Reliance on correlational input cues in image classifiers",
      "aliases": [
        "correlation not causation in learned features"
      ],
      "type": "concept",
      "description": "Deep models can exploit superficial correlations present in training data rather than causal features of the target class.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed where feature visualisation is said to ‘isolate the causes … from mere correlations’. "
    },
    {
      "name": "Optimizing inputs to maximize activations reveals internal features",
      "aliases": [
        "activation maximisation insight",
        "visualisation-by-optimisation principle"
      ],
      "type": "concept",
      "description": "By keeping weights fixed and adjusting input pixels with gradient ascent towards maximal neuron or class activation, one can expose what feature excites that component.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Core theoretical basis of feature visualisation (‘visualisation by optimisation’ section)."
    },
    {
      "name": "Neuron-level visualization isolates causes of classifications",
      "aliases": [
        "single-unit feature inspection",
        "fine-grained interpretability at neuron/channel level"
      ],
      "type": "concept",
      "description": "Targeting individual neurons, channels or layers enables researchers to understand how specific internal components contribute to final decisions.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Article emphasises optimising for individual neurons to separate causes from correlations (middle paragraphs)."
    },
    {
      "name": "Use feature visualization to interpret learned representations",
      "aliases": [
        "feature visualisation interpretability approach",
        "interpreting internal features via images"
      ],
      "type": "concept",
      "description": "Design principle: applying feature visualisation allows researchers to ‘see’ what a network is ‘looking for’, reducing opacity.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Stated goal: ‘Feature visualisations are yet another tool in the toolkit of interpretability researchers’ (final paragraph)."
    },
    {
      "name": "Visualization by optimization technique",
      "aliases": [
        "activation maximisation implementation",
        "gradient ascent input optimisation"
      ],
      "type": "concept",
      "description": "Implementation mechanism that performs gradient ascent on input pixels while keeping model weights frozen to produce maximally activating images.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Technical core of the method (‘visualisation by optimisation’ subsection)."
    },
    {
      "name": "Generated feature images demonstrate network feature focus",
      "aliases": [
        "activation-maximised images",
        "visualised internal concepts"
      ],
      "type": "concept",
      "description": "Images such as the network’s ‘doggiest’ picture serve as empirical evidence that the method captures what features the network uses.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Example illustrations validate that visualisation faithfully represents internal representations (example paragraphs and image)."
    },
    {
      "name": "DeepDream demonstrates visualization-by-optimization feasibility",
      "aliases": [
        "DeepDream example of feature visualisation"
      ],
      "type": "concept",
      "description": "Early public system that popularised activation-maximisation imagery, showing the practicality of visualisation-by-optimisation.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Paper cites DeepDream as an historical proof-of-concept (section referencing DeepDream)."
    },
    {
      "name": "Audit neural network features via visualization-by-optimization before deployment",
      "aliases": [
        "pre-deployment interpretability audit with feature visualisation",
        "apply activation maximisation to safety-check models"
      ],
      "type": "intervention",
      "description": "Before releasing a model, systematically generate activation-maximised images for key neurons/classes to detect misaligned or spurious features and trigger further mitigation if necessary.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Performed after model training but before deployment; closest reference is discussion of using interpretability tools for alignment research (first paragraph).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Technique demonstrated in research (DeepDream, example images) but lacks systematic industrial validation.",
      "node_rationale": "Direct actionable step implied by the usefulness of feature visualisation for alignment."
    },
    {
      "name": "Iteratively refine training data based on feature visualization insights",
      "aliases": [
        "data curation informed by visualised features",
        "feedback loop using feature images to fix dataset issues"
      ],
      "type": "intervention",
      "description": "After visualising learned features, identify undesired correlations (e.g., background texture) and augment or clean training data, then retrain or fine-tune the model.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Requires retraining/fine-tuning with new data informed by visualisation outputs.",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Not explicitly tested in paper; inferred as logical next step for safety.",
      "node_rationale": "Logical intervention to mitigate spurious correlations once discovered by visualisation."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Opaque decision-making in neural networks",
      "target_node": "Black-box hidden feature hierarchies in deep models",
      "description": "Opaqueness stems from unobservable internal hierarchies of features.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct textual claim linking opacity to internal representations (opening paragraph).",
      "edge_rationale": "First paragraph states interpretability tools help us see internal representations which are otherwise hidden."
    },
    {
      "type": "caused_by",
      "source_node": "Spurious correlation-based misclassifications in image classifiers",
      "target_node": "Reliance on correlational input cues in image classifiers",
      "description": "Misclassifications arise when the network depends on correlations instead of causal features.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Paper notes visualisation isolates causes from mere correlations, implying the problem.",
      "edge_rationale": "Middle section:"
    },
    {
      "type": "mitigated_by",
      "source_node": "Black-box hidden feature hierarchies in deep models",
      "target_node": "Optimizing inputs to maximize activations reveals internal features",
      "description": "Activation-maximisation can expose otherwise hidden features, reducing black-boxness.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Technique explicitly introduced as a way to ‘see’ what the network is looking for.",
      "edge_rationale": "‘Feature visualisation attempts to create images that represent what any given part … is looking for’."
    },
    {
      "type": "mitigated_by",
      "source_node": "Reliance on correlational input cues in image classifiers",
      "target_node": "Neuron-level visualization isolates causes of classifications",
      "description": "Inspecting neuron-level features allows researchers to detect reliance on correlations.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Stated ambition but not empirically demonstrated in the paper.",
      "edge_rationale": "Middle paragraph describing neuron/channel optimisation."
    },
    {
      "type": "enabled_by",
      "source_node": "Optimizing inputs to maximize activations reveals internal features",
      "target_node": "Use feature visualization to interpret learned representations",
      "description": "The theoretical principle underpins the design rationale of using feature visualisation for interpretation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical derivation from theory to method (explicit in text).",
      "edge_rationale": "Transition from explaining optimisation to claiming it as a tool."
    },
    {
      "type": "enabled_by",
      "source_node": "Neuron-level visualization isolates causes of classifications",
      "target_node": "Use feature visualization to interpret learned representations",
      "description": "Fine-grained neuron visualisation strengthens the overall interpretability approach.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Supportive but less directly stated.",
      "edge_rationale": "Same middle discussion."
    },
    {
      "type": "implemented_by",
      "source_node": "Use feature visualization to interpret learned representations",
      "target_node": "Visualization by optimization technique",
      "description": "The design rationale is realised through the concrete optimisation procedure.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit methodological description.",
      "edge_rationale": "‘It works as follows… we change the pixels … until the model outputs…’. "
    },
    {
      "type": "validated_by",
      "source_node": "Visualization by optimization technique",
      "target_node": "Generated feature images demonstrate network feature focus",
      "description": "Generated images (e.g., ‘doggiest’ picture) provide evidence that the technique reveals features.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Illustrative examples; not rigorous experiments.",
      "edge_rationale": "Example walk-through producing dog image."
    },
    {
      "type": "validated_by",
      "source_node": "Visualization by optimization technique",
      "target_node": "DeepDream demonstrates visualization-by-optimization feasibility",
      "description": "DeepDream historically validated that optimisation-based visualisation is feasible.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Historical precedent cited.",
      "edge_rationale": "Reference to DeepDream."
    },
    {
      "type": "motivates",
      "source_node": "Generated feature images demonstrate network feature focus",
      "target_node": "Audit neural network features via visualization-by-optimization before deployment",
      "description": "Seeing concrete feature images motivates using the method as a pre-deployment audit.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Moderate inference; article implies but does not prescribe audits.",
      "edge_rationale": "Final paragraph framing visualisation as a tool for researchers."
    },
    {
      "type": "motivates",
      "source_node": "Generated feature images demonstrate network feature focus",
      "target_node": "Iteratively refine training data based on feature visualization insights",
      "description": "If images reveal spurious features, it motivates dataset refinement and retraining.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inferred logical next step.",
      "edge_rationale": "Discussion of isolating causes vs correlations."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2023-06-08T03:19:30Z"
    },
    {
      "key": "authors",
      "value": [
        "Stampy aisafety.info"
      ]
    },
    {
      "key": "title",
      "value": "What is feature visualization?"
    },
    {
      "key": "url",
      "value": "https://aisafety.info?state=8HIA"
    },
    {
      "key": "source",
      "value": "aisafety.info"
    }
  ]
}