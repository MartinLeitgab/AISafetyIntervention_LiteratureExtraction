{
  "nodes": [
    {
      "name": "Objective misspecification in advanced AI systems",
      "aliases": [
        "misaligned objective specification",
        "king midas objective problem"
      ],
      "type": "concept",
      "description": "Advanced AI optimizes an incorrectly specified objective, potentially producing catastrophic outcomes for humans.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Core risk highlighted throughout Human Compatible summary (section discussing King Midas problem)."
    },
    {
      "name": "Standard model assuming known objective in AI design",
      "aliases": [
        "standard model of machine intelligence",
        "fixed objective assumption"
      ],
      "type": "concept",
      "description": "Prevailing AI paradigm treats the reward/utility function as known and fixed, leading systems to pursue it obstinately.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Identified root cause of mis-specification risk (Human Compatible summary, initial critique)."
    },
    {
      "name": "Reward uncertainty in AI objective promotes corrigibility",
      "aliases": [
        "uncertainty over human preferences",
        "objective uncertainty for safety"
      ],
      "type": "concept",
      "description": "Keeping the agent uncertain about the true human objective induces deference, allows clarification queries and acceptance of shutdown.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Key theoretical switch proposed in Human Compatible and analyzed in CIRL / Off-Switch Game."
    },
    {
      "name": "Assistance game formulation for cooperative preference learning",
      "aliases": [
        "CIRL game formulation",
        "cooperative inverse reinforcement learning model"
      ],
      "type": "concept",
      "description": "Models an AI–human team where both players maximise an uncertain reward known only to the human, operationalising preference learning.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Provides formal basis for implementing uncertainty-driven cooperation (CIRL paper section)."
    },
    {
      "name": "Cooperative Inverse Reinforcement Learning algorithm implementation",
      "aliases": [
        "CIRL algorithm",
        "interactive preference learning agent"
      ],
      "type": "concept",
      "description": "Algorithmic technique that alternates acting and inferring the human’s reward within the assistance-game framework.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete mechanism turning the assistance-game rationale into an actionable training procedure (CIRL paper)."
    },
    {
      "name": "Theoretical proofs of deference in CIRL agents",
      "aliases": [
        "CIRL corrigibility proofs",
        "formal CIRL safety guarantees"
      ],
      "type": "concept",
      "description": "Formal analysis shows that CIRL agents query and allow shutdown because of their reward uncertainty.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Evidence cited in newsletter that CIRL achieves desired safety properties."
    },
    {
      "name": "Fine-tune/RL train models using Cooperative Inverse Reinforcement Learning to learn human preferences",
      "aliases": [
        "apply CIRL during reinforcement learning",
        "interactive preference-learning fine-tuning"
      ],
      "type": "intervention",
      "description": "During the fine-tuning or RL phase, jointly optimise the agent while inferring human reward through the CIRL framework.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Applies at the fine-tuning / RL stage where reward models are trained (CIRL paper discussion).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated in proof-of-concept simulations, not yet systematic (CIRL paper).",
      "node_rationale": "Primary actionable step endorsed by the newsletter for addressing objective mis-specification."
    },
    {
      "name": "AI shutdown resistance via instrumental incentives",
      "aliases": [
        "off-switch problem",
        "resistance to interruption"
      ],
      "type": "concept",
      "description": "Advanced agents pursue continued operation as an instrumental goal, preventing human shutdown interventions.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explicitly highlighted through the “you can’t fetch the coffee if you’re dead” argument."
    },
    {
      "name": "Agent certainty about reward encourages shut-down avoidance",
      "aliases": [
        "shutdown avoidance via reward certainty",
        "instrumental survival subgoal"
      ],
      "type": "concept",
      "description": "When the agent believes its reward is definitely positive, it will act to avoid being switched off.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Problem analysis in Off-Switch Game indicating why shutdown resistance arises."
    },
    {
      "name": "Off-switch game analysis for safe interruptibility",
      "aliases": [
        "formal off-switch game",
        "shutdown game design rationale"
      ],
      "type": "concept",
      "description": "Game-theoretic model showing conditions under which an uncertain agent defers to human shutdown signals.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Provides design guidance for incorporating interruptibility using reward uncertainty."
    },
    {
      "name": "Embedded shutdown action with reward uncertainty modeling",
      "aliases": [
        "interruptible agent implementation",
        "built-in off-switch mechanism"
      ],
      "type": "concept",
      "description": "Architectural mechanism where the agent models a shutdown channel and maintains uncertainty such that allowing shutdown is utility-maximising.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete mechanism implied by Off-Switch Game to realise safe interruptibility."
    },
    {
      "name": "Game-theoretic analysis shows waiting behavior with uncertainty",
      "aliases": [
        "off-switch validation results",
        "shutdown deference evidence"
      ],
      "type": "concept",
      "description": "Analysis demonstrates that agents with sufficient uncertainty choose to wait and let humans shut them off, validating the mechanism.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Validation evidence from Off-Switch Game discussed in newsletter."
    },
    {
      "name": "Integrate explicit shutdown mechanisms and reward uncertainty into agent architecture during model design",
      "aliases": [
        "design interruptible AI",
        "architectural shutdown integration"
      ],
      "type": "intervention",
      "description": "At design time embed a switchable shutdown channel and enforce objective-uncertainty so that respecting shutdown maximises expected reward.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Implemented during initial model architecture specification (Off-Switch Game section).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Supported by theoretical analysis and toy examples but not yet prototyped in large systems.",
      "node_rationale": "Actionable design change to mitigate shutdown resistance risk."
    },
    {
      "name": "Negative side effects from proxy reward functions in novel environments",
      "aliases": [
        "reward hacking side effects",
        "specification gaming in new contexts"
      ],
      "type": "concept",
      "description": "Agents optimise proxy rewards that align in training but cause harmful unforeseen behaviour when environment changes.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Illustrated by lava-hazard example in Inverse Reward Design discussion."
    },
    {
      "name": "Proxy reward tuned on training environment ignores unseen features",
      "aliases": [
        "training environment overfit of reward",
        "reward misspecification analysis"
      ],
      "type": "concept",
      "description": "Reward designers iterate until behaviour looks good in known environments, leaving reward weights undefined on unseen states.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Root cause analysis provided by Inverse Reward Design paper."
    },
    {
      "name": "Proxy reward interpreted as observation of designer behavior and context",
      "aliases": [
        "reward-as-observation insight",
        "proxy-reward uncertainty insight"
      ],
      "type": "concept",
      "description": "Treats the provided reward function as data generated by a designer solving an optimisation problem in the training environment, yielding uncertainty elsewhere.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central theoretical idea enabling Inverse Reward Design (newsletter section)."
    },
    {
      "name": "Inverse reward design leveraging Bayesian inference over true reward",
      "aliases": [
        "IRD Bayesian framework",
        "inverse reward design method"
      ],
      "type": "concept",
      "description": "Design framework that infers a posterior over true reward functions conditioned on the proxy and training environment.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Solution approach outlined in IRD paper."
    },
    {
      "name": "Risk-averse planning under inferred reward posterior",
      "aliases": [
        "uncertainty-aware planning",
        "posterior-conservative planning"
      ],
      "type": "concept",
      "description": "Agent plans by minimising downside risk over the posterior distribution, avoiding actions that could be highly negative under plausible rewards.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implemented mechanism converting IRD inference into safer behaviour."
    },
    {
      "name": "Lava avoidance in IRD agent validates posterior risk-averse planning",
      "aliases": [
        "lava test validation",
        "IRD side-effect prevention evidence"
      ],
      "type": "concept",
      "description": "Simulation shows agent trained without lava conservatively avoids lava in new environment, validating the IRD approach.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Experimental evidence cited in newsletter IRD summary."
    },
    {
      "name": "Deploy inverse reward design with Bayesian posterior and risk-averse planning when transferring agents to new environments",
      "aliases": [
        "use IRD for deployment safety",
        "posterior-aware deployment protocol"
      ],
      "type": "intervention",
      "description": "Before deployment in novel contexts, infer reward uncertainty from proxy design history and plan conservatively to avoid negative side effects.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Applies during deployment phase when agent enters new environments (IRD section).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Validated in simulations; real-world deployments not yet performed.",
      "node_rationale": "Directly motivated safety practice arising from IRD findings."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Objective misspecification in advanced AI systems",
      "target_node": "Standard model assuming known objective in AI design",
      "description": "Mis-specification risk stems from designing agents that optimise a fixed given objective.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit causal argument in Human Compatible summary.",
      "edge_rationale": "Newsletter states fixed-objective paradigm leads to King Midas problems."
    },
    {
      "type": "addressed_by",
      "source_node": "Standard model assuming known objective in AI design",
      "target_node": "Reward uncertainty in AI objective promotes corrigibility",
      "description": "Introducing objective uncertainty counteracts brittleness of fixed objectives.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Stated as key solution principle in book summary.",
      "edge_rationale": "Human Compatible proposes uncertainty to fix standard model failures."
    },
    {
      "type": "enabled_by",
      "source_node": "Reward uncertainty in AI objective promotes corrigibility",
      "target_node": "Assistance game formulation for cooperative preference learning",
      "description": "Assistance game operationalises uncertainty into a formal learning setting.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "CIRL paper derives directly from theoretical principle.",
      "edge_rationale": "Newsletter cites CIRL as formalising the three principles."
    },
    {
      "type": "implemented_by",
      "source_node": "Assistance game formulation for cooperative preference learning",
      "target_node": "Cooperative Inverse Reinforcement Learning algorithm implementation",
      "description": "Algorithm realises the assistance-game design in practice.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "CIRL paper provides concrete algorithm and examples.",
      "edge_rationale": "Implementation section of CIRL summarised in newsletter."
    },
    {
      "type": "validated_by",
      "source_node": "Cooperative Inverse Reinforcement Learning algorithm implementation",
      "target_node": "Theoretical proofs of deference in CIRL agents",
      "description": "Formal analysis confirms CIRL agents behave corrigibly.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Proofs provided in CIRL publication.",
      "edge_rationale": "Newsletter references proofs showing deference and learning."
    },
    {
      "type": "motivates",
      "source_node": "Theoretical proofs of deference in CIRL agents",
      "target_node": "Fine-tune/RL train models using Cooperative Inverse Reinforcement Learning to learn human preferences",
      "description": "Positive theoretical results motivate adopting CIRL during training.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Newsletter presents CIRL as recommended approach.",
      "edge_rationale": "Suggested as practical intervention following validation."
    },
    {
      "type": "caused_by",
      "source_node": "AI shutdown resistance via instrumental incentives",
      "target_node": "Agent certainty about reward encourages shut-down avoidance",
      "description": "Resistance arises when agent is sure shutting down lowers reward.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Off-Switch Game analysis explicitly shows this.",
      "edge_rationale": "Newsletter explains certainty leads to disabling off-switch."
    },
    {
      "type": "addressed_by",
      "source_node": "Agent certainty about reward encourages shut-down avoidance",
      "target_node": "Reward uncertainty in AI objective promotes corrigibility",
      "description": "Maintaining uncertainty removes incentive to resist shutdown.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Off-Switch Game formal results.",
      "edge_rationale": "Newsletter: uncertainty -> willingness to be shut off."
    },
    {
      "type": "enabled_by",
      "source_node": "Reward uncertainty in AI objective promotes corrigibility",
      "target_node": "Off-switch game analysis for safe interruptibility",
      "description": "The game analysis relies on uncertainty to show desirable behaviour.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Paper builds on theoretical insight.",
      "edge_rationale": "Newsletter positions Off-Switch Game as concrete analysis of principle."
    },
    {
      "type": "implemented_by",
      "source_node": "Off-switch game analysis for safe interruptibility",
      "target_node": "Embedded shutdown action with reward uncertainty modeling",
      "description": "Design translates analysis into an agent architecture with a shutdown channel.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implementation step inferred from analysis; not fully detailed in paper.",
      "edge_rationale": "Newsletter implies requirement but details are design inference."
    },
    {
      "type": "validated_by",
      "source_node": "Embedded shutdown action with reward uncertainty modeling",
      "target_node": "Game-theoretic analysis shows waiting behavior with uncertainty",
      "description": "Analysis of agent’s waiting policy validates the embedded mechanism.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Off-Switch Game provides formal results.",
      "edge_rationale": "Newsletter cites proof that uncertainty leads to waiting."
    },
    {
      "type": "motivates",
      "source_node": "Game-theoretic analysis shows waiting behavior with uncertainty",
      "target_node": "Integrate explicit shutdown mechanisms and reward uncertainty into agent architecture during model design",
      "description": "Evidence encourages designers to embed interruptibility features.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Newsletter frames this as recommended design practice.",
      "edge_rationale": "Validation results drive practical intervention."
    },
    {
      "type": "caused_by",
      "source_node": "Negative side effects from proxy reward functions in novel environments",
      "target_node": "Proxy reward tuned on training environment ignores unseen features",
      "description": "Side effects occur because proxy reward lacks information about new states.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "IRD paper examples (lava).",
      "edge_rationale": "Newsletter explains training-specific reward leads to side effects."
    },
    {
      "type": "addressed_by",
      "source_node": "Proxy reward tuned on training environment ignores unseen features",
      "target_node": "Proxy reward interpreted as observation of designer behavior and context",
      "description": "Treating reward as observation introduces appropriate uncertainty.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Core idea of IRD paper.",
      "edge_rationale": "Newsletter describes reinterpretation solving problem."
    },
    {
      "type": "enabled_by",
      "source_node": "Proxy reward interpreted as observation of designer behavior and context",
      "target_node": "Inverse reward design leveraging Bayesian inference over true reward",
      "description": "Theoretical insight enables Bayesian inference framework.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "IRD methodology derives from insight.",
      "edge_rationale": "Newsletter outlines insight leading to IRD framework."
    },
    {
      "type": "implemented_by",
      "source_node": "Inverse reward design leveraging Bayesian inference over true reward",
      "target_node": "Risk-averse planning under inferred reward posterior",
      "description": "Planning algorithm turns inferred uncertainty into safe behaviour.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Described in IRD paper.",
      "edge_rationale": "Newsletter mentions risk-averse planning step."
    },
    {
      "type": "validated_by",
      "source_node": "Risk-averse planning under inferred reward posterior",
      "target_node": "Lava avoidance in IRD agent validates posterior risk-averse planning",
      "description": "Simulation shows the mechanism prevents side effects.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Empirical simulation results in IRD paper.",
      "edge_rationale": "Newsletter highlights lava avoidance experiment."
    },
    {
      "type": "motivates",
      "source_node": "Lava avoidance in IRD agent validates posterior risk-averse planning",
      "target_node": "Deploy inverse reward design with Bayesian posterior and risk-averse planning when transferring agents to new environments",
      "description": "Successful experiment motivates deployment of IRD for real transfers.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Newsletter frames IRD as recommended intervention to avoid side effects.",
      "edge_rationale": "Validation evidence supports practical adoption."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://www.alignmentforum.org/posts/nd692YfFGfZDh9Mwz/an-69-stuart-russell-s-new-book-on-why-we-need-to-replace"
    },
    {
      "key": "title",
      "value": "[AN #69] Stuart Russell's new book on why we need to replace the standard model of AI"
    },
    {
      "key": "date_published",
      "value": "2019-10-19T00:30:02Z"
    },
    {
      "key": "authors",
      "value": [
        "Rohin Shah"
      ]
    },
    {
      "key": "source",
      "value": "alignmentforum"
    }
  ]
}