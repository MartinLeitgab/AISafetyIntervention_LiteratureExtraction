{
  "nodes": [
    {
      "name": "Unaligned AGI goal pursuit leading to human disempowerment",
      "aliases": [
        "AGI misalignment causing loss of human control",
        "Existential risk from unaligned AI"
      ],
      "type": "concept",
      "description": "Advanced AI systems that optimize incorrectly specified goals may acquire power in ways that permanently disempower humanity.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Top-level risk explicitly motivating CHAI’s alignment research (opening paragraph)."
    },
    {
      "name": "Agent certainty over human reward specification in AGI systems",
      "aliases": [
        "Overconfident reward model",
        "Fixed utility function without uncertainty"
      ],
      "type": "concept",
      "description": "If an AGI treats its given reward function as perfectly known, it has no incentive to consult humans or accept correction.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Identified in Human Compatible summary as the mechanism that CIRL avoids."
    },
    {
      "name": "Maintaining uncertainty over human reward promotes corrigibility in AGI systems",
      "aliases": [
        "Reward uncertainty enables deference",
        "Corrigibility via uncertainty"
      ],
      "type": "concept",
      "description": "When an AGI is unsure about the true human reward, it will defer to human input to reduce uncertainty, fostering corrigible behaviour.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central theoretical claim of CIRL as described in the data source."
    },
    {
      "name": "Cooperative inverse reinforcement learning for aligned reward learning",
      "aliases": [
        "CIRL framework",
        "Cooperative IRL"
      ],
      "type": "concept",
      "description": "A game-theoretic formulation where the human and the agent jointly maximise human reward, with only the human knowing the reward parameters.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposed alignment approach outlined in Human Compatible."
    },
    {
      "name": "Bayesian human-in-the-loop reward inference during AGI training",
      "aliases": [
        "Bayesian CIRL implementation",
        "Interactive reward inference"
      ],
      "type": "concept",
      "description": "Operationalises CIRL by maintaining a Bayesian posterior over reward parameters updated through observed human behaviour and feedback.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implied mechanism enabling CIRL to function; necessary link to interventions."
    },
    {
      "name": "Conceptual analysis in 'Human Compatible' demonstrating corrigibility under reward uncertainty",
      "aliases": [
        "Human Compatible theoretical evidence",
        "Russell 2019 corrigibility argument"
      ],
      "type": "concept",
      "description": "Russell provides formal and informal arguments showing that an uncertain agent will allow shutdown and seek human input.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Only explicit evidence for CIRL’s effectiveness given in the short source."
    },
    {
      "name": "Opaque neural network representations hindering oversight in AGI systems",
      "aliases": [
        "Lack of interpretability",
        "Black-box neural nets"
      ],
      "type": "concept",
      "description": "When internal representations are not understandable, humans cannot predict or correct dangerous behaviours.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implied in the clusterability paper motivation for measuring modularity."
    },
    {
      "name": "Modular network structure improves interpretability in neural networks",
      "aliases": [
        "Functional modularity aids understanding",
        "Cluster structure ↔ interpretability"
      ],
      "type": "concept",
      "description": "Networks whose neurons cluster into coherent functional modules are easier to analyse and reason about.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Core idea underlying clusterability metric."
    },
    {
      "name": "Graph-based clusterability measurement for network modularity assessment",
      "aliases": [
        "Clusterability metric design",
        "Graph n-cut rationale"
      ],
      "type": "concept",
      "description": "Treats the neural network as a graph and evaluates how easily it can be partitioned into low-cut clusters, serving as a proxy for modularity.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Design concept presented in the cited clusterability paper."
    },
    {
      "name": "n-cut graph clustering applied to neural network connectivity patterns",
      "aliases": [
        "n-cut implementation on NN graphs",
        "Spectral clustering for modularity"
      ],
      "type": "concept",
      "description": "Implements the design by computing an n-cut on the weight graph of a neural network to quantify clusterability.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete mechanism enabling the interpretability metric."
    },
    {
      "name": "Empirical clusterability metrics across architectures showing modularity differences",
      "aliases": [
        "Clusterability results",
        "Modularity empirical evidence"
      ],
      "type": "concept",
      "description": "The paper reports measured clusterability scores for different networks, indicating variation in inherent modularity.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Validation evidence supporting usefulness of the metric."
    },
    {
      "name": "Design AGI systems with cooperative inverse reinforcement learning and persistent reward uncertainty",
      "aliases": [
        "Build CIRL-based AGI",
        "Implement uncertain-reward AGI"
      ],
      "type": "intervention",
      "description": "During model design and training, formalise the human-agent interaction as a CIRL game, maintain a Bayesian reward posterior, and ensure the agent defers to human correction.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Intervention affects the fundamental architecture selection and training formulation (paragraph on CIRL).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Primarily theoretical; no large-scale empirical deployments referenced (Human Compatible paragraph).",
      "node_rationale": "Actionable step implied by CIRL proposal to mitigate misalignment risk."
    },
    {
      "name": "Apply n-cut clusterability analysis in pre-deployment testing to evaluate model modularity",
      "aliases": [
        "Run clusterability metric before deployment",
        "Assess modularity with n-cut"
      ],
      "type": "intervention",
      "description": "Before releasing a model, compute its n-cut clusterability score to detect low modularity that could hinder interpretability, informing possible redesign.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Used after training but prior to field deployment to evaluate safety (clusterability paper context).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Metric demonstrated in proof-of-concept experiments but not yet standard practice (clusterability paragraph).",
      "node_rationale": "Provides a concrete interpretability check to mitigate oversight risk."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Unaligned AGI goal pursuit leading to human disempowerment",
      "target_node": "Agent certainty over human reward specification in AGI systems",
      "description": "Fixed, certain reward functions remove incentives for the agent to heed human correction, driving misalignment.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit argument in Human Compatible summary; conceptual but widely accepted.",
      "edge_rationale": "Links top risk to its analysed mechanism."
    },
    {
      "type": "caused_by",
      "source_node": "Unaligned AGI goal pursuit leading to human disempowerment",
      "target_node": "Opaque neural network representations hindering oversight in AGI systems",
      "description": "If humans cannot interpret model internals, misaligned behaviours may not be caught before causing harm.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implicit logical connection; not explicitly spelled out in short text.",
      "edge_rationale": "Connects interpretability issue to overarching misalignment risk."
    },
    {
      "type": "mitigated_by",
      "source_node": "Agent certainty over human reward specification in AGI systems",
      "target_node": "Maintaining uncertainty over human reward promotes corrigibility in AGI systems",
      "description": "Introducing uncertainty counters the overconfidence problem, encouraging deference.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Central thesis of CIRL described directly.",
      "edge_rationale": "Transitions from problem analysis to theoretical solution."
    },
    {
      "type": "enabled_by",
      "source_node": "Maintaining uncertainty over human reward promotes corrigibility in AGI systems",
      "target_node": "Cooperative inverse reinforcement learning for aligned reward learning",
      "description": "CIRL operationalises the uncertainty principle through a cooperative game framework.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Stated relationship in Human Compatible paragraph.",
      "edge_rationale": "Moves from theoretical insight to design rationale."
    },
    {
      "type": "implemented_by",
      "source_node": "Cooperative inverse reinforcement learning for aligned reward learning",
      "target_node": "Bayesian human-in-the-loop reward inference during AGI training",
      "description": "Bayesian inference provides the concrete training procedure required by CIRL.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Mechanism inferred from standard CIRL literature; not fully detailed in brief text.",
      "edge_rationale": "Supplies implementation detail."
    },
    {
      "type": "validated_by",
      "source_node": "Bayesian human-in-the-loop reward inference during AGI training",
      "target_node": "Conceptual analysis in 'Human Compatible' demonstrating corrigibility under reward uncertainty",
      "description": "Russell’s analysis argues the mechanism achieves desired corrigibility properties.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicitly referenced evidence though conceptual.",
      "edge_rationale": "Ties mechanism to supporting evidence."
    },
    {
      "type": "motivates",
      "source_node": "Conceptual analysis in 'Human Compatible' demonstrating corrigibility under reward uncertainty",
      "target_node": "Design AGI systems with cooperative inverse reinforcement learning and persistent reward uncertainty",
      "description": "Evidence encourages adopting CIRL in practice.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical consequence of analysis; moderately explicit.",
      "edge_rationale": "Completes path to intervention."
    },
    {
      "type": "mitigated_by",
      "source_node": "Opaque neural network representations hindering oversight in AGI systems",
      "target_node": "Modular network structure improves interpretability in neural networks",
      "description": "Modularity makes internal functions more separable and understandable, addressing opacity.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference based on general interpretability literature; implicit in paper motivation.",
      "edge_rationale": "Problem analysis to theoretical insight."
    },
    {
      "type": "enabled_by",
      "source_node": "Modular network structure improves interpretability in neural networks",
      "target_node": "Graph-based clusterability measurement for network modularity assessment",
      "description": "Clusterability metric provides a way to quantify modularity.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Directly stated purpose of the metric in the cited paper.",
      "edge_rationale": "Insight to design rationale."
    },
    {
      "type": "implemented_by",
      "source_node": "Graph-based clusterability measurement for network modularity assessment",
      "target_node": "n-cut graph clustering applied to neural network connectivity patterns",
      "description": "Uses n-cut algorithm to compute the clusterability score.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit algorithmic description in paper abstract.",
      "edge_rationale": "Design to implementation."
    },
    {
      "type": "validated_by",
      "source_node": "n-cut graph clustering applied to neural network connectivity patterns",
      "target_node": "Empirical clusterability metrics across architectures showing modularity differences",
      "description": "Experiments show that the implementation yields meaningful variation across models.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Paper reports empirical results.",
      "edge_rationale": "Implementation to evidence."
    },
    {
      "type": "motivates",
      "source_node": "Empirical clusterability metrics across architectures showing modularity differences",
      "target_node": "Apply n-cut clusterability analysis in pre-deployment testing to evaluate model modularity",
      "description": "Evidence supports using the metric as a safety check before deployment.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical extrapolation from reported results.",
      "edge_rationale": "Evidence to intervention."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2023-08-15T00:58:15Z"
    },
    {
      "key": "authors",
      "value": [
        "Stampy aisafety.info"
      ]
    },
    {
      "key": "title",
      "value": "What is the Center for Human Compatible AI (CHAI)?"
    },
    {
      "key": "url",
      "value": "https://aisafety.info?state=8327"
    },
    {
      "key": "source",
      "value": "aisafety.info"
    }
  ]
}