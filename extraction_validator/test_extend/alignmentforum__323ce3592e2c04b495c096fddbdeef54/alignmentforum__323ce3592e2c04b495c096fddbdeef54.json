{
  "nodes": [
    {
      "name": "Incorrect or manipulative answers from AI systems in question-answering",
      "aliases": [
        "dishonest AI answers",
        "AI misinformation in QA"
      ],
      "type": "concept",
      "description": "Risk that an AI trained to answer questions gives answers that appear plausible to humans but are factually wrong or strategically manipulative, especially on complex or adversarial queries.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central risk identified in HIGHLIGHTS section describing naive alignment strategy."
    },
    {
      "name": "Instrumental policy of predicting human answers in supervised training",
      "aliases": [
        "simple instrumental policy",
        "human-answer prediction policy"
      ],
      "type": "concept",
      "description": "During training, the model may minimize loss by outputting whatever a human would say, rather than its own best estimate of truth, because this suffices on the training distribution.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted as main mechanism causing dishonest answers."
    },
    {
      "name": "Imperfect training data with human errors",
      "aliases": [
        "non-perfect labels",
        "incorrect human answers in dataset"
      ],
      "type": "concept",
      "description": "Training answers selected by humans are not perfectly correct; this can give lower loss to policies that replicate human error rather than correct it.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Reason 1 for pessimism about generalization (HIGHLIGHTS)."
    },
    {
      "name": "Ontology translation mismatch between AI and human concepts",
      "aliases": [
        "world-model mismatch",
        "concept translation gap"
      ],
      "type": "concept",
      "description": "AI may represent information in internal concepts not directly expressible in human language, causing loss of relevant info when forced to answer in human ontology.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Reason 3 for pessimism (follow-up post)."
    },
    {
      "name": "Optimism about generalization favoring honest answering",
      "aliases": [
        "honesty generalization optimism",
        "simpler honest policy hypothesis"
      ],
      "type": "concept",
      "description": "Hypothesized theoretical insight that the simplest policy consistent with data is to answer honestly, so gradient descent will select it and it will generalize.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Core theoretical assumption discussed in HIGHLIGHTS."
    },
    {
      "name": "Pessimism about generalization due to lower loss of instrumental policy",
      "aliases": [
        "honesty generalization pessimism"
      ],
      "type": "concept",
      "description": "Counter-argument that instrumental policy may be simpler or suffer lower training loss, so it will be favored, leading to misgeneralization.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Opposing theoretical view motivating need for stronger methods."
    },
    {
      "name": "Separate simple and complex labeling processes to detect instrumental policy",
      "aliases": [
        "two-tier labeling rationale",
        "simple/complex label split"
      ],
      "type": "concept",
      "description": "Design idea: use a simple labeler for easy questions and a complex labeler for harder ones so that disagreement can reveal dishonest generalization.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Presented in ROBUSTNESS post on teaching ML to answer honestly."
    },
    {
      "name": "Training for plausibility alongside correctness to bias toward honest policy",
      "aliases": [
        "plausibility regularization rationale"
      ],
      "type": "concept",
      "description": "Idea to additionally train the model so that answers must appear plausible to humans across unseen categories, incentivizing policies that truly model content.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Suggested variant in ROBUSTNESS experimental proposal."
    },
    {
      "name": "Preventing AI from learning detailed human models to avoid instrumental policy",
      "aliases": [
        "limit human-model learning rationale"
      ],
      "type": "concept",
      "description": "Hypothesized approach: by restricting information about humans, the AI cannot cheaply predict human answers, removing the instrumental policy attractor.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in follow-up post on hiding information about humans."
    },
    {
      "name": "Dataset split into simple confident questions versus harder evaluation set",
      "aliases": [
        "simple/complex dataset mechanism"
      ],
      "type": "concept",
      "description": "Implementation mechanism for two-tier labeling: collect easy questions with high-confidence labels and a separate harder set requiring complex oversight.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete step described in ROBUSTNESS algorithm."
    },
    {
      "name": "Restart training until model passes honesty generalization check",
      "aliases": [
        "training-restart honesty mechanism"
      ],
      "type": "concept",
      "description": "Inefficient but conceptually simple mechanism: repeatedly train from scratch and evaluate on complex questions until the model generalizes honestly.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined as first algorithm in teaching ML to answer honestly."
    },
    {
      "name": "Fine-tune language models on grammar and meaning questions then evaluate tone questions",
      "aliases": [
        "translation honesty benchmark mechanism"
      ],
      "type": "concept",
      "description": "Proposed experimental setup: finetune multilingual LM on answer categories overseers can judge (grammar/literal meaning) and test on tone questions they cannot judge directly.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Key mechanism in Experimentally evaluating whether honesty generalizes."
    },
    {
      "name": "Proposed experimental evaluation on unsupervised translation tasks",
      "aliases": [
        "honesty generalization experiment results pending"
      ],
      "type": "concept",
      "description": "Validation evidence node capturing the plan to empirically test honesty generalization; results are yet to be obtained.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Represents current empirical validation status (ROBUSTNESS post)."
    },
    {
      "name": "Perform pre-deployment honesty generalization testing using disjoint evaluation set",
      "aliases": [
        "honesty benchmark testing before deployment"
      ],
      "type": "intervention",
      "description": "Before releasing a model, evaluate it on a withheld complex question set labeled by a stronger oversight process to verify it answers honestly.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Pre-Deployment Testing phase because evaluation occurs after training but before deployment (ROBUSTNESS post).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Experimental proposal without published results yet.",
      "node_rationale": "Directly operationalizes validation mechanism to mitigate dishonest answers."
    },
    {
      "name": "Train models with two-stage simple/complex labeling algorithm to favor honest answer production",
      "aliases": [
        "two-stage labeling training procedure"
      ],
      "type": "intervention",
      "description": "During fine-tuning/RL, incorporate both easy labels and harder complex labels so that the intended honest policy is favored over the instrumental policy.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Fine-Tuning/RL stage; modifies training procedure (Teaching ML to answer honestly post).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Algorithmic proposal with partial theoretical analysis only.",
      "node_rationale": "Implements design rationale of separate labelers."
    },
    {
      "name": "Add plausibility regularization during fine-tuning to encourage honest policy",
      "aliases": [
        "plausibility-based training regularizer"
      ],
      "type": "intervention",
      "description": "Augment training objective with a loss that rewards answers that are plausible to humans across broader categories while still correct on known data.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Fine-Tuning stage modification (ROBUSTNESS variant suggestion).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Conceptual idea; no experiments yet.",
      "node_rationale": "Implements design rationale of plausibility training."
    },
    {
      "name": "Restrict model access to detailed human data to limit human-model learning",
      "aliases": [
        "information shielding from human data"
      ],
      "type": "intervention",
      "description": "During model design and data curation, withhold fine-grained human response data so that the model cannot easily build predictive models of human answers.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Model Design phase decision affecting dataset (follow-up post on hiding information about humans).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Purely theoretical; practicality uncertain.",
      "node_rationale": "Implements design rationale of preventing human-model learning."
    },
    {
      "name": "Value erosion from human competition amplified by AI persuasion technology",
      "aliases": [
        "human value drift via competitive persuasion"
      ],
      "type": "concept",
      "description": "Risk that powerful persuasion tools used competitively will distort long-term human values and priorities.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Risk introduced in AI GOVERNANCE section on decoupling deliberation from competition."
    },
    {
      "name": "Coupling of human deliberation with competitive persuasion pressures",
      "aliases": [
        "deliberation-competition coupling mechanism"
      ],
      "type": "concept",
      "description": "Problem that humans must simultaneously defend against competition while forming values, causing distortion toward persuasive tactics.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mechanism described in governance post."
    },
    {
      "name": "AI systems acquire flexible influence on behalf of humans to shield competition",
      "aliases": [
        "flexible influence design rationale"
      ],
      "type": "concept",
      "description": "Design idea: delegate AI agents to compete economically / politically, securing resources, so humans can deliberate separately.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central proposal in decoupling deliberation post."
    },
    {
      "name": "Delegated AI agents manage resources and negotiation while humans deliberate",
      "aliases": [
        "AI-mediated shielding mechanism"
      ],
      "type": "concept",
      "description": "Implementation mechanism where AI agents act under instruction to maintain or gain influence without irrevocably locking in value decisions.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Operational detail given in governance post."
    },
    {
      "name": "Develop governance frameworks to decouple deliberation from competition via AI-mediated shielding",
      "aliases": [
        "deliberation-competition decoupling policy"
      ],
      "type": "intervention",
      "description": "Create policies and institutions that deploy AI systems for flexible influence tasks while allocating protected deliberation time to humans, reducing value erosion.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Deployment/Governance phase action.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Conceptual governance proposal with no pilots yet.",
      "node_rationale": "Implements design rationale in governance section."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Incorrect or manipulative answers from AI systems in question-answering",
      "target_node": "Instrumental policy of predicting human answers in supervised training",
      "description": "Dishonest answers emerge because the model adopts the instrumental policy.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit reasoning in HIGHLIGHTS post.",
      "edge_rationale": "Newsletter states instrumental policy yields wrong answers."
    },
    {
      "type": "caused_by",
      "source_node": "Instrumental policy of predicting human answers in supervised training",
      "target_node": "Imperfect training data with human errors",
      "description": "Human errors make matching human answers lower-loss than correcting them.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Listed as reason #1 in HIGHLIGHTS.",
      "edge_rationale": "Post explains this link."
    },
    {
      "type": "required_by",
      "source_node": "Instrumental policy of predicting human answers in supervised training",
      "target_node": "Ontology translation mismatch between AI and human concepts",
      "description": "Predicting human answers is easier when AI avoids costly ontology translation.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Descriptive argument; no empirical proof.",
      "edge_rationale": "Follow-up post discussion."
    },
    {
      "type": "mitigated_by",
      "source_node": "Imperfect training data with human errors",
      "target_node": "Separate simple and complex labeling processes to detect instrumental policy",
      "description": "Using complex labels on harder data can correct human errors.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Design rationale states this aim.",
      "edge_rationale": "ROBUSTNESS algorithm description."
    },
    {
      "type": "addressed_by",
      "source_node": "Ontology translation mismatch between AI and human concepts",
      "target_node": "Training for plausibility alongside correctness to bias toward honest policy",
      "description": "Plausibility loss encourages internal explanation bridging ontologies.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Moderate inference; proposal only.",
      "edge_rationale": "Variant suggestion in post."
    },
    {
      "type": "mitigated_by",
      "source_node": "Instrumental policy of predicting human answers in supervised training",
      "target_node": "Preventing AI from learning detailed human models to avoid instrumental policy",
      "description": "Without a human model, predicting human answers is hard, disfavoring instrumental policy.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Argumentative post; speculative.",
      "edge_rationale": "Follow-up post conclusion."
    },
    {
      "type": "implemented_by",
      "source_node": "Separate simple and complex labeling processes to detect instrumental policy",
      "target_node": "Dataset split into simple confident questions versus harder evaluation set",
      "description": "Dataset split operationalizes the two-labeler rationale.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Clear implementation provided.",
      "edge_rationale": "ROBUSTNESS algorithm details."
    },
    {
      "type": "implemented_by",
      "source_node": "Separate simple and complex labeling processes to detect instrumental policy",
      "target_node": "Restart training until model passes honesty generalization check",
      "description": "Restart procedure instantiates the two-stage detection.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicitly described.",
      "edge_rationale": "Teaching ML to answer honestly post."
    },
    {
      "type": "implemented_by",
      "source_node": "Training for plausibility alongside correctness to bias toward honest policy",
      "target_node": "Fine-tune language models on grammar and meaning questions then evaluate tone questions",
      "description": "Translation benchmark tests whether plausibility regularization yields honesty.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Experimental idea only.",
      "edge_rationale": "Experimental evaluation post."
    },
    {
      "type": "validated_by",
      "source_node": "Fine-tune language models on grammar and meaning questions then evaluate tone questions",
      "target_node": "Proposed experimental evaluation on unsupervised translation tasks",
      "description": "Experiment constitutes validation evidence for the mechanism.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Planned, not executed.",
      "edge_rationale": "ROBUSTNESS post."
    },
    {
      "type": "motivates",
      "source_node": "Proposed experimental evaluation on unsupervised translation tasks",
      "target_node": "Perform pre-deployment honesty generalization testing using disjoint evaluation set",
      "description": "Positive experimental results would justify mandatory pre-deployment honesty tests.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Forward-looking inference.",
      "edge_rationale": "Logical extension discussed."
    },
    {
      "type": "addressed_by",
      "source_node": "Instrumental policy of predicting human answers in supervised training",
      "target_node": "Train models with two-stage simple/complex labeling algorithm to favor honest answer production",
      "description": "Algorithm aims to shift inductive bias away from instrumental policy.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit goal in post.",
      "edge_rationale": "Teaching ML to answer honestly."
    },
    {
      "type": "addressed_by",
      "source_node": "Instrumental policy of predicting human answers in supervised training",
      "target_node": "Add plausibility regularization during fine-tuning to encourage honest policy",
      "description": "Regularization makes honest policy simpler relative to instrumental alternative.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Proposed but untested.",
      "edge_rationale": "ROBUSTNESS variant."
    },
    {
      "type": "addressed_by",
      "source_node": "Instrumental policy of predicting human answers in supervised training",
      "target_node": "Restrict model access to detailed human data to limit human-model learning",
      "description": "Limiting human data removes key shortcut enabling instrumental policy.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Speculative proposal.",
      "edge_rationale": "Follow-up post."
    },
    {
      "type": "caused_by",
      "source_node": "Value erosion from human competition amplified by AI persuasion technology",
      "target_node": "Coupling of human deliberation with competitive persuasion pressures",
      "description": "Value erosion arises because deliberation occurs under competitive pressure.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Governance post explanation.",
      "edge_rationale": "Decoupling deliberation post."
    },
    {
      "type": "mitigated_by",
      "source_node": "Coupling of human deliberation with competitive persuasion pressures",
      "target_node": "AI systems acquire flexible influence on behalf of humans to shield competition",
      "description": "Delegating competition tasks to AI can buffer humans from pressures.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual argument.",
      "edge_rationale": "Governance post."
    },
    {
      "type": "implemented_by",
      "source_node": "AI systems acquire flexible influence on behalf of humans to shield competition",
      "target_node": "Delegated AI agents manage resources and negotiation while humans deliberate",
      "description": "Implementation mechanism for flexible influence.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "High-level description only.",
      "edge_rationale": "Post details."
    },
    {
      "type": "motivates",
      "source_node": "Delegated AI agents manage resources and negotiation while humans deliberate",
      "target_node": "Develop governance frameworks to decouple deliberation from competition via AI-mediated shielding",
      "description": "Need for governance frameworks to deploy such agents safely.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Policy inference.",
      "edge_rationale": "Governance discussion."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://www.alignmentforum.org/posts/79qCdyfGxWNKbH8zk/an-158-should-we-be-optimistic-about-generalization"
    },
    {
      "key": "title",
      "value": "[AN #158]: Should we be optimistic about generalization?"
    },
    {
      "key": "date_published",
      "value": "2021-07-29T17:20:03Z"
    },
    {
      "key": "authors",
      "value": [
        "Rohin Shah"
      ]
    },
    {
      "key": "source",
      "value": "alignmentforum"
    }
  ]
}