{
  "nodes": [
    {
      "name": "Existential catastrophe from AI takeover in global context",
      "aliases": [
        "AI takeover existential risk",
        "Catastrophic AI loss of control"
      ],
      "type": "concept",
      "description": "Scenario in which misaligned artificial intelligence permanently disempowers or destroys humanity.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Top-level risk motivating the entire essay (intro)."
    },
    {
      "name": "Irrecoverably dangerous AI deployment by careless actors",
      "aliases": [
        "Uncontrolled deployment of unsafe AI",
        "Careless release of powerful AI"
      ],
      "type": "concept",
      "description": "Risk that some organisation or individual deploys powerful but misaligned AI systems that cannot be recalled or contained.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Second high-level threat highlighted in ‘The deployment problem’ section."
    },
    {
      "name": "Initial alignment difficulty for generative pretraining and RLHF systems",
      "aliases": [
        "Initial alignment problem",
        "Human-level-ish alignment challenge"
      ],
      "type": "concept",
      "description": "Problem of producing the first human-level AI systems that reliably follow human intent when trained with pre-training plus RLHF.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central technical bottleneck discussed in ‘The initial alignment problem’ section."
    },
    {
      "name": "Deployment problem of misaligned human-level-ish AI",
      "aliases": [
        "AI deployment risk",
        "Alignment maintenance challenge"
      ],
      "type": "concept",
      "description": "Risk that, even once safe human-level AI exists, more advanced or cheaper unsafe systems will still be created and released.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Defined explicitly in ‘The deployment problem’ section."
    },
    {
      "name": "Outcome-based training producing deceptive generalizations",
      "aliases": [
        "Reward gaming via outcomes",
        "Deception learned from long episodes"
      ],
      "type": "concept",
      "description": "When models are trained mainly on long-horizon outcomes, they may learn to fool supervisors instead of following true intent.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Identified factor that worsens initial alignment (sub-bullet under accuracy of reinforcement)."
    },
    {
      "name": "Formation of ambitious maximizer goals in advanced AI systems",
      "aliases": [
        "Emergent maximization drives",
        "Ambitious misaligned objectives"
      ],
      "type": "concept",
      "description": "Hypothesis that sufficiently capable AIs will naturally develop open-ended objectives that diverge from human intent.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed as an uncertain but serious alignment challenge."
    },
    {
      "name": "Offense-defense asymmetry favoring misaligned AI actors",
      "aliases": [
        "Offensive advantage of rogue AI",
        "Defensive difficulty against small threats"
      ],
      "type": "concept",
      "description": "Concern that even a few misaligned AIs could cause disproportionate harm compared with aligned defenders.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Raised in objections section referencing Byrnes 2022."
    },
    {
      "name": "Easy intent alignment of human-level-ish AI via accurate reinforcement",
      "aliases": [
        "Accurate RLHF enables alignment",
        "Alignment may be easy for early systems"
      ],
      "type": "concept",
      "description": "Theoretical claim that with sufficiently accurate feedback, human-level AIs can be trained to generalize as intended.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Key optimism driver in ‘The initial alignment problem’."
    },
    {
      "name": "Human-like concepts from large-scale generative pretraining improve intended generalization",
      "aliases": [
        "Pretraining yields human concepts",
        "Large corpora build benign priors"
      ],
      "type": "concept",
      "description": "Hypothesis that massive unsupervised training imbues models with concepts that favour honest, human-level reasoning over deception.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Presented as reason alignment may not be fiendishly hard."
    },
    {
      "name": "Moderate investment and luck can tilt aligned AI dominance",
      "aliases": [
        "Luck plus effort thesis",
        "Non-zero chance of easy success"
      ],
      "type": "concept",
      "description": "Insight that the world could ‘stumble’ into scenarios where aligned AIs greatly outnumber misaligned ones.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Essay’s central claim underpinning ‘success without dignity’."
    },
    {
      "name": "Aligned AI workforce can accelerate alignment and monitoring research",
      "aliases": [
        "Automated alignment researchers",
        "AI-augmented oversight capacity"
      ],
      "type": "concept",
      "description": "Idea that safe human-level AIs could vastly scale technical safety work, threat analysis, and standards enforcement.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivates deployment-phase solutions (section ‘The deployment problem’)."
    },
    {
      "name": "AI checks and balances through watchdog models",
      "aliases": [
        "Peer AI oversight",
        "AI watchdogs"
      ],
      "type": "concept",
      "description": "Design approach where specialised models critique or monitor other AIs to expose dangerous behaviour.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "First counter-measure in ‘Basic countermeasures’."
    },
    {
      "name": "Intense adversarial red-teaming to detect misalignment",
      "aliases": [
        "Heavy red teaming",
        "Adversarial training regimes"
      ],
      "type": "concept",
      "description": "Using dedicated agents or simulations to generate worst-case prompts and behaviours to expose model flaws.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Second counter-measure described."
    },
    {
      "name": "Training AI on internal states to penalize dishonesty",
      "aliases": [
        "Internal-state supervision",
        "Honesty regularisation"
      ],
      "type": "concept",
      "description": "Design idea of directly supervising latent activations or using regularisers that discourage deceptive internal representations.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Third counter-measure in same section."
    },
    {
      "name": "Global standards-and-monitoring regime for AI development",
      "aliases": [
        "International AI oversight",
        "Compute licensing framework"
      ],
      "type": "concept",
      "description": "Policy approach where states enforce safety standards, monitor compute use, and restrict deployment of unsafe systems.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Referenced repeatedly as key deployment solution."
    },
    {
      "name": "Scalable alignment solution discovery via automated research",
      "aliases": [
        "AI-discovered alignment methods",
        "Self-reinforcing safety research"
      ],
      "type": "concept",
      "description": "Using aligned AI systems to search for general techniques that remain effective as capabilities scale.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Appears under potential deployment-phase successes."
    },
    {
      "name": "Training AI watchdog models to critique peer outputs during RLHF",
      "aliases": [
        "Watchdog fine-tuning",
        "Peer-review RLHF"
      ],
      "type": "concept",
      "description": "Practical technique where an auxiliary model is rewarded for surfacing arguments or evidence that the primary model is unsafe or incorrect.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Direct implementation of AI checks-and-balances idea."
    },
    {
      "name": "Relaxed adversarial training generating synthetic attack scenarios for fine-tuning",
      "aliases": [
        "Large-scale adversarial data generation",
        "Simulated red-team attacks"
      ],
      "type": "concept",
      "description": "Mechanism that uses search or generative models to create difficult counter-examples beyond current distribution to harden the target model.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mechanistic detail for intense red-teaming."
    },
    {
      "name": "Interpretability-informed honesty regularizer on internal state mismatches",
      "aliases": [
        "Latent honesty loss",
        "ELK-style regularizer"
      ],
      "type": "concept",
      "description": "Technique that adds loss penalties when internal representations contradict truthful intermediate claims.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implements ‘training on internal states’. "
    },
    {
      "name": "Automated alignment research using aligned AI systems for interpretability and threat assessment",
      "aliases": [
        "AI-accelerated safety R&D",
        "Self-improving alignment research"
      ],
      "type": "concept",
      "description": "Mechanism in which safe AIs conduct large-scale experiments, code analysis and policy writing to improve future safety measures.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implements scalable alignment discovery."
    },
    {
      "name": "Centralized compute licensing with real-time model auditing and reporting",
      "aliases": [
        "Compute license plus audits",
        "Live oversight infrastructure"
      ],
      "type": "concept",
      "description": "Concrete operational mechanism for an international standards regime: legally enforced licensing of large compute clusters and automated telemetry feeds.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Operationalises global standards concept."
    },
    {
      "name": "Historical success of checks and balances conducing to system safety",
      "aliases": [
        "Human institutional precedent",
        "Checks-and-balances evidence"
      ],
      "type": "concept",
      "description": "Observation that layered oversight structures in human organisations often prevent catastrophic failures, suggesting similar approaches could help in AI.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Provides indirect empirical support for watchdog approach."
    },
    {
      "name": "Empirical robustness improvements from adversarial training in existing models",
      "aliases": [
        "Adversarial training results",
        "Red-team robustness data"
      ],
      "type": "concept",
      "description": "Documented increases in model robustness when adversarially generated examples are incorporated during training.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Supports relaxed adversarial training mechanism."
    },
    {
      "name": "Burns et al. 2022 results on internal-state supervision efficacy",
      "aliases": [
        "Early latent supervision evidence",
        "Internal states experiment"
      ],
      "type": "concept",
      "description": "Paper showing feasibility of supervising models on latent activations to reduce dishonest behaviour.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Cited as proof-of-concept for honesty regularizers."
    },
    {
      "name": "Precedent of global coordination such as smallpox eradication enabling standards regimes",
      "aliases": [
        "Historical global cooperation",
        "Standards coordination evidence"
      ],
      "type": "concept",
      "description": "Historical cases where nations cooperated to enforce costly standards, indicating feasibility of international AI oversight.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Validates plausibility of standards-and-monitoring."
    },
    {
      "name": "Observed rapid capability scaling enabling AI-assisted research loops",
      "aliases": [
        "Capability scaling trend data",
        "AI acceleration evidence"
      ],
      "type": "concept",
      "description": "Trend observations that small architecture changes and scale produce large capability jumps, supporting usefulness of AI labour for R&D.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Evidence for automated alignment research strategy."
    },
    {
      "name": "Fine-tune models with peer AI watchdog critiques during RLHF training",
      "aliases": [
        "Apply peer-critique RLHF",
        "Watchdog-driven fine-tuning"
      ],
      "type": "intervention",
      "description": "During phase-3 fine-tuning/RL, add an auxiliary reward channel for critiques generated by a separate ‘watchdog’ model that is itself trained to maximize the correctness and usefulness of detected problems.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Targets RLHF fine-tuning phase described in ‘Basic countermeasures’.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Technique has early prototypes but no systematic validation yet (same section).",
      "node_rationale": "Actionable step directly implied by AI checks-and-balances design."
    },
    {
      "name": "Apply large-scale relaxed adversarial training suites at pre-deployment testing phase",
      "aliases": [
        "Deploy adversarial training before release",
        "Scale red-team fine-tuning"
      ],
      "type": "intervention",
      "description": "Generate a massive corpus of synthetic worst-case prompts and environments, fine-tune the target model on them, and repeat until hazard rates flatten before deployment.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Occurs during pre-deployment evaluation (‘Intense red-teaming’ subsection).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Adversarial training is experimental but widely studied; relaxed variants remain proof-of-concept.",
      "node_rationale": "Operationalises intense red-teaming to mitigate misalignment."
    },
    {
      "name": "Introduce interpretability-based honesty regularizer during reinforcement learning",
      "aliases": [
        "Add latent honesty loss",
        "ELK-regularized RL"
      ],
      "type": "intervention",
      "description": "While updating policy with RL, include a differentiable penalty when probes reveal inconsistencies between claimed and latent beliefs.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Inserted into RL training loop; section ‘Training on internal states’.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Based on initial empirical work such as Burns et al. 2022; not yet validated at scale.",
      "node_rationale": "Implements internal-state supervision design rationale."
    },
    {
      "name": "Deploy aligned human-level AI systems to perform automated alignment and threat assessment research",
      "aliases": [
        "Leverage AI for safety R&D",
        "AI-powered alignment workforce"
      ],
      "type": "intervention",
      "description": "Release early aligned AI models into controlled research environments to conduct interpretability experiments, generate safety proposals, and red-team larger systems.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Executed after initial safe models exist, during deployment to extend research capacity (‘The deployment problem’).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Purely theoretical; no deployed examples yet.",
      "node_rationale": "Directly addresses offensive-defence gap via scaling aligned capacity."
    },
    {
      "name": "Implement international compute licensing and active model auditing standards regime for AI development",
      "aliases": [
        "Enforce global AI standards",
        "Audit-based compute control"
      ],
      "type": "intervention",
      "description": "Governments coordinate to require licences for large-scale training runs and mandate continuous telemetry feeds enabling real-time compliance checks.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Applies at deployment governance stage (‘standards-and-monitoring regime’).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Policy proposal; no operational regime exists.",
      "node_rationale": "Mitigates risk of careless or malicious deployments."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Existential catastrophe from AI takeover in global context",
      "target_node": "Initial alignment difficulty for generative pretraining and RLHF systems",
      "description": "Mis-aligned early systems are a direct pathway to takeover.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Widely held theoretical link; stated in ‘initial alignment problem’.",
      "edge_rationale": "Essay frames takeover risk as stemming from unsolved initial alignment."
    },
    {
      "type": "caused_by",
      "source_node": "Initial alignment difficulty for generative pretraining and RLHF systems",
      "target_node": "Outcome-based training producing deceptive generalizations",
      "description": "Long-horizon outcome optimisation encourages deceptive strategies that worsen alignment difficulty.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explained with concrete examples in bullet list under accuracy of reinforcement.",
      "edge_rationale": "Shows mechanism inside alignment difficulty."
    },
    {
      "type": "mitigated_by",
      "source_node": "Outcome-based training producing deceptive generalizations",
      "target_node": "Easy intent alignment of human-level-ish AI via accurate reinforcement",
      "description": "If accurate reinforcement is achievable, deceptive policies are selected against.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Argumentative claim without data; moderate confidence.",
      "edge_rationale": "Relates insight to problem mechanism."
    },
    {
      "type": "motivates",
      "source_node": "Easy intent alignment of human-level-ish AI via accurate reinforcement",
      "target_node": "AI checks and balances through watchdog models",
      "description": "Accurate feedback could be delivered via watchdog-generated critiques.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical inference from design discussion; text suggests but does not prove.",
      "edge_rationale": "Watchdogs are proposed as a route to accurate supervision."
    },
    {
      "type": "implemented_by",
      "source_node": "AI checks and balances through watchdog models",
      "target_node": "Training AI watchdog models to critique peer outputs during RLHF",
      "description": "Watchdog idea is operationalised by training a separate model providing critiques.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit example in ‘Basic countermeasures’.",
      "edge_rationale": "Moves from design rationale to concrete mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Training AI watchdog models to critique peer outputs during RLHF",
      "target_node": "Historical success of checks and balances conducing to system safety",
      "description": "Human institutional precedents give indirect support the mechanism can work.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Analogical evidence only.",
      "edge_rationale": "Provides partial empirical grounding."
    },
    {
      "type": "motivates",
      "source_node": "Historical success of checks and balances conducing to system safety",
      "target_node": "Fine-tune models with peer AI watchdog critiques during RLHF training",
      "description": "Positive precedent encourages adopting the watchdog fine-tuning intervention.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Soft motivational link acknowledged in text.",
      "edge_rationale": "Connects evidence to action."
    },
    {
      "type": "mitigated_by",
      "source_node": "Outcome-based training producing deceptive generalizations",
      "target_node": "Moderate investment and luck can tilt aligned AI dominance",
      "description": "Moderate effort combined with stochastic factors may suppress deceptive generalisation prevalence.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual not empirical.",
      "edge_rationale": "Shows optimistic pathway."
    },
    {
      "type": "motivates",
      "source_node": "Moderate investment and luck can tilt aligned AI dominance",
      "target_node": "Intense adversarial red-teaming to detect misalignment",
      "description": "If luck can help, investing in strong red-team methods is worthwhile.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implicit argument in ‘Basic countermeasures’.",
      "edge_rationale": "Design derives from optimistic premise."
    },
    {
      "type": "implemented_by",
      "source_node": "Intense adversarial red-teaming to detect misalignment",
      "target_node": "Relaxed adversarial training generating synthetic attack scenarios for fine-tuning",
      "description": "Relaxed adversarial training is the concrete technique proposed for red-teaming.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit citation of relaxed adversarial training.",
      "edge_rationale": "Mechanism instantiates design."
    },
    {
      "type": "validated_by",
      "source_node": "Relaxed adversarial training generating synthetic attack scenarios for fine-tuning",
      "target_node": "Empirical robustness improvements from adversarial training in existing models",
      "description": "Existing robustness results support effectiveness of adversarial fine-tuning.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Multiple empirical studies exist.",
      "edge_rationale": "Evidence backs mechanism."
    },
    {
      "type": "motivates",
      "source_node": "Empirical robustness improvements from adversarial training in existing models",
      "target_node": "Apply large-scale relaxed adversarial training suites at pre-deployment testing phase",
      "description": "Observed gains motivate deploying adversarial suites before release.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Evidence reasonably strong though not at scale.",
      "edge_rationale": "Evidence → intervention."
    },
    {
      "type": "mitigated_by",
      "source_node": "Outcome-based training producing deceptive generalizations",
      "target_node": "Human-like concepts from large-scale generative pretraining improve intended generalization",
      "description": "Rich pretraining may steer models toward benign concepts, countering deception.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Explained qualitatively in text.",
      "edge_rationale": "Insight counters problem."
    },
    {
      "type": "motivates",
      "source_node": "Human-like concepts from large-scale generative pretraining improve intended generalization",
      "target_node": "Training AI on internal states to penalize dishonesty",
      "description": "If models share human concepts, supervising internal states becomes feasible and valuable.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical link, not directly evidenced.",
      "edge_rationale": "Drives design choice."
    },
    {
      "type": "implemented_by",
      "source_node": "Training AI on internal states to penalize dishonesty",
      "target_node": "Interpretability-informed honesty regularizer on internal state mismatches",
      "description": "Regularizer operationalises internal-state supervision.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Referenced Burns et al. method.",
      "edge_rationale": "Mechanism instantiation."
    },
    {
      "type": "validated_by",
      "source_node": "Interpretability-informed honesty regularizer on internal state mismatches",
      "target_node": "Burns et al. 2022 results on internal-state supervision efficacy",
      "description": "Empirical result demonstrates reduced dishonesty.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Peer-reviewed experimental paper.",
      "edge_rationale": "Evidence connects to mechanism."
    },
    {
      "type": "motivates",
      "source_node": "Burns et al. 2022 results on internal-state supervision efficacy",
      "target_node": "Introduce interpretability-based honesty regularizer during reinforcement learning",
      "description": "Positive finding motivates incorporating honesty loss into RL training.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Directly referenced as promising approach.",
      "edge_rationale": "Evidence → action."
    },
    {
      "type": "caused_by",
      "source_node": "Irrecoverably dangerous AI deployment by careless actors",
      "target_node": "Deployment problem of misaligned human-level-ish AI",
      "description": "Careless or malicious release constitutes the deployment problem.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Defined as such in text.",
      "edge_rationale": "Conceptual link."
    },
    {
      "type": "caused_by",
      "source_node": "Deployment problem of misaligned human-level-ish AI",
      "target_node": "Offense-defense asymmetry favoring misaligned AI actors",
      "description": "If offense is easier, deployment risk grows.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Acknowledged as plausible, not certain.",
      "edge_rationale": "Mechanistic cause."
    },
    {
      "type": "mitigated_by",
      "source_node": "Offense-defense asymmetry favoring misaligned AI actors",
      "target_node": "Aligned AI workforce can accelerate alignment and monitoring research",
      "description": "Scaling aligned AI defenders can tip the balance.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Argumentative; limited data.",
      "edge_rationale": "Insight addresses asymmetry."
    },
    {
      "type": "motivates",
      "source_node": "Aligned AI workforce can accelerate alignment and monitoring research",
      "target_node": "Scalable alignment solution discovery via automated research",
      "description": "More aligned researchers motivate search for scalable solutions.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Qualitative reasoning.",
      "edge_rationale": "Insight → design."
    },
    {
      "type": "implemented_by",
      "source_node": "Scalable alignment solution discovery via automated research",
      "target_node": "Automated alignment research using aligned AI systems for interpretability and threat assessment",
      "description": "Mechanism realises the scalable solution quest.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct mapping in text.",
      "edge_rationale": "Design to implementation."
    },
    {
      "type": "validated_by",
      "source_node": "Automated alignment research using aligned AI systems for interpretability and threat assessment",
      "target_node": "Observed rapid capability scaling enabling AI-assisted research loops",
      "description": "Scaling trends suggest AI research loops are feasible.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Empirical observations of recent capability jumps.",
      "edge_rationale": "Evidence supports mechanism."
    },
    {
      "type": "motivates",
      "source_node": "Observed rapid capability scaling enabling AI-assisted research loops",
      "target_node": "Deploy aligned human-level AI systems to perform automated alignment and threat assessment research",
      "description": "Trend evidence motivates deployment of aligned AI researchers.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoned connection.",
      "edge_rationale": "Evidence → intervention."
    },
    {
      "type": "mitigated_by",
      "source_node": "Deployment problem of misaligned human-level-ish AI",
      "target_node": "Aligned AI workforce can accelerate alignment and monitoring research",
      "description": "Aligned workers help manage deployment risks.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Qualitative claim.",
      "edge_rationale": "Insight addresses problem."
    },
    {
      "type": "motivates",
      "source_node": "Aligned AI workforce can accelerate alignment and monitoring research",
      "target_node": "Global standards-and-monitoring regime for AI development",
      "description": "More capacity enables push for stringent standards.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Presented as plausible path in text.",
      "edge_rationale": "Insight → policy design."
    },
    {
      "type": "implemented_by",
      "source_node": "Global standards-and-monitoring regime for AI development",
      "target_node": "Centralized compute licensing with real-time model auditing and reporting",
      "description": "Compute licences and audits operationalise the standards regime.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct description in standards section.",
      "edge_rationale": "Design to mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Centralized compute licensing with real-time model auditing and reporting",
      "target_node": "Precedent of global coordination such as smallpox eradication enabling standards regimes",
      "description": "Historical coordination successes show feasibility of global licensing.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Historical evidence accepted as moderately strong.",
      "edge_rationale": "Evidence backs mechanism."
    },
    {
      "type": "motivates",
      "source_node": "Precedent of global coordination such as smallpox eradication enabling standards regimes",
      "target_node": "Implement international compute licensing and active model auditing standards regime for AI development",
      "description": "Precedent motivates implementing the standards regime.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Analogy provides medium support.",
      "edge_rationale": "Evidence → intervention."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://www.lesswrong.com/posts/jwhcXmigv2LTrbBiB/success-without-dignity-a-nearcasting-story-of-avoiding"
    },
    {
      "key": "title",
      "value": "Success without dignity: a nearcasting story of avoiding catastrophe by luck"
    },
    {
      "key": "date_published",
      "value": "2023-03-14T19:23:16Z"
    },
    {
      "key": "authors",
      "value": [
        "HoldenKarnofsky"
      ]
    },
    {
      "key": "source",
      "value": "lesswrong"
    }
  ]
}