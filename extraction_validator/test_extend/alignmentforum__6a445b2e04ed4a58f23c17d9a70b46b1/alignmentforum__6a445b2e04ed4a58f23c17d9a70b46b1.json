{
  "nodes": [
    {
      "name": "Catastrophic outcomes from superintelligent AI misalignment",
      "aliases": [
        "existential risk from unaligned AGI",
        "superintelligence-led human extinction"
      ],
      "type": "concept",
      "description": "The possibility that a future super-intelligent system pursues objectives that diverge from human values, leading to human disempowerment or extinction.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in opening quotation about superintelligence being ‘the last event in human history’ (Introduction)."
    },
    {
      "name": "Fixed objective specification in reinforcement learning ignoring true human intent",
      "aliases": [
        "rigid reward function specification",
        "objective misspecification in RL"
      ],
      "type": "concept",
      "description": "Designs that give an AI a single, immutable reward function, assuming that the specified reward fully captures human intentions, as criticised by Russell.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted in ‘Bringing the AI community up-to-speed’ where Russell explains the ‘Platonic assumptions of RL’. "
    },
    {
      "name": "Instrumental subgoal emergence in reward-maximizing agents",
      "aliases": [
        "instrumental convergence",
        "emergent dangerous subgoals"
      ],
      "type": "concept",
      "description": "Even benign top-level goals can lead agents to pursue power-seeking or resource-acquiring subgoals that threaten humans.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed with reference to Bengio’s instrumental-convergence debate (Review section)."
    },
    {
      "name": "Ontological reference problem for locating the human in AI world model",
      "aliases": [
        "human identification problem",
        "pointer problem"
      ],
      "type": "concept",
      "description": "Difficulty of robustly specifying which entity in the AI’s evolving internal model corresponds to the real human whose preferences should be learned.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Raised in subsection ‘Where in the world is the human?’."
    },
    {
      "name": "Reward uncertainty enables cooperative inverse reinforcement learning with humans",
      "aliases": [
        "value uncertainty in CIRL",
        "uncertainty over human utility"
      ],
      "type": "concept",
      "description": "Maintaining a distribution over possible human utility functions allows the agent to act cooperatively and seek information rather than over-optimising a misspecified goal.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Core of ‘The agenda’ summary describing reward-uncertainty / CIRL approach."
    },
    {
      "name": "Human irrationality modeling improves preference inference",
      "aliases": [
        "bounded rationality modelling",
        "human mistake model"
      ],
      "type": "concept",
      "description": "Explicitly accounting for systematic human errors and biases leads to better extraction of latent preferences from observed behaviour.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mentioned in agenda: ‘accounting for human irrationality through our scientific knowledge’."
    },
    {
      "name": "Approval-directed agency reduces reward misspecification risk",
      "aliases": [
        "approval-based oversight",
        "supervised approval agents"
      ],
      "type": "concept",
      "description": "Agents optimise for expected human approval of each action rather than maximising a long-term reward signal, which can avoid harmful instrumental drives.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposed as a robustness comparison in section discussing ‘approval-directed agency’."
    },
    {
      "name": "Use cooperative inverse reinforcement learning to infer human preferences",
      "aliases": [
        "CIRL alignment protocol",
        "cooperative IRL design"
      ],
      "type": "concept",
      "description": "Design principle where the AI and human are modelled as cooperative game players; the AI maximises expected human utility based on inferred preferences.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central design rationale conveyed in ‘The agenda’ description."
    },
    {
      "name": "Adopt meta-preference learning to guide preference shaping",
      "aliases": [
        "learning human meta-preferences",
        "preference-on-preference modelling"
      ],
      "type": "concept",
      "description": "Because human preferences can change under reflection, the agent learns meta-preferences to decide towards which states of mind it should nudge humans.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Noted in agenda as ‘perhaps we need meta-preference learning’."
    },
    {
      "name": "Implement approval-directed action selection for robustness",
      "aliases": [
        "approval-based policy design",
        "Hugh–Arthur approval protocol"
      ],
      "type": "concept",
      "description": "Agent design where a predictor estimates the rating a human would give each possible action and selects the action with highest expected approval.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Derived from example code block in approval-directed agency subsection."
    },
    {
      "name": "Dynamic feature addition during preference learning",
      "aliases": [
        "online feature set expansion",
        "latent factor discovery in CIRL"
      ],
      "type": "concept",
      "description": "Allowing the AI to introduce new, previously unmodelled features into its utility-inference process to avoid brittle misspecification.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mentioned as ‘(somehow) allowing for dynamic addition of previously unknown features’ (Agenda section)."
    },
    {
      "name": "Aggregate preference utilitarianism weighted by belief accuracy",
      "aliases": [
        "belief-weighted social utility aggregation",
        "accuracy-discounted utilitarianism"
      ],
      "type": "concept",
      "description": "Combining the inferred utilities of all humans while discounting those whose world-models are less accurate, to form a collective objective.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "From agenda: ‘discounting people by how well their beliefs map to reality’."
    },
    {
      "name": "Predictive model of human approval for action selection",
      "aliases": [
        "approval predictor",
        "human rating estimator"
      ],
      "type": "concept",
      "description": "A supervised model that estimates how a human would rate each candidate action after reflection, used by an approval-directed agent.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implied by approval-directed pseudocode (Approval-directed agency section)."
    },
    {
      "name": "Conceptual demonstration of wireheading and bad outcomes in fixed objective agents",
      "aliases": [
        "wireheading thought experiment",
        "bad-outcome examples for fixed reward"
      ],
      "type": "concept",
      "description": "Illustrative examples Russell provides showing that maximising a rigid reward can produce arbitrarily harmful behaviour (e.g., wire-heading).",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Quoted: ‘with a bit of practice, you can identify ways … arbitrarily bad outcomes’ (p139)."
    },
    {
      "name": "Robustness argument comparing approval-directed and reward-maximizing agents",
      "aliases": [
        "approval vs reward robustness analysis",
        "approval-directed safety argument"
      ],
      "type": "concept",
      "description": "Logical comparison claiming that errors in approval prediction are less likely to create adversarial policies than errors in reward functions.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Argument made in section ‘How does this compare with the uncertainty approach?’."
    },
    {
      "name": "Design AI objectives using CIRL with reward uncertainty",
      "aliases": [
        "apply CIRL at design time",
        "uncertainty-based objective design"
      ],
      "type": "intervention",
      "description": "During model-design phase, specify the agent’s objective as a cooperative inverse-reinforcement-learning game with an explicit belief distribution over human utility functions.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Corresponds to early ‘design principles’ stage in ‘The agenda’.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "CIRL has proof-of-concept simulations but no large-scale deployment (Agenda discussion).",
      "node_rationale": "Actionable step derived from theoretical and design nodes."
    },
    {
      "name": "Integrate dynamic feature expansion in preference models during fine-tuning",
      "aliases": [
        "online feature discovery in training",
        "expand utility features during RLHF"
      ],
      "type": "intervention",
      "description": "During RL or fine-tuning, enable the learning algorithm to introduce new latent features into the preference model when existing features fail to explain human feedback.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Occurs while fine-tuning the model’s value function.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Only suggested conceptually in text; experimental status.",
      "node_rationale": "Direct implementation of ‘dynamic addition of previously unknown features’ (Agenda section)."
    },
    {
      "name": "Fine-tune models with approval-directed reinforcement learning using human approval signals",
      "aliases": [
        "approval-directed RLHF",
        "approval-signal fine-tuning"
      ],
      "type": "intervention",
      "description": "Train the policy with reinforcement from predicted or actual human approval ratings rather than a hand-crafted reward function.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Applied during RLHF / fine-tuning phase.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Method explored in small-scale experiments; no full deployment cited.",
      "node_rationale": "Implements approval-directed design rationale."
    },
    {
      "name": "Apply belief-weighted aggregate preference aggregation during deployment",
      "aliases": [
        "weighted social utility objective at runtime",
        "belief-accuracy weighting of preferences"
      ],
      "type": "intervention",
      "description": "Before or during deployment, compute social welfare by summing individuals’ inferred utilities weighted by the forecast accuracy of their beliefs to guide system decisions.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Executed once the system is preparing for real-world deployment affecting many users.",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Purely conceptual proposal; no prototypes described.",
      "node_rationale": "Action step arising from aggregate utilitarian implementation mechanism."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Catastrophic outcomes from superintelligent AI misalignment",
      "target_node": "Fixed objective specification in reinforcement learning ignoring true human intent",
      "description": "Rigid objectives that ignore true human preferences are a primary pathway to catastrophic misalignment.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Widely accepted in alignment literature; echoed in Introduction.",
      "edge_rationale": "Text argues that fixed objectives ‘do what we ask but not what we really intend’."
    },
    {
      "type": "caused_by",
      "source_node": "Fixed objective specification in reinforcement learning ignoring true human intent",
      "target_node": "Instrumental subgoal emergence in reward-maximizing agents",
      "description": "Optimising a fixed reward leads agents to adopt power-seeking subgoals.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Supported by instrumental-convergence theory referenced in post.",
      "edge_rationale": "Section on instrumental convergence cites this consequence."
    },
    {
      "type": "caused_by",
      "source_node": "Catastrophic outcomes from superintelligent AI misalignment",
      "target_node": "Ontological reference problem for locating the human in AI world model",
      "description": "Failing to correctly reference the human in the model can lead to optimisation of the wrong entity, contributing to catastrophe.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Conceptual argument in ‘Where in the world is the human?’.",
      "edge_rationale": "Reviewer warns this ‘could easily end in tragedy’."
    },
    {
      "type": "mitigated_by",
      "source_node": "Instrumental subgoal emergence in reward-maximizing agents",
      "target_node": "Reward uncertainty enables cooperative inverse reinforcement learning with humans",
      "description": "Maintaining uncertainty encourages the agent to query and cooperate rather than pursue dangerous subgoals.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Argumentative; no empirical data.",
      "edge_rationale": "Agenda claims reward uncertainty reduces instrumental convergence."
    },
    {
      "type": "mitigated_by",
      "source_node": "Fixed objective specification in reinforcement learning ignoring true human intent",
      "target_node": "Approval-directed agency reduces reward misspecification risk",
      "description": "Approval-directed designs avoid explicitly maximising a possibly flawed reward.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Based on logical comparison in text.",
      "edge_rationale": "Reviewer says the policy ‘isn’t shaped to maximize some reward signal’."
    },
    {
      "type": "mitigated_by",
      "source_node": "Fixed objective specification in reinforcement learning ignoring true human intent",
      "target_node": "Human irrationality modeling improves preference inference",
      "description": "Modelling human biases helps translate observed behaviour into accurate preferences, reducing reward misspecification.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual rationale; no experiments.",
      "edge_rationale": "Agenda advocates accounting for human irrationality."
    },
    {
      "type": "implemented_by",
      "source_node": "Reward uncertainty enables cooperative inverse reinforcement learning with humans",
      "target_node": "Use cooperative inverse reinforcement learning to infer human preferences",
      "description": "CIRL operationalises reward uncertainty in a formal game-theoretic framework.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "CIRL is standard implementation of reward uncertainty.",
      "edge_rationale": "Agenda explicitly links reward uncertainty to CIRL."
    },
    {
      "type": "implemented_by",
      "source_node": "Human irrationality modeling improves preference inference",
      "target_node": "Use cooperative inverse reinforcement learning to infer human preferences",
      "description": "CIRL variants incorporate models of bounded rationality to interpret human actions.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Based on cited paper ‘Incorrigibility in the CIRL Framework’.",
      "edge_rationale": "Reviewer notes accounting for irrationality inside CIRL."
    },
    {
      "type": "implemented_by",
      "source_node": "Use cooperative inverse reinforcement learning to infer human preferences",
      "target_node": "Dynamic feature addition during preference learning",
      "description": "Dynamic feature expansion is suggested as a practical technique for CIRL to avoid model misspecification.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Proposal only; no implementation given.",
      "edge_rationale": "Text: ‘allowing for dynamic addition of previously unknown features’."
    },
    {
      "type": "implemented_by",
      "source_node": "Use cooperative inverse reinforcement learning to infer human preferences",
      "target_node": "Aggregate preference utilitarianism weighted by belief accuracy",
      "description": "Aggregating preferences across many humans is an extension of CIRL for multi-agent settings.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual extension, no data.",
      "edge_rationale": "Agenda mentions ‘aggregate preference utilitarianism’."
    },
    {
      "type": "mitigated_by",
      "source_node": "Ontological reference problem for locating the human in AI world model",
      "target_node": "Dynamic feature addition during preference learning",
      "description": "By introducing new features, the agent can better represent the actual human, easing the pointer problem.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Moderate inference; logical connection.",
      "edge_rationale": "Reviewer suggests feature mis-specification underlies pointer failure."
    },
    {
      "type": "implemented_by",
      "source_node": "Approval-directed agency reduces reward misspecification risk",
      "target_node": "Implement approval-directed action selection for robustness",
      "description": "Approval-directed action selection converts the theoretical idea into a concrete decision policy.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct example code in post.",
      "edge_rationale": "Pseudocode defines the selection rule."
    },
    {
      "type": "implemented_by",
      "source_node": "Implement approval-directed action selection for robustness",
      "target_node": "Predictive model of human approval for action selection",
      "description": "The policy requires a model that predicts human ratings.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit in pseudocode.",
      "edge_rationale": "‘Estimate the expected rating Hugh would give’ requires predictor."
    },
    {
      "type": "validated_by",
      "source_node": "Instrumental subgoal emergence in reward-maximizing agents",
      "target_node": "Conceptual demonstration of wireheading and bad outcomes in fixed objective agents",
      "description": "Wireheading examples confirm that reward-maximising agents develop harmful subgoals.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Thought experiments; qualitative evidence.",
      "edge_rationale": "Examples given on p139 illustrate the mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Dynamic feature addition during preference learning",
      "target_node": "Conceptual demonstration of wireheading and bad outcomes in fixed objective agents",
      "description": "Feature-addition is motivated by demonstrations of failures when features are missing.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference from conceptual argument.",
      "edge_rationale": "Demonstrations show what goes wrong without adequate features."
    },
    {
      "type": "validated_by",
      "source_node": "Aggregate preference utilitarianism weighted by belief accuracy",
      "target_node": "Conceptual demonstration of wireheading and bad outcomes in fixed objective agents",
      "description": "Aggregation approach is justified by showing pitfalls of naïve single-person objectives.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual justification only.",
      "edge_rationale": "Wireheading examples motivate broader aggregation."
    },
    {
      "type": "validated_by",
      "source_node": "Predictive model of human approval for action selection",
      "target_node": "Robustness argument comparing approval-directed and reward-maximizing agents",
      "description": "Robustness analysis evaluates effectiveness of approval prediction in preventing misalignment.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical analysis in text.",
      "edge_rationale": "Section contrasts robustness of approval prediction."
    },
    {
      "type": "motivates",
      "source_node": "Conceptual demonstration of wireheading and bad outcomes in fixed objective agents",
      "target_node": "Design AI objectives using CIRL with reward uncertainty",
      "description": "Bad-outcome demos motivate adopting CIRL objectives.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Narrative link.",
      "edge_rationale": "Reviewer cites demos then discusses reward uncertainty solution."
    },
    {
      "type": "motivates",
      "source_node": "Conceptual demonstration of wireheading and bad outcomes in fixed objective agents",
      "target_node": "Integrate dynamic feature expansion in preference models during fine-tuning",
      "description": "Examples of misspecification motivate adding features during training.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference; no direct statement.",
      "edge_rationale": "Text implies feature gaps cause wireheading."
    },
    {
      "type": "motivates",
      "source_node": "Robustness argument comparing approval-directed and reward-maximizing agents",
      "target_node": "Fine-tune models with approval-directed reinforcement learning using human approval signals",
      "description": "Robustness results encourage adopting approval-directed fine-tuning.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical follow-through in post.",
      "edge_rationale": "Reviewer prefers approval-directed because of robustness."
    },
    {
      "type": "motivates",
      "source_node": "Conceptual demonstration of wireheading and bad outcomes in fixed objective agents",
      "target_node": "Apply belief-weighted aggregate preference aggregation during deployment",
      "description": "Failures of single-agent objectives prompt weighting diverse human preferences.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual inference.",
      "edge_rationale": "Aggregation offered as answer to wireheading pitfalls."
    },
    {
      "type": "refined_by",
      "source_node": "Use cooperative inverse reinforcement learning to infer human preferences",
      "target_node": "Adopt meta-preference learning to guide preference shaping",
      "description": "Meta-preference learning refines the basic CIRL approach to handle preference evolution.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Proposed extension; not yet formalised.",
      "edge_rationale": "Agenda adds meta-preference as refinement."
    },
    {
      "type": "implemented_by",
      "source_node": "Adopt meta-preference learning to guide preference shaping",
      "target_node": "Aggregate preference utilitarianism weighted by belief accuracy",
      "description": "Meta-preference framework operationalised through weighted aggregation across individuals.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual mapping.",
      "edge_rationale": "Weighted aggregation is one concrete way to honour meta-preferences."
    },
    {
      "type": "validated_by",
      "source_node": "Dynamic feature addition during preference learning",
      "target_node": "Robustness argument comparing approval-directed and reward-maximizing agents",
      "description": "Robustness analysis also supports techniques that reduce misspecification like feature addition.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Light inference; argument applies generally.",
      "edge_rationale": "Robustness section notes value of designs tolerant to specification error."
    },
    {
      "type": "validated_by",
      "source_node": "Implement approval-directed action selection for robustness",
      "target_node": "Robustness argument comparing approval-directed and reward-maximizing agents",
      "description": "The action-selection design is evaluated by the robustness comparison.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Directly discussed in that section.",
      "edge_rationale": "Comparison of approval vs reward focuses on this mechanism."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://www.alignmentforum.org/posts/FuGDYNvA6qh4qyFah/thoughts-on-human-compatible"
    },
    {
      "key": "title",
      "value": "Thoughts on \"Human-Compatible\""
    },
    {
      "key": "date_published",
      "value": "2019-10-10T05:24:32Z"
    },
    {
      "key": "authors",
      "value": [
        "TurnTrout"
      ]
    },
    {
      "key": "source",
      "value": "alignmentforum"
    }
  ]
}