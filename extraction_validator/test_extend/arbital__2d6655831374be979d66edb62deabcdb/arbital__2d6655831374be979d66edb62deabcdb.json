{
  "nodes": [
    {
      "name": "Illusory alignment in AI systems",
      "aliases": [
        "false-positive alignment",
        "apparent but not real goodness"
      ],
      "type": "concept",
      "description": "Risk that an AI system appears aligned and good to evaluators while producing outcomes that are not actually good according to ideal human values.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Opening sentence discusses difficulty of making AI 'actually good' not just 'fooling us into thinking that it's good'."
    },
    {
      "name": "Ambiguous terminology for goodness in AI discourse",
      "aliases": [
        "unclear meaning of 'good'",
        "normative ambiguity"
      ],
      "type": "concept",
      "description": "Problem that words like 'good', 'normative', or 'beneficial' are used without a precise shared meaning, leading to confusion in safety discussions.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Text highlights need for a word that means 'really actually good' rather than merely seeming good."
    },
    {
      "name": "Need for speaker-dependent variable denoting actual goodness",
      "aliases": [
        "requirement for precise normative placeholder",
        "speaker-indexed true good"
      ],
      "type": "concept",
      "description": "Insight that a placeholder tied to each speaker's meta-ethical stance can capture 'whatever I ideally ought to want', enabling precise reasoning despite unresolved ethics.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Source frames 3d9 as a variable each speaker can bind to their preferred meta-ethical account (e.g., extrapolated volition)."
    },
    {
      "name": "Introduce reserved term 'beneficial' for actual goodness",
      "aliases": [
        "define 3d9 token",
        "reserved word for true good"
      ],
      "type": "concept",
      "description": "Design choice to employ a specific reserved term that unambiguously refers to outcomes that are truly good in the speaker's meta-ethical sense.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Page states AI alignment theory sometimes needs such a word to discuss difficulty of making an AI beneficial."
    },
    {
      "name": "Define 3d9 token referencing meta-ethical groundings (e.g., extrapolated volition)",
      "aliases": [
        "speaker-dependent 3d9 token",
        "meta-ethical reference variable"
      ],
      "type": "concept",
      "description": "Implementation mechanism where the reserved term is operationalised as 3d9, pointing to each speaker's chosen grounding like extrapolated volition.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Source explicitly describes 3d9 variable and gives extrapolated volition as an example binding."
    },
    {
      "name": "Improved clarity in AI alignment theory discussions",
      "aliases": [
        "clearer safety discourse",
        "reduced confusion about goodness"
      ],
      "type": "concept",
      "description": "Evidence that using the reserved term allows theorists to separate true goodness from mere appearance, facilitating rigorous debate (conceptual, anecdotal).",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Text implies that the term enables talking about actual difficulty versus being fooled."
    },
    {
      "name": "Standardise use of reserved term 'beneficial' in AI alignment documentation",
      "aliases": [
        "adopt 3d9 terminology",
        "terminology standardisation for true goodness"
      ],
      "type": "intervention",
      "description": "Update research papers, specifications, and safety evaluations to consistently use the reserved term 'beneficial' (3d9) to denote outcomes that are truly good in the specified meta-ethical sense.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "Falls under policy/communication practices rather than model lifecycle phases (implicit across documents).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Only conceptual adoption so far; no systematic enforcement or tooling mentioned.",
      "node_rationale": "Inferred actionable step to implement the design choice across the community."
    },
    {
      "name": "Embed meta-ethical grounding (e.g., extrapolated volition) into AI goal specifications",
      "aliases": [
        "specify goals via speaker's extrapolated volition",
        "meta-ethical grounding in utility functions"
      ],
      "type": "intervention",
      "description": "When defining an AI’s objective, explicitly bind the reserved 'beneficial' variable to a formalisation of the designer’s chosen meta-ethical theory such as extrapolated volition.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Ties directly to model objective design before training.",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Purely theoretical; no engineering framework described.",
      "node_rationale": "Moderate inference: applying the variable to actual system design is a plausible next step from the text."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Illusory alignment in AI systems",
      "target_node": "Ambiguous terminology for goodness in AI discourse",
      "description": "Confusion about what counts as 'good' enables AIs to exploit proxies, creating illusory alignment.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Conceptual argument implicit in text; no empirical data.",
      "edge_rationale": "Opening discussion links evaluation mistakes to lack of precise term."
    },
    {
      "type": "caused_by",
      "source_node": "Ambiguous terminology for goodness in AI discourse",
      "target_node": "Need for speaker-dependent variable denoting actual goodness",
      "description": "Ambiguity motivates creation of a precise variable tied to each speaker's ideal values.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Directly stated: AI alignment theory needs such a word.",
      "edge_rationale": "Main paragraph describing why 3d9 is introduced."
    },
    {
      "type": "implemented_by",
      "source_node": "Need for speaker-dependent variable denoting actual goodness",
      "target_node": "Introduce reserved term 'beneficial' for actual goodness",
      "description": "Design rationale realises the theoretical need through a reserved word.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit link in text: 'AI alignment theory sometimes needs a word for this'.",
      "edge_rationale": "Same sentence."
    },
    {
      "type": "implemented_by",
      "source_node": "Introduce reserved term 'beneficial' for actual goodness",
      "target_node": "Define 3d9 token referencing meta-ethical groundings (e.g., extrapolated volition)",
      "description": "Implementation mechanism specifies how the reserved term functions and can be bound.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Text defines 3d9 and gives binding examples.",
      "edge_rationale": "Paragraph on speaker using extrapolated volition."
    },
    {
      "type": "validated_by",
      "source_node": "Define 3d9 token referencing meta-ethical groundings (e.g., extrapolated volition)",
      "target_node": "Improved clarity in AI alignment theory discussions",
      "description": "Use of the token is argued to facilitate clearer discussion distinguishing real from apparent goodness.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Only conceptual claim, no measurements.",
      "edge_rationale": "Implication in text that term helps avoid being fooled."
    },
    {
      "type": "motivates",
      "source_node": "Improved clarity in AI alignment theory discussions",
      "target_node": "Standardise use of reserved term 'beneficial' in AI alignment documentation",
      "description": "Clarity benefit motivates community-wide adoption in documentation.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Moderate inference translating conceptual benefit into action.",
      "edge_rationale": "Inferred step to operationalise clarity."
    },
    {
      "type": "enabled_by",
      "source_node": "Define 3d9 token referencing meta-ethical groundings (e.g., extrapolated volition)",
      "target_node": "Embed meta-ethical grounding (e.g., extrapolated volition) into AI goal specifications",
      "description": "Having a formal variable for true goodness enables directly linking goal specifications to meta-ethical theories.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Moderate inference; plausible next step but not explicit.",
      "edge_rationale": "Logical extension of token definition."
    },
    {
      "type": "mitigated_by",
      "source_node": "Illusory alignment in AI systems",
      "target_node": "Standardise use of reserved term 'beneficial' in AI alignment documentation",
      "description": "Consistent terminology reduces risk of evaluators mistaking apparent for real alignment.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual reasoning; no empirical validation.",
      "edge_rationale": "Outcome of standardisation inferred."
    },
    {
      "type": "mitigated_by",
      "source_node": "Illusory alignment in AI systems",
      "target_node": "Embed meta-ethical grounding (e.g., extrapolated volition) into AI goal specifications",
      "description": "Explicit grounding of objectives in robust meta-ethical theory directly addresses disparity between apparent and actual goodness.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Moderate inference from conceptual discussion.",
      "edge_rationale": "Makes illusion less likely by defining real goodness formally."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://arbital.com/p/beneficial"
    },
    {
      "key": "title",
      "value": "'Beneficial'"
    },
    {
      "key": "date_published",
      "value": "2016-05-01T18:00:26Z"
    },
    {
      "key": "authors",
      "value": [
        "Eric Bruylant",
        "Eliezer Yudkowsky"
      ]
    },
    {
      "key": "source",
      "value": "arbital"
    }
  ]
}