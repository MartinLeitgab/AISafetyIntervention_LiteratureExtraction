{
  "nodes": [
    {
      "name": "Loss of control over AGI behavior due to inadequate understanding of its learning architecture",
      "aliases": [
        "AGI control failure from unknown learning algorithms",
        "Unpredictable AGI behavior risk"
      ],
      "type": "concept",
      "description": "If developers cannot predict how an AGI's learning algorithms respond to situations or updates, they may be unable to keep the system aligned or safe, leading to potentially catastrophic outcomes.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in opening section 'Here is the ultimate challenge'; forms the top-level risk driving the entire argument."
    },
    {
      "name": "Unfamiliar foundational learning algorithms reducing predictability of AGI behavior",
      "aliases": [
        "Unknown base learning strategy unpredictability",
        "Low familiarity learning algorithms"
      ],
      "type": "concept",
      "description": "When the AGI relies on learning paradigms (e.g., novel beyond RL/SL) that researchers have little experience with, its internal dynamics and decision patterns become difficult to forecast.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in 'Foundational learning algorithm' section stressing gaps in familiarity."
    },
    {
      "name": "High post-deployment learning capacity causing unpredictable updates in AGI",
      "aliases": [
        "Online learning unpredictability",
        "Continual self-modification risk"
      ],
      "type": "concept",
      "description": "If an AGI continues to learn heavily after deployment, its policies and world model can shift in unforeseen ways, complicating monitoring and control.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined in 'Post-deployment learning capacity' axis as a key uncertainty for safety."
    },
    {
      "name": "Misalignment between AGI general intelligence level and human control assumptions",
      "aliases": [
        "Incorrect intelligence-level assumption",
        "General intelligence mismatch"
      ],
      "type": "concept",
      "description": "Safety measures presuming human-level or superhuman capability may fail if the deployed AGI exhibits subhuman or differently distributed general intelligence, or vice-versa.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "From 'Level of general intelligence' axis distinguishing sub-, human- and super-human AGI."
    },
    {
      "name": "Understanding learning algorithm architecture enables behavior prediction and control in AGI",
      "aliases": [
        "Architecture comprehension for control",
        "Predictive understanding of learning strategies"
      ],
      "type": "concept",
      "description": "Gaining a principled grasp of the AGI’s underlying learning mechanisms is posited as a prerequisite for forecasting its actions and implementing reliable control schemes.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central claim reiterated in 'Zooming back out'."
    },
    {
      "name": "Mapping AGI architecture predictions using axes of foundational algorithm, post-deployment learning capacity, and general intelligence to inform control proposals",
      "aliases": [
        "Three-axis prediction mapping",
        "Architecture tri-axis localisation"
      ],
      "type": "concept",
      "description": "Researchers can project where an anticipated AGI sits in a three-dimensional space defined by learning paradigm, online/offline capacity, and intelligence level, then tailor safety strategies accordingly.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivated in sections introducing the three axes and their utility for unifying varied safety viewpoints."
    },
    {
      "name": "Focusing safety research on less familiar RL and online learning architectures to close knowledge gaps",
      "aliases": [
        "Prioritise RL-centric online architectures",
        "Target unfamiliar architectures for safety"
      ],
      "type": "concept",
      "description": "Given lower familiarity and higher potential risk, safety work should preferentially investigate RL-heavy and strongly online AGI designs.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implied contrast between 'prosaic' SL-offline focus and 'brain-based' RL-online concerns."
    },
    {
      "name": "Three-dimensional taxonomy tool for positioning AGI projects along axes",
      "aliases": [
        "3-D architecture taxonomy",
        "Tri-axis framework tool"
      ],
      "type": "concept",
      "description": "A practical representation (graph/visual or checklist) that allows teams to plot candidate AGI systems against the three axes, generating a concrete reference point for alignment planning.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Embodied in figures and discussion comparing prosaic vs. brain-based AGI."
    },
    {
      "name": "Illustrative mapping of Tesla Autopilot and human brain validates framework utility",
      "aliases": [
        "Tesla vs. brain case study",
        "Framework illustration examples"
      ],
      "type": "concept",
      "description": "Placing Tesla’s offline SL system and the human brain’s online RL-ish system within the framework demonstrates its coverage and clarifies axis interpretation.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Examples provided under 'Post-deployment learning capacity' and later figures serve as informal validation."
    },
    {
      "name": "Apply three-axis architecture framework during AGI project scoping to tailor alignment strategies",
      "aliases": [
        "Use architecture taxonomy in model design",
        "Incorporate tri-axis analysis in planning"
      ],
      "type": "intervention",
      "description": "During early model-design discussions, teams systematically locate their proposed AGI within the three-axis space and generate architecture-specific alignment requirements before heavy development begins.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Proposed for the conception/model-design stage referenced in framework presentation sections.",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Framework is theoretical; no empirical deployments noted ('Zooming back out').",
      "node_rationale": "Direct actionable step inferred from paper’s claim that localisation is prerequisite for later control work."
    },
    {
      "name": "Increase foundational research into RL and online learning safety to raise familiarity pre-deployment",
      "aliases": [
        "Prioritise RL/online safety research",
        "Bridge familiarity gap via research"
      ],
      "type": "intervention",
      "description": "Allocate funding and effort to studying safety properties, interpretability, and control techniques for RL-dominant and online-learning AGI systems before they are built.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "Represents meta-level research agenda setting, not tied to a specific development phase.",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Positioned as a forward-looking research priority without current prototypes (sections comparing prosaic vs. brain-based).",
      "node_rationale": "Implied by design rationale to close knowledge gaps on unfamiliar architectures."
    },
    {
      "name": "Develop architecture-specific control mechanisms aligned to predicted AGI point in framework",
      "aliases": [
        "Tailor controls to framework position",
        "Design axis-aligned safety measures"
      ],
      "type": "intervention",
      "description": "For each located point (e.g., RL-online-superhuman), create bespoke monitoring, alignment, or interpretability techniques that address the unique risks of that cluster.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Envisioned during system design once the framework position is known.",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Conceptual; no systematic validation yet cited.",
      "node_rationale": "Follows logically from framework’s stated purpose of grounding control proposals in architecture predictions."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Loss of control over AGI behavior due to inadequate understanding of its learning architecture",
      "target_node": "Unfamiliar foundational learning algorithms reducing predictability of AGI behavior",
      "description": "Unfamiliar or novel learning paradigms make it harder to foresee AGI behavior, directly contributing to loss of control.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit argument in 'Foundational learning algorithm' section; conceptual but well-reasoned.",
      "edge_rationale": "Author links lack of familiarity to inability to control."
    },
    {
      "type": "caused_by",
      "source_node": "Loss of control over AGI behavior due to inadequate understanding of its learning architecture",
      "target_node": "High post-deployment learning capacity causing unpredictable updates in AGI",
      "description": "Continual online learning post-deployment introduces unpredictable behavioral drift, heightening control failure risk.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Discussed in 'Post-deployment learning capacity' with concrete examples.",
      "edge_rationale": "Uncontrolled updates make safety guarantees hard."
    },
    {
      "type": "caused_by",
      "source_node": "Loss of control over AGI behavior due to inadequate understanding of its learning architecture",
      "target_node": "Misalignment between AGI general intelligence level and human control assumptions",
      "description": "Designing controls for the wrong intelligence level undermines their effectiveness, leading to loss of control.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoned in 'Level of general intelligence' section though not empirically tested.",
      "edge_rationale": "Mismatch invalidates control models."
    },
    {
      "type": "mitigated_by",
      "source_node": "Unfamiliar foundational learning algorithms reducing predictability of AGI behavior",
      "target_node": "Understanding learning algorithm architecture enables behavior prediction and control in AGI",
      "description": "Gaining architectural understanding directly addresses unpredictability stemming from unfamiliar algorithms.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical mitigation proposed, but no empirical data.",
      "edge_rationale": "Understanding removes unfamiliarity."
    },
    {
      "type": "mitigated_by",
      "source_node": "High post-deployment learning capacity causing unpredictable updates in AGI",
      "target_node": "Understanding learning algorithm architecture enables behavior prediction and control in AGI",
      "description": "A principled grasp of the learning process aids in predicting and constraining online updates.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual connection in same section.",
      "edge_rationale": "Framework provides insight into dynamic behavior."
    },
    {
      "type": "mitigated_by",
      "source_node": "Misalignment between AGI general intelligence level and human control assumptions",
      "target_node": "Understanding learning algorithm architecture enables behavior prediction and control in AGI",
      "description": "Architectural understanding encompasses intelligence-level considerations, reducing assumption mismatch.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Reasoned argument, no direct evidence.",
      "edge_rationale": "Better models correct mistaken assumptions."
    },
    {
      "type": "implemented_by",
      "source_node": "Understanding learning algorithm architecture enables behavior prediction and control in AGI",
      "target_node": "Mapping AGI architecture predictions using axes of foundational algorithm, post-deployment learning capacity, and general intelligence to inform control proposals",
      "description": "The three-axis mapping operationalises the high-level insight into a concrete strategy.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Framework explicitly presented as practical embodiment.",
      "edge_rationale": "Mapping turns insight into actionable design rationale."
    },
    {
      "type": "implemented_by",
      "source_node": "Understanding learning algorithm architecture enables behavior prediction and control in AGI",
      "target_node": "Focusing safety research on less familiar RL and online learning architectures to close knowledge gaps",
      "description": "Comprehension goal motivates prioritising research where understanding is weakest.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference from discussion comparing researcher focus areas.",
      "edge_rationale": "Research focus is strategic implementation of insight."
    },
    {
      "type": "implemented_by",
      "source_node": "Mapping AGI architecture predictions using axes of foundational algorithm, post-deployment learning capacity, and general intelligence to inform control proposals",
      "target_node": "Three-dimensional taxonomy tool for positioning AGI projects along axes",
      "description": "The taxonomy tool embodies the mapping approach, providing a concrete artefact (graph/checklist).",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Figures and explanations act as prototype of tool.",
      "edge_rationale": "Tool realises mapping design rationale."
    },
    {
      "type": "implemented_by",
      "source_node": "Focusing safety research on less familiar RL and online learning architectures to close knowledge gaps",
      "target_node": "Three-dimensional taxonomy tool for positioning AGI projects along axes",
      "description": "The same taxonomy supports identifying under-explored regions (RL-online) for targeted research.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical, but only lightly implied.",
      "edge_rationale": "Tool highlights research priorities."
    },
    {
      "type": "validated_by",
      "source_node": "Three-dimensional taxonomy tool for positioning AGI projects along axes",
      "target_node": "Illustrative mapping of Tesla Autopilot and human brain validates framework utility",
      "description": "Example placements serve as informal validation that the tool can represent real systems.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Validation is illustrative, not empirical.",
      "edge_rationale": "Examples demonstrate feasibility."
    },
    {
      "type": "motivates",
      "source_node": "Illustrative mapping of Tesla Autopilot and human brain validates framework utility",
      "target_node": "Apply three-axis architecture framework during AGI project scoping to tailor alignment strategies",
      "description": "Seeing the framework work on examples encourages adoption early in new AGI projects.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Reasonable inference from illustrative success.",
      "edge_rationale": "Demonstrated utility spurs usage."
    },
    {
      "type": "motivates",
      "source_node": "Illustrative mapping of Tesla Autopilot and human brain validates framework utility",
      "target_node": "Increase foundational research into RL and online learning safety to raise familiarity pre-deployment",
      "description": "The case study highlights gaps (online RL) prompting more foundational research.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Derived from contrast example; inference.",
      "edge_rationale": "Framework reveals weakly understood zones."
    },
    {
      "type": "motivates",
      "source_node": "Illustrative mapping of Tesla Autopilot and human brain validates framework utility",
      "target_node": "Develop architecture-specific control mechanisms aligned to predicted AGI point in framework",
      "description": "Successful mapping suggests tailoring control methods to each located point is viable.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Illustrative evidence supports feasibility; still conceptual.",
      "edge_rationale": "Framework indicates need for bespoke controls."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://www.lesswrong.com/posts/snwpyAfzoFKdfnEDj/question-1-predicted-architecture-of-agi-learning-algorithm"
    },
    {
      "key": "title",
      "value": "Question 1: Predicted architecture of AGI learning algorithm(s)"
    },
    {
      "key": "date_published",
      "value": "2022-02-10T17:22:24Z"
    },
    {
      "key": "authors",
      "value": [
        "Cameron Berg"
      ]
    },
    {
      "key": "source",
      "value": "lesswrong"
    }
  ]
}