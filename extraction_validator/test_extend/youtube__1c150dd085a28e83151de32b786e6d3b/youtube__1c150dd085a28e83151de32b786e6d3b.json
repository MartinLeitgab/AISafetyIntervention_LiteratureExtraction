{
  "nodes": [
    {
      "name": "Incorrect or unsafe decisions by AI systems in complex domains",
      "aliases": [
        "AI decision safety failures",
        "Unreliable AI behaviour"
      ],
      "type": "concept",
      "description": "Top-level safety risk that AI systems make erroneous or harmful decisions when operating in large, complex search spaces or when their internal reasoning cannot be trusted.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Stated motivation throughout talk: need to ensure AI behaviour is correct and trustworthy (opening and closing remarks)."
    },
    {
      "name": "Intractable search spaces in AI reasoning and planning",
      "aliases": [
        "Exponential branching in planning",
        "Combinatorial explosion in search"
      ],
      "type": "concept",
      "description": "Many AI tasks such as long-horizon planning or game play have search spaces of size 2^n or worse, making exhaustive reasoning impossible.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Speaker emphasises exponential space (10^350) for Erdős sequences and 2^15000 SAT examples as core difficulty (mid-talk)."
    },
    {
      "name": "Opaque reasoning without machine-checkable justification",
      "aliases": [
        "Lack of verifiable proofs for AI outputs",
        "Unexplained AI decisions"
      ],
      "type": "concept",
      "description": "Current AI systems, especially deep learning models or heuristic search agents, often output answers without any concise proof or explanation that can be independently verified.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in sections on four-colour theorem controversy and need for checkable witnesses (late talk)."
    },
    {
      "name": "Heuristic-guided randomized search reduces effective reasoning complexity",
      "aliases": [
        "Monte Carlo evaluation insight",
        "UCT theoretical insight"
      ],
      "type": "concept",
      "description": "Using random roll-outs and heuristic value estimates gives useful information about positions, dramatically shrinking the practical search tree.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained when introducing UCT in 2006 as key idea that enabled progress in Go (two-thirds into talk)."
    },
    {
      "name": "Use Monte Carlo Tree Search with UCT selection to explore large search trees efficiently",
      "aliases": [
        "UCT-based MCTS design rationale",
        "Probabilistic tree search strategy"
      ],
      "type": "concept",
      "description": "Design approach: balance exploration and exploitation via Upper-Confidence-bounds-for-Trees, guided by roll-out statistics.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Presented as explicit solution architecture succeeding where minimax failed (Go segment)."
    },
    {
      "name": "Reinforcement learning–enhanced Monte Carlo Tree Search combining policy and value networks",
      "aliases": [
        "AlphaGo-style MCTS implementation",
        "RL-guided UCT search"
      ],
      "type": "concept",
      "description": "Implementation mechanism where deep neural networks supply prior probabilities and value estimates to an MCTS core, further shrinking search requirements.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed during AlphaGo discussion: deep learning removes need for handcrafted roll-outs."
    },
    {
      "name": "AlphaGo victories over professional players demonstrate scalable machine reasoning",
      "aliases": [
        "AlphaGo performance evidence",
        "Go benchmark success"
      ],
      "type": "concept",
      "description": "Empirical validation that RL-augmented MCTS can outperform top human experts, indicating tractability gains translate to real-world performance.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Referenced as landmark evidence of effectiveness (Go segment)."
    },
    {
      "name": "Deploy RL-guided Monte Carlo Tree Search for planning in safety-critical AI systems",
      "aliases": [
        "Apply AlphaGo-style planning to robotics",
        "Use RL-MCTS in autonomous decision making"
      ],
      "type": "intervention",
      "description": "Integrate reinforcement-learning-boosted MCTS modules into the planning stack of robots or other autonomous systems to obtain near-optimal long-horizon plans without exhaustive enumeration.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Affects algorithmic planning design phase before training specific models (\"planning will impact robotics\" segment).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Prototype-level validation in AlphaGo; limited deployment in general safety-critical contexts yet (Go discussion).",
      "node_rationale": "Represents actionable step derived from validated mechanism."
    },
    {
      "name": "Clause-learning SAT solvers can generate compact machine-checkable proof traces",
      "aliases": [
        "CDCL proof-producing SAT insight",
        "Short witnesses via SAT"
      ],
      "type": "concept",
      "description": "Observation that modern Conflict-Driven Clause-Learning (CDCL) SAT engines discover proofs many orders of magnitude shorter than brute-force search and can emit them for external checking.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in depth using Erdős discrepancy encoding and factor 10^339 reduction example."
    },
    {
      "name": "Integrate SAT-based reasoning modules with proof logging for verifiable outputs",
      "aliases": [
        "Design rationale for proof logging",
        "Verification-oriented SAT integration"
      ],
      "type": "concept",
      "description": "Design choice to embed SAT solvers that emit DIMACS CNF proofs, ensuring every result is accompanied by a concise witness.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Stated need to 'throw away solver and still verify' motivates this architecture (Erdős proof segment)."
    },
    {
      "name": "CDCL SAT solver with proof trace generation and independent checker",
      "aliases": [
        "Proof-logging CDCL implementation",
        "DIMACS CNF + 50-line checker"
      ],
      "type": "concept",
      "description": "Concrete mechanism: run a modern CDCL solver configured to output every resolution step; verify trace using a minimal, trusted program.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Described macbook-air experiment (hour for n=1160, 10 h unsat for 1161) and 50-line checker."
    },
    {
      "name": "8 GB proof trace for Erdős-discrepancy 1161 unsatisfiable verified independently",
      "aliases": [
        "Erdős discrepancy proof evidence",
        "Billion-step certified proof"
      ],
      "type": "concept",
      "description": "Empirical evidence: the unsat instance generated a ~8 GB, billion-step proof that any user can download and check, demonstrating practicality of the approach.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted as key success showing verifiable large-scale reasoning (mid-talk)."
    },
    {
      "name": "Embed proof-logging CDCL SAT solvers in AI pipelines to provide verifiable reasoning guarantees",
      "aliases": [
        "Use certified SAT reasoning in deployment",
        "Proof-producing reasoning integration"
      ],
      "type": "intervention",
      "description": "During model design or pre-deployment testing, direct all logical sub-tasks through CDCL solvers that emit proof traces, and require automated proof checking before accepting results.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Used in pre-deployment verification/testing to certify reasoning components (verification sections).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Technique proven in academic/industrial verification but not yet routine in broader AI pipelines.",
      "node_rationale": "Actionable safety measure derived from validated mechanism."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Incorrect or unsafe decisions by AI systems in complex domains",
      "target_node": "Intractable search spaces in AI reasoning and planning",
      "description": "Explosive combinatorial spaces make exhaustive reasoning impossible, leading to potential decision errors.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Speaker repeatedly cites exponential spaces as barrier to safe decisions.",
      "edge_rationale": "Problem analysis directly explains origin of risk."
    },
    {
      "type": "caused_by",
      "source_node": "Incorrect or unsafe decisions by AI systems in complex domains",
      "target_node": "Opaque reasoning without machine-checkable justification",
      "description": "When AI outputs cannot be independently verified, undetected mistakes can propagate to unsafe actions.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit comparison with four-colour-theorem controversy.",
      "edge_rationale": "Lack of verificability identified as root cause."
    },
    {
      "type": "mitigated_by",
      "source_node": "Intractable search spaces in AI reasoning and planning",
      "target_node": "Heuristic-guided randomized search reduces effective reasoning complexity",
      "description": "Heuristic roll-outs provide informative estimates, shrinking effective branching factor.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Talk describes UCT as key breakthrough enabling tractability.",
      "edge_rationale": "Theoretical insight addresses the complexity problem."
    },
    {
      "type": "implemented_by",
      "source_node": "Heuristic-guided randomized search reduces effective reasoning complexity",
      "target_node": "Use Monte Carlo Tree Search with UCT selection to explore large search trees efficiently",
      "description": "UCT operationalises the theoretical principle within a concrete search algorithm.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Widely documented algorithm, explained in talk.",
      "edge_rationale": "Design embodies the insight."
    },
    {
      "type": "implemented_by",
      "source_node": "Use Monte Carlo Tree Search with UCT selection to explore large search trees efficiently",
      "target_node": "Reinforcement learning–enhanced Monte Carlo Tree Search combining policy and value networks",
      "description": "Deep policy/value networks supply priors and roll-out replacements, realising the design at scale.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "AlphaGo architecture openly published; speaker details combination.",
      "edge_rationale": "Implementation realises design rationale."
    },
    {
      "type": "validated_by",
      "source_node": "Reinforcement learning–enhanced Monte Carlo Tree Search combining policy and value networks",
      "target_node": "AlphaGo victories over professional players demonstrate scalable machine reasoning",
      "description": "Empirical performance in world-class matches confirms effectiveness.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Well-documented tournament results referenced.",
      "edge_rationale": "Evidence node supports implementation mechanism."
    },
    {
      "type": "motivates",
      "source_node": "AlphaGo victories over professional players demonstrate scalable machine reasoning",
      "target_node": "Deploy RL-guided Monte Carlo Tree Search for planning in safety-critical AI systems",
      "description": "Demonstrated capability motivates adoption in broader, safety-relevant contexts.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical extension proposed by speaker; no direct experiments yet.",
      "edge_rationale": "Validation evidence drives intervention."
    },
    {
      "type": "mitigated_by",
      "source_node": "Opaque reasoning without machine-checkable justification",
      "target_node": "Clause-learning SAT solvers can generate compact machine-checkable proof traces",
      "description": "Proof-producing solvers provide short witnesses that make reasoning transparent.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Speaker presents proof trace as solution to earlier verification worries.",
      "edge_rationale": "Insight addresses opacity problem."
    },
    {
      "type": "enabled_by",
      "source_node": "Clause-learning SAT solvers can generate compact machine-checkable proof traces",
      "target_node": "Integrate SAT-based reasoning modules with proof logging for verifiable outputs",
      "description": "The insight underpins the design choice to embed proof logging.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Design follows directly from theoretical capability.",
      "edge_rationale": "Design operationalises insight."
    },
    {
      "type": "implemented_by",
      "source_node": "Integrate SAT-based reasoning modules with proof logging for verifiable outputs",
      "target_node": "CDCL SAT solver with proof trace generation and independent checker",
      "description": "Concrete mechanism realises design via CDCL solver and minimal checker.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Speaker describes MacBook implementation and 50-line checker.",
      "edge_rationale": "Implementation mechanism realises design rationale."
    },
    {
      "type": "validated_by",
      "source_node": "CDCL SAT solver with proof trace generation and independent checker",
      "target_node": "8 GB proof trace for Erdős-discrepancy 1161 unsatisfiable verified independently",
      "description": "Large proof successfully generated and independently checked, demonstrating practicality.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit empirical result in talk.",
      "edge_rationale": "Evidence confirms implementation effectiveness."
    },
    {
      "type": "motivates",
      "source_node": "8 GB proof trace for Erdős-discrepancy 1161 unsatisfiable verified independently",
      "target_node": "Embed proof-logging CDCL SAT solvers in AI pipelines to provide verifiable reasoning guarantees",
      "description": "Successful proof verification motivates deploying approach for general AI verification.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical extrapolation suggested by speaker.",
      "edge_rationale": "Validation evidence drives intervention."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2016-10-21T20:26:46Z"
    },
    {
      "key": "authors",
      "value": [
        "Machine Intelligence Research Institute"
      ]
    },
    {
      "key": "title",
      "value": "Bart Selman – Non-Human Intelligence – CSRBAI 2016"
    },
    {
      "key": "url",
      "value": "https://www.youtube.com/watch?v=35BWlvPcYvg"
    },
    {
      "key": "source",
      "value": "youtube"
    }
  ]
}