{
  "nodes": [
    {
      "name": "Patch resistant value misalignment in advanced AI systems",
      "aliases": [
        "patch-resistant misalignment",
        "value misalignment despite training"
      ],
      "type": "concept",
      "description": "Advanced AI may appear aligned on many training problems yet diverge on a novel problem because earlier coverage failed to constrain all Humean degrees of freedom.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Represents the top-level risk motivating the paper (main text first paragraph)"
    },
    {
      "name": "Uncorrelated coverage across Humean value degrees of freedom in AI training",
      "aliases": [
        "uncorrelated value coverage",
        "missing value dimensions"
      ],
      "type": "concept",
      "description": "Because later problems may depend on latent value dimensions not exercised during training, success on earlier tasks need not predict success on future tasks.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explains mechanism causing patch-resistant misalignment (paragraphs on 1001st question)"
    },
    {
      "name": "Correlated coverage principle for robust alignment",
      "aliases": [
        "correlated coverage concept",
        "coverage that generalises"
      ],
      "type": "concept",
      "description": "If an AI embodies a compact core idea whose application generalises, solving many training problems will highly correlate with solving future ones in the same domain.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central theoretical construct of the essay (title sentence)"
    },
    {
      "name": "Adoption of core alignment principles to achieve correlated coverage",
      "aliases": [
        "core-principle alignment design",
        "principle-centric safety"
      ],
      "type": "concept",
      "description": "Rather than enumerating forbidden impacts, designers should encode a single compact principle expected to cover all relevant cases.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "General design strategy flowing from the correlated coverage insight (paragraph on gradient from blacklists to low-impact)"
    },
    {
      "name": "Low-impact penalty objective in AI utility function",
      "aliases": [
        "low-impact regularisation",
        "impact minimisation term"
      ],
      "type": "concept",
      "description": "Embed a penalty for causing large unintended changes, hoping that the simplicity of ‘low impact’ generalises better than listing every bad outcome.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Specific mechanism illustrating a core principle (low-impact discussion)"
    },
    {
      "name": "Conceptual argument that low-impact is compact and generalisable",
      "aliases": [
        "low-impact compactness reasoning",
        "low-impact correlated coverage evidence"
      ],
      "type": "concept",
      "description": "Author asserts that ‘low impact in general’ may be simple enough to generalise across sharp capability gains, unlike listing bad impacts.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Provides evidence supporting the low-impact mechanism (gradient paragraph)"
    },
    {
      "name": "Apply low-impact regularization during fine-tuning and pre-deployment testing",
      "aliases": [
        "train with low-impact penalty",
        "low-impact fine-tuning"
      ],
      "type": "intervention",
      "description": "During RLHF or similar fine-tuning phases, include an explicit low-impact penalty term and evaluate models in stress tests for unintended side-effects.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Penalty is introduced while adjusting reward models and can be tested before release (main text gradient example)",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Only conceptual proposals exist; no systematic validation cited (main text)",
      "node_rationale": "Actionable step derived from low-impact mechanism"
    },
    {
      "name": "Anapartistic reasoning for external correction",
      "aliases": [
        "anapartistic core principle",
        "deference-based alignment"
      ],
      "type": "concept",
      "description": "Design the AI around a core idea of letting an overseer correct it without manipulating their safety measures.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Second candidate principle proposed for correlated coverage (paragraph on anapartistic reasoning)"
    },
    {
      "name": "Agent architecture deferring to overseer corrections (anapartistic implementation)",
      "aliases": [
        "correction-deference architecture",
        "anapartistic mechanism"
      ],
      "type": "concept",
      "description": "Structure the agent (B) so that it actively preserves the ability of another agent (A) to intervene and correct it.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete mechanism realising anapartistic reasoning (same paragraph)"
    },
    {
      "name": "Conceptual proof-of-concept reasoning for anapartistic coverage",
      "aliases": [
        "anapartistic compactness argument"
      ],
      "type": "concept",
      "description": "Author argues that if 100 problems are solved by the same deference principle, remaining problems may also be solved, indicating correlated coverage.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Provides evidence supporting anapartistic implementation (paragraph on 100 problems right)"
    },
    {
      "name": "Design models with anapartistic reasoning architecture enabling overseer corrections",
      "aliases": [
        "build deference-enabled models"
      ],
      "type": "intervention",
      "description": "At model-design time embed an architecture that preserves and honours external correction channels, preventing interference with safety measures.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Requires architectural choices before pre-training (anapartistic paragraph)",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Only theoretical; no prototypes referenced",
      "node_rationale": "Actionable design derived from anapartistic mechanism"
    },
    {
      "name": "Do What I Mean principle for user-aligned action selection",
      "aliases": [
        "DWIM core principle",
        "model user intentions"
      ],
      "type": "concept",
      "description": "Base behaviour on modelling the user and only performing actions whose consequences it is confident the user would approve.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Third candidate core principle (DWIM paragraph)"
    },
    {
      "name": "User-modelling with uncertainty-avoidant action selection (DWIM implementation)",
      "aliases": [
        "DWIM mechanism",
        "user intent modelling"
      ],
      "type": "concept",
      "description": "Implement internal user models and abstain or query when consequence approval is uncertain.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mechanism realising DWIM principle (DWIM paragraph)"
    },
    {
      "name": "Conceptual argument that DWIM reduces complexity versus full value encoding",
      "aliases": [
        "DWIM compactness argument"
      ],
      "type": "concept",
      "description": "Author claims DWIM may achieve correlated coverage with less complexity than encoding all human values explicitly.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Evidence supporting DWIM mechanism (DWIM paragraph)"
    },
    {
      "name": "Implement DWIM-based user-intention modelling during deployment",
      "aliases": [
        "deploy DWIM agents"
      ],
      "type": "intervention",
      "description": "In deployed systems maintain live user-modelling modules that veto or seek clarification when confidence in user approval is low.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Module operates at runtime in deployment environment (DWIM paragraph)",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "No empirical evaluations cited; conceptual stage",
      "node_rationale": "Actionable deployment step derived from DWIM mechanism"
    },
    {
      "name": "Observed human general epistemology enabled by Bayesian updating and simplicity prior",
      "aliases": [
        "human epistemic generalisation evidence",
        "Bayesian-simplicity human example"
      ],
      "type": "concept",
      "description": "Humans, selected for simple survival tasks, could later derive General Relativity, suggesting a core epistemic principle can generalise widely.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Serves as empirical-ish analogy validating correlated coverage concept (human epistemology paragraph)"
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Patch resistant value misalignment in advanced AI systems",
      "target_node": "Uncorrelated coverage across Humean value degrees of freedom in AI training",
      "description": "Misalignment arises because training success fails to constrain all value dimensions.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit textual argument but qualitative (1001st query example)",
      "edge_rationale": "Connects top risk to underlying mechanism (same paragraph)"
    },
    {
      "type": "addressed_by",
      "source_node": "Uncorrelated coverage across Humean value degrees of freedom in AI training",
      "target_node": "Correlated coverage principle for robust alignment",
      "description": "Establishing correlated coverage directly tackles the uncorrelated coverage problem.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Stated as conceptual remedy; no data",
      "edge_rationale": "Main thesis of article"
    },
    {
      "type": "validated_by",
      "source_node": "Correlated coverage principle for robust alignment",
      "target_node": "Observed human general epistemology enabled by Bayesian updating and simplicity prior",
      "description": "Human epistemic success is cited as evidence that a core principle can generalise.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Analogy rather than controlled evidence",
      "edge_rationale": "Paragraph on humans discovering General Relativity"
    },
    {
      "type": "enabled_by",
      "source_node": "Correlated coverage principle for robust alignment",
      "target_node": "Adoption of core alignment principles to achieve correlated coverage",
      "description": "Designing around a compact principle operationalises correlated coverage.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical derivation; no empirical data",
      "edge_rationale": "Gradient paragraph"
    },
    {
      "type": "implemented_by",
      "source_node": "Adoption of core alignment principles to achieve correlated coverage",
      "target_node": "Low-impact penalty objective in AI utility function",
      "description": "Low-impact is a concrete instantiation of a core alignment principle.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Directly stated in text",
      "edge_rationale": "Low-impact discussion"
    },
    {
      "type": "validated_by",
      "source_node": "Low-impact penalty objective in AI utility function",
      "target_node": "Conceptual argument that low-impact is compact and generalisable",
      "description": "Argument provides supporting evidence that low-impact will generalise.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual reasoning only",
      "edge_rationale": "Same paragraph"
    },
    {
      "type": "motivates",
      "source_node": "Conceptual argument that low-impact is compact and generalisable",
      "target_node": "Apply low-impact regularization during fine-tuning and pre-deployment testing",
      "description": "If argument holds, practitioners should incorporate low-impact penalties in training and testing.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference from conceptual evidence to practice",
      "edge_rationale": "Gradient paragraph, moderate inference"
    },
    {
      "type": "enabled_by",
      "source_node": "Correlated coverage principle for robust alignment",
      "target_node": "Anapartistic reasoning for external correction",
      "description": "Anapartistic reasoning is posed as another core principle realising correlated coverage.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicitly specified in text",
      "edge_rationale": "Anapartistic paragraph"
    },
    {
      "type": "implemented_by",
      "source_node": "Anapartistic reasoning for external correction",
      "target_node": "Agent architecture deferring to overseer corrections (anapartistic implementation)",
      "description": "Deference-preserving architecture realises anapartistic reasoning.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct conceptual mapping",
      "edge_rationale": "Same paragraph"
    },
    {
      "type": "validated_by",
      "source_node": "Agent architecture deferring to overseer corrections (anapartistic implementation)",
      "target_node": "Conceptual proof-of-concept reasoning for anapartistic coverage",
      "description": "Reasoning about 100 correct problems and next 1000 provides support.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual example only",
      "edge_rationale": "Same paragraph"
    },
    {
      "type": "motivates",
      "source_node": "Conceptual proof-of-concept reasoning for anapartistic coverage",
      "target_node": "Design models with anapartistic reasoning architecture enabling overseer corrections",
      "description": "Supporting reasoning encourages implementing such architectures.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference from conceptual argument to design step",
      "edge_rationale": "Anapartistic paragraph"
    },
    {
      "type": "enabled_by",
      "source_node": "Correlated coverage principle for robust alignment",
      "target_node": "Do What I Mean principle for user-aligned action selection",
      "description": "DWIM is presented as a third pathway to correlated coverage.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit in text",
      "edge_rationale": "DWIM paragraph"
    },
    {
      "type": "implemented_by",
      "source_node": "Do What I Mean principle for user-aligned action selection",
      "target_node": "User-modelling with uncertainty-avoidant action selection (DWIM implementation)",
      "description": "User modelling and cautious execution instantiate DWIM.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct mapping from principle to mechanism",
      "edge_rationale": "DWIM paragraph"
    },
    {
      "type": "validated_by",
      "source_node": "User-modelling with uncertainty-avoidant action selection (DWIM implementation)",
      "target_node": "Conceptual argument that DWIM reduces complexity versus full value encoding",
      "description": "Argument gives conceptual evidence of DWIM’s generality.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual only",
      "edge_rationale": "DWIM paragraph"
    },
    {
      "type": "motivates",
      "source_node": "Conceptual argument that DWIM reduces complexity versus full value encoding",
      "target_node": "Implement DWIM-based user-intention modelling during deployment",
      "description": "If DWIM is compact and generalisable, practitioners should deploy user-modelling modules.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference from conceptual argument",
      "edge_rationale": "DWIM paragraph"
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://arbital.com/p/correlated_coverage"
    },
    {
      "key": "title",
      "value": "Correlated coverage"
    },
    {
      "key": "date_published",
      "value": "2016-06-26T23:37:41Z"
    },
    {
      "key": "authors",
      "value": [
        "Eric Bruylant",
        "Eliezer Yudkowsky"
      ]
    },
    {
      "key": "source",
      "value": "arbital"
    }
  ]
}