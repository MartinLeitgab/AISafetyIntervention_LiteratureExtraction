{
  "nodes": [
    {
      "name": "incorrect task execution in novel environments by language-conditioned agents",
      "aliases": [
        "poor generalisation of instruction-following agents",
        "task failure in unseen houses"
      ],
      "type": "concept",
      "description": "Agents trained to follow language commands may execute the wrong behaviour when evaluated in new environments, threatening safety and usability.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Top-level safety risk implicitly motivating the paper (Section 1 Introduction)."
    },
    {
      "name": "coupled language understanding and control learning in language-conditioned policy training",
      "aliases": [
        "joint language-policy learning",
        "direct language-to-action mapping"
      ],
      "type": "concept",
      "description": "Policies that map observations and commands directly to actions must simultaneously interpret language and plan, making zero-shot transfer fragile.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Identified mechanism causing failures (Section 1 Introduction)."
    },
    {
      "name": "manual reward specification difficulty in vision-based tasks",
      "aliases": [
        "reward engineering burden",
        "hand-designed task detectors"
      ],
      "type": "concept",
      "description": "Designing reward functions for complex, image-based environments is tedious and error-prone, limiting scalability.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Stated obstacle that motivates learning language-conditioned rewards (Section 1 Introduction)."
    },
    {
      "name": "sparse reward signal in RL re-optimization of learned rewards",
      "aliases": [
        "exploration challenge with learned rewards",
        "scarce feedback during DQN training"
      ],
      "type": "concept",
      "description": "When re-optimising learned rewards with model-free RL, agents rarely observe positive feedback, hindering learning.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explains low DQN success without shaping (Section 6.3 Experimental Results, Table 2)."
    },
    {
      "name": "decoupling language and control via reward function grounding",
      "aliases": [
        "language-to-reward mapping",
        "goal grounding in rewards"
      ],
      "type": "concept",
      "description": "Mapping language commands to rewards lets the agent learn control separately, reducing reliance on zero-shot policy generalisation.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Key hypothesis articulated in Introduction."
    },
    {
      "name": "inverse reinforcement learning from demonstrations infers reward functions",
      "aliases": [
        "IRL to learn rewards",
        "reward inference from expert trajectories"
      ],
      "type": "concept",
      "description": "Maximum-Entropy IRL can learn a reward that explains expert demonstrations, avoiding manual reward design.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central theoretical tool (Section 3 Background)."
    },
    {
      "name": "potential-based reward shaping can aid exploration in sparse reward tasks",
      "aliases": [
        "value-function shaping for exploration",
        "dense shaping signals"
      ],
      "type": "concept",
      "description": "Adding a potential-based term derived from an optimal value function provides denser feedback, facilitating learning.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivated in Section 6.3 where shaping improves DQN performance."
    },
    {
      "name": "multi-task language-conditioned IRL for reward generalization",
      "aliases": [
        "shared reward across tasks",
        "LC-RL objective"
      ],
      "type": "concept",
      "description": "Training a single language-conditioned reward network across many tasks enables it to generalise to novel instructions and scenes.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Core design choice (Section 4 Multi-Task IRL, Eq. 3)."
    },
    {
      "name": "training with known dynamics enables exact IRL optimization",
      "aliases": [
        "simulation with full dynamics",
        "lab-only exact IRL training"
      ],
      "type": "concept",
      "description": "Restricting training to environments whose transitions are known allows use of dynamic programming for exact MaxEnt IRL, avoiding unstable nested RL.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Important rationale given in Section 5.1."
    },
    {
      "name": "use optimal value function potentials for shaping",
      "aliases": [
        "design of shaping term",
        "value-based potential function"
      ],
      "type": "concept",
      "description": "Shaping term Φ(s)=V*(s) is selected because it is potential-based and preserves optimal policies while providing dense rewards.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Design detail for exploration aid (Section 6.2)."
    },
    {
      "name": "exact MaxEnt IRL with dynamic programming occupancy computation",
      "aliases": [
        "exact IRL inner loop",
        "Q-iteration + forward algorithm"
      ],
      "type": "concept",
      "description": "Optimal Q-values and occupancy measures are computed exactly via dynamic programming during training to obtain IRL gradients.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implemented in Algorithm 1, line 5."
    },
    {
      "name": "language-conditioned CNN-LSTM reward network processing panoramic semantic images",
      "aliases": [
        "reward network architecture",
        "panoramic semantic CNN + LSTM"
      ],
      "type": "concept",
      "description": "A shared-weights CNN processes four semantic images; an LSTM encodes language. Element-wise multiplication of embeddings yields the reward.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Architecture detailed in Section 5.2 and Figure 2."
    },
    {
      "name": "stochastic multi-task gradient updates of reward network",
      "aliases": [
        "mini-batch task sampling",
        "SGD across environments"
      ],
      "type": "concept",
      "description": "Each IRL gradient step samples a task and demonstrations to update shared parameters, enabling scalable training.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Step 4-6 of Algorithm 1."
    },
    {
      "name": "potential-based shaping using optimal value function",
      "aliases": [
        "implementation of shaping term",
        "Φ(s)=V*(s) shaping"
      ],
      "type": "concept",
      "description": "During RL re-optimisation, reward r'(s,a)=r(s,a)+γΦ(s')−Φ(s) is used, where Φ is pre-computed from known dynamics.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation mentioned in Section 6.2 and Table 2."
    },
    {
      "name": "LC-RL outperforms policy baselines on test tasks and houses",
      "aliases": [
        "LC-RL success rates Table 1",
        "empirical performance gain"
      ],
      "type": "concept",
      "description": "Measured success 52% on Test-Task and 36% on Test-House versus 20% and 9% for cloning, showing substantial improvement.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Evidence provided in Table 1 (Section 6.3)."
    },
    {
      "name": "learned reward functions generalize across unseen environments",
      "aliases": [
        "reward generalisation evidence",
        "cross-house success"
      ],
      "type": "concept",
      "description": "LC-RL maintains meaningful performance on entirely new houses, proving reward network generalisation.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in Section 6.3 and Table 1."
    },
    {
      "name": "value-function shaping doubles DQN reoptimization success rates",
      "aliases": [
        "shaping improves RL",
        "Table 2 shaping result"
      ],
      "type": "concept",
      "description": "Success with DQN rises from ~8 % to ~14 % when shaping is added, indicating exploration benefits.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Shown in Table 2 (Section 6.3)."
    },
    {
      "name": "Implement LC-RL pipeline to learn language-conditioned reward functions from demonstrations before deployment",
      "aliases": [
        "apply LC-RL training",
        "pre-deploy LC-RL"
      ],
      "type": "intervention",
      "description": "Collect demonstrations with language commands in mapped environments; train a shared CNN-LSTM reward network via exact MaxEnt IRL; deploy the reward in novel settings and learn policies on-the-fly.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Training and fine-tuning of models prior to deployment corresponds to Fine-Tuning/RL phase (Section 5 Algorithm 1).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated only in simulation with proof-of-concept experiments (Section 6 Evaluation).",
      "node_rationale": "Primary actionable step derived from validation evidence."
    },
    {
      "name": "Use potential-based reward shaping when reoptimizing learned rewards",
      "aliases": [
        "apply value-function shaping",
        "dense shaping term"
      ],
      "type": "intervention",
      "description": "During RL in new environments, augment the learned reward with a potential Φ(s)=V*(s) to supply dense feedback and improve exploration.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Applied during policy learning on top of learned reward (Section 6.2, DQN with shaping).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Evaluated experimentally but not yet validated at scale (Table 2).",
      "node_rationale": "Suggested by empirical improvement with shaping."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "incorrect task execution in novel environments by language-conditioned agents",
      "target_node": "coupled language understanding and control learning in language-conditioned policy training",
      "description": "Jointly learning language interpretation and control makes policies brittle to environment changes, leading to failures.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoning stated in Section 1; no quantitative proof.",
      "edge_rationale": "Links observed risk to identified mechanism."
    },
    {
      "type": "caused_by",
      "source_node": "incorrect task execution in novel environments by language-conditioned agents",
      "target_node": "manual reward specification difficulty in vision-based tasks",
      "description": "When designers cannot hand-craft precise rewards, agents may act unpredictably in new settings.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Argument made qualitatively in Introduction.",
      "edge_rationale": "Connects risk to second problem analysis."
    },
    {
      "type": "caused_by",
      "source_node": "incorrect task execution in novel environments by language-conditioned agents",
      "target_node": "sparse reward signal in RL re-optimization of learned rewards",
      "description": "Even with a learned reward, sparse feedback can prevent agents from discovering correct behaviours, resulting in task failure.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Based on experimental observation; moderate inference.",
      "edge_rationale": "Explains another pathway to risk."
    },
    {
      "type": "mitigated_by",
      "source_node": "coupled language understanding and control learning in language-conditioned policy training",
      "target_node": "decoupling language and control via reward function grounding",
      "description": "Separating goal interpretation from control removes the need for zero-shot policy transfer.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit claim in Section 1 with supporting empirical results.",
      "edge_rationale": "Theoretical solution to identified problem."
    },
    {
      "type": "addressed_by",
      "source_node": "manual reward specification difficulty in vision-based tasks",
      "target_node": "inverse reinforcement learning from demonstrations infers reward functions",
      "description": "IRL learns the reward automatically from examples, eliminating manual design.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Standard IRL rationale described in Section 3.",
      "edge_rationale": "Shows how theoretical insight tackles the problem."
    },
    {
      "type": "mitigated_by",
      "source_node": "sparse reward signal in RL re-optimization of learned rewards",
      "target_node": "potential-based reward shaping can aid exploration in sparse reward tasks",
      "description": "Shaping supplies additional dense feedback, easing exploration difficulties.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Supported by experimental improvement in Table 2.",
      "edge_rationale": "Provides theoretical mitigation."
    },
    {
      "type": "implemented_by",
      "source_node": "decoupling language and control via reward function grounding",
      "target_node": "multi-task language-conditioned IRL for reward generalization",
      "description": "LC-RL realises the decoupling by learning a reward network shared across tasks.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Design articulated in Sections 4-5.",
      "edge_rationale": "Moves from theory to concrete approach."
    },
    {
      "type": "implemented_by",
      "source_node": "inverse reinforcement learning from demonstrations infers reward functions",
      "target_node": "multi-task language-conditioned IRL for reward generalization",
      "description": "LC-RL applies MaxEnt IRL across many tasks to infer rewards from demonstrations.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Method section provides details.",
      "edge_rationale": "Same theoretical idea instantiated."
    },
    {
      "type": "refined_by",
      "source_node": "multi-task language-conditioned IRL for reward generalization",
      "target_node": "training with known dynamics enables exact IRL optimization",
      "description": "Restricting training environments to known dynamics refines the LC-RL design to be computationally tractable.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicitly argued in Section 5.1.",
      "edge_rationale": "Design refinement relationship."
    },
    {
      "type": "implemented_by",
      "source_node": "training with known dynamics enables exact IRL optimization",
      "target_node": "exact MaxEnt IRL with dynamic programming occupancy computation",
      "description": "Exact Q-iteration and forward algorithms are used within known dynamics to compute gradients.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Algorithm 1 line 5 shows implementation.",
      "edge_rationale": "Maps rationale to mechanism."
    },
    {
      "type": "implemented_by",
      "source_node": "multi-task language-conditioned IRL for reward generalization",
      "target_node": "language-conditioned CNN-LSTM reward network processing panoramic semantic images",
      "description": "The shared reward is parameterised by a CNN-LSTM architecture consuming images and language.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 5.2 details architecture.",
      "edge_rationale": "Concrete mechanism for design."
    },
    {
      "type": "implemented_by",
      "source_node": "multi-task language-conditioned IRL for reward generalization",
      "target_node": "stochastic multi-task gradient updates of reward network",
      "description": "Sampling tasks and demonstrations provides stochastic gradients for shared parameters.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Algorithm 1 lines 4-6.",
      "edge_rationale": "Training procedure mechanism."
    },
    {
      "type": "implemented_by",
      "source_node": "potential-based reward shaping can aid exploration in sparse reward tasks",
      "target_node": "use optimal value function potentials for shaping",
      "description": "Selecting the optimal value function as potential realises the shaping insight without altering optimal policies.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Design choice explained in Section 6.2.",
      "edge_rationale": "Mechanises theoretical insight."
    },
    {
      "type": "implemented_by",
      "source_node": "use optimal value function potentials for shaping",
      "target_node": "potential-based shaping using optimal value function",
      "description": "The potential is computed offline from known dynamics and added during DQN training.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Implementation described in Experimental Procedure.",
      "edge_rationale": "Connects design choice to concrete computation."
    },
    {
      "type": "validated_by",
      "source_node": "exact MaxEnt IRL with dynamic programming occupancy computation",
      "target_node": "LC-RL outperforms policy baselines on test tasks and houses",
      "description": "Performance gains evidence the effectiveness of exact IRL inner loop.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Empirical results Table 1.",
      "edge_rationale": "Implementation success shown by data."
    },
    {
      "type": "validated_by",
      "source_node": "language-conditioned CNN-LSTM reward network processing panoramic semantic images",
      "target_node": "LC-RL outperforms policy baselines on test tasks and houses",
      "description": "Architecture contributes to strong test performance.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Architecture ablation discussed Section 5.2.",
      "edge_rationale": "Link between mechanism and evidence."
    },
    {
      "type": "validated_by",
      "source_node": "stochastic multi-task gradient updates of reward network",
      "target_node": "LC-RL outperforms policy baselines on test tasks and houses",
      "description": "Multi-task SGD provides generalisation evidenced by success rates.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Not isolated experimentally but integral to method.",
      "edge_rationale": "Credible contribution to validation."
    },
    {
      "type": "validated_by",
      "source_node": "multi-task language-conditioned IRL for reward generalization",
      "target_node": "learned reward functions generalize across unseen environments",
      "description": "Shared reward learning directly leads to cross-scene generalisation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Supported by Test-House performance Table 1.",
      "edge_rationale": "Direct validation of design goal."
    },
    {
      "type": "validated_by",
      "source_node": "potential-based shaping using optimal value function",
      "target_node": "value-function shaping doubles DQN reoptimization success rates",
      "description": "Shaping implementation improves DQN success from 7-8 % to ~15 %.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Data in Table 2.",
      "edge_rationale": "Shows mechanism’s benefit."
    },
    {
      "type": "motivates",
      "source_node": "LC-RL outperforms policy baselines on test tasks and houses",
      "target_node": "Implement LC-RL pipeline to learn language-conditioned reward functions from demonstrations before deployment",
      "description": "Empirical superiority encourages adoption of LC-RL in practice.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Clear quantitative advantage Table 1.",
      "edge_rationale": "Evidence drives intervention."
    },
    {
      "type": "motivates",
      "source_node": "learned reward functions generalize across unseen environments",
      "target_node": "Implement LC-RL pipeline to learn language-conditioned reward functions from demonstrations before deployment",
      "description": "Generalisation property makes the pipeline suitable for deployment in unfamiliar settings.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Test-House success demonstrates generalisation.",
      "edge_rationale": "Evidence to intervention."
    },
    {
      "type": "motivates",
      "source_node": "value-function shaping doubles DQN reoptimization success rates",
      "target_node": "Use potential-based reward shaping when reoptimizing learned rewards",
      "description": "Observed improvement suggests adopting shaping to enhance policy learning efficiency.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Quantitative evidence in Table 2.",
      "edge_rationale": "Validation leads to actionable step."
    }
  ],
  "meta": [
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "date_published",
      "value": "2019-02-20T00:00:00Z"
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1902.07742"
    },
    {
      "key": "authors",
      "value": [
        "Justin Fu",
        "Anoop Korattikara",
        "Sergey Levine",
        "Sergio Guadarrama"
      ]
    },
    {
      "key": "title",
      "value": "From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following"
    }
  ]
}