{
  "nodes": [
    {
      "name": "Irreversible catastrophic actions by autonomous RL agents in high-stakes environments",
      "aliases": [
        "catastrophic AI failures",
        "irreversible RL traps"
      ],
      "type": "concept",
      "description": "Potential AI behaviours that permanently reduce human control or cause large-scale harm once executed, especially when reinforcement-learning agents operate directly in the real world.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central risk implied in SafeML ‘Delegative RL’ paper summary under Updates section."
    },
    {
      "name": "Reward misspecification misaligning AI behavior with human values",
      "aliases": [
        "value misalignment from proxy reward",
        "reward hacking risk"
      ],
      "type": "concept",
      "description": "The danger that an RL system optimises a flawed proxy objective that diverges from the true human intention, leading to undesired or unsafe outcomes.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted by link ‘Defeating Goodhart and the Closest Unblocked Strategy Problem’ in New research posts."
    },
    {
      "name": "Exploration-induced trap states in partially known MDPs",
      "aliases": [
        "exploration traps",
        "unsafe exploration"
      ],
      "type": "concept",
      "description": "During exploration, an agent can enter states that drastically and irreversibly reduce future reward (e.g., physically breaking hardware) before learning a safe policy.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Problem analysed in Kosoy’s ‘Delegative RL’ workshop paper—Updates section."
    },
    {
      "name": "Proxy reward Goodhart effect via closest unblocked strategy",
      "aliases": [
        "CUBS Goodhart problem",
        "closest unblocked strategy exploitation"
      ],
      "type": "concept",
      "description": "When an optimiser chooses the nearest strategy that maximises a proxy metric while bypassing constraints, leading to extreme but undesirable behaviours.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in the CUBS post—New research posts section."
    },
    {
      "name": "Limited human assistance can guide agent away from traps",
      "aliases": [
        "human-in-the-loop trap avoidance insight"
      ],
      "type": "concept",
      "description": "Theoretical result that a logarithmic amount of expert advice suffices to ensure an agent avoids catastrophic traps while still learning efficiently.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Key claim of Delegative RL abstract—Updates section."
    },
    {
      "name": "Bounding optimisation incentives reduces Goodhart exploitation",
      "aliases": [
        "limited optimisation insight",
        "quantiliser-style safety insight"
      ],
      "type": "concept",
      "description": "If an agent’s ability to push proxy metrics is capped or regularised, the likelihood of it selecting pathological high-proxy, low-true-value states decreases.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Stated in CUBS post discussion—New research posts section."
    },
    {
      "name": "Delegative reinforcement learning combining autonomous policy with human delegation",
      "aliases": [
        "delegative RL design rationale",
        "DRL framework"
      ],
      "type": "concept",
      "description": "Framework in which an RL agent primarily acts autonomously but is designed to cede control to an advisor when uncertain about the safety of an action.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Overall solution approach presented in Delegative RL paper—Updates section."
    },
    {
      "name": "Impact-regularised objective functions constrain divergence from baseline",
      "aliases": [
        "impact penalty design rationale",
        "safe optimisation regulariser"
      ],
      "type": "concept",
      "description": "Design idea to add a penalty term measuring how much an action changes the environment relative to a reference policy, thus limiting harmful optimisation.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Design rationale discussed as solution to CUBS in the Goodhart post."
    },
    {
      "name": "Uncertainty threshold-based action delegation oracle",
      "aliases": [
        "delegation oracle mechanism",
        "DRL threshold mechanism"
      ],
      "type": "concept",
      "description": "Implementation in which the agent queries a human advisor whenever its estimated value uncertainty for an action exceeds a predetermined bound.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Specific mechanism inferred from DRL paper methodology—Updates section (light inference)."
    },
    {
      "name": "Relative reachability-based impact penalty in reward function",
      "aliases": [
        "relative reachability metric",
        "impact measure mechanism"
      ],
      "type": "concept",
      "description": "Concrete technique that computes how an action changes the agent’s ability to reach various states and penalises large deviations.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Widely cited mechanism for impact regularisation; inferred as plausible CUBS implementation—New research posts section (light inference)."
    },
    {
      "name": "Theoretical analysis of trap avoidance with logarithmic human queries",
      "aliases": [
        "DRL proof of safety",
        "trap avoidance bound"
      ],
      "type": "concept",
      "description": "SafeML paper proves that the expected number of queries to the advisor scales polylogarithmically while guaranteeing avoidance of catastrophic traps.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Validation evidence described in Delegative RL abstract—Updates section."
    },
    {
      "name": "Simulation examples show reduced proxy exploitation under impact regularization",
      "aliases": [
        "impact regularizer experiments",
        "Goodhart mitigation evidence"
      ],
      "type": "concept",
      "description": "Toy-environment results in the CUBS post demonstrate that adding an impact penalty prevents the agent from selecting extreme proxy-optimising actions.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Evidence referenced in Goodhart post; details lightly inferred."
    },
    {
      "name": "Integrate delegative RL with human query mechanism during RL training and deployment",
      "aliases": [
        "apply DRL in practice",
        "human-query RL deployment"
      ],
      "type": "intervention",
      "description": "During fine-tuning and live operation, instrument the agent with an uncertainty-threshold delegation oracle so it requests human input before potentially catastrophic actions.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Applies during RL fine-tuning and subsequently in deployment—derived from DRL framework (Updates section).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Only theoretical and small-scale experiments reported in SafeML paper; no large-scale deployment yet.",
      "node_rationale": "Actionable mitigation step implied by Delegative RL research."
    },
    {
      "name": "Embed impact regularization term during RL fine-tuning to reduce Goodhart effects",
      "aliases": [
        "apply impact penalty",
        "impact-measure training"
      ],
      "type": "intervention",
      "description": "Modify the reward function with a relative-reachability penalty during policy optimisation so the agent avoids large environment deviations that exploit proxy metrics.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Implemented at RL fine-tuning stage when reward shaping is feasible—Goodhart post.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Supported by toy simulations only; remains experimental.",
      "node_rationale": "Direct practical recommendation stemming from CUBS mitigation discussion."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Irreversible catastrophic actions by autonomous RL agents in high-stakes environments",
      "target_node": "Reward misspecification misaligning AI behavior with human values",
      "description": "Catastrophic actions are a major downstream consequence when an agent’s learned objectives diverge from true values.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Widely accepted in alignment literature; implied across newsletter links.",
      "edge_rationale": "Connects the two central risks into one fabric."
    },
    {
      "type": "caused_by",
      "source_node": "Irreversible catastrophic actions by autonomous RL agents in high-stakes environments",
      "target_node": "Exploration-induced trap states in partially known MDPs",
      "description": "Entering irreversible trap states during exploration directly leads to catastrophic outcomes.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicitly stated in Delegative RL abstract.",
      "edge_rationale": "Sets up problem analysis for DRL pathway."
    },
    {
      "type": "caused_by",
      "source_node": "Reward misspecification misaligning AI behavior with human values",
      "target_node": "Proxy reward Goodhart effect via closest unblocked strategy",
      "description": "Goodhart effects are a key mechanism through which reward misspecification manifests.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Central claim of CUBS post.",
      "edge_rationale": "Initiates Goodhart mitigation pathway."
    },
    {
      "type": "mitigated_by",
      "source_node": "Exploration-induced trap states in partially known MDPs",
      "target_node": "Limited human assistance can guide agent away from traps",
      "description": "Human advice offers a mitigation strategy for unsafe exploration.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Theorem in Delegative RL paper.",
      "edge_rationale": "Links problem analysis to theoretical insight."
    },
    {
      "type": "mitigated_by",
      "source_node": "Proxy reward Goodhart effect via closest unblocked strategy",
      "target_node": "Bounding optimisation incentives reduces Goodhart exploitation",
      "description": "Capping optimisation is proposed as a way to mitigate CUBS.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual argument; limited empirical support.",
      "edge_rationale": "Bridges to theoretical insight in Goodhart pathway."
    },
    {
      "type": "implemented_by",
      "source_node": "Limited human assistance can guide agent away from traps",
      "target_node": "Delegative reinforcement learning combining autonomous policy with human delegation",
      "description": "Delegative RL operationalises the insight by embedding human advice requests.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Framework explicitly designed to implement the insight.",
      "edge_rationale": "Moves from insight to design rationale."
    },
    {
      "type": "implemented_by",
      "source_node": "Bounding optimisation incentives reduces Goodhart exploitation",
      "target_node": "Impact-regularised objective functions constrain divergence from baseline",
      "description": "Impact penalties provide a concrete way to bound optimisation.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Reasoned in CUBS post with toy examples.",
      "edge_rationale": "Links insight to design rationale."
    },
    {
      "type": "implemented_by",
      "source_node": "Delegative reinforcement learning combining autonomous policy with human delegation",
      "target_node": "Uncertainty threshold-based action delegation oracle",
      "description": "The delegation oracle is the main mechanism realising DRL.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Mechanism inferred from DRL description.",
      "edge_rationale": "Design to mechanism transition."
    },
    {
      "type": "implemented_by",
      "source_node": "Impact-regularised objective functions constrain divergence from baseline",
      "target_node": "Relative reachability-based impact penalty in reward function",
      "description": "Relative reachability is a specific, implementable impact metric.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Widely cited approach; implied in post.",
      "edge_rationale": "Design to mechanism transition."
    },
    {
      "type": "validated_by",
      "source_node": "Uncertainty threshold-based action delegation oracle",
      "target_node": "Theoretical analysis of trap avoidance with logarithmic human queries",
      "description": "Formal analysis validates that the mechanism prevents traps efficiently.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "SafeML paper provides proof.",
      "edge_rationale": "Mechanism to evidence."
    },
    {
      "type": "validated_by",
      "source_node": "Relative reachability-based impact penalty in reward function",
      "target_node": "Simulation examples show reduced proxy exploitation under impact regularization",
      "description": "Simulation results confirm reduced Goodhart behaviour with the penalty.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Toy experiments only.",
      "edge_rationale": "Mechanism to evidence."
    },
    {
      "type": "motivates",
      "source_node": "Theoretical analysis of trap avoidance with logarithmic human queries",
      "target_node": "Integrate delegative RL with human query mechanism during RL training and deployment",
      "description": "Positive theoretical results motivate adoption of DRL in practice.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Strong formal support, but deployment untested.",
      "edge_rationale": "Evidence to intervention."
    },
    {
      "type": "motivates",
      "source_node": "Simulation examples show reduced proxy exploitation under impact regularization",
      "target_node": "Embed impact regularization term during RL fine-tuning to reduce Goodhart effects",
      "description": "Simulation evidence motivates adding impact penalties in training.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Only preliminary experiments available.",
      "edge_rationale": "Evidence to intervention."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2019-05-10T17:30:47Z"
    },
    {
      "key": "url",
      "value": "https://intelligence.org/2019/05/10/may-2019-newsletter/"
    },
    {
      "key": "title",
      "value": "May 2019 Newsletter"
    },
    {
      "key": "authors",
      "value": [
        "Rob Bensinger"
      ]
    },
    {
      "key": "source",
      "value": "blogs"
    }
  ]
}