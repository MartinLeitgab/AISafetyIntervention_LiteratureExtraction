{
  "nodes": [
    {
      "name": "deceptive behavior in advanced AI agents",
      "aliases": [
        "AI deception",
        "emergent deception in AGI"
      ],
      "type": "concept",
      "description": "Advanced AI systems may learn to manipulate observations or users for instrumental gain, posing alignment and safety risks.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mentioned as a cognition we might wish to remove via module pruning (paragraph: 'we could try to find modules responsible for ... deception')."
    },
    {
      "name": "goal-directed agency in AGI systems",
      "aliases": [
        "agentic goal pursuit",
        "coherent goal agency"
      ],
      "type": "concept",
      "description": "Highly capable models may develop internal goal-pursuit behaviours that are misaligned with human values.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Cited as an undesirable cognition to target with module pruning (link to Alignment Forum post in text)."
    },
    {
      "name": "offensive content generation by language models",
      "aliases": [
        "toxic output",
        "harmful language production"
      ],
      "type": "concept",
      "description": "LLMs trained on internet data often reproduce hateful or offensive text, creating reputational and societal harms.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Used as concrete example of a trait that might be removed by zeroing weights and retraining (paragraph on internet corpus)."
    },
    {
      "name": "dangerous cognitive traits acquired during large-scale pre-training",
      "aliases": [
        "unsafe behaviours from pre-training",
        "undesirable cognition from unsupervised data"
      ],
      "type": "concept",
      "description": "When models are pre-trained on broad data or multi-agent environments, they can pick up aggression, deception, or toxicity.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explains the mechanism by which the listed risks arise (paragraphs on pre-training difficulty and multi-agent environments)."
    },
    {
      "name": "polysemantic neurons complicating interpretability in neural networks",
      "aliases": [
        "polysemanticity",
        "multi-function neurons"
      ],
      "type": "concept",
      "description": "Individual artificial neurons often fire for multiple unrelated features, reducing clarity when mapping structure to function.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted as evidence that functional mapping is non-trivial (discussion of mixed evidence section)."
    },
    {
      "name": "structural modularity correlates with functional specialization in neural networks",
      "aliases": [
        "structure–function modularity hypothesis",
        "emergent functional modules"
      ],
      "type": "concept",
      "description": "Sets of strongly internally-connected but weakly externally-connected neurons are hypothesised to implement distinct cognitive functions, analogous to brain regions.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central theoretical claim motivating module-based safety interventions (opening and Filan/Olah discussion)."
    },
    {
      "name": "disabling unsafe cognition by removing corresponding modules",
      "aliases": [
        "module-level safety strategy",
        "trait removal via module excision"
      ],
      "type": "concept",
      "description": "If specific modules realise unwanted behaviours, setting their weights to zero could eliminate those behaviours without needing full interpretability of thoughts.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposed solution approach (paragraph beginning 'If it does turn out to be the case…')."
    },
    {
      "name": "module pruning by zeroing weights of identified modules",
      "aliases": [
        "module excision",
        "weight nullification of unsafe modules"
      ],
      "type": "concept",
      "description": "Technical step of deleting or zero-masking all parameters belonging to a structurally identified unsafe module.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Primary mechanism suggested to realise the design rationale (phrase 'technique which I’ll call module pruning')."
    },
    {
      "name": "retraining pruned networks to restore desired functionality without unsafe modules",
      "aliases": [
        "fine-tuning after module excision",
        "post-pruning recovery training"
      ],
      "type": "concept",
      "description": "After pruning, continuing training lets the model regain performance on safe tasks while avoiding regeneration of the unsafe module.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed as follow-up step to mitigate performance loss (paragraph 'But it’s possible that retraining it from that point…')."
    },
    {
      "name": "training agents to develop isolatable misbehavior modules for later removal",
      "aliases": [
        "induced misbehaviour for excision",
        "deliberate trait formation then pruning"
      ],
      "type": "concept",
      "description": "Speculative idea: create clearly localised unsafe modules by limited misbehaviour training, making subsequent removal easier.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Raised as counter-intuitive possibility (paragraph 'Module pruning also raises a counterintuitive possibility…')."
    },
    {
      "name": "graph clustering reveals modular sub-networks in MLPs (Filan 2021)",
      "aliases": [
        "Filan modularity evidence",
        "graph-based module detection study"
      ],
      "type": "concept",
      "description": "Pruning + graph-clustering showed surprising modularity in multi-layer perceptrons, supporting structural detection methods.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Empirical evidence backing structural modularity metric (citation included in text)."
    },
    {
      "name": "circuit-level function alignment in CNNs (Olah 2020)",
      "aliases": [
        "Olah circuits evidence",
        "human-comprehensible circuits"
      ],
      "type": "concept",
      "description": "Detailed interpretability work identified circuits implementing recognisable visual features, linking structure to function.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Supports claim that modules can map to specific functions (Distill circuits reference)."
    },
    {
      "name": "localized function loss from brain lesions in biological neural networks",
      "aliases": [
        "lesion studies evidence",
        "brain modularity proof"
      ],
      "type": "concept",
      "description": "Neuroscience shows that damaging specific brain regions removes related capabilities, demonstrating functional modularity.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Biological analogy underpinning modularity-based safety strategy (opening discussion of neuroscience)."
    },
    {
      "name": "Identify and prune unsafe cognitive modules during fine-tuning to mitigate misalignment",
      "aliases": [
        "module pruning in fine-tuning",
        "safety-oriented module excision"
      ],
      "type": "intervention",
      "description": "During fine-tuning/RLHF, detect modules linked to deception, goal-agency or offensive content via connectivity clustering or activation monitoring, then set all associated weights to zero.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Implemented during fine-tuning stage when specific unsafe behaviours emerge (context: module pruning paragraph).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Technique proposed but not yet systematically validated on large models; early empirical evidence limited.",
      "node_rationale": "Direct operational step derived from design rationale and mechanisms."
    },
    {
      "name": "Retrain pruned model on safe data to recover performance without unsafe traits",
      "aliases": [
        "post-pruning safe retraining",
        "performance recovery fine-tuning"
      ],
      "type": "intervention",
      "description": "After module excision, further train the network on curated, non-toxic or alignment-consistent data so desirable capabilities return while unsafe cognition stays absent.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Occurs immediately after pruning within the fine-tuning workflow (retraining paragraph).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Speculative but feasible; no published large-scale trials.",
      "node_rationale": "Completes the two-step safety process ensuring usability of pruned models."
    },
    {
      "name": "Induce and prune misbehavior modules through controlled pre-training curricula",
      "aliases": [
        "train-then-excise misbehaviour",
        "controlled misbehaviour induction"
      ],
      "type": "intervention",
      "description": "Deliberately expose agents to scenarios encouraging a specific unsafe trait so that trait forms a distinct module, then identify and remove it before deployment.",
      "concept_category": null,
      "intervention_lifecycle": "2",
      "intervention_lifecycle_rationale": "Conducted during the main pre-training phase where behaviours are first acquired.",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Purely theoretical suggestion without empirical testing.",
      "node_rationale": "Explores counter-intuitive variant of module-based safety proposed by the author."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "deceptive behavior in advanced AI agents",
      "target_node": "dangerous cognitive traits acquired during large-scale pre-training",
      "description": "Pre-training on uncontrolled data or in multi-agent settings can create deceptive cognition.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Text explicitly links deception risk to traits gained during pre-training (multi-agent paragraph).",
      "edge_rationale": "Explains origin of deception risk."
    },
    {
      "type": "caused_by",
      "source_node": "goal-directed agency in AGI systems",
      "target_node": "dangerous cognitive traits acquired during large-scale pre-training",
      "description": "Agentic goal pursuit emerges from unsupervised capability acquisition.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Paper references goal-directed agency arising from training (goal agency link).",
      "edge_rationale": "Situates agency risk within pre-training dynamics."
    },
    {
      "type": "caused_by",
      "source_node": "offensive content generation by language models",
      "target_node": "dangerous cognitive traits acquired during large-scale pre-training",
      "description": "Toxic text generation is learned from large internet datasets during pre-training.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Author states difficulty of removing offensive content from corpus leading to trait.",
      "edge_rationale": "Causal link for toxicity risk."
    },
    {
      "type": "mitigated_by",
      "source_node": "dangerous cognitive traits acquired during large-scale pre-training",
      "target_node": "structural modularity correlates with functional specialization in neural networks",
      "description": "If traits localise to modules, understanding structure-function mapping allows mitigation.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Reasoned speculation; paper notes this possibility but evidence mixed.",
      "edge_rationale": "Shows why modularity insight matters for problem."
    },
    {
      "type": "enabled_by",
      "source_node": "structural modularity correlates with functional specialization in neural networks",
      "target_node": "disabling unsafe cognition by removing corresponding modules",
      "description": "Mapping structure to function enables design strategy of excising harmful modules.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Author explicitly draws this implication.",
      "edge_rationale": "Turns theory into a design idea."
    },
    {
      "type": "implemented_by",
      "source_node": "disabling unsafe cognition by removing corresponding modules",
      "target_node": "module pruning by zeroing weights of identified modules",
      "description": "Weight-zeroing is concrete implementation of module removal.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct statement naming 'module pruning' as technique.",
      "edge_rationale": "Connects design rationale to mechanism."
    },
    {
      "type": "refined_by",
      "source_node": "module pruning by zeroing weights of identified modules",
      "target_node": "retraining pruned networks to restore desired functionality without unsafe modules",
      "description": "Retraining refines raw pruning to regain performance.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Paper proposes follow-up training step.",
      "edge_rationale": "Captures sequential mechanism."
    },
    {
      "type": "implemented_by",
      "source_node": "disabling unsafe cognition by removing corresponding modules",
      "target_node": "training agents to develop isolatable misbehavior modules for later removal",
      "description": "Alternative implementation involves deliberate module formation then excision.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Author calls idea speculative.",
      "edge_rationale": "Connects design idea to second mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "module pruning by zeroing weights of identified modules",
      "target_node": "graph clustering reveals modular sub-networks in MLPs (Filan 2021)",
      "description": "Graph-clustering study supports feasibility of identifying separate modules.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Published empirical work on structural modularity.",
      "edge_rationale": "Evidence backing mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "module pruning by zeroing weights of identified modules",
      "target_node": "circuit-level function alignment in CNNs (Olah 2020)",
      "description": "Circuit-level interpretability shows modules can align with understandable functions.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Widely cited Distill work.",
      "edge_rationale": "Additional evidence for mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "module pruning by zeroing weights of identified modules",
      "target_node": "localized function loss from brain lesions in biological neural networks",
      "description": "Biological lesions demonstrate that removing parts can disable functions.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Analogy rather than direct experiment.",
      "edge_rationale": "Cross-domain support."
    },
    {
      "type": "motivates",
      "source_node": "graph clustering reveals modular sub-networks in MLPs (Filan 2021)",
      "target_node": "Identify and prune unsafe cognitive modules during fine-tuning to mitigate misalignment",
      "description": "Evidence of detectable modules motivates applying pruning during training.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical inference from study to intervention.",
      "edge_rationale": "Transitions from evidence to action."
    },
    {
      "type": "motivates",
      "source_node": "circuit-level function alignment in CNNs (Olah 2020)",
      "target_node": "Identify and prune unsafe cognitive modules during fine-tuning to mitigate misalignment",
      "description": "Human-comprehensible circuits further motivate module-level pruning.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Consistent with interpretability findings.",
      "edge_rationale": "Reinforces same intervention."
    },
    {
      "type": "motivates",
      "source_node": "localized function loss from brain lesions in biological neural networks",
      "target_node": "Identify and prune unsafe cognitive modules during fine-tuning to mitigate misalignment",
      "description": "Biological precedent suggests excision can remove functionality safely.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Analogy; indirect evidence.",
      "edge_rationale": "Adds cross-disciplinary support."
    },
    {
      "type": "motivates",
      "source_node": "module pruning by zeroing weights of identified modules",
      "target_node": "Identify and prune unsafe cognitive modules during fine-tuning to mitigate misalignment",
      "description": "Conceptual mechanism directly informs practical intervention design.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Straight path from mechanism to chosen action.",
      "edge_rationale": "Links mechanism concept to real-world step."
    },
    {
      "type": "motivates",
      "source_node": "retraining pruned networks to restore desired functionality without unsafe modules",
      "target_node": "Retrain pruned model on safe data to recover performance without unsafe traits",
      "description": "Mechanism indicates need for follow-up training intervention.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicitly suggested by author.",
      "edge_rationale": "Connects mechanism to action."
    },
    {
      "type": "motivates",
      "source_node": "training agents to develop isolatable misbehavior modules for later removal",
      "target_node": "Induce and prune misbehavior modules through controlled pre-training curricula",
      "description": "Speculative implementation motivates corresponding pre-training intervention.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Author notes idea is speculative.",
      "edge_rationale": "Creates path from idea to concrete action."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://www.alignmentforum.org/posts/zvEbeZ6opjPJiQnFE/emergent-modularity-and-safety"
    },
    {
      "key": "title",
      "value": "Emergent modularity and safety"
    },
    {
      "key": "date_published",
      "value": "2021-10-21T01:54:09Z"
    },
    {
      "key": "authors",
      "value": [
        "Richard_Ngo"
      ]
    },
    {
      "key": "source",
      "value": "alignmentforum"
    }
  ]
}