Summary
Data source overview: The paper “RvS: What is Essential for Offline RL via Supervised Learning?” empirically studies “reinforcement-learning-via-supervised-learning” (RvS) as a simple alternative to value-based offline RL. It shows that feed-forward conditional behavior-cloning policies, if given sufficient capacity, light regularisation and an appropriate conditioning variable, match or exceed state-of-the-art value-based methods on diverse D4RL benchmarks.

Inference strategy justification: The paper is not written from an AI-safety perspective, so light inference (edge confidence = 2) was applied to frame the performance/reliability issues of complex offline RL algorithms as safety-relevant risks. All other edges are grounded in explicit empirical results (confidence ≥ 3).

Extraction completeness: 23 nodes and 33 edges cover the full causal chain from the top-level risk (unsafe, unstable offline RL) through problem analyses (hyper-parameter sensitivity, need for compositionality, unclear RvS components), theoretical insights (simple supervised learning suffices, capacity-regularisation trade-off, importance of conditioning variable), design rationales, implementation mechanisms (RvS-G, RvS-R, categorical outputs, dropout), validation evidence, and five actionable interventions. All nodes inter-connect; no satellites remain.

Key limitations: 1) Results are limited to simulated robotics/locomotion tasks, so external validity is uncertain; 2) hyper-parameter tuning remains manual, so the proposed automated-selection intervention is speculative (maturity = 1); 3) the safety framing relies on inferred rather than explicit statements.

EXTRACTION CONFIDENCE[86]

Instruction-set improvement: Provide explicit guidance on how to frame performance/robustness papers (that do not mention safety) as AI-safety-relevant risks to reduce inference ambiguity; add archetypal examples.

JSON knowledge fabric