{
  "nodes": [
    {
      "name": "Factual hallucinations in language model outputs",
      "aliases": [
        "hallucinated facts in LLMs",
        "incorrect continuations from language models"
      ],
      "type": "concept",
      "description": "Language models frequently generate statements that are not supported by real-world facts, leading to misinformation and unsafe outcomes.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted by comparison samples where baseline model produces wrong digits (Figure 3)."
    },
    {
      "name": "Excessive training energy consumption in large dense transformers",
      "aliases": [
        "high compute cost of scaling parameters",
        "environmental cost of 100B+ parameter LLMs"
      ],
      "type": "concept",
      "description": "Growing model size to hundreds of billions of parameters drives substantial energy use and CO₂ emissions.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mentioned in introduction: increasing parameters raises training energy cost."
    },
    {
      "name": "Low interpretability of transformer predictions",
      "aliases": [
        "opaque LLM reasoning",
        "lack of provenance in outputs"
      ],
      "type": "concept",
      "description": "Standard dense Transformers offer little transparency into why particular tokens are produced, hindering safety auditing.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Paper claims RETRO 'increases the interpretability of model predictions'."
    },
    {
      "name": "Limited context window and parametric memorization in dense transformers",
      "aliases": [
        "finite attention context",
        "over-reliance on internal parameters"
      ],
      "type": "concept",
      "description": "Dense models must store most knowledge in weights and can only attend to a limited sequence, fostering hallucinations.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explains why factual errors arise when relevant information lies outside training parameters."
    },
    {
      "name": "Parameter-proportional compute and memory demands in dense transformers",
      "aliases": [
        "quadratic scaling cost",
        "compute tied to parameter count"
      ],
      "type": "concept",
      "description": "Model accuracy traditionally improves only by adding parameters, directly driving energy use.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Paper contrasts RETRO with costly 175-280B parameter baselines."
    },
    {
      "name": "Opaque latent reasoning without provenance in dense transformers",
      "aliases": [
        "hidden activations not human-readable"
      ],
      "type": "concept",
      "description": "Predictions arise from distributed internal states, offering no explicit references users can inspect.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Sets stage for interpretability benefits of retrieval citations."
    },
    {
      "name": "External retrieval provides accurate contextual knowledge beyond model parameters",
      "aliases": [
        "database look-up supplies facts",
        "retrieval augments context"
      ],
      "type": "concept",
      "description": "Fetching nearest-neighbour passages can supply verifiable information the model did not store internally.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Described in Figure 1 narrative: model not limited to data seen during training."
    },
    {
      "name": "Retrieval-based performance scaling decouples accuracy from parameter count",
      "aliases": [
        "performance grows with database size not parameters"
      ],
      "type": "concept",
      "description": "By leveraging a growing retrieval corpus, models can improve without proportional parameter growth, reducing compute.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Figure 2 shows gains as retrieval database scales to 2T tokens."
    },
    {
      "name": "Retrieved citations enhance transparency of generated tokens",
      "aliases": [
        "retrieval gives provenance",
        "evidence-linked outputs"
      ],
      "type": "concept",
      "description": "Displaying retrieved passages alongside generated text makes model reasoning more interpretable.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Paper states RETRO 'increases the interpretability of model predictions'."
    },
    {
      "name": "Integrate nearest-neighbor retrieval with transformer language modeling",
      "aliases": [
        "retrieval-augmented LM design",
        "combining search and attention"
      ],
      "type": "concept",
      "description": "Architectural design where a Transformer alternates regular self-attention with cross-attention to retrieved neighbours.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Described in high-level overview and Figure 1."
    },
    {
      "name": "Nearest-neighbor search over chunked text passage database in RETRO",
      "aliases": [
        "ANN retrieval over passages",
        "search index for RETRO"
      ],
      "type": "concept",
      "description": "Each input chunk queries a large database to retrieve similar sequences and their continuations.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mechanism providing external context, referenced in paragraph describing retrieval."
    },
    {
      "name": "Cross-attention fusion of retrieved neighbors with self-attention in RETRO",
      "aliases": [
        "interleaved cross-attention",
        "fusion of retrieval and context"
      ],
      "type": "concept",
      "description": "Model layers combine information from internal context and retrieved passages to predict next tokens.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation detail described in architecture overview."
    },
    {
      "name": "7.5B RETRO surpasses 175B dense models on Pile benchmark accuracy",
      "aliases": [
        "RETRO accuracy evidence",
        "benchmark wins over Jurassic-1"
      ],
      "type": "concept",
      "description": "Empirical result: 7.5 B RETRO beats 175 B Jurassic-1 on 10/16 datasets and 280 B Gopher on 9/16.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Core quantitative validation of architecture, Figure 2 and text."
    },
    {
      "name": "RETRO achieves competitive performance with >20× fewer parameters reducing compute",
      "aliases": [
        "compute efficiency evidence",
        "energy saving validation"
      ],
      "type": "concept",
      "description": "Shows that smaller RETRO models match or beat much larger ones, implying lower training energy.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Derived from performance comparison in results section."
    },
    {
      "name": "RETRO outputs show on-topic continuations with accurate digits and explicit sources",
      "aliases": [
        "qualitative interpretability evidence",
        "sample quality evidence"
      ],
      "type": "concept",
      "description": "Qualitative samples (Figures 3–4) display factual accuracy and topic adherence when retrieval is used.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Demonstrates interpretability and factual improvements."
    },
    {
      "name": "Design and train language models with RETRO retrieval architecture",
      "aliases": [
        "implement RETRO",
        "use retrieval-enhanced transformers"
      ],
      "type": "intervention",
      "description": "During model design and pre-training, incorporate nearest-neighbour retrieval and cross-attention as in RETRO to improve factuality and efficiency.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Applies at model architecture design stage described in 'high-level overview'.",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Demonstrated in research prototypes (7.5 B model) but not yet widespread operational deployment.",
      "node_rationale": "Primary actionable step proposed by paper."
    },
    {
      "name": "Curate and update retrieval database to remove unsafe or incorrect content post-deployment",
      "aliases": [
        "edit RETRO database for safety",
        "database curation intervention"
      ],
      "type": "intervention",
      "description": "After deployment, continuously audit and edit the retrieval corpus to eliminate harmful content or inject corrections, directly steering model outputs.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Performed during deployment when database can be updated without retraining.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Suggested as future safety route; no large-scale validation yet.",
      "node_rationale": "Paper notes retrieval provides 'a route for direct interventions through the retrieval database to improve the safety of text continuation'."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Factual hallucinations in language model outputs",
      "target_node": "Limited context window and parametric memorization in dense transformers",
      "description": "Hallucinations arise because models cannot reference information outside training weights or limited context.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Widely accepted mechanism and implied in introduction.",
      "edge_rationale": "Paper frames retrieval as solution to factual errors caused by context limitations."
    },
    {
      "type": "caused_by",
      "source_node": "Excessive training energy consumption in large dense transformers",
      "target_node": "Parameter-proportional compute and memory demands in dense transformers",
      "description": "Energy usage scales with parameter count in dense models.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Compute scaling laws are well-documented; referenced in introduction.",
      "edge_rationale": "Sets up need for more efficient approach."
    },
    {
      "type": "caused_by",
      "source_node": "Low interpretability of transformer predictions",
      "target_node": "Opaque latent reasoning without provenance in dense transformers",
      "description": "Lack of explicit citations leads to opaque outputs.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Qualitative reasoning presented.",
      "edge_rationale": "Establishes interpretability problem."
    },
    {
      "type": "mitigated_by",
      "source_node": "Limited context window and parametric memorization in dense transformers",
      "target_node": "External retrieval provides accurate contextual knowledge beyond model parameters",
      "description": "Fetching passages supplies missing context, reducing hallucinations.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Central claim supported by results.",
      "edge_rationale": "Core theoretical insight."
    },
    {
      "type": "mitigated_by",
      "source_node": "Parameter-proportional compute and memory demands in dense transformers",
      "target_node": "Retrieval-based performance scaling decouples accuracy from parameter count",
      "description": "Shifts accuracy driver from parameters to database size.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Evidence in Figure 2.",
      "edge_rationale": "Explains efficiency benefit."
    },
    {
      "type": "mitigated_by",
      "source_node": "Opaque latent reasoning without provenance in dense transformers",
      "target_node": "Retrieved citations enhance transparency of generated tokens",
      "description": "Citations make reasoning traceable.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Supported by qualitative samples.",
      "edge_rationale": "Addresses interpretability risk."
    },
    {
      "type": "enabled_by",
      "source_node": "External retrieval provides accurate contextual knowledge beyond model parameters",
      "target_node": "Integrate nearest-neighbor retrieval with transformer language modeling",
      "description": "Design integrates retrieval into model to realize insight.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Design follows directly from insight.",
      "edge_rationale": "Design rationale derived from theoretical insight."
    },
    {
      "type": "enabled_by",
      "source_node": "Retrieval-based performance scaling decouples accuracy from parameter count",
      "target_node": "Integrate nearest-neighbor retrieval with transformer language modeling",
      "description": "Same design implements parameter-efficient scaling.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical connection made in text.",
      "edge_rationale": "Shared design rationale."
    },
    {
      "type": "enabled_by",
      "source_node": "Retrieved citations enhance transparency of generated tokens",
      "target_node": "Integrate nearest-neighbor retrieval with transformer language modeling",
      "description": "Integration provides surface for showing citations.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Implied in interpretability claim.",
      "edge_rationale": "Design serves interpretability goal."
    },
    {
      "type": "implemented_by",
      "source_node": "Integrate nearest-neighbor retrieval with transformer language modeling",
      "target_node": "Nearest-neighbor search over chunked text passage database in RETRO",
      "description": "Search mechanism operationalizes retrieval component.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit architecture description.",
      "edge_rationale": "Implementation mechanism realises design."
    },
    {
      "type": "implemented_by",
      "source_node": "Integrate nearest-neighbor retrieval with transformer language modeling",
      "target_node": "Cross-attention fusion of retrieved neighbors with self-attention in RETRO",
      "description": "Cross-attention layer incorporates retrieved information.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Detailed in architecture overview.",
      "edge_rationale": "Second implementation mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Cross-attention fusion of retrieved neighbors with self-attention in RETRO",
      "target_node": "7.5B RETRO surpasses 175B dense models on Pile benchmark accuracy",
      "description": "Accuracy gains confirm effectiveness of fusion mechanism.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Quantitative benchmark results.",
      "edge_rationale": "Empirical validation."
    },
    {
      "type": "validated_by",
      "source_node": "Nearest-neighbor search over chunked text passage database in RETRO",
      "target_node": "RETRO achieves competitive performance with >20× fewer parameters reducing compute",
      "description": "Efficiency evidence demonstrates benefit of search mechanism.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Parameter comparison data.",
      "edge_rationale": "Empirical validation."
    },
    {
      "type": "validated_by",
      "source_node": "Cross-attention fusion of retrieved neighbors with self-attention in RETRO",
      "target_node": "RETRO outputs show on-topic continuations with accurate digits and explicit sources",
      "description": "Qualitative samples show improved factuality and interpretability.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Qualitative evidence; no statistical tests.",
      "edge_rationale": "Validation for interpretability claim."
    },
    {
      "type": "motivates",
      "source_node": "7.5B RETRO surpasses 175B dense models on Pile benchmark accuracy",
      "target_node": "Design and train language models with RETRO retrieval architecture",
      "description": "Superior accuracy motivates adopting RETRO.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Strong benchmark evidence.",
      "edge_rationale": "Transforms validation into actionable step."
    },
    {
      "type": "motivates",
      "source_node": "RETRO achieves competitive performance with >20× fewer parameters reducing compute",
      "target_node": "Design and train language models with RETRO retrieval architecture",
      "description": "Compute savings encourage implementation.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Performance-per-compute data.",
      "edge_rationale": "Cost benefit drives intervention."
    },
    {
      "type": "motivates",
      "source_node": "RETRO outputs show on-topic continuations with accurate digits and explicit sources",
      "target_node": "Curate and update retrieval database to remove unsafe or incorrect content post-deployment",
      "description": "Demonstrates that manipulating retrieval data influences output, suggesting safety curation.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Moderate inference from qualitative examples.",
      "edge_rationale": "Links evidence to safety-oriented intervention."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2021-12-08T00:00:00Z"
    },
    {
      "key": "url",
      "value": "https://www.deepmind.com/blog/improving-language-models-by-retrieving-from-trillions-of-tokens"
    },
    {
      "key": "title",
      "value": "Improving language models by retrieving from trillions of tokens"
    },
    {
      "key": "authors",
      "value": [
        "Sebastian Borgeaud",
        "Arthur Mensch",
        "Jordan Hoffmann",
        "Laurent Sifre"
      ]
    },
    {
      "key": "source",
      "value": "blogs"
    }
  ]
}