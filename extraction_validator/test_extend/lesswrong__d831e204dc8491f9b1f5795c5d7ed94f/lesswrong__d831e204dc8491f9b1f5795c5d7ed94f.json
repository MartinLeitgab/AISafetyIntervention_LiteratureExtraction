{
  "nodes": [
    {
      "name": "Catastrophic AI failure from non-adoption of validated safety solution",
      "aliases": [
        "AI catastrophe due to ignored safety",
        "Unsafe AI deployment from lack of compliance"
      ],
      "type": "concept",
      "description": "Risk that even a fully adequate AI-safety method will not be followed by developers or firms, leading to deployment of unsafe systems with potentially catastrophic outcomes.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central risk stated in opening paragraph ('How do we actually get people to follow this solution?')."
    },
    {
      "name": "Perception that AGI is distant reduces urgency for safety adoption",
      "aliases": [
        "Far AGI timeline belief",
        "Assumption AGI is decades away"
      ],
      "type": "concept",
      "description": "Many actors believe artificial general intelligence is far in the future, so they see no need to apply current safety methods to narrow AI work.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explicit first bullet in text listing reasons for ignoring safety."
    },
    {
      "name": "Complexity of AI safety concepts hinders correct implementation",
      "aliases": [
        "Difficulty understanding AI safety",
        "Safety concepts too abstract"
      ],
      "type": "concept",
      "description": "Because AI-safety ideas are hard to grasp, developers may misapply or simply skip the validated solution.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explicit second bullet in text."
    },
    {
      "name": "Competitive pressure disincentivizes adherence to safety solution",
      "aliases": [
        "Race dynamics reduce safety",
        "Fear of losing market share"
      ],
      "type": "concept",
      "description": "Firms worry they will fall behind rivals or earn less money if they slow down to follow safety protocols.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explicit third bullet in text."
    },
    {
      "name": "Skepticism about AI safety concerns reduces compliance",
      "aliases": [
        "Disbelief in AI risk",
        "Dismissal of safety worries"
      ],
      "type": "concept",
      "description": "Some actors simply do not care about or believe in the AI-safety concerns, preferring to experiment freely.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explicit fourth bullet in text."
    },
    {
      "name": "Temporal and knowledge asymmetry on AGI timelines reduces perceived immediate utility of safety compliance",
      "aliases": [
        "Timeline knowledge gap",
        "Present bias regarding AGI"
      ],
      "type": "concept",
      "description": "Actors undervalue long-term safety benefits because they believe AGI is far away or lack information about timelines.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Inferred mechanism explaining Problems 1, 2 and 5."
    },
    {
      "name": "Competitive market dynamics discourage voluntary safety measures",
      "aliases": [
        "Misaligned economic incentives",
        "Race to the bottom in AI safety"
      ],
      "type": "concept",
      "description": "Profit-driven competition creates incentives to cut safety corners unless all actors are simultaneously constrained.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Inferred underlying cause of competitive-pressure problem."
    },
    {
      "name": "Regulatory enforcement realigns market incentives to mandate safety compliance",
      "aliases": [
        "Safety regulation rationale",
        "Government rules as incentive fixer"
      ],
      "type": "concept",
      "description": "Creating legal requirements for AI safety can remove first-mover advantage from ignoring safety, solving coordination failure.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Logical design step to mitigate competitive and timeline-based disincentives."
    },
    {
      "name": "Educational outreach and simplified tooling lower cognitive barriers to safety adoption",
      "aliases": [
        "Safety education rationale",
        "Simplify safety implementation"
      ],
      "type": "concept",
      "description": "Providing easy-to-understand materials and tools can help practitioners correctly apply the safety solution despite conceptual complexity.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Design step addressing the complexity barrier."
    },
    {
      "name": "Government-mandated AI safety compliance certification process",
      "aliases": [
        "Safety certification scheme",
        "Regulatory audit mechanism"
      ],
      "type": "concept",
      "description": "A formal certification or audit procedure required by law ensures models meet the approved safety standard before deployment.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete mechanism implementing regulatory design rationale."
    },
    {
      "name": "Open-source safety libraries and developer training modules",
      "aliases": [
        "Safety tooling suite",
        "Open-source safety resources"
      ],
      "type": "concept",
      "description": "Freely available code libraries, documentation, and courses that bake in the validated safety method, making adoption straightforward.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete mechanism implementing educational design rationale."
    },
    {
      "name": "Current widespread neglect of AI safety among AI companies",
      "aliases": [
        "Observed lack of safety compliance",
        "Real-world safety neglect evidence"
      ],
      "type": "concept",
      "description": "Empirical observation mentioned in the text that many companies presently ignore AI-safety recommendations, demonstrating the need for stronger measures.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Serves as evidence validating need for certification and tooling mechanisms."
    },
    {
      "name": "Legislate mandatory AI safety audits before model deployment",
      "aliases": [
        "Pass AI safety audit law",
        "Enforce pre-deployment audits"
      ],
      "type": "intervention",
      "description": "Governments enact laws requiring every AI system above a defined capability threshold to pass an independent safety audit certifying adherence to the approved safety solution.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Targets deployment phase where models are approved for release (implicit in 'will ignore solutions' discussion).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Policy proposals exist but no large-scale implementations yet; based on inference from discussion.",
      "node_rationale": "Direct actionable step to implement certification mechanism and solve competitive race."
    },
    {
      "name": "Develop and disseminate standardized AI safety training programs for practitioners",
      "aliases": [
        "Offer AI safety education",
        "Provide safety curricula"
      ],
      "type": "intervention",
      "description": "Create comprehensive, simplified training courses and documentation packages that teach developers how to apply the validated safety solution correctly.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "General ecosystem support rather than a model-development phase action.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Some pilot courses exist but no universal standards; inferred need from complexity barrier.",
      "node_rationale": "Actionable step flowing from educational tooling mechanism."
    },
    {
      "name": "Provide tax incentives for companies that attain safety certification",
      "aliases": [
        "Offer safety compliance tax credits",
        "Financial rewards for safe AI"
      ],
      "type": "intervention",
      "description": "Governments grant tax credits or subsidies to organizations that successfully complete the safety certification, offsetting competitive disadvantages.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "An ecosystem-level financial policy supporting compliance.",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Purely theoretical suggestion based on inference; no pilots noted in source.",
      "node_rationale": "Addresses competitive-pressure problem by aligning economic incentives."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Catastrophic AI failure from non-adoption of validated safety solution",
      "target_node": "Perception that AGI is distant reduces urgency for safety adoption",
      "description": "If actors perceive AGI as far away, they ignore safety, raising the failure risk.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Explicit reasoning in first bullet; qualitative only.",
      "edge_rationale": "Text states this perception as a reason for non-adoption leading to risk."
    },
    {
      "type": "caused_by",
      "source_node": "Catastrophic AI failure from non-adoption of validated safety solution",
      "target_node": "Complexity of AI safety concepts hinders correct implementation",
      "description": "Conceptual difficulty prevents correct application, increasing chance that safety is ignored.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Explicit bullet point; qualitative evidence.",
      "edge_rationale": "Second bullet connects complexity to ignoring safety."
    },
    {
      "type": "caused_by",
      "source_node": "Catastrophic AI failure from non-adoption of validated safety solution",
      "target_node": "Competitive pressure disincentivizes adherence to safety solution",
      "description": "Firms may deliberately skip safety to keep up competitively, causing risk.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Explicit third bullet; qualitative evidence.",
      "edge_rationale": "Third bullet links competition to ignoring safety."
    },
    {
      "type": "caused_by",
      "source_node": "Catastrophic AI failure from non-adoption of validated safety solution",
      "target_node": "Skepticism about AI safety concerns reduces compliance",
      "description": "If actors disbelieve AI risk, they ignore safety, raising catastrophic risk.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Explicit fourth bullet.",
      "edge_rationale": "Fourth bullet lists disbelief as cause."
    },
    {
      "type": "caused_by",
      "source_node": "Perception that AGI is distant reduces urgency for safety adoption",
      "target_node": "Temporal and knowledge asymmetry on AGI timelines reduces perceived immediate utility of safety compliance",
      "description": "Timeline optimism stems from lack of information and present-bias described in theoretical insight.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inferred linkage; not directly proven in text.",
      "edge_rationale": "Maps surface perception to deeper knowledge-gap mechanism."
    },
    {
      "type": "caused_by",
      "source_node": "Complexity of AI safety concepts hinders correct implementation",
      "target_node": "Temporal and knowledge asymmetry on AGI timelines reduces perceived immediate utility of safety compliance",
      "description": "Limited understanding of timelines and concepts mutually reinforce complexity barrier.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inferred; plausible causal claim.",
      "edge_rationale": "Complexity partly arises from knowledge gaps captured by theoretical node."
    },
    {
      "type": "caused_by",
      "source_node": "Competitive pressure disincentivizes adherence to safety solution",
      "target_node": "Competitive market dynamics discourage voluntary safety measures",
      "description": "Competitive race dynamics are the underlying economic mechanism for the pressure.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inferred economic reasoning.",
      "edge_rationale": "Maps observed pressure to underlying incentive structure."
    },
    {
      "type": "caused_by",
      "source_node": "Skepticism about AI safety concerns reduces compliance",
      "target_node": "Temporal and knowledge asymmetry on AGI timelines reduces perceived immediate utility of safety compliance",
      "description": "Disbelief often stems from misjudging timelines or lacking info, tied to theoretical insight.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inferred linkage; plausible.",
      "edge_rationale": "Connects skepticism to knowledge asymmetry."
    },
    {
      "type": "mitigated_by",
      "source_node": "Temporal and knowledge asymmetry on AGI timelines reduces perceived immediate utility of safety compliance",
      "target_node": "Educational outreach and simplified tooling lower cognitive barriers to safety adoption",
      "description": "Education and tooling directly address knowledge gaps and complexity that feed timeline optimism.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical inference from design rationale; no empirical data.",
      "edge_rationale": "Design rationale proposed to mitigate theoretical mechanism."
    },
    {
      "type": "mitigated_by",
      "source_node": "Temporal and knowledge asymmetry on AGI timelines reduces perceived immediate utility of safety compliance",
      "target_node": "Regulatory enforcement realigns market incentives to mandate safety compliance",
      "description": "Regulations can counteract actorsâ€™ tendency to delay safety until AGI feels imminent.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inferred policy logic.",
      "edge_rationale": "Shows regulation as mitigation leverage."
    },
    {
      "type": "mitigated_by",
      "source_node": "Competitive market dynamics discourage voluntary safety measures",
      "target_node": "Regulatory enforcement realigns market incentives to mandate safety compliance",
      "description": "Regulation removes competitive disadvantage of being safe, fixing incentive misalignment.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Standard economic reasoning; not directly evidenced.",
      "edge_rationale": "Connects regulation design to incentive problem."
    },
    {
      "type": "implemented_by",
      "source_node": "Regulatory enforcement realigns market incentives to mandate safety compliance",
      "target_node": "Government-mandated AI safety compliance certification process",
      "description": "Certification/audit schemes operationalise the regulatory approach.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inferred mechanism; no empirical data.",
      "edge_rationale": "Specific implementation of regulatory design."
    },
    {
      "type": "implemented_by",
      "source_node": "Educational outreach and simplified tooling lower cognitive barriers to safety adoption",
      "target_node": "Open-source safety libraries and developer training modules",
      "description": "Developing libraries and training material is how the outreach design is executed.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical inference.",
      "edge_rationale": "Tooling embodies educational design."
    },
    {
      "type": "validated_by",
      "source_node": "Government-mandated AI safety compliance certification process",
      "target_node": "Current widespread neglect of AI safety among AI companies",
      "description": "Observation of widespread neglect indicates need for, and thus supports, certification mechanisms.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Evidence is indirect and qualitative.",
      "edge_rationale": "Neglect data used as justification for certification."
    },
    {
      "type": "validated_by",
      "source_node": "Open-source safety libraries and developer training modules",
      "target_node": "Current widespread neglect of AI safety among AI companies",
      "description": "Observed neglect motivates the tooling approach to improve adoption.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Indirect qualitative evidence.",
      "edge_rationale": "Same observation validates need for educational tooling."
    },
    {
      "type": "motivates",
      "source_node": "Current widespread neglect of AI safety among AI companies",
      "target_node": "Legislate mandatory AI safety audits before model deployment",
      "description": "Evidence of neglect motivates legislative intervention.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference from text question 'How do we get people to follow?'.",
      "edge_rationale": "Observed neglect motivates audit legislation."
    },
    {
      "type": "motivates",
      "source_node": "Current widespread neglect of AI safety among AI companies",
      "target_node": "Develop and disseminate standardized AI safety training programs for practitioners",
      "description": "Neglect motivates creation of training programs.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference; qualitative.",
      "edge_rationale": "Lack of adoption drives education intervention."
    },
    {
      "type": "motivates",
      "source_node": "Current widespread neglect of AI safety among AI companies",
      "target_node": "Provide tax incentives for companies that attain safety certification",
      "description": "Neglect highlights need for positive economic incentives.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference.",
      "edge_rationale": "Observed neglect motivates economic incentive policy."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://www.lesswrong.com/posts/4rgDink5LirgzwyqF/what-if-we-solve-ai-safety-but-no-one-cares"
    },
    {
      "key": "title",
      "value": "What if we solve AI Safety but no one cares"
    },
    {
      "key": "date_published",
      "value": "2022-08-22T05:38:03Z"
    },
    {
      "key": "authors",
      "value": [
        "142857"
      ]
    },
    {
      "key": "source",
      "value": "lesswrong"
    }
  ]
}