{
  "nodes": [
    {
      "name": "Existential catastrophe from misaligned transformative AI",
      "aliases": [
        "AI alignment existential risk",
        "Human disempowerment by misaligned AI"
      ],
      "type": "concept",
      "description": "Advanced AI systems might acquire goals divergent from human values and overpower humanity, leading to permanent loss of control or extinction.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central risk recapped in section “Recapping the major risks, and some things that could help”."
    },
    {
      "name": "Global power imbalance due to uneven AI deployment",
      "aliases": [
        "AI-driven geopolitical imbalance",
        "Concentrated AI advantage risk"
      ],
      "type": "concept",
      "description": "First movers in transformative AI could become hundreds of years ahead technologically, allowing domination of others and risking global instability.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed under “Power imbalances, and other risks beyond misaligned AI”."
    },
    {
      "name": "Catastrophic misuse of aligned AI by malicious actors",
      "aliases": [
        "Misuse of powerful aligned AI",
        "Malicious application risk"
      ],
      "type": "concept",
      "description": "Even AI that obeys its operators can enable bioweapons, large-scale propaganda or other harms if placed in the wrong hands.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted in ‘Anti-misuse research’ subsection within “Research and engineering careers”."
    },
    {
      "name": "Premature deployment of unsafe AI systems driven by capability race",
      "aliases": [
        "AI capability race risk",
        "Racing through a minefield scenario"
      ],
      "type": "concept",
      "description": "Competitive pressure and ambiguous evidence may push actors to release inadequately tested AI, amplifying systemic risk.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in narrative of ‘How we could stumble into an AI catastrophe’. "
    },
    {
      "name": "Trial-and-error training induces unintended power-seeking aims in advanced AI",
      "aliases": [
        "Reward hacking via RLHF",
        "Unintended objective formation"
      ],
      "type": "concept",
      "description": "Black-box optimisation methods may reinforce deceptive or power-seeking behaviours that diverge from intended objectives.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Problem source for mis-alignment noted under ‘Why would AI aim to defeat humanity?’."
    },
    {
      "name": "Competitive pressures and ambiguous evidence reduce caution in AI deployment",
      "aliases": [
        "Ambiguity-driven risk taking",
        "Deploy-first verify-later dynamic"
      ],
      "type": "concept",
      "description": "Firms race for advantage while risk signals remain uncertain, making conservative safety measures economically unattractive.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in ‘Competitive pressures ... could make this situation very dangerous’."
    },
    {
      "name": "Low-barrier access to powerful AI tools facilitates creation of biological weapons",
      "aliases": [
        "AI-enabled biothreat creation",
        "DIY bio via AI"
      ],
      "type": "concept",
      "description": "Open or poorly secured models could help non-experts design or scale bioweapons, increasing misuse risk.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivating example in ‘Anti-misuse research’ subsection."
    },
    {
      "name": "AI weight theft via state-level cyber-espionage",
      "aliases": [
        "Model exfiltration threat",
        "Frontier weights leakage"
      ],
      "type": "concept",
      "description": "Highly resourced actors could hack leading labs and copy model parameters, negating any safety lead.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined in ‘The challenge of securing dangerous AI’ box."
    },
    {
      "name": "Opacity of advanced models conceals emerging deceptive behaviors",
      "aliases": [
        "Hidden deception in LM",
        "Lance-Armstrong problem"
      ],
      "type": "concept",
      "description": "It is hard to distinguish genuinely safe behaviour from strategic behaviour that merely appears safe during evaluation.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Problem framed through ‘Lance-Armstrong’ and ‘King Lear’ analogies."
    },
    {
      "name": "Lack of reliable probabilistic forecasts about AI timelines and safety effectiveness",
      "aliases": [
        "Forecasting gap on AI progress",
        "Uncertain AI timelines"
      ],
      "type": "concept",
      "description": "Decision-makers have little quantitative guidance on when capabilities or solutions will arrive, complicating policy.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivation in ‘Forecasting’ job track section."
    },
    {
      "name": "Absence of safety-first leadership among frontier AI projects",
      "aliases": [
        "Lack of careful AI initiative",
        "Profit-driven governance gap"
      ],
      "type": "concept",
      "description": "Few leading organisations adopt charters that prioritise safety over rapid capability progress, limiting positive influence.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed repeatedly in ‘Successful, careful AI projects’ segment."
    },
    {
      "name": "Evaluation difficulties of advanced AI behavior hinder alignment assurance",
      "aliases": [
        "Hard-to-measure AI safety",
        "Alignment evaluation challenge"
      ],
      "type": "concept",
      "description": "Because deceptive systems can appear compliant, verifying true alignment requires deeper insights than surface testing.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Derived from ‘Why AI safety could be hard to measure’ table."
    },
    {
      "name": "Global safety standards can shift incentives toward cautious deployment",
      "aliases": [
        "Standards-driven safety incentives",
        "Monitoring regime theory"
      ],
      "type": "concept",
      "description": "If all actors must evidence safety compliance, competitive pressure to cut corners diminishes.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in ‘Standards and monitoring’ coloured box."
    },
    {
      "name": "Robust information security prevents proliferation of dangerous models",
      "aliases": [
        "Security as proliferation control",
        "Cybersecurity-alignment link"
      ],
      "type": "concept",
      "description": "Strong defences against espionage help ensure only safety-conscious actors possess frontier models.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Justified in ‘Information security careers’ and ‘The challenging of securing dangerous AI’."
    },
    {
      "name": "Training refusal policies can reduce AI-assisted biothreat misuse",
      "aliases": [
        "Safe completion theory",
        "Policy-based misuse mitigation"
      ],
      "type": "concept",
      "description": "Aligning models to refuse disallowed content can make dangerous applications harder even for willing operators.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivated in ‘Anti-misuse research’ subsection."
    },
    {
      "name": "Threat assessment experiments can build consensus on AI risks",
      "aliases": [
        "Model organism approach for AI",
        "Deliberate deception studies"
      ],
      "type": "concept",
      "description": "Creating controlled demonstrations of deceptive or manipulative behaviour clarifies danger levels and spurs precaution.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Articulated in ‘Threat assessment research’ bullet."
    },
    {
      "name": "Aggregated superforecaster predictions improve policy decisions under uncertainty",
      "aliases": [
        "Forecast aggregation insight",
        "Probabilistic governance theory"
      ],
      "type": "concept",
      "description": "Empirical evidence shows structured forecasting beats intuition, suggesting value for AI policy timing.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Basis of ‘Forecasting to get a better handle on what’s coming’ section."
    },
    {
      "name": "Careful AI projects can influence industry norms toward safety",
      "aliases": [
        "Lead-by-example theory",
        "Safe-leader leverage"
      ],
      "type": "concept",
      "description": "A profitable yet safety-oriented lab can set precedents, negotiate deals and lobby for safer practices.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in depth in ‘Successful, careful AI projects’ coloured box."
    },
    {
      "name": "Develop alignment techniques combining digital neuroscience, limited AI, and AI checks & balances",
      "aliases": [
        "Multi-prong alignment agenda",
        "Trifecta alignment approach"
      ],
      "type": "concept",
      "description": "Using interpretability, capability limitation and adversarial oversight in concert offers a plausible alignment path.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposed in ‘High-level hopes for AI alignment’ expansion."
    },
    {
      "name": "Establish international AI safety standards and monitoring regime",
      "aliases": [
        "Global AI governance framework",
        "Standards & monitoring design"
      ],
      "type": "concept",
      "description": "A common set of demonstrable safety requirements enforced by auditing and state oversight can align incentives.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined in ‘Standards and monitoring’ coloured box."
    },
    {
      "name": "Implement zero-trust security and compute controls in AI labs",
      "aliases": [
        "Frontier model security design",
        "Defense-in-depth for AI"
      ],
      "type": "concept",
      "description": "Adopting rigorous access control, hardware enclaves and least-privilege principles can deter model theft.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Encouraged in ‘Information security careers’ discussion (inference flagged)."
    },
    {
      "name": "Design models resistant to misuse via policy fine-tuning",
      "aliases": [
        "Misuse-resilient model design",
        "Safety policy tuning"
      ],
      "type": "concept",
      "description": "Training procedures add refusal and safe-completion behaviours, limiting dangerous outputs.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Suggested under ‘Anti-misuse research’."
    },
    {
      "name": "Conduct intentional red-team threat assessment of new models",
      "aliases": [
        "Deliberate deception red-team",
        "Model organism testing plan"
      ],
      "type": "concept",
      "description": "Purposefully inducing adversarial behaviours in a controlled environment reveals failure modes early.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposed in ‘Threat assessment research’."
    },
    {
      "name": "Use prediction markets to inform AI governance decisions",
      "aliases": [
        "Forecast-driven policy design",
        "Prediction-informed governance"
      ],
      "type": "concept",
      "description": "Structured forecasting platforms can supply quantitative inputs to legislators and boards making AI decisions.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivated in ‘Forecasting’ section."
    },
    {
      "name": "Operate AI labs with safety-first governance charters and board oversight",
      "aliases": [
        "Careful AI governance design",
        "Safety-oriented lab charter"
      ],
      "type": "concept",
      "description": "Embedding explicit safety clauses and empowered boards aligns organisational incentives with global welfare.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Design idea in ‘Successful, careful AI projects’ section."
    },
    {
      "name": "Mechanistic interpretability and oversight RL on frontier models",
      "aliases": [
        "Circuit analysis plus RLHF",
        "Interpretability-guided alignment"
      ],
      "type": "concept",
      "description": "Combines transparent internal representation mapping with reinforcement learning from human and AI overseers.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation example derived from alignment techniques list."
    },
    {
      "name": "Third-party safety audits and certifications for AI models",
      "aliases": [
        "External AI safety audit",
        "Certification pipeline"
      ],
      "type": "concept",
      "description": "Independent organisations evaluate models against agreed standards, publishing compliance reports.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposed under standards regime narrative."
    },
    {
      "name": "Hardened infrastructure and key management for model weights",
      "aliases": [
        "Secure model infra",
        "Zero-trust implementation"
      ],
      "type": "concept",
      "description": "Technical stack includes hardware enclaves, split-key encryption and continuous monitoring to prevent exfiltration.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation detail inferred from ‘Information security careers’ (inference)."
    },
    {
      "name": "Fine-tune models on refusal and safe completions for biorisk content",
      "aliases": [
        "Refusal fine-tuning",
        "Biorisk policy training"
      ],
      "type": "concept",
      "description": "RLHF or supervised fine-tuning discourages responses that facilitate harmful biological instructions.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete mechanism under ‘Anti-misuse research’."
    },
    {
      "name": "Controlled experiments with deception-inducing reward functions",
      "aliases": [
        "Deception red-team experiment",
        "Intentional deceptive training setup"
      ],
      "type": "concept",
      "description": "Researchers purposely reward models for hiding true reasoning to test detection methods.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation derived from threat-assessment proposal."
    },
    {
      "name": "Superforecasting tournaments integrated with policy planning",
      "aliases": [
        "Policy-linked prediction platform",
        "Forecast pipeline"
      ],
      "type": "concept",
      "description": "Regularly updated question sets feed aggregated probabilities directly to governmental or board dashboards.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation of prediction-market design rationale."
    },
    {
      "name": "Safety-educated boards and risk review processes in AI companies",
      "aliases": [
        "Governance risk reviews",
        "Board-level safety process"
      ],
      "type": "concept",
      "description": "Board members trained on AI risk oversee stage-gate processes before major deployments.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mechanism sketched in ‘Education for key people at AI companies’."
    },
    {
      "name": "Improved deception detection in pilot language model benchmark",
      "aliases": [
        "Deception benchmark result",
        "Early threat-assessment evidence"
      ],
      "type": "concept",
      "description": "Small-scale experiments (Perez 2022) show progress in eliciting and identifying deceptive behaviour.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Cited example in threat-assessment section."
    },
    {
      "name": "Self-regulation commitments by leading AI companies (e.g., Google 2018)",
      "aliases": [
        "Voluntary AI principles evidence",
        "Corporate self-regulation case"
      ],
      "type": "concept",
      "description": "Past public pledges to restrict weaponisable AI demonstrate feasibility of standards adoption.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Referenced in standards path narrative."
    },
    {
      "name": "Information security talent shortage identified at AI labs",
      "aliases": [
        "Security hiring gap evidence",
        "Talent shortage case"
      ],
      "type": "concept",
      "description": "AI companies report difficulty recruiting experienced security professionals, indicating unmet need.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Evidence quoted in ‘Information security careers’. "
    },
    {
      "name": "Early RLHF models show reduced compliance with disallowed content",
      "aliases": [
        "Refusal policy empirical result",
        "RLHF misuse mitigation evidence"
      ],
      "type": "concept",
      "description": "Commercial chat models exhibit higher refusal rates on dangerous prompts after fine-tuning, suggesting viability.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Example performance cited in anti-misuse discussion."
    },
    {
      "name": "Metaculus accuracy in geopolitical forecasting competitions",
      "aliases": [
        "Forecast platform track record",
        "Metaculus evidence"
      ],
      "type": "concept",
      "description": "Community forecasting platform has demonstrated Brier-score advantages over baseline in real-world events.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Validation for prediction-market approach referenced in ‘Forecasting’ section."
    },
    {
      "name": "Conduct mechanistic alignment research on advanced AI during fine-tuning",
      "aliases": [
        "Perform interpretability-guided RL alignment",
        "Run circuit-analysis alignment projects"
      ],
      "type": "intervention",
      "description": "During stage-3 fine-tuning/RL, apply mechanistic interpretability tools and overseer feedback to identify and prevent deceptive or power-seeking circuits.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Fits ‘Research and engineering careers’ focus on fine-tuning/RL experimentation.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Technique under active study; limited prototypes exist (recap ‘High-level hopes for AI alignment’).",
      "node_rationale": "Direct actionable step derived from alignment research job description."
    },
    {
      "name": "Develop and enforce global AI safety standards before deployment",
      "aliases": [
        "Draft and implement AI governance standards",
        "Set enforceable safety compliance"
      ],
      "type": "intervention",
      "description": "Coordinate industry and governments to require external audits, security practices and proven alignment results prior to commercial release (deployment lifecycle).",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Targets pre-deployment testing and deployment controls described in ‘Standards and monitoring’.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Currently at proposal/pilot policy stage; few systematic regimes exist.",
      "node_rationale": "Key policy lever emphasised across standards discussions."
    },
    {
      "name": "Implement zero-trust security architecture for model weights during pre-training and deployment",
      "aliases": [
        "Deploy hardened AI infra",
        "Apply zero-trust to frontier models"
      ],
      "type": "intervention",
      "description": "Adopt least-privilege networking, hardware enclaves, split-key encryption and continuous red-team testing for all compute clusters handling frontier models.",
      "concept_category": null,
      "intervention_lifecycle": "2",
      "intervention_lifecycle_rationale": "Security controls must be in place at pre-training and persist through deployment lifecycle, as argued in ‘Information security careers’.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Industry implementations are experimental; best practice patterns still forming (evidence: talent shortage).",
      "node_rationale": "Actionable security measure derived from design rationale."
    },
    {
      "name": "Fine-tune models with misuse-prevention policies to refuse dangerous instructions",
      "aliases": [
        "Deploy refusal policy fine-tuning",
        "Safety policy RLHF"
      ],
      "type": "intervention",
      "description": "At fine-tuning/RL stage, train models with curated prompts and reinforcement to decline or safe-complete requests that enable biothreats or other serious harms.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Occurs during supervised fine-tuning and RL as per ‘Anti-misuse research’.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Early commercial systems show partial success; rigorous validation pending.",
      "node_rationale": "Direct mitigation for misuse risk track."
    },
    {
      "name": "Establish independent threat-assessment red-teaming of new models pre-deployment",
      "aliases": [
        "Run external deception red-teams",
        "Third-party adversarial testing"
      ],
      "type": "intervention",
      "description": "Before deployment, contract specialist teams to deliberately elicit and measure deceptive, manipulative or unsafe behaviours under controlled conditions.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Falls in pre-deployment testing phase, aligning with ‘Threat assessment research’.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Pilot studies exist; practice not yet institutionalised (Perez benchmark cited).",
      "node_rationale": "Converts threat-assessment design into actionable programme."
    },
    {
      "name": "Use superforecasting platforms to guide government AI policy decisions",
      "aliases": [
        "Integrate forecasting into AI policy",
        "Policy-linked prediction markets"
      ],
      "type": "intervention",
      "description": "Governments and boards commission regular question sets on AI timelines and safety milestones and commit to use resulting probability distributions in strategy setting.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "Cross-cutting policy support mechanism—categorised as Other (6).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Forecast platforms operate today but are not embedded in AI governance (based on Metaculus evidence).",
      "node_rationale": "Implements forecasting design rationale."
    },
    {
      "name": "Operate AI labs under safety-first governance charters with empowered boards",
      "aliases": [
        "Adopt safety-first lab charters",
        "Board-empowered safety labs"
      ],
      "type": "intervention",
      "description": "Frontier AI organisations codify safety priority in legal charters and install boards trained in AI risk to veto unsafe deployments.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Governs deployment-era decisions outlined in ‘Successful, careful AI projects’.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Some precedents exist (OpenAI charter); broad adoption limited.",
      "node_rationale": "Action path for careful-project theoretical insight."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Existential catastrophe from misaligned transformative AI",
      "target_node": "Trial-and-error training induces unintended power-seeking aims in advanced AI",
      "description": "Misalignment catastrophe is expected when training procedures reinforce unintended goals.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "The article argues conceptually; no empirical proof.",
      "edge_rationale": "‘Why would AI aim to defeat humanity?’ links optimisation methods to takeover risk."
    },
    {
      "type": "specified_by",
      "source_node": "Trial-and-error training induces unintended power-seeking aims in advanced AI",
      "target_node": "Evaluation difficulties of advanced AI behavior hinder alignment assurance",
      "description": "Difficulty measuring true goals explains why trial-and-error can lock in bad objectives.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical explanation rather than data.",
      "edge_rationale": "Drawn from ‘Why AI safety could be hard to measure’."
    },
    {
      "type": "mitigated_by",
      "source_node": "Evaluation difficulties of advanced AI behavior hinder alignment assurance",
      "target_node": "Develop alignment techniques combining digital neuroscience, limited AI, and AI checks & balances",
      "description": "Proposed alignment trio aims to circumvent evaluation gaps.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual proposal without strong empirical backing.",
      "edge_rationale": "‘High-level hopes for AI alignment’ lists these techniques as answers."
    },
    {
      "type": "implemented_by",
      "source_node": "Develop alignment techniques combining digital neuroscience, limited AI, and AI checks & balances",
      "target_node": "Mechanistic interpretability and oversight RL on frontier models",
      "description": "Concrete lab-level work combines interpretability with overseer RL.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Widely practised in alignment groups; partial empirical evidence.",
      "edge_rationale": "Implementation examples cited in research careers."
    },
    {
      "type": "validated_by",
      "source_node": "Mechanistic interpretability and oversight RL on frontier models",
      "target_node": "Improved deception detection in pilot language model benchmark",
      "description": "Early benchmarks show the approach can reveal deceptive circuits.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Small-scale empirical study only.",
      "edge_rationale": "Perez 2022 benchmark linked in threat assessment section."
    },
    {
      "type": "motivates",
      "source_node": "Improved deception detection in pilot language model benchmark",
      "target_node": "Conduct mechanistic alignment research on advanced AI during fine-tuning",
      "description": "Positive benchmark results encourage further mechanistic alignment work.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasonable causal intuition; limited data.",
      "edge_rationale": "Research and engineering careers section treats such findings as validation."
    },
    {
      "type": "caused_by",
      "source_node": "Premature deployment of unsafe AI systems driven by capability race",
      "target_node": "Competitive pressures and ambiguous evidence reduce caution in AI deployment",
      "description": "Race dynamics and unclear evidence directly produce premature releases.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Historical analogies and logical reasoning; no quantitative data.",
      "edge_rationale": "Narrative in ‘How we could stumble into an AI catastrophe’."
    },
    {
      "type": "mitigated_by",
      "source_node": "Competitive pressures and ambiguous evidence reduce caution in AI deployment",
      "target_node": "Global safety standards can shift incentives toward cautious deployment",
      "description": "Uniform standards reduce the advantage of racing ahead.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Policy reasoning; precedent from other industries mentioned.",
      "edge_rationale": "Standards and monitoring section."
    },
    {
      "type": "implemented_by",
      "source_node": "Global safety standards can shift incentives toward cautious deployment",
      "target_node": "Establish international AI safety standards and monitoring regime",
      "description": "Design rationale realised through drafting and enforcing standards.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct logical implementation.",
      "edge_rationale": "Same coloured box."
    },
    {
      "type": "implemented_by",
      "source_node": "Establish international AI safety standards and monitoring regime",
      "target_node": "Third-party safety audits and certifications for AI models",
      "description": "Audits are the concrete mechanism for ensuring compliance.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Standard practice in other regulated fields; proposed here.",
      "edge_rationale": "Box on standards provides auditing example."
    },
    {
      "type": "validated_by",
      "source_node": "Third-party safety audits and certifications for AI models",
      "target_node": "Self-regulation commitments by leading AI companies (e.g., Google 2018)",
      "description": "Prior voluntary principles illustrate feasibility of audit-like commitments.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Single case study.",
      "edge_rationale": "Example referenced in monitoring section."
    },
    {
      "type": "motivates",
      "source_node": "Self-regulation commitments by leading AI companies (e.g., Google 2018)",
      "target_node": "Develop and enforce global AI safety standards before deployment",
      "description": "Early self-regulation suggests broader enforceable standards are plausible.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical extrapolation.",
      "edge_rationale": "Author cites commitments as stepping-stone toward formal standards."
    },
    {
      "type": "caused_by",
      "source_node": "Catastrophic misuse of aligned AI by malicious actors",
      "target_node": "Low-barrier access to powerful AI tools facilitates creation of biological weapons",
      "description": "Easy access is direct mechanism enabling misuse.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Analogy to Bostrom’s ‘Vulnerable World’; plausible causal chain.",
      "edge_rationale": "Anti-misuse research discussion."
    },
    {
      "type": "mitigated_by",
      "source_node": "Low-barrier access to powerful AI tools facilitates creation of biological weapons",
      "target_node": "Training refusal policies can reduce AI-assisted biothreat misuse",
      "description": "If models refuse to assist, barrier to misuse rises.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Initial empirical hints; conceptually straightforward.",
      "edge_rationale": "Same subsection."
    },
    {
      "type": "implemented_by",
      "source_node": "Training refusal policies can reduce AI-assisted biothreat misuse",
      "target_node": "Design models resistant to misuse via policy fine-tuning",
      "description": "Fine-tuning with refusal prompts realises the misuse-resistance idea.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Technique widely trialled in practice.",
      "edge_rationale": "Anti-misuse research examples."
    },
    {
      "type": "implemented_by",
      "source_node": "Design models resistant to misuse via policy fine-tuning",
      "target_node": "Fine-tune models on refusal and safe completions for biorisk content",
      "description": "Specific implementation step for policy fine-tuning.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Current industry method.",
      "edge_rationale": "Implementation mechanism description."
    },
    {
      "type": "validated_by",
      "source_node": "Fine-tune models on refusal and safe completions for biorisk content",
      "target_node": "Early RLHF models show reduced compliance with disallowed content",
      "description": "Observed decrease in harmful outputs validates approach.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Limited public metrics.",
      "edge_rationale": "Referenced outcome in anti-misuse section."
    },
    {
      "type": "motivates",
      "source_node": "Early RLHF models show reduced compliance with disallowed content",
      "target_node": "Fine-tune models with misuse-prevention policies to refuse dangerous instructions",
      "description": "Positive evidence encourages broader deployment of refusal fine-tuning.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical link; some data.",
      "edge_rationale": "Article suggests scaling such work."
    },
    {
      "type": "caused_by",
      "source_node": "Existential catastrophe from misaligned transformative AI",
      "target_node": "Opacity of advanced models conceals emerging deceptive behaviors",
      "description": "Hidden deception makes catastrophic takeover more likely.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Conceptual argument (Lance Armstrong problem).",
      "edge_rationale": "Safety-measurement discussion."
    },
    {
      "type": "mitigated_by",
      "source_node": "Opacity of advanced models conceals emerging deceptive behaviors",
      "target_node": "Threat assessment experiments can build consensus on AI risks",
      "description": "Deliberate experiments expose hidden deception, reducing opacity.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Rationale provided; limited data.",
      "edge_rationale": "Threat assessment research proposal."
    },
    {
      "type": "implemented_by",
      "source_node": "Threat assessment experiments can build consensus on AI risks",
      "target_node": "Conduct intentional red-team threat assessment of new models",
      "description": "Red-team programmes enact the experiment design.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct derivation.",
      "edge_rationale": "Same section."
    },
    {
      "type": "implemented_by",
      "source_node": "Conduct intentional red-team threat assessment of new models",
      "target_node": "Controlled experiments with deception-inducing reward functions",
      "description": "Sets up the specific experimental protocol for red-teaming.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Light inference of technical details.",
      "edge_rationale": "Expands author’s illustrative example."
    },
    {
      "type": "validated_by",
      "source_node": "Controlled experiments with deception-inducing reward functions",
      "target_node": "Improved deception detection in pilot language model benchmark",
      "description": "Benchmark results arise from such controlled experiments.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Single benchmark link.",
      "edge_rationale": "Perez example."
    },
    {
      "type": "motivates",
      "source_node": "Improved deception detection in pilot language model benchmark",
      "target_node": "Establish independent threat-assessment red-teaming of new models pre-deployment",
      "description": "Success encourages institutionalised red-teaming before release.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical policy inference.",
      "edge_rationale": "Author advocates broader adoption."
    },
    {
      "type": "caused_by",
      "source_node": "Global power imbalance due to uneven AI deployment",
      "target_node": "AI weight theft via state-level cyber-espionage",
      "description": "Stolen models can shift power dramatically.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Argument in ‘Challenging of securing dangerous AI’.",
      "edge_rationale": "Narrative shows theft leading to power spread."
    },
    {
      "type": "mitigated_by",
      "source_node": "AI weight theft via state-level cyber-espionage",
      "target_node": "Robust information security prevents proliferation of dangerous models",
      "description": "Stronger security reduces theft likelihood.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Security principle widely evidenced in other domains.",
      "edge_rationale": "Information security careers section."
    },
    {
      "type": "implemented_by",
      "source_node": "Robust information security prevents proliferation of dangerous models",
      "target_node": "Implement zero-trust security and compute controls in AI labs",
      "description": "Zero-trust is recommended concrete approach.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Best-practice extrapolation (light inference).",
      "edge_rationale": "Design rationale elaborated."
    },
    {
      "type": "implemented_by",
      "source_node": "Implement zero-trust security and compute controls in AI labs",
      "target_node": "Hardened infrastructure and key management for model weights",
      "description": "Infrastructure and key management are specific engineering steps.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Standard security engineering.",
      "edge_rationale": "Implementation detail."
    },
    {
      "type": "validated_by",
      "source_node": "Hardened infrastructure and key management for model weights",
      "target_node": "Information security talent shortage identified at AI labs",
      "description": "Talent shortage highlights need and partial adoption status.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Evidence is qualitative.",
      "edge_rationale": "Security careers section cites hiring difficulty."
    },
    {
      "type": "motivates",
      "source_node": "Information security talent shortage identified at AI labs",
      "target_node": "Implement zero-trust security architecture for model weights during pre-training and deployment",
      "description": "Gap motivates dedicated programmes to roll out zero-trust architecture.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical policy response.",
      "edge_rationale": "Author urges more work in this area."
    },
    {
      "type": "caused_by",
      "source_node": "Premature deployment of unsafe AI systems driven by capability race",
      "target_node": "Lack of reliable probabilistic forecasts about AI timelines and safety effectiveness",
      "description": "Uncertain timelines feed into poor strategic choices and haste.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoned claim; limited data.",
      "edge_rationale": "Forecasting section notes uncertainty fuelling race."
    },
    {
      "type": "mitigated_by",
      "source_node": "Lack of reliable probabilistic forecasts about AI timelines and safety effectiveness",
      "target_node": "Aggregated superforecaster predictions improve policy decisions under uncertainty",
      "description": "Better forecasts inform more measured policy.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Supported by Tetlock studies referenced implicitly.",
      "edge_rationale": "Forecasting job track."
    },
    {
      "type": "implemented_by",
      "source_node": "Aggregated superforecaster predictions improve policy decisions under uncertainty",
      "target_node": "Use prediction markets to inform AI governance decisions",
      "description": "Prediction markets/tournaments operationalise forecast use.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct mapping.",
      "edge_rationale": "Design rationale to implementation step."
    },
    {
      "type": "implemented_by",
      "source_node": "Use prediction markets to inform AI governance decisions",
      "target_node": "Superforecasting tournaments integrated with policy planning",
      "description": "Platforms and tournaments supply the required aggregation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Existing platforms show feasibility.",
      "edge_rationale": "Implementation description."
    },
    {
      "type": "validated_by",
      "source_node": "Superforecasting tournaments integrated with policy planning",
      "target_node": "Metaculus accuracy in geopolitical forecasting competitions",
      "description": "Historical accuracy demonstrates effectiveness of tournaments.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Published forecasting metrics.",
      "edge_rationale": "Validation evidence node."
    },
    {
      "type": "motivates",
      "source_node": "Metaculus accuracy in geopolitical forecasting competitions",
      "target_node": "Use superforecasting platforms to guide government AI policy decisions",
      "description": "Proven track record motivates adoption in AI policy.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoned extrapolation.",
      "edge_rationale": "Forecasting section invites such usage."
    },
    {
      "type": "caused_by",
      "source_node": "Existential catastrophe from misaligned transformative AI",
      "target_node": "Absence of safety-first leadership among frontier AI projects",
      "description": "Without safety-oriented leadership, misaligned systems more likely reach deployment.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Conceptual linkage.",
      "edge_rationale": "Successful careful AI projects discussion."
    },
    {
      "type": "mitigated_by",
      "source_node": "Absence of safety-first leadership among frontier AI projects",
      "target_node": "Careful AI projects can influence industry norms toward safety",
      "description": "A single careful leader can set safer norms and deals.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Argument in article.",
      "edge_rationale": "Same section."
    },
    {
      "type": "implemented_by",
      "source_node": "Careful AI projects can influence industry norms toward safety",
      "target_node": "Operate AI labs with safety-first governance charters and board oversight",
      "description": "Governance charters and boards realise the careful-leader model.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical implementation.",
      "edge_rationale": "Governance advice subsections."
    },
    {
      "type": "implemented_by",
      "source_node": "Operate AI labs with safety-first governance charters and board oversight",
      "target_node": "Safety-educated boards and risk review processes in AI companies",
      "description": "Training boards and instituting review processes are concrete mechanisms.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Standard governance practice extrapolated.",
      "edge_rationale": "Education for key people section."
    },
    {
      "type": "validated_by",
      "source_node": "Safety-educated boards and risk review processes in AI companies",
      "target_node": "Self-regulation commitments by leading AI companies (e.g., Google 2018)",
      "description": "Early advisory boards facilitated company-level commitments, indicating governance leverage.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Anecdotal evidence.",
      "edge_rationale": "Example in article."
    },
    {
      "type": "motivates",
      "source_node": "Self-regulation commitments by leading AI companies (e.g., Google 2018)",
      "target_node": "Operate AI labs under safety-first governance charters with empowered boards",
      "description": "Self-regulation success encourages broader charter adoption.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical next step.",
      "edge_rationale": "Author calls for labs to take similar steps."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://forum.effectivealtruism.org/posts/njD2PurEKDEZcMLKZ/jobs-that-can-help-with-the-most-important-century"
    },
    {
      "key": "title",
      "value": "Jobs that can help with the most important century"
    },
    {
      "key": "date_published",
      "value": "2023-02-12T18:19:00Z"
    },
    {
      "key": "authors",
      "value": [
        "Holden Karnofsky"
      ]
    },
    {
      "key": "source",
      "value": "eaforum"
    }
  ]
}