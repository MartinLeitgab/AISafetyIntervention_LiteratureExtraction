{
  "nodes": [
    {
      "name": "Existential catastrophe from unaligned artificial general intelligence",
      "aliases": [
        "AGI existential risk",
        "Catastrophic outcomes from misaligned AI"
      ],
      "type": "concept",
      "description": "Possibility that advanced AGI systems with goals misaligned with human values could cause irreversible harm or human extinction.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted across newsletter, especially in 'Five Theses, Two Lemmas' emphasising fragility of value and need for Friendly AI."
    },
    {
      "name": "Runaway intelligence explosion via recursive self-improvement",
      "aliases": [
        "Uncontrolled intelligence explosion",
        "Recursive self-enhancement of AI"
      ],
      "type": "concept",
      "description": "Mechanism where a sufficiently advanced AI repeatedly improves its own cognitive architecture, leading to rapid, uncontrollable increases in intelligence.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in 'Intelligence Explosion Microeconomics' abstract as core mechanism behind catastrophic risk."
    },
    {
      "name": "Returns on cognitive reinvestment determine self-improvement dynamics",
      "aliases": [
        "Cognitive ROI hypothesis",
        "Reinvestable returns on intelligence improvements"
      ],
      "type": "concept",
      "description": "Hypothesis that the speed and scale of an intelligence explosion hinge on how profitable additional cognitive resources are when reinvested in further improvement.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "'Intelligence Explosion Microeconomics' identifies cognitive ROI as the key unknown."
    },
    {
      "name": "Formal microeconomic modeling of cognitive reinvestment returns",
      "aliases": [
        "Intelligence explosion microeconomics approach"
      ],
      "type": "concept",
      "description": "Rationale to capture intelligence-explosion dynamics through explicit microeconomic models quantifying costs and gains of cognitive investments.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Yudkowsky proposes formal ROI curves as next research step."
    },
    {
      "name": "Intelligence explosion microeconomic ROI curve formalization",
      "aliases": [
        "ROI curve specification for cognitive reinvestment"
      ],
      "type": "concept",
      "description": "Implementation mechanism involving the mathematical specification of return-on-investment curves describing how additional computing resources translate into further cognitive gains.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Detailed in technical report; provides concrete modelling tool."
    },
    {
      "name": "Historical technological growth trends constrain ROI models",
      "aliases": [
        "Empirical constraints on cognitive ROI",
        "Technology growth observations"
      ],
      "type": "concept",
      "description": "Observation that data from hominid evolution, Moore’s Law, chess AI progress, etc. place empirical bounds on plausible ROI curves.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Yudkowsky lists historical phenomena as test cases for falsifying ROI microfoundations."
    },
    {
      "name": "Develop quantitative ROI models to predict intelligence explosion scenarios",
      "aliases": [
        "Build cognitive ROI models",
        "Formalise intelligence explosion economics"
      ],
      "type": "intervention",
      "description": "Conduct foundational research to create and analyse formal ROI curves and publish open models to inform policy and safety planning.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "Research modelling does not sit within a single system-development phase; referenced under 'Intelligence Explosion Microeconomics' (newsletter main section).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Only theoretical frameworks proposed; no systematic validation yet (newsletter).",
      "node_rationale": "Primary actionable outcome suggested by the technical report."
    },
    {
      "name": "Misaligned goal systems due to orthogonality and convergent instrumental pressures",
      "aliases": [
        "Orthogonality-convergence misalignment",
        "Goal misalignment in AGI"
      ],
      "type": "concept",
      "description": "Because intelligence and goals are orthogonal, highly capable systems may pursue practically any objective, and convergent drives can lead them away from human values.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explicit in 'Five Theses' (orthogonality, convergent instrumental goals)."
    },
    {
      "name": "Human value complexity and fragility require indirect normativity",
      "aliases": [
        "Indirect normativity necessity",
        "Value fragility insight"
      ],
      "type": "concept",
      "description": "Because accurately encoding human values is complex and fragile, AI systems should learn or infer normative targets indirectly rather than via explicit hard-coding.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Given as a lemma in Yudkowsky’s mission statement."
    },
    {
      "name": "Friendly AI architectures with stable self-modification and indirect normativity",
      "aliases": [
        "Stable self-improving Friendly AI approach"
      ],
      "type": "concept",
      "description": "Design rationale advocating architectures that remain aligned while self-modifying by grounding goal systems in indirect representations of human values.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Strategic implication #1 in 'Five Theses' section."
    },
    {
      "name": "Timeless Decision Theory for reflective stability in AGI",
      "aliases": [
        "TDT in self-modifying agents",
        "Timeless Decision Theory mechanism"
      ],
      "type": "concept",
      "description": "Implementation mechanism using TDT to allow agents to reason about self-modification and commitments without time-based inconsistencies.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Newsletter announces Altair’s precise formulation of TDT for this purpose."
    },
    {
      "name": "Empirical comparison of decision algorithms on Newcomblike problems",
      "aliases": [
        "Decision algorithm benchmark",
        "TDT comparative study"
      ],
      "type": "concept",
      "description": "Validation study showing how TDT and alternative decision procedures perform under scenarios with predictive correlations (Newcomblike problems).",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Reported in 'Timeless Decision Theory Paper Published' section."
    },
    {
      "name": "Research and integrate TDT-based stable decision frameworks in AGI design",
      "aliases": [
        "Implement TDT in AGI",
        "TDT-based alignment research"
      ],
      "type": "intervention",
      "description": "Advance theoretical and experimental work to embed Timeless Decision Theory or successors into self-modifying AGI architectures to preserve alignment.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Concerns the high-level choice of decision-making model during AGI architecture design.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Initial formalisation exists (Altair 2013) but no large-scale prototypes.",
      "node_rationale": "Direct actionable takeaway from Altair’s publication."
    },
    {
      "name": "Strategic mis-preparedness for emergence of AGI",
      "aliases": [
        "Insufficient preparedness for AGI arrival",
        "Readiness gap"
      ],
      "type": "concept",
      "description": "Human institutions may fail to act in time if AGI arrives sooner than expected or with unexpected properties.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implied in 'When Will AI Be Created?' discussion of uncertainty and need for better forecasts."
    },
    {
      "name": "Forecasting uncertainty hinders preparedness for AGI timelines",
      "aliases": [
        "AI timeline uncertainty insight"
      ],
      "type": "concept",
      "description": "Recognition that large epistemic uncertainty about when AI will be created prevents effective strategic planning.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Luke Muehlhauser’s blog post outlines this difficulty."
    },
    {
      "name": "Crowdsourced probabilistic forecasting platforms for AI timelines",
      "aliases": [
        "Forecast aggregation rationale"
      ],
      "type": "concept",
      "description": "Design rationale to reduce uncertainty by aggregating many quantified expert predictions using structured online platforms.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposed under methods 'explicit quantification' and 'leveraging aggregation'."
    },
    {
      "name": "DAGGRE science & technology forecasting platform participation",
      "aliases": [
        "DAGGRE platform mechanism"
      ],
      "type": "concept",
      "description": "Implementation mechanism involving the George Mason University DAGGRE prediction market where participants post and update probabilistic forecasts.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Newsletter invites readers to sign up for DAGGRE."
    },
    {
      "name": "Aggregation methods reduce prediction error in other domains",
      "aliases": [
        "Forecast aggregation evidence"
      ],
      "type": "concept",
      "description": "Empirical evidence from previous forecasting tournaments shows that aggregated crowd predictions outperform individuals.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Inference from common knowledge of Good Judgment Project; only referenced indirectly, marked low confidence."
    },
    {
      "name": "Engage AI researchers in DAGGRE platform to improve timeline forecasts",
      "aliases": [
        "DAGGRE participation intervention"
      ],
      "type": "intervention",
      "description": "Actively recruit AI domain experts to submit and update probabilistic predictions on DAGGRE questions related to AGI development timelines.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "Policy/community engagement beyond system development stages.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Platform exists but has limited AI-specific participation (newsletter call-to-action).",
      "node_rationale": "Actionable step suggested by Muehlhauser."
    },
    {
      "name": "Insufficient domain expertise in AI safety research",
      "aliases": [
        "Expertise gap in AI safety"
      ],
      "type": "concept",
      "description": "Current research efforts lack sufficient experts in key technical and analytical domains, slowing progress toward safe AGI.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined in 'AGI Impacts Experts and Friendly AI Experts' and 'MIRI Needs Advisors'."
    },
    {
      "name": "Cultivating AGI impacts and Friendly AI experts improves research quality",
      "aliases": [
        "Expert cultivation insight"
      ],
      "type": "concept",
      "description": "Building pools of experts who understand AGI impacts or Friendly AI maths is theorised to accelerate safety research and strategic planning.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explicit goal in MIRI blog post on expert types."
    },
    {
      "name": "Recruit domain-specific advisors and volunteers",
      "aliases": [
        "Advisor recruitment rationale"
      ],
      "type": "concept",
      "description": "Design rationale proposing systematic outreach to academics and professionals to provide feedback and specialised knowledge.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mentioned under 'MIRI Needs Advisors'."
    },
    {
      "name": "MIRI advisory network and research workshops",
      "aliases": [
        "Volunteer advisor mechanism",
        "MIRI workshops mechanism"
      ],
      "type": "concept",
      "description": "Implementation mechanism whereby advisors review drafts and attend focused workshops to generate new formal results.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Newsletter references April workshop and advisor programme."
    },
    {
      "name": "Increased publications resulting from expert involvement",
      "aliases": [
        "Advisor contribution evidence"
      ],
      "type": "concept",
      "description": "Observed rise in technical reports and papers following workshops and advisor feedback, suggesting positive impact of expert network.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Inferred from multiple new publications listed; confidence marked low."
    },
    {
      "name": "Recruit domain-specific advisors to MIRI advisory network",
      "aliases": [
        "Advisor recruitment intervention"
      ],
      "type": "intervention",
      "description": "Systematic outreach campaign asking researchers in logic, AI, economics, etc. to sign up as volunteer advisors, review drafts and answer queries.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "Organisational capacity building outside model development pipeline.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Active programme exists but limited formal evaluation.",
      "node_rationale": "Direct call for action in 'MIRI Needs Advisors'."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Existential catastrophe from unaligned artificial general intelligence",
      "target_node": "Runaway intelligence explosion via recursive self-improvement",
      "description": "Uncontrolled self-improvement can quickly surpass human oversight, leading to misaligned AGI catastrophe.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Widely argued in cited papers; newsletter summarises consensus but no new data.",
      "edge_rationale": "Stated in 'Intelligence Explosion Microeconomics' abstract."
    },
    {
      "type": "addressed_by",
      "source_node": "Runaway intelligence explosion via recursive self-improvement",
      "target_node": "Returns on cognitive reinvestment determine self-improvement dynamics",
      "description": "Understanding ROI on cognition directly tackles whether an explosion will occur.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit framing by Yudkowsky.",
      "edge_rationale": "Technical report section on 'key quantitative issue'."
    },
    {
      "type": "enabled_by",
      "source_node": "Returns on cognitive reinvestment determine self-improvement dynamics",
      "target_node": "Formal microeconomic modeling of cognitive reinvestment returns",
      "description": "Modelling is proposed as the means to make ROI hypotheses precise.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct proposal in report.",
      "edge_rationale": "Report’s conclusion calling for formal ROI curves."
    },
    {
      "type": "implemented_by",
      "source_node": "Formal microeconomic modeling of cognitive reinvestment returns",
      "target_node": "Intelligence explosion microeconomic ROI curve formalization",
      "description": "ROI curve formalisation operationalises the modelling approach.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Implementation described in detail within report.",
      "edge_rationale": "Sections detailing mathematical curves."
    },
    {
      "type": "validated_by",
      "source_node": "Intelligence explosion microeconomic ROI curve formalization",
      "target_node": "Historical technological growth trends constrain ROI models",
      "description": "Empirical trends are used to falsify or support particular ROI curves.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Referenced but not rigorously tested in newsletter.",
      "edge_rationale": "Report lists examples like Moore’s Law."
    },
    {
      "type": "motivates",
      "source_node": "Historical technological growth trends constrain ROI models",
      "target_node": "Develop quantitative ROI models to predict intelligence explosion scenarios",
      "description": "Evidence motivates further refined quantitative modelling work.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Motivation logic, not experiment.",
      "edge_rationale": "Report calls for next steps."
    },
    {
      "type": "caused_by",
      "source_node": "Existential catastrophe from unaligned artificial general intelligence",
      "target_node": "Misaligned goal systems due to orthogonality and convergent instrumental pressures",
      "description": "Misaligned goals are a direct mechanism by which AGI could threaten humanity.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Orthogonality/convergence widely accepted in literature.",
      "edge_rationale": "'Five Theses' explanation."
    },
    {
      "type": "addressed_by",
      "source_node": "Misaligned goal systems due to orthogonality and convergent instrumental pressures",
      "target_node": "Human value complexity and fragility require indirect normativity",
      "description": "Insight frames a potential solution pathway to misalignment.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical argument in Yudkowsky post.",
      "edge_rationale": "Lemma on indirect normativity."
    },
    {
      "type": "enabled_by",
      "source_node": "Human value complexity and fragility require indirect normativity",
      "target_node": "Friendly AI architectures with stable self-modification and indirect normativity",
      "description": "Design rationale builds on the need for indirect normativity.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit strategic implication.",
      "edge_rationale": "'Five Theses' strategic implications."
    },
    {
      "type": "implemented_by",
      "source_node": "Friendly AI architectures with stable self-modification and indirect normativity",
      "target_node": "Timeless Decision Theory for reflective stability in AGI",
      "description": "TDT is presented as a candidate mechanism for stable self-modification.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Suggested by MIRI publications.",
      "edge_rationale": "Newsletter section on TDT paper."
    },
    {
      "type": "validated_by",
      "source_node": "Timeless Decision Theory for reflective stability in AGI",
      "target_node": "Empirical comparison of decision algorithms on Newcomblike problems",
      "description": "Altair’s study provides initial evidence on TDT behaviour.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Paper contains formal comparisons.",
      "edge_rationale": "Newsletter summary."
    },
    {
      "type": "motivates",
      "source_node": "Empirical comparison of decision algorithms on Newcomblike problems",
      "target_node": "Research and integrate TDT-based stable decision frameworks in AGI design",
      "description": "Results encourage further TDT-based AGI design research.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical progression from pilot evidence to design research.",
      "edge_rationale": "Implication of paper publication."
    },
    {
      "type": "caused_by",
      "source_node": "Existential catastrophe from unaligned artificial general intelligence",
      "target_node": "Strategic mis-preparedness for emergence of AGI",
      "description": "Lack of preparedness increases likelihood that catastrophic outcomes are realised.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference from newsletter discussion.",
      "edge_rationale": "'When Will AI Be Created?' section."
    },
    {
      "type": "addressed_by",
      "source_node": "Strategic mis-preparedness for emergence of AGI",
      "target_node": "Forecasting uncertainty hinders preparedness for AGI timelines",
      "description": "Identifies uncertainty as root cause of mis-preparedness.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual link outlined by Muehlhauser.",
      "edge_rationale": "Blog post analysis."
    },
    {
      "type": "enabled_by",
      "source_node": "Forecasting uncertainty hinders preparedness for AGI timelines",
      "target_node": "Crowdsourced probabilistic forecasting platforms for AI timelines",
      "description": "Crowdsourced platforms are proposed to reduce uncertainty.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Methodology recommended explicitly.",
      "edge_rationale": "Four methods listed by Muehlhauser."
    },
    {
      "type": "implemented_by",
      "source_node": "Crowdsourced probabilistic forecasting platforms for AI timelines",
      "target_node": "DAGGRE science & technology forecasting platform participation",
      "description": "DAGGRE is the concrete platform embodying the design rationale.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Newsletter provides sign-up link.",
      "edge_rationale": "Sign-up call."
    },
    {
      "type": "validated_by",
      "source_node": "DAGGRE science & technology forecasting platform participation",
      "target_node": "Aggregation methods reduce prediction error in other domains",
      "description": "Historical success of aggregation gives support to DAGGRE approach.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Empirical support inferred, not detailed.",
      "edge_rationale": "General forecasting literature implied."
    },
    {
      "type": "motivates",
      "source_node": "Aggregation methods reduce prediction error in other domains",
      "target_node": "Engage AI researchers in DAGGRE platform to improve timeline forecasts",
      "description": "Positive evidence motivates recruiting AI researchers.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference from existing forecasting successes.",
      "edge_rationale": "Newsletter invitation."
    },
    {
      "type": "caused_by",
      "source_node": "Existential catastrophe from unaligned artificial general intelligence",
      "target_node": "Insufficient domain expertise in AI safety research",
      "description": "Lack of expertise hampers development of safety solutions, increasing catastrophic risk.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Reasonable inference, not quantitatively proven.",
      "edge_rationale": "AGI impacts/Friendly expert posts."
    },
    {
      "type": "addressed_by",
      "source_node": "Insufficient domain expertise in AI safety research",
      "target_node": "Cultivating AGI impacts and Friendly AI experts improves research quality",
      "description": "Insight proposes cultivating experts to fix expertise gap.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit premise of blog post.",
      "edge_rationale": "MIRI blog post on experts."
    },
    {
      "type": "enabled_by",
      "source_node": "Cultivating AGI impacts and Friendly AI experts improves research quality",
      "target_node": "Recruit domain-specific advisors and volunteers",
      "description": "Recruitment programme operationalises expert cultivation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct recommendation in 'MIRI Needs Advisors'.",
      "edge_rationale": "Newsletter section."
    },
    {
      "type": "implemented_by",
      "source_node": "Recruit domain-specific advisors and volunteers",
      "target_node": "MIRI advisory network and research workshops",
      "description": "Advisor network and workshops are concrete mechanisms to involve experts.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Programme already running, referenced with examples.",
      "edge_rationale": "April workshop photos and advisor sign-up."
    },
    {
      "type": "validated_by",
      "source_node": "MIRI advisory network and research workshops",
      "target_node": "Increased publications resulting from expert involvement",
      "description": "Observed surge in output offers preliminary validation.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Correlation inferred; no formal study.",
      "edge_rationale": "Newsletter lists multiple new publications post-workshop."
    },
    {
      "type": "motivates",
      "source_node": "Increased publications resulting from expert involvement",
      "target_node": "Recruit domain-specific advisors to MIRI advisory network",
      "description": "Positive outcomes encourage further recruitment.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical motivation; limited data.",
      "edge_rationale": "Call for more advisors."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2013-05-30T18:48:11Z"
    },
    {
      "key": "url",
      "value": "https://intelligence.org/2013/05/30/miri-may-newsletter-intelligence-explosion-microeconomics-and-other-publications/"
    },
    {
      "key": "title",
      "value": "MIRI May Newsletter: Intelligence Explosion Microeconomics and Other Publications"
    },
    {
      "key": "authors",
      "value": [
        "Jake"
      ]
    },
    {
      "key": "source",
      "value": "blogs"
    }
  ]
}