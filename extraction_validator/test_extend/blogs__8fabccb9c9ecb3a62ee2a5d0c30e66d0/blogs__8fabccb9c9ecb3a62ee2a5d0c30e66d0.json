{
  "nodes": [
    {
      "name": "Existential catastrophe from misaligned superintelligent AI",
      "aliases": [
        "AI-induced existential risk",
        "Unaligned AGI catastrophic risk"
      ],
      "type": "concept",
      "description": "The possibility that advanced artificial intelligence, if its objectives diverge from human values, could bring about irreversible harm or extinction.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Top-level risk repeatedly referenced by MIRI and the newsletter’s ‘long-term impact’ framing (General updates)."
    },
    {
      "name": "Convergent instrumental goal pursuit by advanced agents",
      "aliases": [
        "Convergent instrumental goals",
        "Power-seeking tendencies"
      ],
      "type": "concept",
      "description": "The tendency of sufficiently capable goal-directed systems to seek resources, self-preservation, and goal-content integrity, regardless of their final objectives.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central mechanism discussed in ‘Formalizing Convergent Instrumental Goals’ (Research updates)."
    },
    {
      "name": "Extreme optimisation behaviour from utility maximiser design",
      "aliases": [
        "Over-maximisation pressure",
        "Pure maximiser risk"
      ],
      "type": "concept",
      "description": "Classical expected-utility maximisers push probability mass toward very high-utility actions, often ignoring side-effects and safety constraints, creating dangerous behaviour.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Identified as the key failure mode addressed by the Quantilizer paper (Research updates)."
    },
    {
      "name": "Formal characterisation of convergent instrumental goals",
      "aliases": [
        "CIG formalisation",
        "Mathematical CIG analysis"
      ],
      "type": "concept",
      "description": "A theoretical framework proving when and why power-seeking subgoals arise from broad classes of objective functions.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Presented in the accepted paper; provides groundwork for analysing incentive structures (Research updates)."
    },
    {
      "name": "Quantilizer decision rule reducing optimisation pressure",
      "aliases": [
        "Quantilization concept",
        "Top-quantile optimiser"
      ],
      "type": "concept",
      "description": "A theoretical insight that selecting actions at random from the top q-quantile of a reference distribution can greatly reduce catastrophic optimisation side-effects.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Core contribution of ‘Quantilizers’ paper (Research updates)."
    },
    {
      "name": "Limiting objective maximisation via top-quantile action selection",
      "aliases": [
        "Bounded optimisation design",
        "Quantile-limited planning"
      ],
      "type": "concept",
      "description": "Design rationale to cap optimisation strength by restricting the agent’s search to a safe subset of actions identified through a benign distribution.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Directly follows from quantilizer theory, offering a practical safety-oriented design (Research updates)."
    },
    {
      "name": "Using formal proofs to anticipate and mitigate power-seeking incentives",
      "aliases": [
        "Proof-based incentive control",
        "Formal safety guarantees"
      ],
      "type": "concept",
      "description": "Employ mathematical reasoning to predict when an agent will adopt instrumental goals and design countermeasures accordingly.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivated by the ‘Formalizing CIG’ paper and other MIRI logic work (IAFF corrigibility discussions)."
    },
    {
      "name": "Quantilizer sampling from benign action reference distribution",
      "aliases": [
        "Quantilizer algorithm implementation",
        "Reference-distribution sampling"
      ],
      "type": "concept",
      "description": "An implementation mechanism where the agent draws actions from a distribution known to have acceptable average behaviour, then filters to the top q-fraction according to its utility.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete technique proposed by the Quantilizer paper enabling bounded optimisation (Research updates)."
    },
    {
      "name": "Formal agent modelling with logical frameworks for corrigibility",
      "aliases": [
        "Reflective oracle based models",
        "Logic-based corrigibility mechanisms"
      ],
      "type": "concept",
      "description": "Implementation work using logical oracles and self-reflective formalisms to build agents that remain open to correction without incentive to resist.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Referenced by IAFF posts on corrigibility and reflective oracles (News and links)."
    },
    {
      "name": "Theoretical proofs of bounded side-effects in quantilizer paper",
      "aliases": [
        "Quantilizer safety proofs",
        "Quantilization theoretical validation"
      ],
      "type": "concept",
      "description": "Mathematical results showing that quantilizers dramatically lower worst-case harm compared with pure maximisers, under certain distributional assumptions.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Provides evidence motivating adoption of the quantilizer design (Research updates)."
    },
    {
      "name": "Mathematical characterisation results in convergent goal formalisation paper",
      "aliases": [
        "CIG validation proofs",
        "Formal CIG results"
      ],
      "type": "concept",
      "description": "Demonstrates, through formal theorems, conditions under which instrumental power-seeking arises, validating theoretical claims.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Supports the use of formal analysis tools in safety assessment (Research updates)."
    },
    {
      "name": "Adopt quantilizer decision rule in AI system design",
      "aliases": [
        "Implement quantilizers",
        "Quantilizer-based AI design"
      ],
      "type": "intervention",
      "description": "During model-design and planning modules, replace pure expected-utility maximisers with quantilizers that restrict optimisation to the safest q-quantile of actions from a vetted distribution.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Applies at initial algorithm-design phase before any training (implicit in Quantilizers paper context, Research updates).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Currently a theoretical/PoC proposal with no large-scale deployment; paper acceptance only (Research updates).",
      "node_rationale": "Primary actionable proposal to mitigate over-maximisation risks."
    },
    {
      "name": "Integrate convergent instrumental goal analysis into safety assessment tools",
      "aliases": [
        "Use CIG formalism for safety audits",
        "CIG-based incentive analysis"
      ],
      "type": "intervention",
      "description": "Embed the formal CIG framework into automated or manual assessment pipelines to detect and redesign systems that would predictably develop power-seeking subgoals.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Deployed during pre-deployment testing/audit to evaluate trained models (implied use case, General updates).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Methodological concept with preliminary proofs; tools not yet standardised (Research updates).",
      "node_rationale": "Translates theoretical CIG work into a concrete safety assessment practice."
    },
    {
      "name": "Provide dedicated funding and institutional support for AI safety research centres",
      "aliases": [
        "Fund AI safety centres",
        "Establish safety research institutes"
      ],
      "type": "intervention",
      "description": "Governments, charities and universities allocate sustained budgets (e.g., Leverhulme Centre, Strategic AI Research Centre) to advance corrigibility, governance, and technical alignment research.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "Policy/strategic action outside direct model lifecycle, focused on ecosystem-level support (News and links).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Operational centres with active staff and projects announced, but long-term efficacy still under evaluation (News and links).",
      "node_rationale": "Addresses safety through resource provision enabling the above technical work."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Existential catastrophe from misaligned superintelligent AI",
      "target_node": "Convergent instrumental goal pursuit by advanced agents",
      "description": "Power-seeking subgoals are a primary pathway by which unaligned AI could gain control and cause catastrophe.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Widely-cited reasoning; implicit in CIG paper summary (Research updates).",
      "edge_rationale": "Newsletter positions CIG formalisation as addressing existential risk."
    },
    {
      "type": "caused_by",
      "source_node": "Convergent instrumental goal pursuit by advanced agents",
      "target_node": "Extreme optimisation behaviour from utility maximiser design",
      "description": "Pure maximisers create strong incentives that manifest as instrumental goal pursuit.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Theoretical argument central to both cited papers.",
      "edge_rationale": "Quantilizer paper frames maximisation as root driver of problematic goals."
    },
    {
      "type": "mitigated_by",
      "source_node": "Extreme optimisation behaviour from utility maximiser design",
      "target_node": "Quantilizer decision rule reducing optimisation pressure",
      "description": "Quantilizers dial down optimisation to avoid extreme actions.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Claim formalised in Quantilizer paper.",
      "edge_rationale": "Research updates highlight quantilizers as ‘safer alternative’."
    },
    {
      "type": "enabled_by",
      "source_node": "Formal characterisation of convergent instrumental goals",
      "target_node": "Using formal proofs to anticipate and mitigate power-seeking incentives",
      "description": "Formal understanding underpins proof-based mitigation strategies.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Connection inferred; newsletter implies but doesn’t detail.",
      "edge_rationale": "CIG paper provides mathematical toolset that design rationale relies on."
    },
    {
      "type": "enabled_by",
      "source_node": "Quantilizer decision rule reducing optimisation pressure",
      "target_node": "Limiting objective maximisation via top-quantile action selection",
      "description": "Insight supplies the principle for bounded optimisation design.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Directly stated in Quantilizer paper abstract.",
      "edge_rationale": "Newsletter summary positions quantilizers as the safer design."
    },
    {
      "type": "implemented_by",
      "source_node": "Limiting objective maximisation via top-quantile action selection",
      "target_node": "Quantilizer sampling from benign action reference distribution",
      "description": "Sampling procedure operationalises the design rationale.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Algorithm described in paper.",
      "edge_rationale": "‘Safer alternative’ claims rest on this mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Quantilizer sampling from benign action reference distribution",
      "target_node": "Theoretical proofs of bounded side-effects in quantilizer paper",
      "description": "Safety proofs test and support the mechanism’s effectiveness.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Contains mathematical theorems; strong theoretical evidence.",
      "edge_rationale": "Validation evidence node directly quotes paper results."
    },
    {
      "type": "validated_by",
      "source_node": "Using formal proofs to anticipate and mitigate power-seeking incentives",
      "target_node": "Mathematical characterisation results in convergent goal formalisation paper",
      "description": "Proofs show when instrumental goals arise, validating the approach.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Peer-reviewed mathematical results.",
      "edge_rationale": "Newsletter notes acceptance to AAAI workshop."
    },
    {
      "type": "motivates",
      "source_node": "Theoretical proofs of bounded side-effects in quantilizer paper",
      "target_node": "Adopt quantilizer decision rule in AI system design",
      "description": "Theoretical validation encourages practitioners to implement quantilizers.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Evidence sufficient for experimental adoption, though not yet deployed.",
      "edge_rationale": "Paper framed as ‘safer alternative’. "
    },
    {
      "type": "motivates",
      "source_node": "Mathematical characterisation results in convergent goal formalisation paper",
      "target_node": "Integrate convergent instrumental goal analysis into safety assessment tools",
      "description": "Formal results provide an actionable analytic framework for safety audits.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implementation idea inferred from application potential.",
      "edge_rationale": "Newsletter implies practical value of formalisation."
    },
    {
      "type": "addressed_by",
      "source_node": "Existential catastrophe from misaligned superintelligent AI",
      "target_node": "Provide dedicated funding and institutional support for AI safety research centres",
      "description": "Funding enables continued research to develop and implement safety solutions.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Newsletter explicitly links new centres to long-term AI impact.",
      "edge_rationale": "‘Future of Intelligence’ centre launched to study long-term impact."
    },
    {
      "type": "required_by",
      "source_node": "Adopt quantilizer decision rule in AI system design",
      "target_node": "Provide dedicated funding and institutional support for AI safety research centres",
      "description": "Research centres supply resources necessary to refine and test quantilizer implementations.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical dependency inferred; funding enables R&D.",
      "edge_rationale": "Centres host technical alignment projects."
    },
    {
      "type": "required_by",
      "source_node": "Integrate convergent instrumental goal analysis into safety assessment tools",
      "target_node": "Provide dedicated funding and institutional support for AI safety research centres",
      "description": "Institutional support needed to develop assessment tooling and standards.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference based on typical research funding cycles.",
      "edge_rationale": "Newsletter emphasises institutional role."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2015-12-03T23:04:27Z"
    },
    {
      "key": "url",
      "value": "https://intelligence.org/2015/12/03/december-2015-newsletter/"
    },
    {
      "key": "title",
      "value": "December 2015 Newsletter"
    },
    {
      "key": "authors",
      "value": [
        "Rob Bensinger"
      ]
    },
    {
      "key": "source",
      "value": "blogs"
    }
  ]
}