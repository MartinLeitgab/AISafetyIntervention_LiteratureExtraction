{
  "nodes": [
    {
      "name": "Cryptographic vulnerability from integer factoring capability growth",
      "aliases": [
        "RSA-breaking risk from improved factoring",
        "Threat to encryption from larger numbers being factored"
      ],
      "type": "concept",
      "description": "Risk that confidential data encrypted with RSA or similar schemes becomes readable because adversaries can factor the underlying large semiprimes.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced implicitly in Background and Relevance sections as the motivating security concern behind tracking factoring progress."
    },
    {
      "name": "RSA security dependence on integer factoring hardness",
      "aliases": [
        "RSA relies on factoring difficulty",
        "Encryption strength tied to factoring infeasibility"
      ],
      "type": "concept",
      "description": "The security of widely-used RSA cryptography is premised on the assumption that factoring large semiprimes is computationally infeasible with general-purpose algorithms.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explicitly stated in Background: knowledge of factoring difficulty informs cryptographic key choices."
    },
    {
      "name": "Steady growth in factorable integer size using general purpose algorithms",
      "aliases": [
        "Increasing digits factored annually",
        "Factoring capability trend"
      ],
      "type": "concept",
      "description": "Historical records show that the largest number factored has risen from ~20 digits in 1970 to 232 digits in 2009, averaging 4.5–6 additional digits per year.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Extracted from Digits-by-year and Trends figures indicating the empirical trajectory that threatens RSA security."
    },
    {
      "name": "Combined hardware and algorithm improvements driving factoring progress",
      "aliases": [
        "Hardware-software synergy in factoring",
        "Algorithmic plus computing advances increase factoring ability"
      ],
      "type": "concept",
      "description": "CPU-time requirements for record factorizations dropped roughly in line with a 10,000-fold increase in available computing and several algorithmic innovations.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Hardware inputs section quantifies the shared contribution of cheaper computation and better methods."
    },
    {
      "name": "Projection-based cryptographic key length policy updates",
      "aliases": [
        "Key-size planning via factoring forecasts",
        "Using growth trends to set RSA key lengths"
      ],
      "type": "concept",
      "description": "Approach: extrapolate factoring progress to determine when existing key lengths become unsafe and plan timely upgrades.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implied in Relevance and Further-research sections discussing the need to anticipate future record sizes."
    },
    {
      "name": "Empirical trend extrapolation from factoring record data",
      "aliases": [
        "Curve-fitting factoring records",
        "Data-driven forecasting of factoring"
      ],
      "type": "concept",
      "description": "Technique: collect historical digit-record data, fit growth curves (e.g., 4.5 digits/year) and project forward for risk assessment.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Directly follows from the quantitative plots in Digits-by-year and Hardware-inputs sections."
    },
    {
      "name": "Historical factoring record analysis showing smooth 4.5 digits/year and occasional jumps",
      "aliases": [
        "Validation of factoring growth rate",
        "Observed smooth yet jumpy factoring progress"
      ],
      "type": "concept",
      "description": "Validation evidence: thirteen digit records between 1988–2009 display an average 4.5–6 digit gain per year with a maximum single-jump of 32 digits.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Compiled in Trends and Digits-by-year figures; provides empirical support for projection mechanisms."
    },
    {
      "name": "Increase RSA key lengths ahead of projected factoring breakthroughs",
      "aliases": [
        "Proactively lengthen RSA keys",
        "Pre-emptive RSA key upgrades"
      ],
      "type": "intervention",
      "description": "Action: escalate minimum RSA modulus sizes (e.g., from 2048 to 4096 bits) before trend extrapolations indicate vulnerability.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Applies at Deployment time when systems are configured or reconfigured (‘Background’ and ‘Relevance’ suggest immediate operational changes).",
      "intervention_maturity": "4",
      "intervention_maturity_rationale": "Widely practised security measure; operational industry standards already recommend periodic key-size increases.",
      "node_rationale": "Logical, widely-recognised mitigation for the documented risk."
    },
    {
      "name": "Adopt post-quantum cryptographic algorithms to eliminate factoring vulnerability",
      "aliases": [
        "Transition to PQC",
        "Use factoring-resistant encryption schemes"
      ],
      "type": "intervention",
      "description": "Replace RSA with lattice-based, hash-based or code-based schemes whose security does not rely on integer factoring.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Takes place during or before deployment when cryptographic libraries are selected (‘Further research’ hints at alternative schemes).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Standards are emerging (e.g., NIST PQC process) but large-scale adoption is still pilot/experimental.",
      "node_rationale": "Provides a more permanent mitigation that removes dependence on the risky factorisation assumption."
    },
    {
      "name": "AI safety planning failure from underestimated algorithmic progress",
      "aliases": [
        "Risk of surprise AI capability jumps",
        "Safety shortfall due to poor AI forecasting"
      ],
      "type": "concept",
      "description": "Risk that safety measures lag behind sudden advances in AI capabilities because forecasting models did not anticipate progress patterns.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Relevance section generalises factoring to broader AI progress, noting interest in forecasting jumps."
    },
    {
      "name": "Difficulty predicting algorithmic progress patterns",
      "aliases": [
        "Uncertain algorithmic trajectories",
        "Forecasting challenges for algorithms"
      ],
      "type": "concept",
      "description": "Problem: algorithmic performance improvements vary in smoothness and exhibit occasional large jumps, complicating accurate predictions.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted in Relevance: need to understand ‘how smooth or jumpy’ progress tends to be."
    },
    {
      "name": "Empirical datasets from factoring as analogy for AI progress forecasting",
      "aliases": [
        "Factoring trend as forecasting exemplar",
        "Using factoring data to study progress dynamics"
      ],
      "type": "concept",
      "description": "Theoretical insight that well-documented factoring progress can serve as a prototype dataset to study generic patterns of algorithmic improvement.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Relevance explicitly proposes factoring as a case study to inform expectations for other AI problems."
    },
    {
      "name": "Cross-domain trend analysis to improve AI capability forecasts",
      "aliases": [
        "Design of comparative progress studies",
        "Trend-based AI forecasting rationale"
      ],
      "type": "concept",
      "description": "Design principle: aggregate multiple algorithmic trend lines (starting with factoring) to build more reliable AI progress models.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Follows from theoretical insight and Relevance call for studying how measured performance relates to conceptual insights."
    },
    {
      "name": "Compilation of historical algorithmic progress datasets across domains",
      "aliases": [
        "Dataset collection for algorithm trends",
        "Building multi-domain progress records"
      ],
      "type": "concept",
      "description": "Implementation: gather and normalise records similar to the factoring database (e.g., Cunningham Project) for diverse AI-related tasks.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Further research lists collecting broader data (e.g., Cunningham Project) as a next step."
    },
    {
      "name": "Develop quantitative forecasting models for AI progress timelines",
      "aliases": [
        "Build AI capability growth models",
        "Quantitative AI timeline forecasting"
      ],
      "type": "intervention",
      "description": "Action: use compiled multi-domain datasets to create statistical or simulation models that project future AI capability increments and inform safety planning.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "This is a research activity preceding model design/pre-training phases, aligning with ‘Further research’.",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Foundational theoretical work; systematic validation largely absent.",
      "node_rationale": "Addresses the planning-failure risk by providing evidence-based timelines."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Cryptographic vulnerability from integer factoring capability growth",
      "target_node": "RSA security dependence on integer factoring hardness",
      "description": "RSA’s reliance on factoring hardness creates the avenue for vulnerabilities as capability grows.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Well-established cryptographic principle cited in Background.",
      "edge_rationale": "If RSA depends on factoring being hard, any weakening in that assumption directly causes vulnerability."
    },
    {
      "type": "caused_by",
      "source_node": "RSA security dependence on integer factoring hardness",
      "target_node": "Steady growth in factorable integer size using general purpose algorithms",
      "description": "As more digits can be factored, the foundational hardness assumption is progressively undermined.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical connection supported by Digits-by-year evidence.",
      "edge_rationale": "Growing factorable size erodes the margin RSA relies upon."
    },
    {
      "type": "caused_by",
      "source_node": "Steady growth in factorable integer size using general purpose algorithms",
      "target_node": "Combined hardware and algorithm improvements driving factoring progress",
      "description": "Underlying driver of the steady growth is a combination of better algorithms and cheaper hardware.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Hardware Inputs section quantifies both effects but lacks formal decomposition.",
      "edge_rationale": "Progress trend stems from these compounded improvements."
    },
    {
      "type": "mitigated_by",
      "source_node": "Combined hardware and algorithm improvements driving factoring progress",
      "target_node": "Projection-based cryptographic key length policy updates",
      "description": "Recognising the drivers enables designing policies that pre-emptively adjust key lengths.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Moderate inference; policy implication is not explicitly spelled out.",
      "edge_rationale": "Understanding progress drivers informs mitigation planning."
    },
    {
      "type": "implemented_by",
      "source_node": "Projection-based cryptographic key length policy updates",
      "target_node": "Empirical trend extrapolation from factoring record data",
      "description": "Trend-extrapolation is the concrete mechanism used to enact policy updates.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicitly suggested in Relevance and Further-research.",
      "edge_rationale": "Policy design is operationalised through data-driven projections."
    },
    {
      "type": "validated_by",
      "source_node": "Empirical trend extrapolation from factoring record data",
      "target_node": "Historical factoring record analysis showing smooth 4.5 digits/year and occasional jumps",
      "description": "Historical analysis confirms the accuracy of extrapolation methods on past data.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct quantitative evidence provided in Digits-by-year figure.",
      "edge_rationale": "Validation involves checking projections against known records."
    },
    {
      "type": "motivates",
      "source_node": "Historical factoring record analysis showing smooth 4.5 digits/year and occasional jumps",
      "target_node": "Increase RSA key lengths ahead of projected factoring breakthroughs",
      "description": "Empirical evidence motivates immediate key-size increases to stay ahead of the curve.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Common industry response; implicit in security community practice.",
      "edge_rationale": "Observed trend suggests when current keys become insufficient."
    },
    {
      "type": "motivates",
      "source_node": "Historical factoring record analysis showing smooth 4.5 digits/year and occasional jumps",
      "target_node": "Adopt post-quantum cryptographic algorithms to eliminate factoring vulnerability",
      "description": "Trend shows long-term unsustainability of RSA, motivating shift to PQC.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Moderate inference; PQC not mentioned explicitly.",
      "edge_rationale": "Continued progress implies eventual need to abandon factoring-based schemes."
    },
    {
      "type": "caused_by",
      "source_node": "AI safety planning failure from underestimated algorithmic progress",
      "target_node": "Difficulty predicting algorithmic progress patterns",
      "description": "Forecasting challenges directly cause planning failures.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "General reasoning from Relevance section.",
      "edge_rationale": "If prediction is difficult, plans are likely inadequate."
    },
    {
      "type": "mitigated_by",
      "source_node": "Difficulty predicting algorithmic progress patterns",
      "target_node": "Empirical datasets from factoring as analogy for AI progress forecasting",
      "description": "Using robust datasets can reduce uncertainty in predictions.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Relevance suggests factoring datasets help understand progress shapes.",
      "edge_rationale": "Data aids modelling and reduces difficulty."
    },
    {
      "type": "enabled_by",
      "source_node": "Empirical datasets from factoring as analogy for AI progress forecasting",
      "target_node": "Cross-domain trend analysis to improve AI capability forecasts",
      "description": "Factoring data provides the initial case enabling broader cross-domain analysis.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inferred link from dataset example to general method.",
      "edge_rationale": "Dataset serves as a prototype enabling the analysis approach."
    },
    {
      "type": "implemented_by",
      "source_node": "Cross-domain trend analysis to improve AI capability forecasts",
      "target_node": "Compilation of historical algorithmic progress datasets across domains",
      "description": "Building multi-domain datasets is the concrete implementation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Further-research explicitly calls for collecting more data.",
      "edge_rationale": "Analysis requires assembled datasets."
    },
    {
      "type": "validated_by",
      "source_node": "Compilation of historical algorithmic progress datasets across domains",
      "target_node": "Historical factoring record analysis showing smooth 4.5 digits/year and occasional jumps",
      "description": "Factoring record analysis acts as initial validation showing dataset utility.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Demonstrated in article; factoring dataset already yields insights.",
      "edge_rationale": "Successful insights from factoring validate the compilation approach."
    },
    {
      "type": "motivates",
      "source_node": "Historical factoring record analysis showing smooth 4.5 digits/year and occasional jumps",
      "target_node": "Develop quantitative forecasting models for AI progress timelines",
      "description": "Insights encourage building formal forecasting models for AI.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Moderate inference; article recommends related future work.",
      "edge_rationale": "Empirical success suggests extending to full forecasting models."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2017-03-16T11:03:21Z"
    },
    {
      "key": "url",
      "value": "https://aiimpacts.org/progress-in-general-purpose-factoring/"
    },
    {
      "key": "title",
      "value": "Progress in general purpose factoring"
    },
    {
      "key": "authors",
      "value": [
        "Katja Grace"
      ]
    },
    {
      "key": "source",
      "value": "blogs"
    }
  ]
}