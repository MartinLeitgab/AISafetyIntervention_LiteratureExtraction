{
  "nodes": [
    {
      "name": "Loss of longterm human potential from misaligned transformative AI",
      "aliases": [
        "existential catastrophe from AI",
        "long-term value loss due to AI"
      ],
      "type": "concept",
      "description": "Risk that advanced, misaligned AI systems undermine or eliminate humanity’s ability to realise vast future value, destroying most of the long-term potential.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Top-level risk motivating the entire discussion. (implicit throughout post)"
    },
    {
      "name": "Misprioritized AI safety interventions due to unclear argument specificity",
      "aliases": [
        "misallocation of AI safety resources",
        "sub-optimal cause prioritisation"
      ],
      "type": "concept",
      "description": "When stakeholders do not know whether arguments for AI risk are highly specific or general, they can fund or emphasise the wrong set of interventions.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Identified mechanism by which unclear discourse increases existential risk. (‘Why this distinction matters’)"
    },
    {
      "name": "Unclear specificity levels in AI risk arguments within discourse",
      "aliases": [
        "ambiguity between general vs specific AI risk arguments",
        "unclear threat-model granularity"
      ],
      "type": "concept",
      "description": "Participants often fail to indicate how detailed their AI-risk claims are, leading to confusion over assumptions and relevance.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Immediate cause of misprioritisation highlighted by the author. (‘The distinction’)"
    },
    {
      "name": "Wide disagreement on plausible AI threat models among experts",
      "aliases": [
        "divergent expert views on AI takeover scenarios",
        "uncertainty over AI failure modes"
      ],
      "type": "concept",
      "description": "Surveys and discussions show experts disagree on which AI-catastrophe scenarios are realistic.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Feeds into unclear discourse; explicitly cited via survey link. (‘Why this distinction matters’)"
    },
    {
      "name": "Overemphasis on specific alignment threat models in EA content",
      "aliases": [
        "alignment-centric default narrative",
        "focus on recursively self-improving super-intelligence"
      ],
      "type": "concept",
      "description": "Educational and outreach materials often present highly specific alignment failures as the default AI-risk story, sidelining broader arguments.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Specific structural bias the post wants corrected. (‘Why this distinction matters’)"
    },
    {
      "name": "Explicitly distinguishing general vs specific AI risk arguments improves intervention prioritization",
      "aliases": [
        "clarity on argument granularity aids decision-making",
        "argument-specificity insight"
      ],
      "type": "concept",
      "description": "Recognising and labelling the level of specificity of AI-risk arguments helps allocate resources across a wider or narrower intervention set as appropriate.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central claim offered as solution mechanism. (‘Why this distinction matters’)"
    },
    {
      "name": "Adopt clarity in argument specificity in AI safety discussions",
      "aliases": [
        "promote argument-specificity clarity",
        "encourage explicit threat-model labelling"
      ],
      "type": "concept",
      "description": "Design principle: encourage practitioners to state whether their claims are general or tied to a particular threat model.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Direct design response to theoretical insight. (‘Why this distinction matters’)"
    },
    {
      "name": "Taxonomy of argument specificity embedded in AI safety publications",
      "aliases": [
        "specificity tagging framework",
        "argument-specificity taxonomy"
      ],
      "type": "concept",
      "description": "Implementation approach: require authors to tag whether their argument is highly general, moderately specific, or threat-model-specific.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete way to achieve clarity goal. (inferred from design rationale)"
    },
    {
      "name": "Systematic expert surveys on AI threat scenarios",
      "aliases": [
        "regular AI-risk scenario surveys",
        "expert elicitations on AI takeover likelihood"
      ],
      "type": "concept",
      "description": "Implementation mechanism collecting probabilistic estimates of different AI threat models to inform argument specificity and prioritisation.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mechanism already partially in use (cited survey). (‘Survey on AI existential risk scenarios’ reference)"
    },
    {
      "name": "Survey on AI existential risk scenarios shows broad uncertainty",
      "aliases": [
        "2023 AI risk scenario survey results",
        "expert survey evidence of disagreement"
      ],
      "type": "concept",
      "description": "Validation evidence: published survey results reveal wide variance in expert judgments, demonstrating unclear consensus on threat models.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Empirical data that motivates further clarity measures. (‘Survey on AI existential risk scenarios’)"
    },
    {
      "name": "Observed confusion and miscommunication in AI risk discussions",
      "aliases": [
        "qualitative evidence of discourse confusion",
        "community communication issues"
      ],
      "type": "concept",
      "description": "Qualitative observation that conversations lump different argument types together, leading to misunderstandings.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Anecdotal but relevant validation supporting need for interventions. (‘Why this distinction matters’)"
    },
    {
      "name": "Mandate specificity taxonomy tagging for AI safety research papers",
      "aliases": [
        "require argument-specificity labels in publications"
      ],
      "type": "intervention",
      "description": "Policy requiring journals/conferences to include a standardised field indicating whether each AI-risk argument is general, moderately specific, or threat-model-specific.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "Governance/policy action applied to publication processes. (implicit policy inference from design section)",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "No existing mandates but feasible pilot policies; therefore experimental. (inferred)",
      "node_rationale": "Direct implementation of taxonomy mechanism to create clarity."
    },
    {
      "name": "Conduct regular expert surveys on AI threat model plausibility",
      "aliases": [
        "institutionalise AI-risk expert elicitation"
      ],
      "type": "intervention",
      "description": "Set up recurring, standardised surveys to track changing expert beliefs about diverse AI threat scenarios and feed results into prioritisation frameworks.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "Meta-research and governance activity outside model training pipeline. (reference: ‘Survey on AI existential risk scenarios’)",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Initial surveys exist; scaling to regular, systematic process constitutes pilot/ prototype level.",
      "node_rationale": "Operationalises the survey mechanism and supplies data for clarity."
    },
    {
      "name": "Update EA educational curricula to include generalist AI risk arguments",
      "aliases": [
        "broaden AI-risk narratives in EA fellowships"
      ],
      "type": "intervention",
      "description": "Revise reading lists and teaching materials so that introductory AI-risk courses present both general and specific arguments with explicit labelling.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "Educational content development; policy/coordination phase. (reference: ‘Why this distinction matters’)",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Proposed curricular changes have been trialled informally but lack formal adoption.",
      "node_rationale": "Addresses over-emphasis on specific threat models."
    },
    {
      "name": "Integrate argument specificity criteria into AI safety grantmaking frameworks",
      "aliases": [
        "grantmaking filters based on argument granularity"
      ],
      "type": "intervention",
      "description": "Funding bodies add explicit fields to proposals requiring applicants to state the specificity of their AI-risk theory of change, guiding portfolio balance.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "Applies to governance/resource-allocation phase.",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Currently purely conceptual; no pilots identified.",
      "node_rationale": "Ensures resource allocation aligns with clarified argument types."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Loss of longterm human potential from misaligned transformative AI",
      "target_node": "Misprioritized AI safety interventions due to unclear argument specificity",
      "description": "If interventions are misprioritised, the likelihood of losing long-term potential increases.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical causal chain stated implicitly; no empirical quantification. (‘Why this distinction matters’)",
      "edge_rationale": "Author argues that unclear prioritisation raises existential risk."
    },
    {
      "type": "caused_by",
      "source_node": "Misprioritized AI safety interventions due to unclear argument specificity",
      "target_node": "Unclear specificity levels in AI risk arguments within discourse",
      "description": "Misprioritisation stems from people not indicating where on the general–specific spectrum their arguments lie.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoning directly stated in text. (‘Why this distinction matters’)",
      "edge_rationale": "Author links confusion in argument type to intervention selection problems."
    },
    {
      "type": "caused_by",
      "source_node": "Unclear specificity levels in AI risk arguments within discourse",
      "target_node": "Wide disagreement on plausible AI threat models among experts",
      "description": "Disagreement on threat models feeds into ambiguity about what kind of argument is being made.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inferential connection; survey evidence signals disagreement but causal link is qualitative.",
      "edge_rationale": "Survey results show disagreement leading to unclear specificity."
    },
    {
      "type": "caused_by",
      "source_node": "Unclear specificity levels in AI risk arguments within discourse",
      "target_node": "Overemphasis on specific alignment threat models in EA content",
      "description": "Dominance of specific narratives obscures where arguments sit on specificity spectrum.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Qualitative inference from author’s observation.",
      "edge_rationale": "Post claims default narrative is overly specific, causing confusion."
    },
    {
      "type": "mitigated_by",
      "source_node": "Unclear specificity levels in AI risk arguments within discourse",
      "target_node": "Explicitly distinguishing general vs specific AI risk arguments improves intervention prioritization",
      "description": "Making the distinction directly addresses the confusion.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicitly recommended by author.",
      "edge_rationale": "Theoretical insight offered as remedy."
    },
    {
      "type": "enabled_by",
      "source_node": "Adopt clarity in argument specificity in AI safety discussions",
      "target_node": "Explicitly distinguishing general vs specific AI risk arguments improves intervention prioritization",
      "description": "Design principle derives from and operationalises the theoretical insight.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Straightforward deduction from text.",
      "edge_rationale": "Design states how to put the insight into practice."
    },
    {
      "type": "implemented_by",
      "source_node": "Adopt clarity in argument specificity in AI safety discussions",
      "target_node": "Taxonomy of argument specificity embedded in AI safety publications",
      "description": "Taxonomy provides concrete tooling for implementing clarity.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implementation inferred but directly follows design rationale.",
      "edge_rationale": "Taxonomy is obvious implementation of clarity design."
    },
    {
      "type": "implemented_by",
      "source_node": "Adopt clarity in argument specificity in AI safety discussions",
      "target_node": "Systematic expert surveys on AI threat scenarios",
      "description": "Regular surveys supply data necessary for clear argument classification and prioritisation.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implementation inferred but supported by survey example in text.",
      "edge_rationale": "Surveys inform argument specificity labels."
    },
    {
      "type": "validated_by",
      "source_node": "Taxonomy of argument specificity embedded in AI safety publications",
      "target_node": "Observed confusion and miscommunication in AI risk discussions",
      "description": "Observed confusion indicates need for taxonomy and can test its effectiveness once applied.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Qualitative evidence; weak but relevant.",
      "edge_rationale": "Confusion serves as both motivation and eventual test signal."
    },
    {
      "type": "validated_by",
      "source_node": "Systematic expert surveys on AI threat scenarios",
      "target_node": "Survey on AI existential risk scenarios shows broad uncertainty",
      "description": "Initial survey provides baseline evidence validating usefulness of systematic surveys.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Concrete survey results cited.",
      "edge_rationale": "Survey demonstrates mechanism’s feasibility and relevance."
    },
    {
      "type": "motivates",
      "source_node": "Observed confusion and miscommunication in AI risk discussions",
      "target_node": "Mandate specificity taxonomy tagging for AI safety research papers",
      "description": "Confusion motivates imposing a publication requirement to standardise clarity.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Motivation inferred but well-aligned with observation.",
      "edge_rationale": "Direct policy response to observed issue."
    },
    {
      "type": "motivates",
      "source_node": "Survey on AI existential risk scenarios shows broad uncertainty",
      "target_node": "Conduct regular expert surveys on AI threat model plausibility",
      "description": "Wide uncertainty suggests need for ongoing, regular expert elicitation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Survey explicitly demonstrates uncertainty; justification strong.",
      "edge_rationale": "Evidence points to benefit of repeat surveys."
    },
    {
      "type": "motivates",
      "source_node": "Observed confusion and miscommunication in AI risk discussions",
      "target_node": "Update EA educational curricula to include generalist AI risk arguments",
      "description": "Curricular change is driven by the need to correct discourse imbalance.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Reasonable policy inference from qualitative observation.",
      "edge_rationale": "Education shapes future discourse clarity."
    },
    {
      "type": "motivates",
      "source_node": "Survey on AI existential risk scenarios shows broad uncertainty",
      "target_node": "Integrate argument specificity criteria into AI safety grantmaking frameworks",
      "description": "Uncertainty across threat models indicates funders should explicitly track specificity when allocating resources.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference connecting evidence to funding practice.",
      "edge_rationale": "Evidence suggests portfolio diversification based on argument type."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://forum.effectivealtruism.org/posts/n5vRgmv3iBEe6Xh3P/general-vs-specific-arguments-for-the-longtermist-importance"
    },
    {
      "key": "title",
      "value": "General vs specific arguments for the longtermist importance of shaping AI development"
    },
    {
      "key": "date_published",
      "value": "2021-10-15T14:43:38Z"
    },
    {
      "key": "authors",
      "value": [
        "Sam Clarke"
      ]
    },
    {
      "key": "source",
      "value": "eaforum"
    }
  ]
}