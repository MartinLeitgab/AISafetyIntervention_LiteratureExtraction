Summary  
Data source overview: The article reviews six competing labels for advanced AI systems—frontier AI, advanced AI, general-purpose AI, AGI, TAI and super­intelligence—evaluating how each is perceived by non-technical audiences and policymakers. It argues that careful, audience-specific terminology is necessary for effective AI-risk governance and recommends concrete wording choices.

EXTRACTION CONFIDENCE[87]  
Inference strategy justification: The paper is purely about communication; no direct experimental data exists. I therefore treated the author’s comparative reasoning, examples of real-world term adoption, and expert quotations as “validation evidence” with medium strength (2-3).  
Extraction completeness explanation: All causal-interventional reasoning—from the governance-relevant catastrophic risk, through communication problems and theoretical insights, to specific wording interventions—has been captured; every node is linked, with no isolated satellites.  
Key limitations: • The source does not quantify outcomes, so evidence nodes rely on qualitative claims. • Interventions are communication practices, not technical controls, so lifecycle/maturity assignment required interpretation.  

Instruction-set improvement suggestion: Allow an optional “communication / policy” lifecycle category for interventions that do not fall neatly into the six model-development phases.