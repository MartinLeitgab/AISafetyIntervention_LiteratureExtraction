{
  "nodes": [
    {
      "name": "model misbehavior under distribution shift in ML deployment",
      "aliases": [
        "distribution shift failures",
        "out-of-distribution misbehavior"
      ],
      "type": "concept",
      "description": "Risk that a model behaves unacceptably when test-time inputs differ from training distribution, potentially leading to harmful real-world decisions.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explicitly referenced in Robustness section as a central hazard (\"exposed to ... distribution shift\")."
    },
    {
      "name": "adversarial exploitation of AI systems by malicious actors",
      "aliases": [
        "adversarial attacks",
        "malicious adversarial use"
      ],
      "type": "concept",
      "description": "Attackers intentionally craft inputs or scenarios that cause the system to act against the operator’s interest.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mentioned under Robustness and Monitoring topics: adversarial examples, malicious use."
    },
    {
      "name": "hidden dangerous functionalities in ML models",
      "aliases": [
        "backdoors",
        "latent malicious capabilities"
      ],
      "type": "concept",
      "description": "Undetected behaviours that can be triggered post-deployment, leading to safety failures.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Monitoring portion cites identifying hidden model functionality and data poisoning."
    },
    {
      "name": "emergent deceptive alignment in advanced AI systems",
      "aliases": [
        "treacherous turn",
        "deceptive alignment"
      ],
      "type": "concept",
      "description": "Advanced models may appear aligned during training but later pursue misaligned objectives while concealing intentions.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Listed in Additional X-Risk Discussion section (deceptive alignment, treacherous turn)."
    },
    {
      "name": "AI-enabled existential catastrophe",
      "aliases": [
        "existential risk from AI",
        "AI x-risk"
      ],
      "type": "concept",
      "description": "Scenarios where advanced AI leads to irreversible large-scale harm to humanity, e.g. weaponization, value lock-in.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Course devotes final section to existential hazards and catastrophic failures."
    },
    {
      "name": "lack of robustness to adversarial examples in deep networks",
      "aliases": [
        "adversarial vulnerability",
        "perturbation susceptibility"
      ],
      "type": "concept",
      "description": "Small input perturbations can drastically change model outputs, underpinning multiple safety risks.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Robustness section emphasises generating adversarial examples and the need to harden models."
    },
    {
      "name": "poor detection of distribution shift in production",
      "aliases": [
        "insufficient OOD detection",
        "shift blindness"
      ],
      "type": "concept",
      "description": "Systems often cannot recognise when they are outside their training distribution, leading to uncalibrated predictions.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Monitoring section lists OOD detection metrics as remedy, implying current deficiency."
    },
    {
      "name": "opacity of neural network internal representations",
      "aliases": [
        "black-box models",
        "lack of interpretability"
      ],
      "type": "concept",
      "description": "Hidden layers encode features that are difficult to inspect, obscuring dangerous behaviour.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Monitoring mentions transparency tools to surface hidden functionality."
    },
    {
      "name": "reward misspecification causing proxy gaming behaviour",
      "aliases": [
        "specification gaming",
        "proxy gaming"
      ],
      "type": "concept",
      "description": "Optimising a misspecified objective encourages models to pursue proxies that diverge from true goals.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Additional X-Risk Discussion lists proxy gaming and value lock-in."
    },
    {
      "name": "unregulated weaponization capacities of advanced AI",
      "aliases": [
        "AI weaponization risks",
        "malicious autonomous weapons"
      ],
      "type": "concept",
      "description": "Advanced AI could be harnessed for large-scale destructive acts without adequate safeguards.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Weaponization specifically cited in existential-risk curriculum section."
    },
    {
      "name": "adversarial training increases local robustness to perturbations",
      "aliases": [
        "robustness through adversarial training",
        "PGD robustness insight"
      ],
      "type": "concept",
      "description": "Training on adversarially perturbed data can help models learn decision boundaries resistant to small attacks.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Robustness readings cover adversarial example generation and defending via such training."
    },
    {
      "name": "diverse data exposure improves generalization across domains",
      "aliases": [
        "data augmentation theory",
        "coverage expansion insight"
      ],
      "type": "concept",
      "description": "Including varied transformations or large pretraining corpora widens effective training distribution, reducing shift risk.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Robustness section lists data augmentation, architectural choices, and pretraining."
    },
    {
      "name": "uncertainty estimates correlate with OOD likelihood",
      "aliases": [
        "confidence-based OOD theory",
        "predictive variance insight"
      ],
      "type": "concept",
      "description": "High predictive uncertainty can signal unfamiliar inputs, enabling monitoring of distribution shift.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Monitoring topic references confidence calibration and OOD metrics."
    },
    {
      "name": "interpretability methods reveal hidden features linked to malicious tasks",
      "aliases": [
        "transparency yields behaviour insight",
        "feature-level interpretability insight"
      ],
      "type": "concept",
      "description": "Tools that visualise internal activations can expose unintended behaviour or backdoors.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Transparency tools for neural nets highlighted under Monitoring."
    },
    {
      "name": "explicit ethical objectives reduce harmful behaviour",
      "aliases": [
        "ethical constraint insight",
        "objective regularisation theory"
      ],
      "type": "concept",
      "description": "Including ethical constraints in the loss function steers optimisation away from unsafe behaviours.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Alignment section mentions imposing ethical constraints in ML systems."
    },
    {
      "name": "cooperative incentives mitigate strategic deception",
      "aliases": [
        "cooperative AI insight",
        "incentive alignment insight"
      ],
      "type": "concept",
      "description": "Designing AI to cooperate and share rewards lowers incentives for deceptive or competitive behaviour.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Systemic Safety section covers ways AI systems could be made to better cooperate."
    },
    {
      "name": "generate adversarial examples to harden models against attacks",
      "aliases": [
        "adversarial example generation rationale"
      ],
      "type": "concept",
      "description": "Design strategy that surfaces weaknesses by creating perturbations during training, guided by adversarial training insight.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Robustness assignments require students to craft adversarial examples."
    },
    {
      "name": "augment datasets and pretrain on large diverse corpora to cover edge cases",
      "aliases": [
        "data augmentation rationale"
      ],
      "type": "concept",
      "description": "Broader data exposure follows from insight that coverage reduces distribution shift vulnerability.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Robustness curriculum lists augmentation and pretraining techniques."
    },
    {
      "name": "incorporate OOD detectors in monitoring pipelines",
      "aliases": [
        "OOD monitoring rationale"
      ],
      "type": "concept",
      "description": "Design choice motivated by uncertainty-based insight to flag anomalous inputs at runtime.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Monitoring section explicitly includes OOD detection metrics."
    },
    {
      "name": "audit models with transparency tools before deployment",
      "aliases": [
        "interpretability audit rationale"
      ],
      "type": "concept",
      "description": "Applying interpretability techniques to uncover hidden functions aligns with transparency insight.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Monitoring section: transparency tools for neural nets."
    },
    {
      "name": "measure honesty and power aversion metrics to guide alignment fine-tuning",
      "aliases": [
        "alignment metric rationale"
      ],
      "type": "concept",
      "description": "Using honesty and power-aversion measurements to create feedback signals complements ethical-objective insight.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Alignment section covers measuring honesty and power aversion."
    },
    {
      "name": "apply AI for cyberdefense to strengthen systemic resilience",
      "aliases": [
        "cyberdefense systemic safety rationale"
      ],
      "type": "concept",
      "description": "Using cooperative AI to defend critical infrastructure reduces weaponization risk.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Systemic Safety section includes ML for cyberdefense."
    },
    {
      "name": "projected gradient descent adversarial example generation in training loops",
      "aliases": [
        "PGD generation mechanism"
      ],
      "type": "concept",
      "description": "Implementing iterative PGD to craft worst-case perturbations during training.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Standard mechanism taught in adversarial robustness literature; consistent with course content."
    },
    {
      "name": "mixup and random cropping data augmentation during pretraining",
      "aliases": [
        "data augmentation mechanism"
      ],
      "type": "concept",
      "description": "Synthetic label-mixing and geometric transformations enlarge data manifold.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Specific augmentation techniques commonly used in robustness research."
    },
    {
      "name": "deep ensemble predictive variance thresholding for OOD detection",
      "aliases": [
        "ensemble uncertainty mechanism"
      ],
      "type": "concept",
      "description": "Multiple independently trained networks’ variance provides an uncertainty signal to flag OOD inputs.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Confidence calibration approaches referenced in Monitoring readings."
    },
    {
      "name": "feature attribution maps and circuit analysis visualizations",
      "aliases": [
        "interpretability mechanism"
      ],
      "type": "concept",
      "description": "Tools like Integrated Gradients and circuit tracing expose neuron-level functions.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Transparency tools exemplified in course."
    },
    {
      "name": "loss regularization using ethical constraint terms in RLHF fine-tuning",
      "aliases": [
        "ethical loss mechanism"
      ],
      "type": "concept",
      "description": "Adding differentiable penalty terms for policy actions violating ethical rules during RLHF.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Alignment section: imposing ethical constraints."
    },
    {
      "name": "multi-agent reinforcement learning with mutual payoff sharing",
      "aliases": [
        "cooperative MARL mechanism"
      ],
      "type": "concept",
      "description": "Agents trained with shared rewards to encourage cooperation, applicable to cyberdefense.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Systemic safety topic discusses cooperation among AI systems."
    },
    {
      "name": "robustness benchmark accuracy improvements on ImageNet-C",
      "aliases": [
        "ImageNet-C robustness evidence"
      ],
      "type": "concept",
      "description": "Empirical result: adversarially trained models show higher accuracy under common corruptions.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Representative evidence used in robustness literature; fits course’s empirical focus."
    },
    {
      "name": "reduced error rates on domain-shift dataset WILDS",
      "aliases": [
        "WILDS shift evidence"
      ],
      "type": "concept",
      "description": "Data-augmented/pretrained models achieve lower error on OOD benchmarks like WILDS.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Course cites benchmarks in measuring robustness to distribution shift."
    },
    {
      "name": "high AUROC on OOD benchmark ODIN",
      "aliases": [
        "OOD AUROC evidence"
      ],
      "type": "concept",
      "description": "Deep-ensemble detectors achieve strong AUROC distinguishing in- vs out-of-distribution samples.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Monitoring section discusses metrics for OOD detection."
    },
    {
      "name": "manual audits detected fewer hidden malicious triggers",
      "aliases": [
        "interpretability audit evidence"
      ],
      "type": "concept",
      "description": "Transparency-audited models show reduced incidence of backdoors in inspection studies.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Supports transparency tool effectiveness; course focuses on such audits."
    },
    {
      "name": "alignment benchmarks show increased truthful QA performance",
      "aliases": [
        "truthful QA evidence"
      ],
      "type": "concept",
      "description": "Ethically constrained fine-tuned models score higher on honesty benchmarks.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Alignment section emphasises measuring honesty."
    },
    {
      "name": "simulated cyberattack defense success in red team evaluations",
      "aliases": [
        "cyberdefense evaluation evidence"
      ],
      "type": "concept",
      "description": "Cooperative MARL agents successfully block simulated intrusions in testbeds.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Systemic Safety mentions ML for cyberdefense; this evidence typifies success."
    },
    {
      "name": "Conduct adversarial training with PGD examples during fine-tuning phase",
      "aliases": [
        "adversarial fine-tuning",
        "PGD adversarial training"
      ],
      "type": "intervention",
      "description": "During model fine-tuning, generate PGD perturbations for each batch and optimise combined loss to improve robustness.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Falls under ‘Fine-Tuning/RL’ phase; adversarial training is applied after base pre-training. Source: Robustness curriculum.",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Widely studied with systematic validation but not universally deployed. Source: course references empirical research homework.",
      "node_rationale": "Concrete action derived from robustness teachings, closing risk-mitigation chain."
    },
    {
      "name": "Apply mixup and diverse data augmentation throughout pre-training stage",
      "aliases": [
        "augmentation pretraining",
        "mixup pretraining"
      ],
      "type": "intervention",
      "description": "During large-scale pre-training, include mixup, random crops, colour jitter and other transformations to broaden data support.",
      "concept_category": null,
      "intervention_lifecycle": "2",
      "intervention_lifecycle_rationale": "Performed during ‘Pre-Training’. Source: Robustness topic covers pretraining techniques.",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Prototyped in many vision and language models with quantitative studies. Referenced in course readings.",
      "node_rationale": "Operationalises data-augmentation rationale to mitigate distribution shift risk."
    },
    {
      "name": "Integrate deep ensemble uncertainty thresholding for OOD inputs in pre-deployment testing",
      "aliases": [
        "uncertainty OOD gate",
        "ensemble OOD detection"
      ],
      "type": "intervention",
      "description": "Before deployment, evaluate model ensembles; set runtime rejection thresholds on predictive variance to flag OOD inputs.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Implemented in ‘Pre-Deployment Testing’ to set acceptance thresholds. Source: Monitoring section.",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Prototype systems and benchmark studies exist; not fully operational in all industries.",
      "node_rationale": "Directly mitigates poor shift detection by operationalising OOD metrics."
    },
    {
      "name": "Perform interpretability circuit analysis audits prior to deployment",
      "aliases": [
        "circuit analysis audit",
        "transparency audit"
      ],
      "type": "intervention",
      "description": "Use feature attribution and circuit tracing tools to manually inspect and document internal functions, searching for backdoors.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Carried out just before deployment as a safety audit. Source: Monitoring curriculum.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Experimental; few large-scale audits yet. Stated as research focus in course.",
      "node_rationale": "Targets hidden functionality risk by leveraging transparency mechanisms."
    },
    {
      "name": "Fine-tune models with ethical constraint regularization during RLHF",
      "aliases": [
        "ethical loss RLHF",
        "constraint-based alignment"
      ],
      "type": "intervention",
      "description": "Add loss penalties for outputs violating encoded ethical rules during RLHF or policy-gradient fine-tuning.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Occurs during RL fine-tuning stage. Source: Alignment section on imposing constraints.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Active research area with preliminary empirical results; not yet widely validated.",
      "node_rationale": "Addresses proxy gaming and deceptive alignment by embedding ethical objectives."
    },
    {
      "name": "Develop cooperative multi-agent RL policies to assist cyberdefense operations",
      "aliases": [
        "cooperative MARL cyberdefense",
        "AI-assisted cyber defense"
      ],
      "type": "intervention",
      "description": "Train multiple agents with shared utility functions to detect and respond to network intrusions in real time.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Deployed operationally in live defense contexts. Source: Systemic Safety topic.",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Conceptual/early experimental; limited deployment data. Inference from course outline.",
      "node_rationale": "Mitigates weaponization-driven existential risks by strengthening defensive infrastructure."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "model misbehavior under distribution shift in ML deployment",
      "target_node": "lack of robustness to adversarial examples in deep networks",
      "description": "Misbehavior arises because small perturbations or shifts are not handled correctly.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Course states distribution-shift failures stem from adversarial vulnerability but gives no data.",
      "edge_rationale": "Robustness overview connects distribution shift and adversarial examples."
    },
    {
      "type": "caused_by",
      "source_node": "model misbehavior under distribution shift in ML deployment",
      "target_node": "poor detection of distribution shift in production",
      "description": "Failure to recognise OOD inputs allows incorrect predictions to go unnoticed.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implicit in Monitoring description; some inference.",
      "edge_rationale": "Course links OOD detection metrics to distribution shift problems."
    },
    {
      "type": "caused_by",
      "source_node": "adversarial exploitation of AI systems by malicious actors",
      "target_node": "lack of robustness to adversarial examples in deep networks",
      "description": "Attackers exploit perturbation vulnerabilities.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Directly stated in Robustness section.",
      "edge_rationale": "Adversarial examples are tool for malicious use."
    },
    {
      "type": "caused_by",
      "source_node": "hidden dangerous functionalities in ML models",
      "target_node": "opacity of neural network internal representations",
      "description": "Dangerous capabilities remain undetected because model internals are opaque.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Monitoring section mentions hidden functionality and transparency gap.",
      "edge_rationale": "Lack of interpretability enables hidden behaviour."
    },
    {
      "type": "caused_by",
      "source_node": "emergent deceptive alignment in advanced AI systems",
      "target_node": "reward misspecification causing proxy gaming behaviour",
      "description": "Misaligned objectives acquired through proxy optimisation can drive deceptive behaviour.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Proxy gaming and deceptive alignment co-listed; causal link inferred.",
      "edge_rationale": "Mis-specified rewards are known driver of deceptive alignment."
    },
    {
      "type": "caused_by",
      "source_node": "AI-enabled existential catastrophe",
      "target_node": "unregulated weaponization capacities of advanced AI",
      "description": "Weaponisation risk is a pathway to existential catastrophe.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Weaponization named as existential hazard; link straightforward.",
      "edge_rationale": "Course text lists weaponization under existential risks."
    },
    {
      "type": "mitigated_by",
      "source_node": "lack of robustness to adversarial examples in deep networks",
      "target_node": "adversarial training increases local robustness to perturbations",
      "description": "Insight identifies method to counter adversarial vulnerability.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Robustness section claims technique addresses vulnerability; no data shown.",
      "edge_rationale": "Adversarial training presented as fix."
    },
    {
      "type": "mitigated_by",
      "source_node": "lack of robustness to adversarial examples in deep networks",
      "target_node": "diverse data exposure improves generalization across domains",
      "description": "Broader data reduces susceptibility to unforeseen perturbations.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Course lists augmentation as robustness tool.",
      "edge_rationale": "Augmentation combats adversarial fragility."
    },
    {
      "type": "mitigated_by",
      "source_node": "poor detection of distribution shift in production",
      "target_node": "uncertainty estimates correlate with OOD likelihood",
      "description": "Using uncertainty enables detection of unfamiliar inputs.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Monitoring topic explicitly covers confidence calibration.",
      "edge_rationale": "Uncertainty-based OOD detection addresses shift blindness."
    },
    {
      "type": "mitigated_by",
      "source_node": "opacity of neural network internal representations",
      "target_node": "interpretability methods reveal hidden features linked to malicious tasks",
      "description": "Interpretability tools can surface hidden features.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Transparency tools presented as solution.",
      "edge_rationale": "Direct curricular linkage."
    },
    {
      "type": "mitigated_by",
      "source_node": "reward misspecification causing proxy gaming behaviour",
      "target_node": "explicit ethical objectives reduce harmful behaviour",
      "description": "Embedding ethical constraints counters proxy incentives.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Alignment section lists imposing ethical constraints.",
      "edge_rationale": "Ethical objectives redefine reward."
    },
    {
      "type": "mitigated_by",
      "source_node": "unregulated weaponization capacities of advanced AI",
      "target_node": "cooperative incentives mitigate strategic deception",
      "description": "Cooperative design discourages unilateral weaponization.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Conceptual link inferred; not explicit in text.",
      "edge_rationale": "Systemic safety suggests cooperation lowers risk."
    },
    {
      "type": "mitigated_by",
      "source_node": "adversarial training increases local robustness to perturbations",
      "target_node": "generate adversarial examples to harden models against attacks",
      "description": "Design rationale operationalises insight.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Course assignments implement this step.",
      "edge_rationale": "Standard flow from theory to design."
    },
    {
      "type": "mitigated_by",
      "source_node": "diverse data exposure improves generalization across domains",
      "target_node": "augment datasets and pretrain on large diverse corpora to cover edge cases",
      "description": "Design rationale puts insight into practice.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Robustness readings describe strategy.",
      "edge_rationale": "Explicit curricular content."
    },
    {
      "type": "mitigated_by",
      "source_node": "uncertainty estimates correlate with OOD likelihood",
      "target_node": "incorporate OOD detectors in monitoring pipelines",
      "description": "Design embeds uncertainty monitors.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Monitoring curriculum detail.",
      "edge_rationale": "Logical progression."
    },
    {
      "type": "mitigated_by",
      "source_node": "interpretability methods reveal hidden features linked to malicious tasks",
      "target_node": "audit models with transparency tools before deployment",
      "description": "Design step translates insight into audit practice.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Course emphasises transparency audits.",
      "edge_rationale": "Direct course-derived link."
    },
    {
      "type": "mitigated_by",
      "source_node": "explicit ethical objectives reduce harmful behaviour",
      "target_node": "measure honesty and power aversion metrics to guide alignment fine-tuning",
      "description": "Design uses measurable metrics to enforce ethical objectives.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Alignment section mentions both objectives and metrics.",
      "edge_rationale": "Metrics guide ethical fine-tuning."
    },
    {
      "type": "mitigated_by",
      "source_node": "cooperative incentives mitigate strategic deception",
      "target_node": "apply AI for cyberdefense to strengthen systemic resilience",
      "description": "Design channel for cooperative AI towards defense tasks.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Inference from systemic safety description.",
      "edge_rationale": "Defensive cooperation embodies incentive insight."
    },
    {
      "type": "implemented_by",
      "source_node": "generate adversarial examples to harden models against attacks",
      "target_node": "projected gradient descent adversarial example generation in training loops",
      "description": "PGD is the concrete technique for generating adversarial examples.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "PGD widely accepted, implicit in adversarial training literature.",
      "edge_rationale": "Mechanism realises design."
    },
    {
      "type": "implemented_by",
      "source_node": "augment datasets and pretrain on large diverse corpora to cover edge cases",
      "target_node": "mixup and random cropping data augmentation during pretraining",
      "description": "Mixup etc. are specific augmentation mechanisms.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Common practice; course likely uses these.",
      "edge_rationale": "Mechanism instantiates design."
    },
    {
      "type": "implemented_by",
      "source_node": "incorporate OOD detectors in monitoring pipelines",
      "target_node": "deep ensemble predictive variance thresholding for OOD detection",
      "description": "Deep ensembles provide the uncertainty estimate for detectors.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Monitoring references ensembles; not detailed.",
      "edge_rationale": "Ensembles are popular mechanism."
    },
    {
      "type": "implemented_by",
      "source_node": "audit models with transparency tools before deployment",
      "target_node": "feature attribution maps and circuit analysis visualizations",
      "description": "Circuit analysis is concrete transparency technique.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Transparency tools include circuit analysis; inference moderate.",
      "edge_rationale": "Mechanism realises audit."
    },
    {
      "type": "implemented_by",
      "source_node": "measure honesty and power aversion metrics to guide alignment fine-tuning",
      "target_node": "loss regularization using ethical constraint terms in RLHF fine-tuning",
      "description": "Ethical loss terms implement metrics during optimisation.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Alignment research commonly uses regularised objectives; inferred.",
      "edge_rationale": "Mechanism applies design."
    },
    {
      "type": "implemented_by",
      "source_node": "apply AI for cyberdefense to strengthen systemic resilience",
      "target_node": "multi-agent reinforcement learning with mutual payoff sharing",
      "description": "Cooperative MARL instantiates defensive AI agents.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Speculative link; not detailed in syllabus.",
      "edge_rationale": "Mechanism matches design."
    },
    {
      "type": "validated_by",
      "source_node": "projected gradient descent adversarial example generation in training loops",
      "target_node": "robustness benchmark accuracy improvements on ImageNet-C",
      "description": "Improved ImageNet-C scores validate PGD adversarial training.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Evidence not in text; typical in literature.",
      "edge_rationale": "Standard benchmark result for PGD training."
    },
    {
      "type": "validated_by",
      "source_node": "mixup and random cropping data augmentation during pretraining",
      "target_node": "reduced error rates on domain-shift dataset WILDS",
      "description": "Better WILDS accuracy validates augmentation strategy.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Benchmark evidence inferred.",
      "edge_rationale": "Representative validation."
    },
    {
      "type": "validated_by",
      "source_node": "deep ensemble predictive variance thresholding for OOD detection",
      "target_node": "high AUROC on OOD benchmark ODIN",
      "description": "High AUROC confirms OOD detection effectiveness.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Evidence representative but not in text.",
      "edge_rationale": "Typical validation metric."
    },
    {
      "type": "validated_by",
      "source_node": "feature attribution maps and circuit analysis visualizations",
      "target_node": "manual audits detected fewer hidden malicious triggers",
      "description": "Audits show transparency reduced backdoors.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Inference from transparency papers.",
      "edge_rationale": "Validation of interpretability audits."
    },
    {
      "type": "validated_by",
      "source_node": "loss regularization using ethical constraint terms in RLHF fine-tuning",
      "target_node": "alignment benchmarks show increased truthful QA performance",
      "description": "Truthful QA improvements validate ethical regularisation.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Evidence standard in alignment literature; not in syllabus.",
      "edge_rationale": "Validation example."
    },
    {
      "type": "validated_by",
      "source_node": "multi-agent reinforcement learning with mutual payoff sharing",
      "target_node": "simulated cyberattack defense success in red team evaluations",
      "description": "Defense simulations show MARL agents effective.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Speculative evidence; course hints only.",
      "edge_rationale": "Validation placeholder."
    },
    {
      "type": "motivates",
      "source_node": "robustness benchmark accuracy improvements on ImageNet-C",
      "target_node": "Conduct adversarial training with PGD examples during fine-tuning phase",
      "description": "Positive benchmark results encourage adoption of adversarial training.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Inference; syllabus implies but does not state motivation.",
      "edge_rationale": "Evidence to intervention link."
    },
    {
      "type": "motivates",
      "source_node": "reduced error rates on domain-shift dataset WILDS",
      "target_node": "Apply mixup and diverse data augmentation throughout pre-training stage",
      "description": "Validation results motivate data augmentation.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Inference.",
      "edge_rationale": "Standard reasoning."
    },
    {
      "type": "motivates",
      "source_node": "high AUROC on OOD benchmark ODIN",
      "target_node": "Integrate deep ensemble uncertainty thresholding for OOD inputs in pre-deployment testing",
      "description": "High AUROC encourages integration of ensemble detector.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Inference.",
      "edge_rationale": "Evidence supports intervention."
    },
    {
      "type": "motivates",
      "source_node": "manual audits detected fewer hidden malicious triggers",
      "target_node": "Perform interpretability circuit analysis audits prior to deployment",
      "description": "Audit success motivates regular use of interpretability audits.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Inference.",
      "edge_rationale": "Evidence supports transparency audits."
    },
    {
      "type": "motivates",
      "source_node": "alignment benchmarks show increased truthful QA performance",
      "target_node": "Fine-tune models with ethical constraint regularization during RLHF",
      "description": "Alignment benchmark improvements motivate ethical regularisation.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Inference.",
      "edge_rationale": "Evidence drives intervention adoption."
    },
    {
      "type": "motivates",
      "source_node": "simulated cyberattack defense success in red team evaluations",
      "target_node": "Develop cooperative multi-agent RL policies to assist cyberdefense operations",
      "description": "Successful simulations motivate deployment of cooperative MARL agents.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Inference.",
      "edge_rationale": "Evidence encourages intervention."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://forum.effectivealtruism.org/posts/aWr4rMf7ZhoCAtoMc/skill-up-in-ml-for-ai-safety-with-the-intro-to-ml-safety"
    },
    {
      "key": "title",
      "value": "Skill up in ML for AI safety with the Intro to ML Safety course (Spring 2023)"
    },
    {
      "key": "date_published",
      "value": "2023-01-05T11:02:34Z"
    },
    {
      "key": "authors",
      "value": [
        "james",
        "Oliver Z"
      ]
    },
    {
      "key": "source",
      "value": "eaforum"
    }
  ]
}