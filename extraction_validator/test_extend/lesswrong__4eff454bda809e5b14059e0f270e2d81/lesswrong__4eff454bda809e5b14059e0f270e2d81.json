{
  "nodes": [
    {
      "name": "Value misalignment in AI systems from inaccurate utility modelling of human preferences",
      "aliases": [
        "utility function misalignment in AI",
        "incorrect scalar utility for humans in AI alignment"
      ],
      "type": "concept",
      "description": "Risk that AI systems optimized against a mis-specified scalar utility function will pursue goals that diverge from genuine human values because the utility model fails to reflect real human preferences.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central AI-safety risk implicit in the post’s reflection on failed utility assignment (Main post)."
    },
    {
      "name": "Cardinal utility representation failure for diverse human preferences",
      "aliases": [
        "difficulty assigning numeric utilities",
        "failure of precise utility numbers for varied outcomes"
      ],
      "type": "concept",
      "description": "Problem that many human outcomes across life domains cannot be placed on a single cardinal utility scale, producing incomparabilities and intransitivities.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Author’s attempt to create a numeric utility list failed, revealing this problem (Main post)."
    },
    {
      "name": "Human preferences exhibit incomparability and context dependence",
      "aliases": [
        "intransitive human preferences",
        "context-dependent preference structures"
      ],
      "type": "concept",
      "description": "Theoretical insight that people often cannot rank dissimilar outcomes or maintain consistent trade-offs; preferences change with perspective and available information.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explains why numeric utilities fail, directly inferred from author’s double-‘No’ comparisons (Main post)."
    },
    {
      "name": "Model preferences via pairwise comparisons and contextual factors",
      "aliases": [
        "pairwise preference modelling",
        "context-aware preference design rationale"
      ],
      "type": "concept",
      "description": "Design rationale proposing that instead of cardinal utilities, systems should learn from pairwise preference data augmented with context, better matching actual preference structure.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Natural solution direction suggested once incomparability is acknowledged (Inference from Main post)."
    },
    {
      "name": "Train preference learning algorithms on partial-order data with context features",
      "aliases": [
        "partial-order preference learning",
        "contextual preference learning mechanism"
      ],
      "type": "concept",
      "description": "Implementation mechanism in which machine-learning models are trained on user-supplied pairwise comparisons, learning a partial order that can vary across contexts rather than a single scalar utility.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Operational technique that realizes the design rationale (Inference referencing ML literature; Main post lacks direct mechanism detail)."
    },
    {
      "name": "Anecdotal self-experiment revealing inconsistent numeric valuations",
      "aliases": [
        "author’s failed utility elicitation experiment",
        "personal utility assignment failure evidence"
      ],
      "type": "concept",
      "description": "Validation evidence consisting of the author’s two-month follow-up showing assigned numerical utilities were ‘completely wrong’ across life domains.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Only empirical observation in the data source; supports need for alternative approaches (Main post)."
    },
    {
      "name": "Integrate partial-order preference learning into reward model training to align AI with human preferences",
      "aliases": [
        "apply partial-order preference learning in RLHF",
        "use pairwise preference signals for alignment"
      ],
      "type": "intervention",
      "description": "During the RLHF or similar fine-tuning stage, collect human pairwise comparisons (possibly contextual) and train the reward model to represent a partial order rather than a single utility scalar, thereby reducing misalignment from inaccurate cardinal utilities.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Applies during preference-based RL fine-tuning where reward models are trained (inferred from alignment workflow; Main post provides no lifecycle detail).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Technique is under active research with proof-of-concepts but limited large-scale deployment; inference based on current literature, not on the anecdotal post.",
      "node_rationale": "Actionable step most directly supported by the reasoning chain extracted (Inference from Main post)."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Value misalignment in AI systems from inaccurate utility modelling of human preferences",
      "target_node": "Cardinal utility representation failure for diverse human preferences",
      "description": "Misalignment risk arises when AI systems rely on flawed cardinal utility representations that do not reflect actual human preferences.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Widely accepted alignment argument; partially inferred because post does not mention AI explicitly.",
      "edge_rationale": "Connects the general AI risk to the specific modeling failure identified in the post (Inference combining Main post with alignment context)."
    },
    {
      "type": "addressed_by",
      "source_node": "Cardinal utility representation failure for diverse human preferences",
      "target_node": "Human preferences exhibit incomparability and context dependence",
      "description": "Understanding incomparability and context provides an explanation and starting point to address the failure of cardinal utilities.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Based on author’s observations; limited empirical support.",
      "edge_rationale": "Theoretical insight clarifies the nature of the problem uncovered (Main post)."
    },
    {
      "type": "mitigated_by",
      "source_node": "Human preferences exhibit incomparability and context dependence",
      "target_node": "Model preferences via pairwise comparisons and contextual factors",
      "description": "Using pairwise and context-aware modeling mitigates the issues caused by incomparability in human preferences.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical mitigation inferred; not experimentally confirmed in the post.",
      "edge_rationale": "Design rationale flows naturally from the theoretical insight (Inference from Main post)."
    },
    {
      "type": "implemented_by",
      "source_node": "Model preferences via pairwise comparisons and contextual factors",
      "target_node": "Train preference learning algorithms on partial-order data with context features",
      "description": "Preference-learning algorithms operationalize the design rationale by learning from partial-order data.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Technique documented in ML literature though not in the post; inference with moderate confidence.",
      "edge_rationale": "Shows concrete mechanism implementing the approach (Inference)."
    },
    {
      "type": "validated_by",
      "source_node": "Train preference learning algorithms on partial-order data with context features",
      "target_node": "Anecdotal self-experiment revealing inconsistent numeric valuations",
      "description": "The experiment’s failure to produce consistent numbers serves as preliminary validation for abandoning strict numeric utilities in favor of alternative learning methods.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Evidence is a single anecdote; weak support.",
      "edge_rationale": "Links mechanism need to empirical observation from the post."
    },
    {
      "type": "motivates",
      "source_node": "Anecdotal self-experiment revealing inconsistent numeric valuations",
      "target_node": "Integrate partial-order preference learning into reward model training to align AI with human preferences",
      "description": "The observed inconsistency motivates adopting partial-order preference learning during AI reward-model training.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Motivation is reasonable but based on limited evidence.",
      "edge_rationale": "Demonstrates how evidence leads to the proposed intervention (Inference from Main post)."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://www.lesswrong.com/posts/MvwdPfYLX866vazFJ/post-your-utility-function"
    },
    {
      "key": "title",
      "value": "Post Your Utility Function"
    },
    {
      "key": "date_published",
      "value": "2009-06-04T05:05:18Z"
    },
    {
      "key": "authors",
      "value": [
        "taw"
      ]
    },
    {
      "key": "source",
      "value": "lesswrong"
    }
  ]
}