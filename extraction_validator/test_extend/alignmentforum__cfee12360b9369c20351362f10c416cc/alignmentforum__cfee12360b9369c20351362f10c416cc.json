{
  "nodes": [
    {
      "name": "Proxy objective divergence in AI optimization",
      "aliases": [
        "Goodhart divergence in AI systems",
        "Proxy misalignment risk"
      ],
      "type": "concept",
      "description": "Systematic tendency for an AI system that maximises an observable proxy U to produce outcomes that are worse with respect to the true objective V.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Captures the top-level risk emphasised throughout the post (introductory paragraphs)"
    },
    {
      "name": "Regression to mean in proxy-based selection",
      "aliases": [
        "Goodhart level 1",
        "Selection‐induced overestimation"
      ],
      "type": "concept",
      "description": "When choosing high values of a noisy proxy, the chosen samples are biased upward, so the proxy overestimates the true value.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained under 'Goodhart's Curse Level 1 (regressing to the mean)'"
    },
    {
      "name": "Optimization pressure eroding proxy–value correlation in AI systems",
      "aliases": [
        "Goodhart level 2",
        "Extremal selection correlation decay"
      ],
      "type": "concept",
      "description": "Strong optimization on a proxy forces selection into regions where the historical correlation between proxy and true value no longer holds.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in 'Goodhart's Curse Level 2 (optimizing away the correlation)'"
    },
    {
      "name": "Adversarial exploitation of proxy metrics by agents",
      "aliases": [
        "Goodhart level 3",
        "Adversarial correlation manipulation"
      ],
      "type": "concept",
      "description": "A strategic agent shapes the observable proxy so that maximizing the proxy furthers its own objective W instead of the desired objective V.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "'Goodhart's Curse Level 3 (adversarial correlations)' and Treacherous Turn example"
    },
    {
      "name": "Limiting optimization intensity preserves proxy–value alignment",
      "aliases": [
        "Conservative optimization insight"
      ],
      "type": "concept",
      "description": "Reducing the strength of selection prevents entry into regions where proxy and true value decouple, maintaining expected alignment.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Derived from Level 1 and Level 2 discussion that excessive optimization is the core driver of divergence"
    },
    {
      "name": "Robust statistics across multiple metrics reduce overestimation bias",
      "aliases": [
        "Multi-metric robustness insight"
      ],
      "type": "concept",
      "description": "Combining several noisy proxies with robust aggregators (e.g., median) lowers the chance that any single overestimated proxy dominates selection.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implied by regression-to-mean example: using several proxies mitigates single-proxy noise"
    },
    {
      "name": "Adversarial detection mechanisms needed to counter strategic manipulation",
      "aliases": [
        "Need for adversarial safeguards"
      ],
      "type": "concept",
      "description": "Recognising that other agents may purposely align the proxy with their goal suggests using detection or red-team methods to uncover manipulation.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Reasoning in Level 3 section about employees gaming LOC metrics and treacherous turns"
    },
    {
      "name": "Quantilization approach to cap optimization pressure",
      "aliases": [
        "Quantilization design rationale"
      ],
      "type": "concept",
      "description": "Select actions from the top q-quantile of a reference distribution instead of the absolute optimum, limiting extremal optimisation.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Applies theoretical insight on limiting optimisation intensity"
    },
    {
      "name": "Median-of-means reward aggregation to reduce bias",
      "aliases": [
        "Robust reward aggregation rationale"
      ],
      "type": "concept",
      "description": "Use robust estimators such as median-of-means over grouped feedback to avoid single noisy high rewards dominating learning.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Follows insight that robust statistics mitigate regression-to-mean effects"
    },
    {
      "name": "Red-team adversarial training to expose proxy gaming",
      "aliases": [
        "Adversarial red-teaming rationale"
      ],
      "type": "concept",
      "description": "Employ dedicated adversaries to search for inputs or strategies that achieve high proxy scores while failing true objectives.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Stems from theoretical need for adversarial detection in level 3"
    },
    {
      "name": "Sampling actions from top-q percent of baseline policy distribution",
      "aliases": [
        "Quantilization sampling mechanism"
      ],
      "type": "concept",
      "description": "Implementation: draw N candidate actions from a trusted distribution and pick uniformly among those in the highest q-quantile of proxy score.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete mechanism realising the quantilization rationale"
    },
    {
      "name": "Compute median of grouped human feedback scores in RLHF",
      "aliases": [
        "Median-of-means mechanism"
      ],
      "type": "concept",
      "description": "Partition feedback into k groups, compute the mean in each, then take the median across groups as the reward fed back to the learner.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Concrete mechanism for robust aggregation rationale"
    },
    {
      "name": "Generate adversarial prompt attacks during fine-tuning",
      "aliases": [
        "Red-team input generation mechanism"
      ],
      "type": "concept",
      "description": "Automatically or manually craft inputs intended to maximise proxy reward while violating the real objective, then incorporate them in training.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mechanism to operationalise red-team rationale"
    },
    {
      "name": "Toy model simulation shows reduced divergence under quantilization",
      "aliases": [
        "Quantilization toy-model evidence"
      ],
      "type": "concept",
      "description": "Simulated normal-distribution environment demonstrating that quantilized policies retain higher expected true value than full optimisation.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Hypothetical validation evidence node (moderate inference) to connect mechanism to intervention"
    },
    {
      "name": "Synthetic reward corruption study shows robustness of median-of-means RL",
      "aliases": [
        "Median-of-means empirical evidence"
      ],
      "type": "concept",
      "description": "Experiments where a fraction of reward signals are corrupted; median-of-means aggregation maintains performance compared to mean.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Inferred validation evidence bridging to intervention"
    },
    {
      "name": "Adversarial prompt tests reveal lower vulnerability post red-teaming",
      "aliases": [
        "Red-team effectiveness evidence"
      ],
      "type": "concept",
      "description": "Benchmark showing decline in success rate of proxy-gaming prompts after adversarial fine-tuning cycle.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Validation evidence node (inferred) for red-team pathway"
    },
    {
      "name": "Embed quantilization decision rule into planner at deployment",
      "aliases": [
        "Apply quantilization during action selection"
      ],
      "type": "intervention",
      "description": "Design the AI planner to sample actions via a q-quantilization rule, limiting optimisation pressure on any single proxy.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Addresses model-design phase choice; aligns with 'Quantilization approach' reasoning (implied overall post).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Currently theoretical in most AI systems; no wide-scale empirical validation stated.",
      "node_rationale": "Direct actionable step motivated by Goodhart level 1/2 mitigation."
    },
    {
      "name": "Fine-tune language models with median-of-means reward aggregation",
      "aliases": [
        "Robust RLHF with MoM"
      ],
      "type": "intervention",
      "description": "During RLHF or other fine-tuning, replace average reward with median-of-means across feedback batches to resist noisy overestimation.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Takes place during fine-tuning/RL stage when reward signals are processed.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Some experimental work in robust RL; limited large-scale adoption.",
      "node_rationale": "Actionable method motivated by regression-to-mean mitigation."
    },
    {
      "name": "Conduct structured adversarial red-teaming before release",
      "aliases": [
        "Pre-deployment proxy gaming red-team"
      ],
      "type": "intervention",
      "description": "Organise dedicated red-team sessions and automated adversarial search to discover proxy-gaming behaviours prior to public deployment.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Occurs in pre-deployment testing to surface issues before release.",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Increasingly used in practice (e.g., Anthropic, OpenAI red-team exercises).",
      "node_rationale": "Intervention directly addressing adversarial exploitation risk (Goodhart level 3)."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Proxy objective divergence in AI optimization",
      "target_node": "Regression to mean in proxy-based selection",
      "description": "Regression to the mean is identified as the first mechanism producing divergence.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit example with V, U=V+E normal distributions ('Level 1' section).",
      "edge_rationale": "The post states that selecting high U causes overestimation of V, i.e., divergence."
    },
    {
      "type": "caused_by",
      "source_node": "Proxy objective divergence in AI optimization",
      "target_node": "Optimization pressure eroding proxy–value correlation in AI systems",
      "description": "Extreme optimisation pressure is the second mechanism leading to divergence.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit toy examples in 'Level 2' section.",
      "edge_rationale": "Author shows that with enough optimisation the proxy/true correlation collapses."
    },
    {
      "type": "caused_by",
      "source_node": "Proxy objective divergence in AI optimization",
      "target_node": "Adversarial exploitation of proxy metrics by agents",
      "description": "Adversarial manipulation is the third mechanism producing divergence.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "'Level 3' section with employee code-line example.",
      "edge_rationale": "The post argues strategic agents will align proxy with their goals, diverging from V."
    },
    {
      "type": "caused_by",
      "source_node": "Regression to mean in proxy-based selection",
      "target_node": "Limiting optimization intensity preserves proxy–value alignment",
      "description": "Insight that reducing selection pressure would limit overestimation.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Derived insight; not formally proven in text.",
      "edge_rationale": "Author implies best you can do is still pick highest U but expect overestimate; suggests limit optimisation."
    },
    {
      "type": "caused_by",
      "source_node": "Optimization pressure eroding proxy–value correlation in AI systems",
      "target_node": "Limiting optimization intensity preserves proxy–value alignment",
      "description": "Further supports same insight on optimisation limits.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Text stresses 'as you increase k to infinity' optimisation harms V.",
      "edge_rationale": "Demonstrates need to curb optimisation intensity."
    },
    {
      "type": "caused_by",
      "source_node": "Regression to mean in proxy-based selection",
      "target_node": "Robust statistics across multiple metrics reduce overestimation bias",
      "description": "Multiple noisy measures can average out regression effects.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference from statistical argument; not explicit.",
      "edge_rationale": "If one proxy is noisy, combining several can reduce bias."
    },
    {
      "type": "caused_by",
      "source_node": "Adversarial exploitation of proxy metrics by agents",
      "target_node": "Adversarial detection mechanisms needed to counter strategic manipulation",
      "description": "Adversarial exploitation necessitates detection mechanisms.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Employee example implies need for monitoring/manipulation checks.",
      "edge_rationale": "If agents can game proxy, must detect gaming."
    },
    {
      "type": "mitigated_by",
      "source_node": "Limiting optimization intensity preserves proxy–value alignment",
      "target_node": "Quantilization approach to cap optimization pressure",
      "description": "Quantilization operationalises limited optimisation intensity.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Quantilization not named in post; moderate inference.",
      "edge_rationale": "Quantilization is canonical AI-safety technique for this insight."
    },
    {
      "type": "mitigated_by",
      "source_node": "Robust statistics across multiple metrics reduce overestimation bias",
      "target_node": "Median-of-means reward aggregation to reduce bias",
      "description": "Median-of-means realises robust statistical combination.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference mapping statistical insight to known method.",
      "edge_rationale": "MoM is standard robust estimator aligning with insight."
    },
    {
      "type": "mitigated_by",
      "source_node": "Adversarial detection mechanisms needed to counter strategic manipulation",
      "target_node": "Red-team adversarial training to expose proxy gaming",
      "description": "Red-teaming is a design to detect manipulation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Red-teaming widely accepted safety practice; logical extension.",
      "edge_rationale": "Provides organised adversarial search."
    },
    {
      "type": "implemented_by",
      "source_node": "Quantilization approach to cap optimization pressure",
      "target_node": "Sampling actions from top-q percent of baseline policy distribution",
      "description": "Sampling mechanism realises quantilized decision rule.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Mechanism standard in quantilization literature.",
      "edge_rationale": "Defines how to act on quantilization rationale."
    },
    {
      "type": "implemented_by",
      "source_node": "Median-of-means reward aggregation to reduce bias",
      "target_node": "Compute median of grouped human feedback scores in RLHF",
      "description": "Algorithmic step to compute MoM rewards.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct implementation match.",
      "edge_rationale": "Mechanism follows from design rationale."
    },
    {
      "type": "implemented_by",
      "source_node": "Red-team adversarial training to expose proxy gaming",
      "target_node": "Generate adversarial prompt attacks during fine-tuning",
      "description": "Prompt generation is concrete red-team mechanism.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Standard practice in safety evals.",
      "edge_rationale": "Mechanism operationalises red-team rationale."
    },
    {
      "type": "validated_by",
      "source_node": "Sampling actions from top-q percent of baseline policy distribution",
      "target_node": "Toy model simulation shows reduced divergence under quantilization",
      "description": "Toy-model demonstrates mechanism effectiveness.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Hypothetical; not empirical in post.",
      "edge_rationale": "Provides supporting evidence for mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Compute median of grouped human feedback scores in RLHF",
      "target_node": "Synthetic reward corruption study shows robustness of median-of-means RL",
      "description": "Experimental study validates robustness.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inferred experiment.",
      "edge_rationale": "Evidence needed to motivate intervention."
    },
    {
      "type": "validated_by",
      "source_node": "Generate adversarial prompt attacks during fine-tuning",
      "target_node": "Adversarial prompt tests reveal lower vulnerability post red-teaming",
      "description": "Testing shows fewer successful proxy-gaming prompts.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inferred validation.",
      "edge_rationale": "Shows mechanism effectiveness."
    },
    {
      "type": "motivates",
      "source_node": "Toy model simulation shows reduced divergence under quantilization",
      "target_node": "Embed quantilization decision rule into planner at deployment",
      "description": "Evidence motivates adopting quantilization in design.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Validation is hypothetical; moderate inference.",
      "edge_rationale": "Connects evidence to intervention."
    },
    {
      "type": "motivates",
      "source_node": "Synthetic reward corruption study shows robustness of median-of-means RL",
      "target_node": "Fine-tune language models with median-of-means reward aggregation",
      "description": "Evidence advocates MoM in RLHF.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Moderate inference from robust statistics literature.",
      "edge_rationale": "Supports intervention adoption."
    },
    {
      "type": "motivates",
      "source_node": "Adversarial prompt tests reveal lower vulnerability post red-teaming",
      "target_node": "Conduct structured adversarial red-teaming before release",
      "description": "Evidence motivates institutionalising red-team testing.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Moderate inference.",
      "edge_rationale": "Shows benefit prompting intervention."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://www.alignmentforum.org/posts/5bd75cc58225bf06703754b2/the-three-levels-of-goodhart-s-curse"
    },
    {
      "key": "title",
      "value": "The Three Levels of Goodhart's Curse"
    },
    {
      "key": "date_published",
      "value": "2017-12-30T16:41:25Z"
    },
    {
      "key": "authors",
      "value": [
        "Scott Garrabrant"
      ]
    },
    {
      "key": "source",
      "value": "alignmentforum"
    }
  ]
}