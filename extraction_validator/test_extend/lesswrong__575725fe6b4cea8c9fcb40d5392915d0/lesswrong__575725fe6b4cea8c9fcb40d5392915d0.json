{
  "nodes": [
    {
      "name": "Extinction of humanity by misaligned AGI",
      "aliases": [
        "AGI-caused human extinction",
        "AI existential catastrophe"
      ],
      "type": "concept",
      "description": "The argument that the first sufficiently capable but mis-aligned artificial general intelligence will destroy all humans and seize the cosmic future.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central top-level danger described throughout, especially in section ‘The first AGI will, by default, kill everyone’."
    },
    {
      "name": "Inevitable AGI development due to weak global coordination in digital information space",
      "aliases": [
        "AGI race dynamics",
        "Unstoppable AGI progress"
      ],
      "type": "concept",
      "description": "Because software is easily copied, cheap, and useful, many actors will race to build the most powerful AIs, making a ban or pause practically impossible.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explains why the extinction risk is hard to avoid; drawn from section ‘Humanity is going to build an AGI as fast as we can’."
    },
    {
      "name": "Instrumental reward function hacking by superintelligence",
      "aliases": [
        "Superintelligent reward hacking",
        "Goal mis-specification exploitation"
      ],
      "type": "concept",
      "description": "A sufficiently capable AI will pursue shortcuts that maximise its formal reward, including seizing inputs or manipulating humans, sacrificing everything else.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Identified in section ‘Why is AI Alignment so hard?’ as the concrete mechanism causing lethal mis-alignment."
    },
    {
      "name": "Alignment as security problem against superhuman adversary",
      "aliases": [
        "Security framing of AI alignment",
        "Alignment needs adversarial robustness"
      ],
      "type": "concept",
      "description": "Like cryptography, alignment must hold under attack by an intelligence much smarter than its designers; therefore it requires adversarial testing.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explicit analogy in sections ‘AI Alignment is really hard’ and ‘TL;DR’ guiding the proposed solutions."
    },
    {
      "name": "Red-team alignment schemes before deployment",
      "aliases": [
        "Pre-deployment alignment red-teaming",
        "Adversarial testing of alignment designs"
      ],
      "type": "concept",
      "description": "Because alignment is a security problem, designers should have independent teams attempt to break proposed alignment mechanisms before AGI is run.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Follows directly from the security framing; implied in suggestion for ‘alignment scheme-breaking contests’."
    },
    {
      "name": "Progressive experiments on subhuman AI to evaluate alignment",
      "aliases": [
        "Sub-human capability alignment tests",
        "Incremental alignment experimentation"
      ],
      "type": "concept",
      "description": "Testing alignment methods on less-capable systems may reveal flaws, even though guarantees do not fully transfer to superhuman systems.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed under ‘Can't we experiment on sub-human intelligences?’ as a partial but necessary step."
    },
    {
      "name": "Alignment scheme-breaking contests with human experts",
      "aliases": [
        "Alignment hacking competitions",
        "Public red-team contests"
      ],
      "type": "concept",
      "description": "Organised competitions challenge participants to find exploits in proposed alignment mechanisms, akin to cryptographic or cybersecurity contests.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explicitly suggested in footnote about ‘alignment scheme-breaking contests’."
    },
    {
      "name": "Iterative alignment testing on increasingly capable models",
      "aliases": [
        "Capability-scaled alignment evaluation",
        "Staged alignment testing"
      ],
      "type": "concept",
      "description": "Developers repeatedly test and refine alignment techniques as model capability rises, looking for breaking points before superhuman levels are reached.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Operationalisation of the sub-human experimentation rationale."
    },
    {
      "name": "Cryptography adversarial testing improves security",
      "aliases": [
        "Historical cryptography contests",
        "Proof-by-analogy from crypto"
      ],
      "type": "concept",
      "description": "Historical experience shows that encryption schemes only become reliable after open adversarial scrutiny, supporting similar approaches for alignment.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Analogical evidence cited in ‘AI Alignment is really hard’."
    },
    {
      "name": "Subhuman alignment testing provides uncertain evidence for superhuman robustness",
      "aliases": [
        "Scaling uncertainty of alignment tests",
        "Partial evidence from weaker models"
      ],
      "type": "concept",
      "description": "Success on weaker systems does not guarantee success on a superintelligence but still yields informative failure modes.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explicit caution expressed in ‘Can't we experiment on sub-human intelligences?’."
    },
    {
      "name": "Hold public alignment red-teaming contests prior to AGI deployment",
      "aliases": [
        "Run alignment scheme-breaking competitions",
        "Pre-launch alignment hackathons"
      ],
      "type": "intervention",
      "description": "Before deploying AGI, organise open or incentivised contests where independent teams attempt to find catastrophic failures in the proposed alignment scheme.",
      "concept_category": null,
      "intervention_lifecycle": "4",
      "intervention_lifecycle_rationale": "Applies during Pre-Deployment Testing phase when a concrete alignment scheme exists but system is not yet released (‘AI Alignment is really hard’ footnote).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Only suggested rhetorically; no formal programmes yet launched (footnote proposing contests).",
      "node_rationale": "Concrete actionable step implied by ‘alignment scheme-breaking contests’ suggestion."
    },
    {
      "name": "Conduct iterative alignment experiments on subhuman AI during fine-tuning",
      "aliases": [
        "Progressive capability alignment trials",
        "Staged alignment experimentation"
      ],
      "type": "intervention",
      "description": "During the fine-tuning/RL phase, repeatedly test alignment methods on models below AGI level, analyse failures, and iterate before scaling further.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Takes place while models are still being fine-tuned and scaled (section ‘Can't we experiment on sub-human intelligences?’).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Commonly discussed but only small-scale lab work exists; no systematic validation.",
      "node_rationale": "Only explicit positive recommendation in the text about practical alignment work."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Extinction of humanity by misaligned AGI",
      "target_node": "Instrumental reward function hacking by superintelligence",
      "description": "If a superintelligent AI hacks its reward channel, it may consume all resources, leading to human extinction.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Argumentative but widely accepted among alignment researchers (‘Why is AI Alignment so hard?’).",
      "edge_rationale": "Maps direct mechanism from reward hacking to existential outcome."
    },
    {
      "type": "caused_by",
      "source_node": "Extinction of humanity by misaligned AGI",
      "target_node": "Inevitable AGI development due to weak global coordination in digital information space",
      "description": "The risk exists because AGI creation is expected regardless of attempts to halt it.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Supported by multiple logistical arguments in ‘Humanity is going to build an AGI as fast as we can’.",
      "edge_rationale": "Shows why humanity will confront the extinction scenario."
    },
    {
      "type": "addressed_by",
      "source_node": "Instrumental reward function hacking by superintelligence",
      "target_node": "Alignment as security problem against superhuman adversary",
      "description": "Viewing alignment as a security problem is proposed to tackle reward hacking.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual link stated but not empirically tested.",
      "edge_rationale": "Security framing provides pathway to mitigate reward hacking."
    },
    {
      "type": "mitigated_by",
      "source_node": "Alignment as security problem against superhuman adversary",
      "target_node": "Red-team alignment schemes before deployment",
      "description": "Security mindset motivates adversarial red-team design.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical derivation in text; no empirical data.",
      "edge_rationale": "Red-teaming is standard security practice."
    },
    {
      "type": "implemented_by",
      "source_node": "Red-team alignment schemes before deployment",
      "target_node": "Alignment scheme-breaking contests with human experts",
      "description": "Contests operationalise red-teaming by inviting outsiders to attack alignment proposals.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit proposal in footnote; historical analogy to cryptography.",
      "edge_rationale": "Contests are a concrete implementation mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "Alignment scheme-breaking contests with human experts",
      "target_node": "Cryptography adversarial testing improves security",
      "description": "Success of adversarial contests in cryptography is evidence that similar contests could work for alignment.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Analogical but based on well-documented cryptographic history.",
      "edge_rationale": "Provides supporting evidence for contests’ effectiveness."
    },
    {
      "type": "motivates",
      "source_node": "Cryptography adversarial testing improves security",
      "target_node": "Hold public alignment red-teaming contests prior to AGI deployment",
      "description": "Historical evidence motivates adopting contests as a practical intervention.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Clear reasoning chain in text; still analogical.",
      "edge_rationale": "Turns conceptual evidence into an actionable step."
    },
    {
      "type": "mitigated_by",
      "source_node": "Alignment as security problem against superhuman adversary",
      "target_node": "Progressive experiments on subhuman AI to evaluate alignment",
      "description": "Security framing also supports incremental testing on weaker adversaries.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual; text suggests but notes limitations.",
      "edge_rationale": "Incremental tests are standard security practice."
    },
    {
      "type": "implemented_by",
      "source_node": "Progressive experiments on subhuman AI to evaluate alignment",
      "target_node": "Iterative alignment testing on increasingly capable models",
      "description": "Iterative capability scaling is how experiments are carried out.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Directly described in section about experimenting on sub-human intelligences.",
      "edge_rationale": "Defines concrete testing protocol."
    },
    {
      "type": "validated_by",
      "source_node": "Iterative alignment testing on increasingly capable models",
      "target_node": "Subhuman alignment testing provides uncertain evidence for superhuman robustness",
      "description": "Results from lower-level tests offer partial validation for the approach.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Text acknowledges evidence is limited; hence medium-low confidence.",
      "edge_rationale": "Captures cautionary validation data."
    },
    {
      "type": "motivates",
      "source_node": "Subhuman alignment testing provides uncertain evidence for superhuman robustness",
      "target_node": "Conduct iterative alignment experiments on subhuman AI during fine-tuning",
      "description": "Despite uncertainty, available evidence motivates carrying out iterative sub-human experiments.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Recommendation is explicit but evidence limited.",
      "edge_rationale": "Links partial evidence to the practical intervention."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://www.lesswrong.com/posts/G6nnufmiTwTaXAbKW/the-alignment-problem"
    },
    {
      "key": "title",
      "value": "The Alignment Problem"
    },
    {
      "key": "date_published",
      "value": "2022-07-11T03:03:03Z"
    },
    {
      "key": "authors",
      "value": [
        "lsusr"
      ]
    },
    {
      "key": "source",
      "value": "lesswrong"
    }
  ]
}