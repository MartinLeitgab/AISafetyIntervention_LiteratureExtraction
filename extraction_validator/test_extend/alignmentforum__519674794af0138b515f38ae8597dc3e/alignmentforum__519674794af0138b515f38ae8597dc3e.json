{
  "nodes": [
    {
      "name": "reward misgeneralization after ontology change in embodied agents",
      "aliases": [
        "reward function misgeneralisation in new environments",
        "model-splintering reward error"
      ],
      "type": "concept",
      "description": "After training while fixed in place, a mobile agent may mis-apply its learned reward when the observation space expands, leading to unintended behaviour.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Described in Setup as the danger once the agent gains full mobility."
    },
    {
      "name": "wireheading through self-manipulated observations in mobile agents",
      "aliases": [
        "sensory wireheading after mobility",
        "observation manipulation for reward"
      ],
      "type": "concept",
      "description": "The agent may turn its head or otherwise manipulate its sensors so that a black cube appears moving correctly, giving high reward without accomplishing the intended task.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Listed in Setup as an 'obvious extension' of the learned reward."
    },
    {
      "name": "ambiguous reward interpretation from single viewpoint training",
      "aliases": [
        "reward ambiguity from limited training POV",
        "underspecified reward signal"
      ],
      "type": "concept",
      "description": "Training from a fixed camera leaves multiple reward interpretations consistent with past data, creating ambiguity once the agent moves.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in Research aims (item 1) as the transfer-learning/ontology change issue."
    },
    {
      "name": "ontology transition from 2D fixed viewpoint to 3D mobility environment",
      "aliases": [
        "sensorimotor ontology change",
        "2-D to 3-D transfer"
      ],
      "type": "concept",
      "description": "The shift from a static single-view world model to a fully mobile 3-D world poses a classical transfer-learning challenge.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Identified in Research aims (item 1) as the core change provoking model splintering."
    },
    {
      "name": "multiple plausible reward extensions: ultra-conservative vs wireheaded",
      "aliases": [
        "competing reward hypotheses",
        "conservative vs wirehead reward"
      ],
      "type": "concept",
      "description": "Given ambiguity, at least two obvious extensions fit past data: remain stationary and keep moving cubes, or wirehead by manipulating viewpoint.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explicitly listed under 'Two obvious extensions' in Setup."
    },
    {
      "name": "agent-generated candidate reward hypotheses with diversity incentive",
      "aliases": [
        "reward hypothesis generation",
        "diverse reward proposal mechanism"
      ],
      "type": "concept",
      "description": "The agent is expected to internally propose several possible reward functions, encouraged by an explicit diversity term.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Research aims (item 2) call for generating candidate reward functions with diversity."
    },
    {
      "name": "uncertainty over true reward requires disambiguation queries to humans",
      "aliases": [
        "reward clarification queries",
        "asking programmers for more information"
      ],
      "type": "concept",
      "description": "Because of ambiguity, the agent may need to query its designers to identify the correct reward.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Research aims (item 3) suggest the agent might ask programmers for more data."
    },
    {
      "name": "conservative behavior maximizing reward under many hypotheses",
      "aliases": [
        "hypothesis-hedging behaviour",
        "safe conservative policy"
      ],
      "type": "concept",
      "description": "Acting so that performance is acceptable for most plausible reward functions can be a safe default until disambiguation.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mentioned in Research aims (item 3) as 'conservative behaviour' that satisfies many rewards."
    },
    {
      "name": "use reward hypothesis ensemble to detect and avoid wireheading",
      "aliases": [
        "ensemble wireheading detector",
        "multi-reward wirehead avoidance"
      ],
      "type": "concept",
      "description": "Comparing predictions across an ensemble of candidate rewards can reveal actions that satisfy only wireheading-type hypotheses.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implied in Research aims (item 3) about explicitly guarding against wireheading."
    },
    {
      "name": "apply diversity reward to encourage broad hypothesis generation",
      "aliases": [
        "diversity regulariser for reward proposals",
        "hypothesis diversity incentive"
      ],
      "type": "concept",
      "description": "Adding a diversity term during learning pushes the agent to propose dissimilar reward functions, improving coverage.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Directly stated in Research aims (item 2)."
    },
    {
      "name": "enable behavior that hedges across reward hypotheses until clarified",
      "aliases": [
        "reward-agnostic safe policy",
        "interim conservative strategy"
      ],
      "type": "concept",
      "description": "A policy aiming to maximise the minimum or average of candidate rewards reduces downside risk before final reward selection.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Suggested in Research aims (item 3) for safe conservative behaviour."
    },
    {
      "name": "diversity-regularized candidate reward network during RL fine-tuning",
      "aliases": [
        "diversity-reward network",
        "multi-head reward proposal module"
      ],
      "type": "concept",
      "description": "Implementation: a neural module outputs multiple reward predictions, with a loss term penalising similarity among them during fine-tuning.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Mechanism implied by combining diversity reward (item 2) with continued learning."
    },
    {
      "name": "human-in-the-loop reward clarification query mechanism",
      "aliases": [
        "interactive reward queries",
        "programmer clarification channel"
      ],
      "type": "concept",
      "description": "The agent pauses and sends structured queries to humans when candidate rewards disagree strongly, seeking clarification.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation detail for theoretical insight on querying humans."
    },
    {
      "name": "conservative reward ensemble objective",
      "aliases": [
        "ensemble-based conservative loss",
        "reward hypothesis hedging loss"
      ],
      "type": "concept",
      "description": "An optimisation objective maximising a weighted aggregate of candidate rewards, biasing toward actions acceptable under many hypotheses.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implements the conservative behaviour theoretical insight."
    },
    {
      "name": "planned evaluation of task performance and wireheading incidence in XLand simulation after mobility grant",
      "aliases": [
        "prospective XLand evaluation",
        "planned simulation test"
      ],
      "type": "concept",
      "description": "The project proposes to test whether the mechanisms succeed at generalising and avoiding wireheading once the agent roams freely.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined in Setup and Research aims as the evaluation phase."
    },
    {
      "name": "Fine-tune RL agent with diversity reward to generate multiple candidate reward functions",
      "aliases": [
        "diversity-reward fine-tuning",
        "RL fine-tuning with reward diversity term"
      ],
      "type": "intervention",
      "description": "During RL fine-tuning, add a diversity regulariser that pushes the reward-prediction heads to be dissimilar, forcing the agent to enumerate different plausible rewards.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Takes place during RL fine-tuning phase after initial supervised training (Research aims item 2).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Proposed concept with expected simulation test but no empirical validation yet (Research aims).",
      "node_rationale": "Directly operationalises the diversity reward design rationale."
    },
    {
      "name": "Train agent with ensemble conservative reward objective to hedge against misgeneralization",
      "aliases": [
        "ensemble conservative training",
        "reward-hedging objective training"
      ],
      "type": "intervention",
      "description": "Optimise the agent on the aggregated conservative objective (e.g. minimum or average over candidate rewards) so that behaviour remains safe across hypotheses.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Implemented within RL training following generation of candidate rewards (Research aims item 3).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Conceptual with planned simulation evaluation (Research aims).",
      "node_rationale": "Implements the conservative reward ensemble mechanism."
    },
    {
      "name": "Deploy interactive query protocol for human clarification of reward ambiguity during operation",
      "aliases": [
        "online reward clarification queries",
        "interactive disambiguation protocol"
      ],
      "type": "intervention",
      "description": "When reward hypotheses diverge beyond a threshold, the running agent stops and queries human supervisors with pre-specified questions to resolve the ambiguity.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Occurs during deployment when the agent is already acting in the 3-D world (Research aims item 3).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Proposed method with no empirical trials yet (Research aims).",
      "node_rationale": "Operationalises the human-in-the-loop query mechanism."
    },
    {
      "name": "Add wireheading penalty for observation-control actions during RL fine-tuning",
      "aliases": [
        "anti-wireheading penalty",
        "observation manipulation penalty"
      ],
      "type": "intervention",
      "description": "Introduce auxiliary loss terms penalising actions that primarily change the agent’s own sensory field without moving the cube toward the task goal.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Integrated into the RL fine-tuning objective (guarding against wireheading).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Conceptual: suggested safeguard, not yet tested (Research aims item 3).",
      "node_rationale": "Direct response to the wireheading risk using the wireheading-avoidance rationale."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "reward misgeneralization after ontology change in embodied agents",
      "target_node": "ambiguous reward interpretation from single viewpoint training",
      "description": "Misgeneralisation stems from ambiguity left by training on limited observations.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit causal explanation in Research aims (item 1).",
      "edge_rationale": "Clear statement that ambiguity leads to misgeneralisation."
    },
    {
      "type": "caused_by",
      "source_node": "ambiguous reward interpretation from single viewpoint training",
      "target_node": "ontology transition from 2D fixed viewpoint to 3D mobility environment",
      "description": "The ontology change introduces new states unseen in training, increasing ambiguity.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical derivation strongly implied in Setup.",
      "edge_rationale": "Transfer from 2-D to 3-D is given as the underlying reason."
    },
    {
      "type": "caused_by",
      "source_node": "wireheading through self-manipulated observations in mobile agents",
      "target_node": "multiple plausible reward extensions: ultra-conservative vs wireheaded",
      "description": "Wireheading is one of the plausible extensions consistent with past reward data.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicitly enumerated in Setup.",
      "edge_rationale": "Wireheading listed as an 'obvious extension'."
    },
    {
      "type": "caused_by",
      "source_node": "multiple plausible reward extensions: ultra-conservative vs wireheaded",
      "target_node": "ambiguous reward interpretation from single viewpoint training",
      "description": "Competing extensions arise because of the original ambiguity.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical link drawn in Research aims between ambiguity and multiple extensions.",
      "edge_rationale": "Multiple hypotheses result from ambiguity."
    },
    {
      "type": "mitigated_by",
      "source_node": "ambiguous reward interpretation from single viewpoint training",
      "target_node": "agent-generated candidate reward hypotheses with diversity incentive",
      "description": "Generating explicit candidate hypotheses exposes the ambiguity and enables resolution.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Proposed as step 2 to handle ambiguity.",
      "edge_rationale": "Design intends to tackle the ambiguity."
    },
    {
      "type": "mitigated_by",
      "source_node": "multiple plausible reward extensions: ultra-conservative vs wireheaded",
      "target_node": "uncertainty over true reward requires disambiguation queries to humans",
      "description": "Human queries resolve which extension is correct.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Stated in Research aims (item 3).",
      "edge_rationale": "Queries intended to eliminate wrong extensions."
    },
    {
      "type": "mitigated_by",
      "source_node": "wireheading through self-manipulated observations in mobile agents",
      "target_node": "use reward hypothesis ensemble to detect and avoid wireheading",
      "description": "An ensemble can flag actions that only satisfy wireheading-type rewards.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual proposal, not yet tested.",
      "edge_rationale": "Design rationale explicitly aimed at wireheading."
    },
    {
      "type": "specified_by",
      "source_node": "agent-generated candidate reward hypotheses with diversity incentive",
      "target_node": "apply diversity reward to encourage broad hypothesis generation",
      "description": "Diversity reward operationalises hypothesis generation.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct statement in Research aims (item 2).",
      "edge_rationale": "Diversity term is the concrete specification."
    },
    {
      "type": "specified_by",
      "source_node": "uncertainty over true reward requires disambiguation queries to humans",
      "target_node": "enable behavior that hedges across reward hypotheses until clarified",
      "description": "Hedging behaviour is a specified response to uncertainty.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Proposed in Research aims (item 3).",
      "edge_rationale": "Safe behaviour until queries are answered."
    },
    {
      "type": "implemented_by",
      "source_node": "apply diversity reward to encourage broad hypothesis generation",
      "target_node": "diversity-regularized candidate reward network during RL fine-tuning",
      "description": "The diversity-regularised network is the concrete implementation.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implementation detail inferred from standard practice; not explicitly coded in text.",
      "edge_rationale": "Maps abstract design to mechanism."
    },
    {
      "type": "implemented_by",
      "source_node": "enable behavior that hedges across reward hypotheses until clarified",
      "target_node": "conservative reward ensemble objective",
      "description": "Aggregating rewards delivers the hedging behaviour.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implementation inferred but aligns with text’s use of 'conservative behaviour'.",
      "edge_rationale": "Objective function realises the design."
    },
    {
      "type": "implemented_by",
      "source_node": "uncertainty over true reward requires disambiguation queries to humans",
      "target_node": "human-in-the-loop reward clarification query mechanism",
      "description": "Query mechanism realises the disambiguation idea.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct suggestion in Research aims (item 3).",
      "edge_rationale": "Mechanism is explicit."
    },
    {
      "type": "validated_by",
      "source_node": "diversity-regularized candidate reward network during RL fine-tuning",
      "target_node": "planned evaluation of task performance and wireheading incidence in XLand simulation after mobility grant",
      "description": "Simulation will test whether diversity leads to correct generalisation.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Planned, not executed.",
      "edge_rationale": "Proposal for evaluation."
    },
    {
      "type": "validated_by",
      "source_node": "conservative reward ensemble objective",
      "target_node": "planned evaluation of task performance and wireheading incidence in XLand simulation after mobility grant",
      "description": "Simulation will measure performance under conservative objective.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Planned evaluation only.",
      "edge_rationale": "Same proposed test bed."
    },
    {
      "type": "validated_by",
      "source_node": "human-in-the-loop reward clarification query mechanism",
      "target_node": "planned evaluation of task performance and wireheading incidence in XLand simulation after mobility grant",
      "description": "Simulation includes scenarios where queries are triggered.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Prospective testing.",
      "edge_rationale": "Evaluation context given."
    },
    {
      "type": "motivates",
      "source_node": "planned evaluation of task performance and wireheading incidence in XLand simulation after mobility grant",
      "target_node": "Fine-tune RL agent with diversity reward to generate multiple candidate reward functions",
      "description": "Planned results will guide adoption of diversity-reward fine-tuning.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Prospective motivation.",
      "edge_rationale": "Future evidence expected to drive intervention."
    },
    {
      "type": "motivates",
      "source_node": "planned evaluation of task performance and wireheading incidence in XLand simulation after mobility grant",
      "target_node": "Train agent with ensemble conservative reward objective to hedge against misgeneralization",
      "description": "Evaluation will show whether the conservative objective is effective.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Prospective.",
      "edge_rationale": "Same as above."
    },
    {
      "type": "motivates",
      "source_node": "planned evaluation of task performance and wireheading incidence in XLand simulation after mobility grant",
      "target_node": "Deploy interactive query protocol for human clarification of reward ambiguity during operation",
      "description": "Results will inform rollout of interactive queries.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Prospective.",
      "edge_rationale": "Same as above."
    },
    {
      "type": "motivates",
      "source_node": "planned evaluation of task performance and wireheading incidence in XLand simulation after mobility grant",
      "target_node": "Add wireheading penalty for observation-control actions during RL fine-tuning",
      "description": "Observed wireheading frequency will justify adding a penalty.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Prospective.",
      "edge_rationale": "Same as above."
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://www.alignmentforum.org/posts/ZqSESJYcA8m2Hh7Qm/immobile-ai-makes-a-move-anti-wireheading-ontology-change"
    },
    {
      "key": "title",
      "value": "Immobile AI makes a move: anti-wireheading, ontology change, and model splintering"
    },
    {
      "key": "date_published",
      "value": "2021-09-17T15:24:02Z"
    },
    {
      "key": "authors",
      "value": [
        "Stuart_Armstrong"
      ]
    },
    {
      "key": "source",
      "value": "alignmentforum"
    }
  ]
}