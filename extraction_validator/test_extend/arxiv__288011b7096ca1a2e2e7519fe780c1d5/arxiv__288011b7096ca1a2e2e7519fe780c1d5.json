{
  "nodes": [
    {
      "name": "Reward misspecification in reinforcement learning for complex tasks",
      "aliases": [
        "incorrect reward design in RL",
        "misaligned reward functions"
      ],
      "type": "concept",
      "description": "When hand-crafted reward functions fail to reflect desired objectives, RL agents may learn behaviours that diverge from human intent, especially in real-world or safety-critical domains.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Introduced in 1 Introduction as a core motivation for alternative reward specification."
    },
    {
      "name": "Sample inefficiency in sparse reward reinforcement learning",
      "aliases": [
        "slow exploration with sparse rewards",
        "inefficient learning under sparse signals"
      ],
      "type": "concept",
      "description": "RL agents receiving only goal-state rewards require extensive exploration, leading to very slow or failed learning in environments like Montezuma’s Revenge.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Highlighted in 1 Introduction as a key obstacle to practical deployment."
    },
    {
      "name": "High expertise requirement for manual reward shaping",
      "aliases": [
        "manual dense reward design difficulty",
        "expert crafted shaping burden"
      ],
      "type": "concept",
      "description": "Designing informative dense rewards typically needs domain experts and is labor-intensive, limiting scalability for non-experts.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Discussed in 1 Introduction as the practical barrier to dense reward use."
    },
    {
      "name": "Sparse external rewards in complex RL environments",
      "aliases": [
        "lack of intermediate reinforcement",
        "goal-only reward signals"
      ],
      "type": "concept",
      "description": "Environments often provide only terminal rewards; intermediate progress is un-rewarded, causing exploration difficulties.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in Figure 1 narrative showing Montezuma’s Revenge example."
    },
    {
      "name": "Symbol grounding ambiguity in natural language instructions",
      "aliases": [
        "language grounding problem",
        "mapping words to actions/objects"
      ],
      "type": "concept",
      "description": "A system must learn how linguistic tokens correspond to environment actions and objects; ambiguity and incompleteness complicate this mapping.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Outlined as Challenge 1 in 1 Introduction."
    },
    {
      "name": "Natural language instructions provide easily specified intermediate reward signals",
      "aliases": [
        "language as dense reward source",
        "verbal instruction shaping"
      ],
      "type": "concept",
      "description": "Because humans can effortlessly give instructions, language can encode progress cues that replace manually designed numeric rewards.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposed in 1 Introduction as the central idea."
    },
    {
      "name": "Potential-based shaping maintains optimal policy invariance",
      "aliases": [
        "policy-invariant shaping theory",
        "Ng et al. 1999 potential shaping"
      ],
      "type": "concept",
      "description": "Using a potential function ϕ to derive shaping rewards guarantees that any policy optimal for the original reward remains optimal after shaping.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Referenced in Section 4 when defining R_lang."
    },
    {
      "name": "Mapping language and action history to relatedness potential function",
      "aliases": [
        "language-action relatedness scoring",
        "instruction compliance scoring"
      ],
      "type": "concept",
      "description": "Design principle: predict whether the recent action distribution matches the instruction, and use the difference in related vs. unrelated probabilities as the potential ϕ.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explained in Section 4 as core of reward shaping."
    },
    {
      "name": "Action-frequency vector representation of trajectories",
      "aliases": [
        "bag-of-actions feature",
        "action histogram encoding"
      ],
      "type": "concept",
      "description": "Trajectories are summarized by the frequency of each discrete action between two timesteps, simplifying input for language-action matching.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Defined in Section 3.1 before neural network."
    },
    {
      "name": "Training LEARN with paired trajectory-language data and negative sampling",
      "aliases": [
        "LEARN supervised training procedure",
        "positive/negative relatedness learning"
      ],
      "type": "concept",
      "description": "A neural network is trained on positive (matching) and synthetic negative (non-matching) (action-freq, instruction) pairs using Adam optimisation to predict RELATED vs. UNRELATED.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.1 and 3.2 detail data creation and training."
    },
    {
      "name": "Sentence encoder variants for instruction embedding",
      "aliases": [
        "InferSent/GloVe+RNN/RNNOnly encoders",
        "language embedding choices"
      ],
      "type": "concept",
      "description": "Instructions are embedded via either a fixed InferSent model or trainable GloVe+GRU / GRU-only encoders, balancing prior knowledge and flexibility.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Enumerated in Section 3.1 Language encoder."
    },
    {
      "name": "Computation of shaping reward as discounted potential difference",
      "aliases": [
        "R_lang calculation",
        "ϕ(ft) potential-based reward"
      ],
      "type": "concept",
      "description": "At each step the shaping reward R_lang(ft)=γ·ϕ(ft)−ϕ(ft−1) is computed, ensuring consistency with potential-based shaping theory.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Formula introduced in Section 4."
    },
    {
      "name": "LEARN classifier validation accuracy on held-out rooms",
      "aliases": [
        "LEARN offline evaluation",
        "related/unrelated prediction accuracy"
      ],
      "type": "concept",
      "description": "Using 40 k validation pairs from unseen rooms, the best LEARN model is selected by accuracy prior to RL experiments.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Described in Section 5 Experimental Evaluation dataset split."
    },
    {
      "name": "30 % faster learning and 60 % higher episode success on Montezuma’s Revenge with language shaping",
      "aliases": [
        "Ext+Lang performance gains",
        "sample-efficiency improvement evidence"
      ],
      "type": "concept",
      "description": "Across 15 tasks, Ext+Lang reached baseline success levels after 358 k vs. 500 k steps and doubled final successes, with significance on 11/15 tasks.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Quantitative results in Section 5.1 and Figure 4."
    },
    {
      "name": "Collect human-annotated trajectory-language dataset for LEARN pre-training",
      "aliases": [
        "crowd-sourced instruction dataset creation",
        "offline language-trajectory data gathering"
      ],
      "type": "intervention",
      "description": "Before RL training, gather gameplay trajectories and obtain multiple natural-language descriptions via crowdsourcing, then filter and prepare positive/negative pairs for LEARN supervision.",
      "concept_category": null,
      "intervention_lifecycle": "2",
      "intervention_lifecycle_rationale": "Dataset is built prior to any model training as detailed in Section 3.2 Data collection.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Demonstrated only on one Atari domain; proof-of-concept level.",
      "node_rationale": "Essential prerequisite enabling LEARN training and subsequent reward shaping."
    },
    {
      "name": "Integrate LEARN-generated potential-based language rewards during RL policy optimisation",
      "aliases": [
        "apply R_lang shaping in PPO",
        "language-shaped RL training"
      ],
      "type": "intervention",
      "description": "During online learning (e.g. PPO), augment the extrinsic reward with R_total = R_ext + λ·R_lang, where R_lang is computed from LEARN predictions each timestep.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Implemented during the agent’s fine-tuning/RL training loop (Section 4 and 5).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Evaluated experimentally but not yet deployed in real-world systems.",
      "node_rationale": "Directly operationalises the paper’s proposed solution and shows empirical gains."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Reward misspecification in reinforcement learning for complex tasks",
      "target_node": "High expertise requirement for manual reward shaping",
      "description": "Misaligned rewards often stem from the difficulty non-experts have in specifying dense, correct shaping signals.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Stated rationale in 1 Introduction without quantitative proof.",
      "edge_rationale": "Section 1 links reward design difficulty to mis-specification risk."
    },
    {
      "type": "caused_by",
      "source_node": "Sample inefficiency in sparse reward reinforcement learning",
      "target_node": "Sparse external rewards in complex RL environments",
      "description": "Sparse rewards directly lead to long exploration times and hence sample inefficiency.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Widely accepted in RL literature and illustrated in Figure 1.",
      "edge_rationale": "Figure 1 example demonstrates exploration difficulty under sparse reward."
    },
    {
      "type": "mitigated_by",
      "source_node": "High expertise requirement for manual reward shaping",
      "target_node": "Natural language instructions provide easily specified intermediate reward signals",
      "description": "Using language allows non-experts to specify desired behaviours without manual numeric shaping.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Conceptual argument in Introduction; no direct metrics.",
      "edge_rationale": "Section 1 argues language is easy for humans."
    },
    {
      "type": "mitigated_by",
      "source_node": "Sparse external rewards in complex RL environments",
      "target_node": "Natural language instructions provide easily specified intermediate reward signals",
      "description": "Language-derived signals create dense feedback that counters sparsity.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Reasoned in Introduction; later validated experimentally.",
      "edge_rationale": "Paper positions language shaping as solution to sparse reward."
    },
    {
      "type": "enabled_by",
      "source_node": "Natural language instructions provide easily specified intermediate reward signals",
      "target_node": "Potential-based shaping maintains optimal policy invariance",
      "description": "Implementing language shaping safely requires the theoretical guarantee of potential-based shaping.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implicit logical dependency; moderate inference.",
      "edge_rationale": "Section 4 links language rewards to potential-based formula."
    },
    {
      "type": "implemented_by",
      "source_node": "Potential-based shaping maintains optimal policy invariance",
      "target_node": "Mapping language and action history to relatedness potential function",
      "description": "The potential ϕ is instantiated via a language-action relatedness predictor.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct description in Section 4.",
      "edge_rationale": "ϕ is defined as p_R - p_U from LEARN."
    },
    {
      "type": "refined_by",
      "source_node": "Mapping language and action history to relatedness potential function",
      "target_node": "Action-frequency vector representation of trajectories",
      "description": "The design employs action-frequency vectors to create predictor inputs.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit model description in Section 3.1.",
      "edge_rationale": "Action-frequency vectors are the concrete feature choice."
    },
    {
      "type": "implemented_by",
      "source_node": "Action-frequency vector representation of trajectories",
      "target_node": "Training LEARN with paired trajectory-language data and negative sampling",
      "description": "The vectors feed into a neural network trained with supervised positive/negative pairs.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Detailed training procedure in Sections 3.1–3.2.",
      "edge_rationale": "LEARN training uses these representations."
    },
    {
      "type": "specified_by",
      "source_node": "Training LEARN with paired trajectory-language data and negative sampling",
      "target_node": "Sentence encoder variants for instruction embedding",
      "description": "Sentence embeddings form part of the LEARN architecture during training.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Architecture figures and text list encoder options.",
      "edge_rationale": "Encoders are modular components inside LEARN."
    },
    {
      "type": "enabled_by",
      "source_node": "Training LEARN with paired trajectory-language data and negative sampling",
      "target_node": "Computation of shaping reward as discounted potential difference",
      "description": "A trained LEARN model supplies the probabilities needed to compute ϕ and hence R_lang.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct functional dependency in Section 4.",
      "edge_rationale": "R_lang uses p_R and p_U outputs."
    },
    {
      "type": "validated_by",
      "source_node": "Computation of shaping reward as discounted potential difference",
      "target_node": "LEARN classifier validation accuracy on held-out rooms",
      "description": "Offline accuracy on unseen rooms demonstrates the predictor works before RL.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Validation split results reported in Section 5.",
      "edge_rationale": "Accuracy provides initial evidence of correct mapping."
    },
    {
      "type": "validated_by",
      "source_node": "Computation of shaping reward as discounted potential difference",
      "target_node": "30 % faster learning and 60 % higher episode success on Montezuma’s Revenge with language shaping",
      "description": "Online RL experiments confirm that computed shaping rewards improve learning.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Controlled experiments with statistical tests in Section 5.1.",
      "edge_rationale": "Performance gains directly measure shaping efficacy."
    },
    {
      "type": "motivates",
      "source_node": "LEARN classifier validation accuracy on held-out rooms",
      "target_node": "Collect human-annotated trajectory-language dataset for LEARN pre-training",
      "description": "Need for accurate LEARN requires sufficiently large labelled datasets, motivating data collection.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical inference; data collection discussed but link implicit.",
      "edge_rationale": "Section 3.2 shows dataset importance for model accuracy."
    },
    {
      "type": "required_by",
      "source_node": "Collect human-annotated trajectory-language dataset for LEARN pre-training",
      "target_node": "Training LEARN with paired trajectory-language data and negative sampling",
      "description": "The supervised training step depends on the availability of labelled data.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit in Section 3.2 data pipeline.",
      "edge_rationale": "Dataset feeds the training process."
    },
    {
      "type": "motivates",
      "source_node": "30 % faster learning and 60 % higher episode success on Montezuma’s Revenge with language shaping",
      "target_node": "Integrate LEARN-generated potential-based language rewards during RL policy optimisation",
      "description": "Empirical gains encourage practitioners to adopt language-shaped rewards in RL pipelines.",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Strong experimental evidence across tasks.",
      "edge_rationale": "Section 5.1 concludes that Ext+Lang is superior."
    }
  ],
  "meta": [
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "date_published",
      "value": "2019-03-05T00:00:00Z"
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/1903.02020"
    },
    {
      "key": "authors",
      "value": [
        "Prasoon Goyal",
        "Scott Niekum",
        "Raymond J. Mooney"
      ]
    },
    {
      "key": "title",
      "value": "Using Natural Language for Reward Shaping in Reinforcement Learning"
    }
  ]
}