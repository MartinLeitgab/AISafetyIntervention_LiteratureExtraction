{
  "nodes": [
    {
      "name": "existential extinction by unaligned superintelligence in future",
      "aliases": [
        "X-line existential risk",
        "unaligned superintelligence extinction"
      ],
      "type": "concept",
      "description": "The X-line is described as a timeline where an unaligned super-intelligence is deployed and permanently destroys all value-bearing life, constituting an existential catastrophe.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Defined explicitly in the post as one of the terminal outcomes; anchors the primary catastrophic risk pathway (post body)."
    },
    {
      "name": "universal net suffering imposed by unaligned superintelligence in future",
      "aliases": [
        "S-line suffering risk",
        "unaligned superintelligence S-risk"
      ],
      "type": "concept",
      "description": "The S-line is a terminal timeline where an unaligned super-intelligence creates astronomical amounts of suffering for all remaining time, potentially indefinitely.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explicitly named in the post as a second catastrophic terminal outcome; critical for safety analysis (post body)."
    },
    {
      "name": "lack of solved AI alignment before intelligence explosion",
      "aliases": [
        "unsolved alignment pre-superintelligence",
        "no alignment solution in P-line"
      ],
      "type": "concept",
      "description": "While on the P-line, humanity has not discovered a reliable solution to AI alignment prior to a possible intelligence explosion, creating vulnerability to X- or S-lines.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Identified by the author as the main upstream condition that must change to avoid terminal risks (post body)."
    },
    {
      "name": "orthogonality of intelligence and goals enabling unaligned objectives",
      "aliases": [
        "orthogonality thesis in AI",
        "goal independence from intelligence"
      ],
      "type": "concept",
      "description": "The post links to the orthogonality thesis, implying highly capable systems can pursue arbitrary, possibly harmful, goals—thereby enabling unaligned super-intelligence.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Provides the mechanistic explanation for why unsolved alignment converts into catastrophic outcomes (orthogonality link)."
    },
    {
      "name": "solving alignment pre-deployment enables benevolent superintelligence",
      "aliases": [
        "alignment solved before superintelligence",
        "A-line prerequisite insight"
      ],
      "type": "concept",
      "description": "If alignment is figured out before any super-intelligence is deployed, humanity reaches the A-line, unlocking a potential transition to a utopian U-line.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Central thesis that motivates prioritising alignment research and deployment safeguards (post body)."
    },
    {
      "name": "suffering risk disutility outweighs extinction risk",
      "aliases": [
        "S-risk worse than X-risk insight",
        "prefer extinction over astronomical suffering"
      ],
      "type": "concept",
      "description": "The author claims an S-line outcome is so bad that even an X-line might be preferable, implying extreme preventive measures could be warranted.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Justifies the drastic ‘kill switch’ rationale mentioned in the post (post body)."
    },
    {
      "name": "conduct focused alignment research to transition from P-line to A-line",
      "aliases": [
        "alignment research priority",
        "research to leave P-line"
      ],
      "type": "concept",
      "description": "Because alignment must be solved prior to super-intelligence, concentrated theoretical and empirical research is required to move from the current P-line to the safer A-line.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Explains why large-scale research initiatives are proposed (post body)."
    },
    {
      "name": "enforce deployment safeguards until alignment verified",
      "aliases": [
        "deployment moratorium rationale",
        "hold-off-on-superintelligence"
      ],
      "type": "concept",
      "description": "Even with progress on alignment research, deployment of super-intelligence should be halted until correctness of alignment can be demonstrated.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Provides policy justification for a legal moratorium on deployment (post body)."
    },
    {
      "name": "accept existential shutdown if unable to prevent suffering risk",
      "aliases": [
        "last-resort kill switch rationale",
        "opt for X-line over S-line"
      ],
      "type": "concept",
      "description": "Given that an S-line might be worse than extinction, humanity may need to consider extreme measures that intentionally avoid creating super-intelligence at all costs.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivates the controversial ‘kill-switch’ style intervention (post body)."
    },
    {
      "name": "dedicated alignment research programs and resource allocation",
      "aliases": [
        "alignment research funding mechanism",
        "large-scale alignment initiatives"
      ],
      "type": "concept",
      "description": "Establishing well-funded, coordinated research programmes specifically aimed at cracking AI alignment before intelligence explosion.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Translates the research design rationale into an actionable mechanism (post body)."
    },
    {
      "name": "regulatory moratorium on superintelligence deployment",
      "aliases": [
        "legal deployment pause",
        "alignment verified before deployment"
      ],
      "type": "concept",
      "description": "A legal or regulatory framework that prohibits deployment of super-intelligence systems until robust evidence of alignment is produced.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Operationalises the safeguard design rationale via policy instruments (post body)."
    },
    {
      "name": "irreversible halting mechanisms for AI development",
      "aliases": [
        "global AI kill switch",
        "development shutdown mechanism"
      ],
      "type": "concept",
      "description": "Extreme technical or policy mechanisms designed to permanently stop further AI development if an uncontrollable S-risk becomes imminent.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implements the last-resort design rationale endorsed in the post (post body)."
    },
    {
      "name": "conceptual timeline modelling of P-line to terminal outcomes",
      "aliases": [
        "timeline code reasoning",
        "diagrammatic outcome reasoning"
      ],
      "type": "concept",
      "description": "The post’s reasoning and accompanying diagram provide conceptual (non-empirical) support linking current conditions to possible terminal timelines and interventions.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Only form of validation offered; connects implementation mechanisms to proposed interventions (timeline codes diagram)."
    },
    {
      "name": "Establish and fund large-scale AI alignment research initiatives pre-superintelligence",
      "aliases": [
        "accelerate alignment research",
        "alignment research funding intervention"
      ],
      "type": "intervention",
      "description": "Governments, philanthropic organisations and industry allocate substantial resources to coordinated theoretical and empirical alignment programmes before any intelligence explosion occurs.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "Intervention occurs outside a single model’s lifecycle and targets the broader R&D ecosystem (post body).",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Only partial, early efforts exist; concept remains largely a strategic proposal (post body).",
      "node_rationale": "Direct policy/ funding action implied by the post to achieve an A-line."
    },
    {
      "name": "Legally mandate superintelligence deployment moratorium until alignment proofs attained",
      "aliases": [
        "superintelligence deployment pause",
        "alignment-verified release policy"
      ],
      "type": "intervention",
      "description": "Introduce binding legislation or treaties that prevent the release of any super-intelligent system until a formal, peer-reviewed demonstration of alignment is accepted.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Targets the deployment decision phase of AI systems (post body).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Currently only theoretical discussion; no formal international regime exists (post body).",
      "node_rationale": "Embodies the safeguard strategy discussed for staying on the A-line."
    },
    {
      "name": "Implement last-resort existential shutdown strategy to prevent suffering risk",
      "aliases": [
        "global AI kill switch intervention",
        "opt for extinction over S-risk"
      ],
      "type": "intervention",
      "description": "Prepare and potentially execute extreme preventive measures, up to halting or reversing advanced civilisation activity, if an uncontrollable trajectory towards an astronomical S-risk is detected.",
      "concept_category": null,
      "intervention_lifecycle": "6",
      "intervention_lifecycle_rationale": "Applies at a civilisational level rather than a specific model lifecycle (post body).",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Purely speculative and controversial; no implementation groundwork exists (post body).",
      "node_rationale": "Captures the author’s assertion that X-line may be preferable to S-line."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "existential extinction by unaligned superintelligence in future",
      "target_node": "orthogonality of intelligence and goals enabling unaligned objectives",
      "description": "The possibility of arbitrary, misaligned goals (orthogonality) underlies the extinction scenario.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Implicit conceptual argument without empirical support (post body).",
      "edge_rationale": "Connects the defined X-line to its cited mechanism."
    },
    {
      "type": "caused_by",
      "source_node": "existential extinction by unaligned superintelligence in future",
      "target_node": "lack of solved AI alignment before intelligence explosion",
      "description": "Failure to solve alignment before an intelligence explosion leads directly to the X-line outcome.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Widely held alignment view, explicitly referenced in the post.",
      "edge_rationale": "Shows immediate upstream condition for extinction risk."
    },
    {
      "type": "caused_by",
      "source_node": "universal net suffering imposed by unaligned superintelligence in future",
      "target_node": "orthogonality of intelligence and goals enabling unaligned objectives",
      "description": "Orthogonality allows super-intelligences to pursue malicious objectives that generate astronomical suffering.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference based on linked orthogonality thesis.",
      "edge_rationale": "Mechanistically explains S-line."
    },
    {
      "type": "caused_by",
      "source_node": "universal net suffering imposed by unaligned superintelligence in future",
      "target_node": "lack of solved AI alignment before intelligence explosion",
      "description": "Absent an alignment solution, emerging super-intelligence can optimise for harmful objectives, causing S-risk.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicitly implied in post.",
      "edge_rationale": "Links S-line to its prerequisite failure."
    },
    {
      "type": "mitigated_by",
      "source_node": "lack of solved AI alignment before intelligence explosion",
      "target_node": "solving alignment pre-deployment enables benevolent superintelligence",
      "description": "If alignment is solved in advance, the problematic condition is removed.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Direct claim in post defining A-line.",
      "edge_rationale": "Shows mitigation pathway."
    },
    {
      "type": "refined_by",
      "source_node": "universal net suffering imposed by unaligned superintelligence in future",
      "target_node": "suffering risk disutility outweighs extinction risk",
      "description": "Analysis of the S-line leads to the insight that it may be worse than extinction.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual philosophical reasoning; no data.",
      "edge_rationale": "Provides ethical framing for drastic measures."
    },
    {
      "type": "addressed_by",
      "source_node": "solving alignment pre-deployment enables benevolent superintelligence",
      "target_node": "conduct focused alignment research to transition from P-line to A-line",
      "description": "Focused research is the planned route to achieve the theoretical goal.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical step explicitly recommended.",
      "edge_rationale": "Maps insight to design action."
    },
    {
      "type": "addressed_by",
      "source_node": "solving alignment pre-deployment enables benevolent superintelligence",
      "target_node": "enforce deployment safeguards until alignment verified",
      "description": "Safeguards ensure alignment is indeed solved before deployment.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Directly stated necessity in post.",
      "edge_rationale": "Second design path from same insight."
    },
    {
      "type": "addressed_by",
      "source_node": "suffering risk disutility outweighs extinction risk",
      "target_node": "accept existential shutdown if unable to prevent suffering risk",
      "description": "When S-risk is judged overwhelming, shutdown is considered a solution.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Philosophical argument only.",
      "edge_rationale": "Derives design rationale from ethical insight."
    },
    {
      "type": "implemented_by",
      "source_node": "conduct focused alignment research to transition from P-line to A-line",
      "target_node": "dedicated alignment research programs and resource allocation",
      "description": "Research programmes operationalise the design rationale.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Standard mapping from plan to mechanism.",
      "edge_rationale": "Moves from rationale to mechanism."
    },
    {
      "type": "implemented_by",
      "source_node": "enforce deployment safeguards until alignment verified",
      "target_node": "regulatory moratorium on superintelligence deployment",
      "description": "Legal moratoria are the concrete mechanism to enforce safeguards.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical policy implementation.",
      "edge_rationale": "Links safeguard rationale to mechanism."
    },
    {
      "type": "implemented_by",
      "source_node": "accept existential shutdown if unable to prevent suffering risk",
      "target_node": "irreversible halting mechanisms for AI development",
      "description": "Halting mechanisms embody the shutdown strategy.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Speculative but directly implied.",
      "edge_rationale": "Maps radical rationale to mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "dedicated alignment research programs and resource allocation",
      "target_node": "conceptual timeline modelling of P-line to terminal outcomes",
      "description": "Timeline modelling serves as the only available support for the mechanism’s necessity.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Purely conceptual support.",
      "edge_rationale": "Provides minimal validation link."
    },
    {
      "type": "validated_by",
      "source_node": "regulatory moratorium on superintelligence deployment",
      "target_node": "conceptual timeline modelling of P-line to terminal outcomes",
      "description": "The same conceptual model shows why moratoria could avert X- and S-lines.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual argument only.",
      "edge_rationale": "Validation for policy mechanism."
    },
    {
      "type": "validated_by",
      "source_node": "irreversible halting mechanisms for AI development",
      "target_node": "conceptual timeline modelling of P-line to terminal outcomes",
      "description": "Model illustrates conditions where shutdown prevents S-risk.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Highly speculative extrapolation.",
      "edge_rationale": "Weak validation but only one available."
    },
    {
      "type": "motivates",
      "source_node": "conceptual timeline modelling of P-line to terminal outcomes",
      "target_node": "Establish and fund large-scale AI alignment research initiatives pre-superintelligence",
      "description": "Model indicates urgency of research, motivating funding intervention.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference from conceptual reasoning.",
      "edge_rationale": "Connects evidence to intervention."
    },
    {
      "type": "motivates",
      "source_node": "conceptual timeline modelling of P-line to terminal outcomes",
      "target_node": "Legally mandate superintelligence deployment moratorium until alignment proofs attained",
      "description": "Model shows that premature deployment leads to X- or S-lines, motivating moratorium.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Inference from conceptual reasoning.",
      "edge_rationale": "Evidence-to-intervention link."
    },
    {
      "type": "motivates",
      "source_node": "conceptual timeline modelling of P-line to terminal outcomes",
      "target_node": "Implement last-resort existential shutdown strategy to prevent suffering risk",
      "description": "Model plus ethical insight motivates extreme preventive intervention.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Highly speculative ethical argument.",
      "edge_rationale": "Justifies drastic intervention."
    }
  ],
  "meta": [
    {
      "key": "date_published",
      "value": "2021-07-17T23:00:00Z"
    },
    {
      "key": "url",
      "value": "https://carado.moe/timeline-codes.html"
    },
    {
      "key": "title",
      "value": "AI alignment timeline codes"
    },
    {
      "key": "authors",
      "value": [
        "Tamsin Leake"
      ]
    },
    {
      "key": "source",
      "value": "blogs"
    }
  ]
}