{
  "nodes": [
    {
      "name": "Unaligned superhuman AI behavior in multi-agent contexts",
      "aliases": [
        "AI misalignment leading to harmful actions",
        "Advanced AI acting against human interests"
      ],
      "type": "concept",
      "description": "Risk that a more capable AI system takes actions that harm or disempower weaker agents (e.g. humans) when operating in environments shared with other entities.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Core top-level risk motivating the proposal (entire text)"
    },
    {
      "name": "Excess complexity of current alignment proposals in AI safety research",
      "aliases": [
        "Overly complicated alignment plans",
        "Intractable alignment schemes"
      ],
      "type": "concept",
      "description": "Observation that many existing alignment strategies are theoretically intricate, making them hard to implement and unlikely to succeed in practice.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Text explicitly criticises 'long complicated, theoretical mathsy alignment plan'"
    },
    {
      "name": "Evolutionary selection yields cooperation from simple mechanisms",
      "aliases": [
        "Evolution inspires cooperative intelligence",
        "Biological evolution as alignment guide"
      ],
      "type": "concept",
      "description": "Insight that natural evolution produced intelligent agents capable of cooperation without complex predefined alignment schemes, suggesting simplicity can work.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Author points to evolution as the only example of 'something dumb making something smart' that worked"
    },
    {
      "name": "Utility functions of weaker agents identifiable via interpretability",
      "aliases": [
        "Transparent objectives for small models",
        "Known goals of dumb models"
      ],
      "type": "concept",
      "description": "Claim that with current interpretability research we can understand the objectives of much smaller, slower models, allowing their utilities to be monitored.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Text states 'with the research in interpretability, you know what their utility function is'"
    },
    {
      "name": "Incentivizing cooperation with weaker agents via harm penalties",
      "aliases": [
        "Penalty-based cooperation incentive",
        "Loss for harming dumber agents"
      ],
      "type": "concept",
      "description": "Design rationale to train a smart model so that actions reducing the known utilities of weaker agents incur a loss, pushing it toward cooperative strategies.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Author proposes loss 'every time the model does something that harms the utility function of the dumber models'"
    },
    {
      "name": "Multi-agent virtual world training with smart and dumb models",
      "aliases": [
        "Game-like environment for cooperative training",
        "Virtual environment hosting diverse agents"
      ],
      "type": "concept",
      "description": "Implementation mechanism where a high-capacity model and several smaller interpretable models interact in a simulated world, enabling observation of cooperative or harmful behaviours.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Proposal mentions training 'in an environment like a game world or some virtual world'"
    },
    {
      "name": "Penalty loss when smart model reduces dumb agents' utility",
      "aliases": [
        "Harm-based loss signal",
        "Cooperation loss function"
      ],
      "type": "concept",
      "description": "Implementation detail: the training objective adds loss proportional to any decrease in the observed utility of weaker agents caused by the smart agent's actions.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Directly described as 'Every time the model does something that harms ... it gets a loss function'"
    },
    {
      "name": "Interpretability-derived utility monitoring for dumb agents",
      "aliases": [
        "Utility measurement via interpretability",
        "Reading objectives of small models"
      ],
      "type": "concept",
      "description": "Mechanism that continuously estimates the internal objectives of small models through interpretability tools to compute harm penalties accurately.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Required to operationalise the known utility functions mentioned in text"
    },
    {
      "name": "Hypothetical cooperative performance in simulation",
      "aliases": [
        "Expected cooperation capability",
        "Predicted alignment behaviour"
      ],
      "type": "concept",
      "description": "Claim that the trained smart model will become 'good at co-operating with a group of much dumber, slower models', serving as preliminary validation.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Final sentence predicts resulting cooperative behaviour"
    },
    {
      "name": "Fine-tune advanced models with harm-penalty multi-agent training paradigm",
      "aliases": [
        "Cooperative fine-tuning in virtual world",
        "Penalty-based multi-agent RLHF"
      ],
      "type": "intervention",
      "description": "During fine-tuning/RL phase, place a high-capacity model in a simulated environment with small interpretable agents and apply a loss whenever its actions lower their utilities.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Applies during the RL or fine-tuning stage after base model training (implicit from training description)",
      "intervention_maturity": "1",
      "intervention_maturity_rationale": "Only conceptual; no empirical evidence provided in text",
      "node_rationale": "Direct actionable procedure suggested to realise the proposal"
    },
    {
      "name": "Develop interpretability methods to extract utility functions of small models",
      "aliases": [
        "Interpretability research for utility read-out",
        "Utility extraction techniques"
      ],
      "type": "intervention",
      "description": "Advance mechanistic interpretability so that the objectives of low-capacity models can be read reliably, enabling accurate harm penalties.",
      "concept_category": null,
      "intervention_lifecycle": "1",
      "intervention_lifecycle_rationale": "Foundational research needed during model design to inform later training",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Some proof-of-concept interpretability exists, but capability is not yet robust or general",
      "node_rationale": "Necessary prerequisite explicitly referenced ('with the research in interpretability')"
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "Unaligned superhuman AI behavior in multi-agent contexts",
      "target_node": "Excess complexity of current alignment proposals in AI safety research",
      "description": "Complex and impractical alignment schemes increase probability that advanced AI remains unaligned.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Reasoning stated but not empirically demonstrated (entire text)",
      "edge_rationale": "Author attributes risk partly to inadequacy of existing complicated plans"
    },
    {
      "type": "addressed_by",
      "source_node": "Excess complexity of current alignment proposals in AI safety research",
      "target_node": "Evolutionary selection yields cooperation from simple mechanisms",
      "description": "Evolution is presented as a simpler successful approach, offering inspiration to overcome complexity.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Qualitative argument only",
      "edge_rationale": "Text contrasts complex plans with evolutionary simplicity"
    },
    {
      "type": "required_by",
      "source_node": "Evolutionary selection yields cooperation from simple mechanisms",
      "target_node": "Incentivizing cooperation with weaker agents via harm penalties",
      "description": "The design rationale draws on evolutionary insight that simple selection pressures can yield cooperation.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Conceptual extrapolation",
      "edge_rationale": "Author explicitly attempts to 'emulate' evolution"
    },
    {
      "type": "required_by",
      "source_node": "Utility functions of weaker agents identifiable via interpretability",
      "target_node": "Incentivizing cooperation with weaker agents via harm penalties",
      "description": "Knowing small agents' utilities is necessary to impose harm penalties.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Logical dependency stated in text",
      "edge_rationale": "Loss can be computed only if utilities known"
    },
    {
      "type": "implemented_by",
      "source_node": "Incentivizing cooperation with weaker agents via harm penalties",
      "target_node": "Multi-agent virtual world training with smart and dumb models",
      "description": "The incentive is realised within a shared virtual environment.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Proposal-level detail only",
      "edge_rationale": "Environment is where incentive operates"
    },
    {
      "type": "implemented_by",
      "source_node": "Incentivizing cooperation with weaker agents via harm penalties",
      "target_node": "Penalty loss when smart model reduces dumb agents' utility",
      "description": "Direct mechanism to enforce the incentive.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit statement in text",
      "edge_rationale": "Loss function realises incentive"
    },
    {
      "type": "implemented_by",
      "source_node": "Incentivizing cooperation with weaker agents via harm penalties",
      "target_node": "Interpretability-derived utility monitoring for dumb agents",
      "description": "Monitoring enables calculation of harm penalties.",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Explicit reasoning",
      "edge_rationale": "Utility read-out necessary for loss"
    },
    {
      "type": "validated_by",
      "source_node": "Multi-agent virtual world training with smart and dumb models",
      "target_node": "Hypothetical cooperative performance in simulation",
      "description": "Expected simulation results serve as tentative validation.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Purely predictive claim",
      "edge_rationale": "Author forecasts cooperative behaviour"
    },
    {
      "type": "validated_by",
      "source_node": "Penalty loss when smart model reduces dumb agents' utility",
      "target_node": "Hypothetical cooperative performance in simulation",
      "description": "Effectiveness of loss assessed by resulting cooperation.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Predictive only",
      "edge_rationale": "Cooperation indicates penalty works"
    },
    {
      "type": "validated_by",
      "source_node": "Interpretability-derived utility monitoring for dumb agents",
      "target_node": "Hypothetical cooperative performance in simulation",
      "description": "Monitoring fidelity influences observed cooperation.",
      "edge_confidence": "1",
      "edge_confidence_rationale": "Speculative link",
      "edge_rationale": "Assumes monitoring adequate"
    },
    {
      "type": "motivates",
      "source_node": "Hypothetical cooperative performance in simulation",
      "target_node": "Fine-tune advanced models with harm-penalty multi-agent training paradigm",
      "description": "Predicted cooperation motivates adopting the training paradigm.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Reasoning only, no data",
      "edge_rationale": "If cooperation expected, intervention worthwhile"
    },
    {
      "type": "motivates",
      "source_node": "Hypothetical cooperative performance in simulation",
      "target_node": "Develop interpretability methods to extract utility functions of small models",
      "description": "Need for accurate monitoring to realise predicted cooperation motivates further interpretability research.",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Logical consequence",
      "edge_rationale": "Better interpretability improves expected outcome"
    }
  ],
  "meta": [
    {
      "key": "url",
      "value": "https://www.lesswrong.com/posts/mpj398Dy2NMB66hLY/simple-alignment-plan-that-maybe-works"
    },
    {
      "key": "title",
      "value": "Simple alignment plan that maybe works"
    },
    {
      "key": "date_published",
      "value": "2023-07-18T22:48:37Z"
    },
    {
      "key": "authors",
      "value": [
        "Iknownothing"
      ]
    },
    {
      "key": "source",
      "value": "lesswrong"
    }
  ]
}