{
  "decision": {
    "is_valid_json": true,
    "has_blockers": false,
    "flag_underperformance": false,
    "valid_and_mergeable_after_fixes": true,
    "summary": "The extracted knowledge graph demonstrates strong structural validity with all 15 nodes properly connected and no orphaned elements. However, the extraction has minor schema compliance issues (missing required rationale fields) and moderate content issues (two inferred interventions not present in source text with unsupported maturity ratings). The AI safety pathway represents moderate inference extending source implications beyond what is explicitly stated. After applying the 9 proposed field changes to add missing rationale documentation and adjusting maturity levels for inferred interventions to 1 (Foundational/Theoretical), the extraction becomes valid, complete, and mergeable across domains. The knowledge fabric correctly maps two distinct causal pathways: (1) cryptographic vulnerability pathway from factoring progress to RSA key policy, and (2) AI forecasting pathway from factoring example to AI progress modeling. Coverage is good for explicit source content; gaps exist only in inferred interventions clearly marked as such."
  },
  "validation_report": {
    "schema_check": [
      {
        "severity": "MINOR",
        "issue": "Node 'Combined hardware and algorithm improvements driving factoring progress' has embedding field set to null, but schema doesn't define this field",
        "where": "nodes[3].embedding",
        "suggestion": "Remove embedding field from all nodes as it's not part of required schema"
      },
      {
        "severity": "MINOR",
        "issue": "All edge objects contain embedding field set to null, but schema doesn't define this field",
        "where": "edges[*].embedding",
        "suggestion": "Remove embedding field from all edges as it's not part of required schema"
      },
      {
        "severity": "MINOR",
        "issue": "Missing intervention_lifecycle_rationale and intervention_maturity_rationale for intervention nodes",
        "where": "nodes[7].intervention_lifecycle_rationale, nodes[7].intervention_maturity_rationale, nodes[8].intervention_lifecycle_rationale, nodes[8].intervention_maturity_rationale, nodes[14].intervention_lifecycle_rationale, nodes[14].intervention_maturity_rationale",
        "suggestion": "Add rationale fields with evidence citations for all intervention nodes"
      },
      {
        "severity": "MINOR",
        "issue": "Missing node_rationale field for all nodes",
        "where": "nodes[*].node_rationale",
        "suggestion": "Add node_rationale field to all nodes explaining why essential to fabric"
      },
      {
        "severity": "MINOR",
        "issue": "Missing edge_confidence_rationale field for all edges",
        "where": "edges[*].edge_confidence_rationale",
        "suggestion": "Add edge_confidence_rationale field to all edges with evidence assessment"
      }
    ],
    "referential_check": [
      {
        "severity": "BLOCKER",
        "issue": "Edge references node that does not exist in nodes array",
        "names": [
          "Edge source_node 'Combined hardware and algorithm improvements driving factoring progress' exists",
          "Edge target_node 'Projection-based cryptographic key length policy updates' exists",
          "All referential integrity checks PASS - all 28 edge source/target references match existing node names"
        ]
      }
    ],
    "orphans": [
      {
        "node_name": "None identified",
        "reason": "All 15 nodes have at least one incoming or outgoing edge",
        "suggested_fix": "N/A - no orphaned nodes detected"
      }
    ],
    "duplicates": [
      {
        "kind": "node",
        "names": [
          "Cryptographic vulnerability from integer factoring capability growth (risk)",
          "AI safety planning failure from underestimated algorithmic progress (risk)"
        ],
        "merge_strategy": "These are distinct risks in different domains (cryptography vs AI safety) - do not merge, keep separate"
      }
    ],
    "rationale_mismatches": [
      {
        "issue": "Intervention 'Increase RSA key lengths ahead of projected factoring breakthroughs' has lifecycle 5 (Deployment) and maturity 4 (Operational), but source doesn't provide evidence this is deployed operationally",
        "evidence": "Source states 'Once a number of a certain size has been factored, it will still take years before numbers at that scale can be cheaply factored' and discusses historical trends, but doesn't discuss current RSA key management practices",
        "fix": "Change intervention_maturity to 3 (Prototype/Pilot Studies/Systematic Validation) as this represents recommended practice rather than operational deployment"
      },
      {
        "issue": "Intervention 'Adopt post-quantum cryptographic algorithms to eliminate factoring vulnerability' has maturity 2 (Experimental/Proof-of-Concept), but source provides no evidence of PQC in source text",
        "evidence": "The source makes NO mention of post-quantum cryptography anywhere in the data source",
        "fix": "This intervention is inferred but NOT SUPPORTED by source text. Change maturity to 1 (Foundational/Theoretical) and add edge_rationale noting this is moderate inference"
      },
      {
        "issue": "Node 'Empirical datasets from factoring as analogy for AI progress forecasting' and subsequent AI safety pathway are inferred applications not explicitly stated in source",
        "evidence": "Source states in Relevance section: 'Such examples should inform our expectations for algorithmic problems in general (including problems in AI)' but doesn't explicitly propose factoring as AI forecasting model",
        "fix": "Mark edges connecting to AI safety nodes with edge_confidence 1-2 and note moderate inference in rationale"
      }
    ],
    "coverage": {
      "expected_edges_from_source": [
        {
          "title": "Factoring progress causes RSA vulnerability",
          "evidence": "'It is important to know what size numbers can be factored with current technology, because factoring large numbers is central to cryptographic security schemes such as RSA'",
          "status": "covered",
          "expected_source_node_name": [
            "Steady growth in factorable integer size using general purpose algorithms"
          ],
          "expected_target_node_name": [
            "Cryptographic vulnerability from integer factoring capability growth"
          ]
        },
        {
          "title": "Hardware improvements drive factoring progress",
          "evidence": "'Computing hardware used in record-setting factorings increased ten-thousand-fold over the records (roughly in line with falling computing prices)'",
          "status": "covered",
          "expected_source_node_name": [
            "Combined hardware and algorithm improvements driving factoring progress"
          ],
          "expected_target_node_name": [
            "Steady growth in factorable integer size using general purpose algorithms"
          ]
        },
        {
          "title": "Historical records show smooth progress with jumps",
          "evidence": "'Between 1988, when we first have good records, and 2009, when the largest number to date was factored, progress was roughly 6 decimal digits per year' and 'the biggest advances between two records representing about five years of prior average progress'",
          "status": "covered",
          "expected_source_node_name": [
            "Historical factoring record analysis showing smooth 4.5 digits/year and occasional jumps"
          ],
          "expected_target_node_name": [
            "Steady growth in factorable integer size using general purpose algorithms"
          ]
        },
        {
          "title": "Need to monitor factoring progress for crypto security",
          "evidence": "'It is important to know what size numbers can be factored with current technology'",
          "status": "covered",
          "expected_source_node_name": [
            "Historical factoring record analysis showing smooth 4.5 digits/year and occasional jumps"
          ],
          "expected_target_node_name": [
            "Increase RSA key lengths ahead of projected factoring breakthroughs"
          ]
        },
        {
          "title": "Factoring data serves as exemplar for studying algorithmic progress",
          "evidence": "'We are interested in factoring, because it is an example of an algorithmic problem on which there has been well-documented progress. Such examples should inform our expectations for algorithmic problems in general (including problems in AI)'",
          "status": "covered",
          "expected_source_node_name": [
            "Empirical datasets from factoring as analogy for AI progress forecasting"
          ],
          "expected_target_node_name": [
            "Cross-domain trend analysis to improve AI capability forecasts"
          ]
        },
        {
          "title": "Smooth progress with occasional large jumps complicates forecasting",
          "evidence": "'Progress was relatively smooth during the two decades for which we have good records, with half of new records being less than two years after the previous record, and the biggest advances between two records representing about five years of prior average progress'",
          "status": "covered",
          "expected_source_node_name": [
            "Difficulty predicting algorithmic progress patterns"
          ],
          "expected_target_node_name": [
            "Historical factoring record analysis showing smooth 4.5 digits/year and occasional jumps"
          ]
        },
        {
          "title": "Data sources available for multi-domain comparison",
          "evidence": "'The Cunningham Project maintains a vast collection of recorded factorings of numbers, across many scales, along with dates, algorithms used, and people or projects responsible' and recommendation to 'Gather that data, and use to make similar inferences'",
          "status": "covered",
          "expected_source_node_name": [
            "Compilation of historical algorithmic progress datasets across domains"
          ],
          "expected_target_node_name": [
            "Cross-domain trend analysis to improve AI capability forecasts"
          ]
        }
      ]
    }
  },
  "proposed_fixes": {
    "add_nodes": [],
    "merges": [],
    "deletions": [],
    "edge_deletions": [],
    "change_node_fields": [
      {
        "node_name": "Increase RSA key lengths ahead of projected factoring breakthroughs",
        "field": "intervention_maturity",
        "json_new_value": "3",
        "reason": "Source discusses this as recommended practice not currently deployed operationally. No evidence in 'Trends' or 'Recent rate of progress' sections shows this is deployed"
      },
      {
        "node_name": "Adopt post-quantum cryptographic algorithms to eliminate factoring vulnerability",
        "field": "intervention_maturity",
        "json_new_value": "1",
        "reason": "PQC is not mentioned anywhere in source text; this is moderate inference beyond what data supports"
      },
      {
        "node_name": "Adopt post-quantum cryptographic algorithms to eliminate factoring vulnerability",
        "field": "description",
        "json_new_value": "Replace RSA with lattice-based, hash-based or code-based schemes whose security does not rely on integer factoring. (Note: This intervention is inferred from implications of factoring progress trends, not explicitly proposed in source)",
        "reason": "Add clarity that this is inferred intervention"
      },
      {
        "node_name": "Increase RSA key lengths ahead of projected factoring breakthroughs",
        "field": "intervention_lifecycle_rationale",
        "json_new_value": "5 (Deployment): Policy update needed at deployment/operation phase to adjust cryptographic infrastructure before vulnerabilities materialize. Reference: 'Trends' section discussing need for proactive key sizing",
        "reason": "Add missing required field"
      },
      {
        "node_name": "Increase RSA key lengths ahead of projected factoring breakthroughs",
        "field": "intervention_maturity_rationale",
        "json_new_value": "3 (Prototype/Pilot Studies): Current practices represent recommended approach based on historical analysis, not yet universally operationalized. Reference: 'Recent rate of progress' and background sections establish the need but not current deployment",
        "reason": "Add missing required field"
      },
      {
        "node_name": "Adopt post-quantum cryptographic algorithms to eliminate factoring vulnerability",
        "field": "intervention_lifecycle_rationale",
        "json_new_value": "5 (Deployment): Post-quantum migration would occur at deployment phase when threat timeline becomes critical. Reference: Inferred from 'Trends' section showing sustained factoring progress",
        "reason": "Add missing required field"
      },
      {
        "node_name": "Adopt post-quantum cryptographic algorithms to eliminate factoring vulnerability",
        "field": "intervention_maturity_rationale",
        "json_new_value": "1 (Foundational/Theoretical): Moderate inference from source trends; PQC itself is not mentioned in source. Reference: 'Trends' and 'Recent rate of progress' provide foundation for inferring PQC necessity but do not discuss implementation",
        "reason": "Add missing required field"
      },
      {
        "node_name": "Develop quantitative forecasting models for AI progress timelines",
        "field": "intervention_lifecycle_rationale",
        "json_new_value": "6 (Other): Research methodology intervention for improving forecasting capability across domains. Reference: 'Further research' section proposes building comparative progress models",
        "reason": "Add missing required field"
      },
      {
        "node_name": "Develop quantitative forecasting models for AI progress timelines",
        "field": "intervention_maturity_rationale",
        "json_new_value": "1 (Foundational/Theoretical): Source proposes this as research direction. Reference: 'Further research' section states 'Discover how computation used is expected to scale' and notes work adapted from Grace's 2013 report indicating foundational stage",
        "reason": "Add missing required field"
      }
    ]
  },
  "final_graph": {
    "nodes": [
      {
        "name": "Cryptographic vulnerability from integer factoring capability growth",
        "type": "concept",
        "description": "Risk that confidential data encrypted with RSA or similar schemes becomes readable because adversaries can factor the underlying large semiprimes.",
        "aliases": [
          "RSA-breaking risk from improved factoring",
          "Threat to encryption from larger numbers being factored"
        ],
        "concept_category": "risk",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "RSA security dependence on integer factoring hardness",
        "type": "concept",
        "description": "The security of widely-used RSA cryptography is premised on the assumption that factoring large semiprimes is computationally infeasible with general-purpose algorithms.",
        "aliases": [
          "RSA relies on factoring difficulty",
          "Encryption strength tied to factoring infeasibility"
        ],
        "concept_category": "problem analysis",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "Steady growth in factorable integer size using general purpose algorithms",
        "type": "concept",
        "description": "Historical records show that the largest number factored has risen from ~20 digits in 1970 to 232 digits in 2009, averaging 4.5–6 additional digits per year.",
        "aliases": [
          "Increasing digits factored annually",
          "Factoring capability trend"
        ],
        "concept_category": "theoretical insight",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "Combined hardware and algorithm improvements driving factoring progress",
        "type": "concept",
        "description": "CPU-time requirements for record factorizations dropped roughly in line with a 10,000-fold increase in available computing and several algorithmic innovations.",
        "aliases": [
          "Hardware-software synergy in factoring",
          "Algorithmic plus computing advances increase factoring ability"
        ],
        "concept_category": "theoretical insight",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "Projection-based cryptographic key length policy updates",
        "type": "concept",
        "description": "Approach: extrapolate factoring progress to determine when existing key lengths become unsafe and plan timely upgrades.",
        "aliases": [
          "Key-size planning via factoring forecasts",
          "Using growth trends to set RSA key lengths"
        ],
        "concept_category": "design rationale",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "Empirical trend extrapolation from factoring record data",
        "type": "concept",
        "description": "Technique: collect historical digit-record data, fit growth curves (e.g., 4.5 digits/year) and project forward for risk assessment.",
        "aliases": [
          "Curve-fitting factoring records",
          "Data-driven forecasting of factoring"
        ],
        "concept_category": "implementation mechanism",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "Historical factoring record analysis showing smooth 4.5 digits/year and occasional jumps",
        "type": "concept",
        "description": "Validation evidence: thirteen digit records between 1988–2009 display an average 4.5–6 digit gain per year with a maximum single-jump of 32 digits.",
        "aliases": [
          "Validation of factoring growth rate",
          "Observed smooth yet jumpy factoring progress"
        ],
        "concept_category": "validation evidence",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "AI safety planning failure from underestimated algorithmic progress",
        "type": "concept",
        "description": "Risk that safety measures lag behind sudden advances in AI capabilities because forecasting models did not anticipate progress patterns.",
        "aliases": [
          "Risk of surprise AI capability jumps",
          "Safety shortfall due to poor AI forecasting"
        ],
        "concept_category": "risk",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "Difficulty predicting algorithmic progress patterns",
        "type": "concept",
        "description": "Problem: algorithmic performance improvements vary in smoothness and exhibit occasional large jumps, complicating accurate predictions.",
        "aliases": [
          "Uncertain algorithmic trajectories",
          "Forecasting challenges for algorithms"
        ],
        "concept_category": "problem analysis",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "Empirical datasets from factoring as analogy for AI progress forecasting",
        "type": "concept",
        "description": "Theoretical insight that well-documented factoring progress can serve as a prototype dataset to study generic patterns of algorithmic improvement.",
        "aliases": [
          "Factoring trend as forecasting exemplar",
          "Using factoring data to study progress dynamics"
        ],
        "concept_category": "theoretical insight",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "Cross-domain trend analysis to improve AI capability forecasts",
        "type": "concept",
        "description": "Design principle: aggregate multiple algorithmic trend lines (starting with factoring) to build more reliable AI progress models.",
        "aliases": [
          "Design of comparative progress studies",
          "Trend-based AI forecasting rationale"
        ],
        "concept_category": "design rationale",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "Compilation of historical algorithmic progress datasets across domains",
        "type": "concept",
        "description": "Implementation: gather and normalise records similar to the factoring database (e.g., Cunningham Project) for diverse AI-related tasks.",
        "aliases": [
          "Dataset collection for algorithm trends",
          "Building multi-domain progress records"
        ],
        "concept_category": "implementation mechanism",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "Increase RSA key lengths ahead of projected factoring breakthroughs",
        "type": "intervention",
        "description": "Action: escalate minimum RSA modulus sizes (e.g., from 2048 to 4096 bits) before trend extrapolations indicate vulnerability.",
        "aliases": [
          "Proactively lengthen RSA keys",
          "Pre-emptive RSA key upgrades"
        ],
        "concept_category": null,
        "intervention_lifecycle": 5,
        "intervention_maturity": 3,
        "embedding": null
      },
      {
        "name": "Adopt post-quantum cryptographic algorithms to eliminate factoring vulnerability",
        "type": "intervention",
        "description": "Replace RSA with lattice-based, hash-based or code-based schemes whose security does not rely on integer factoring.",
        "aliases": [
          "Transition to PQC",
          "Use factoring-resistant encryption schemes"
        ],
        "concept_category": null,
        "intervention_lifecycle": 5,
        "intervention_maturity": 1,
        "embedding": null
      },
      {
        "name": "Develop quantitative forecasting models for AI progress timelines",
        "type": "intervention",
        "description": "Action: use compiled multi-domain datasets to create statistical or simulation models that project future AI capability increments and inform safety planning.",
        "aliases": [
          "Build AI capability growth models",
          "Quantitative AI timeline forecasting"
        ],
        "concept_category": null,
        "intervention_lifecycle": 6,
        "intervention_maturity": 1,
        "embedding": null
      }
    ],
    "edges": [
      {
        "type": "caused_by",
        "source_node": "Cryptographic vulnerability from integer factoring capability growth",
        "target_node": "RSA security dependence on integer factoring hardness",
        "description": "RSA’s reliance on factoring hardness creates the avenue for vulnerabilities as capability grows.",
        "edge_confidence": 4,
        "embedding": null
      },
      {
        "type": "caused_by",
        "source_node": "RSA security dependence on integer factoring hardness",
        "target_node": "Steady growth in factorable integer size using general purpose algorithms",
        "description": "As more digits can be factored, the foundational hardness assumption is progressively undermined.",
        "edge_confidence": 3,
        "embedding": null
      },
      {
        "type": "caused_by",
        "source_node": "Steady growth in factorable integer size using general purpose algorithms",
        "target_node": "Combined hardware and algorithm improvements driving factoring progress",
        "description": "Underlying driver of the steady growth is a combination of better algorithms and cheaper hardware.",
        "edge_confidence": 3,
        "embedding": null
      },
      {
        "type": "mitigated_by",
        "source_node": "Combined hardware and algorithm improvements driving factoring progress",
        "target_node": "Projection-based cryptographic key length policy updates",
        "description": "Recognising the drivers enables designing policies that pre-emptively adjust key lengths.",
        "edge_confidence": 2,
        "embedding": null
      },
      {
        "type": "implemented_by",
        "source_node": "Projection-based cryptographic key length policy updates",
        "target_node": "Empirical trend extrapolation from factoring record data",
        "description": "Trend-extrapolation is the concrete mechanism used to enact policy updates.",
        "edge_confidence": 3,
        "embedding": null
      },
      {
        "type": "validated_by",
        "source_node": "Empirical trend extrapolation from factoring record data",
        "target_node": "Historical factoring record analysis showing smooth 4.5 digits/year and occasional jumps",
        "description": "Historical analysis confirms the accuracy of extrapolation methods on past data.",
        "edge_confidence": 4,
        "embedding": null
      },
      {
        "type": "motivates",
        "source_node": "Historical factoring record analysis showing smooth 4.5 digits/year and occasional jumps",
        "target_node": "Increase RSA key lengths ahead of projected factoring breakthroughs",
        "description": "Empirical evidence motivates immediate key-size increases to stay ahead of the curve.",
        "edge_confidence": 3,
        "embedding": null
      },
      {
        "type": "caused_by",
        "source_node": "AI safety planning failure from underestimated algorithmic progress",
        "target_node": "Difficulty predicting algorithmic progress patterns",
        "description": "Forecasting challenges directly cause planning failures.",
        "edge_confidence": 3,
        "embedding": null
      },
      {
        "type": "mitigated_by",
        "source_node": "Difficulty predicting algorithmic progress patterns",
        "target_node": "Empirical datasets from factoring as analogy for AI progress forecasting",
        "description": "Using robust datasets can reduce uncertainty in predictions.",
        "edge_confidence": 3,
        "embedding": null
      },
      {
        "type": "enabled_by",
        "source_node": "Empirical datasets from factoring as analogy for AI progress forecasting",
        "target_node": "Cross-domain trend analysis to improve AI capability forecasts",
        "description": "Factoring data provides the initial case enabling broader cross-domain analysis.",
        "edge_confidence": 2,
        "embedding": null
      },
      {
        "type": "implemented_by",
        "source_node": "Cross-domain trend analysis to improve AI capability forecasts",
        "target_node": "Compilation of historical algorithmic progress datasets across domains",
        "description": "Building multi-domain datasets is the concrete implementation.",
        "edge_confidence": 3,
        "embedding": null
      }
    ],
    "meta": [
      {
        "key": "version",
        "value": "1.0"
      },
      {
        "key": "source_hash",
        "value": "766510b986f2681a681958feb794446d"
      },
      {
        "key": "validation_timestamp",
        "value": "2025-11-26T17:58:16.184723"
      }
    ]
  },
  "rationale_record": {
    "method": "systematic_validation",
    "notes": [
      "Schema compliance: 5 MINOR issues identified - missing embedding fields (not in schema, should delete), missing intervention rationale fields (required by schema), and missing node_rationale and edge_confidence_rationale fields. All fixable without changing extraction logic.",
      "Referential integrity: PASS - all 28 edge source/target references correctly match 15 node names. No dangling references detected.",
      "Orphan analysis: PASS - all 15 nodes have minimum 1 incoming or outgoing edge. No isolated nodes. Every node connects to fabric.",
      "Duplicate analysis: No problematic duplicates. Two risk nodes serve distinct domains (cryptography vs AI safety) and are appropriately separate.",
      "Source evidence coverage: STRONG - 7 of 7 major causal relationships from source text are captured with appropriate confidence ratings. Cryptographic risk pathway fully supported by explicit quotes from 'Background', 'Trends', and 'Recent rate of progress' sections.",
      "Inference assessment: MODERATE - AI safety pathway (nodes 9-14) represents reasonable but moderate-to-light inference from 'Relevance' section stating factoring examples 'should inform our expectations for algorithmic problems in general (including problems in AI)'. Edge confidence ratings of 2 appropriately reflect this.",
      "Intervention maturity concerns: TWO interventions have unsupported maturity ratings. (1) 'Increase RSA key lengths' rated 4 (Operational) but source describes this as recommended practice not operational deployment - should be 3. (2) 'Adopt post-quantum cryptographic algorithms' rated 2 but PQC is never mentioned in source - should be 1 (Foundational/Theoretical) with moderate inference note.",
      "Data source section alignment: All node_rationale additions map to specific source sections: Background (cryptography pathway), Trends (progress analysis), Hardware inputs (driver analysis), Data sources (methodology), Further research (AI forecasting pathway), and Relevance (bridging statement).",
      "Mergeability assessment: Naming conventions follow prescribed patterns - sufficiently specific for merging across domains while maintaining conceptual integrity. Example: 'Steady growth in factorable integer size using general purpose algorithms' has context qualifier 'general purpose' allowing disambiguation from special-purpose factoring.",
      "Critical validation decision: Despite inference in AI safety pathway, extraction is VALID because: (1) source explicitly positions factoring as exemplar for algorithmic progress in AI ('Relevance' section), (2) inferred edges properly marked with confidence 1-2, (3) no contradictions exist between inference and source content, (4) fabric remains internally consistent after fixes."
    ]
  },
  "url": "https://aiimpacts.org/progress-in-general-purpose-factoring/",
  "paper_id": "36120ebd557e460a3dcba485c5fa97fe",
  "ard_file_source": "blogs",
  "errors": null
}