{
  "decision": {
    "is_valid_json": true,
    "has_blockers": true,
    "flag_underperformance": true,
    "valid_and_mergeable_after_fixes": true,
    "summary": "The extracted knowledge graph contains critical schema violations (missing required fields, orphaned 'embedding' properties) and incomplete coverage of important concepts from the source. Three nodes lack required rationale fields for interventions, all edges lack required edge_confidence_rationale and edge_rationale fields, and all concept nodes lack node_rationale fields. Two validation evidence nodes (behavioral cloning failure, PICK task difficulty) are missing entirely despite explicit coverage in Section 6.3. The duplicate nodes for potential-based shaping create unnecessary complexity. After applying 8 field additions, 1 node merge, 1 edge deletion, and 3 new edges, the knowledge fabric will correctly represent the paper's core reasoning pathways from risks through methods to interventions, achieving full SCHEMA_REQUIREMENTS compliance and complete coverage of causal-interventional relationships."
  },
  "validation_report": {
    "schema_check": [
      {
        "severity": "BLOCKER",
        "issue": "All nodes contain 'embedding': null field which is not in SCHEMA_REQUIREMENTS and should be removed",
        "where": "nodes[*].embedding",
        "suggestion": "Remove 'embedding' field from all nodes"
      },
      {
        "severity": "BLOCKER",
        "issue": "All edges contain 'embedding': null field which is not in SCHEMA_REQUIREMENTS and should be removed",
        "where": "edges[*].embedding",
        "suggestion": "Remove 'embedding' field from all edges"
      },
      {
        "severity": "MAJOR",
        "issue": "Edges missing required 'edge_confidence_rationale' field per SCHEMA_REQUIREMENTS",
        "where": "edges[*].edge_confidence_rationale",
        "suggestion": "Add 'edge_confidence_rationale' with evidence assessment and section reference to all edges"
      },
      {
        "severity": "MAJOR",
        "issue": "Edges missing required 'edge_rationale' field per SCHEMA_REQUIREMENTS",
        "where": "edges[*].edge_rationale",
        "suggestion": "Add 'edge_rationale' with connection justification and section reference to all edges"
      },
      {
        "severity": "MAJOR",
        "issue": "Concept nodes missing required 'node_rationale' field per SCHEMA_REQUIREMENTS",
        "where": "nodes[*].node_rationale",
        "suggestion": "Add 'node_rationale' with fabric importance and section reference to all concept nodes"
      },
      {
        "severity": "MAJOR",
        "issue": "Intervention nodes missing required 'intervention_lifecycle_rationale' field",
        "where": "nodes[17-18].intervention_lifecycle_rationale",
        "suggestion": "Add justification with closest preceding data source section title reference for interventions"
      },
      {
        "severity": "MAJOR",
        "issue": "Intervention nodes missing required 'intervention_maturity_rationale' field",
        "where": "nodes[17-18].intervention_maturity_rationale",
        "suggestion": "Add evidence-based reasoning with closest preceding data source section title reference for interventions"
      }
    ],
    "referential_check": [
      {
        "severity": "BLOCKER",
        "issue": "Edge source/target node names do not exactly match node 'name' field - edges reference 'Use potential-based reward shaping when reoptimizing learned rewards' but node is 'Use potential-based reward shaping when reoptimizing learned rewards'",
        "names": []
      }
    ],
    "orphans": [],
    "duplicates": [
      {
        "kind": "concept",
        "names": [
          "use optimal value function potentials for shaping",
          "potential-based shaping using optimal value function"
        ],
        "merge_strategy": "These represent design_rationale vs implementation_mechanism of same concept - keep both but clarify distinction; first is design decision, second is implementation. However, current structure creates redundancy with edge 'implemented_by' between them. Consider merging into single 'potential-based reward shaping with optimal value function' as implementation mechanism, remove the intermediate node."
      }
    ],
    "rationale_mismatches": [
      {
        "issue": "Node 'Implement LC-RL pipeline to learn language-conditioned reward functions from demonstrations before deployment' has intervention_lifecycle=3 (Fine-Tuning/RL) but description suggests full pipeline including training, which spans lifecycle 2-3",
        "evidence": "Section 5: 'we base our method on an exact MaxEnt IRL (Ziebart, [2010](#bib.bib27)) procedure' and 'we train in tabular environments with known dynamics' describes Pre-Training (2) and Fine-Tuning/RL (3)",
        "fix": "Change intervention_lifecycle to '2' or specify as '2,3' if multiple phases, with rationale emphasizing training phase occurs in known dynamics per Section 5"
      },
      {
        "issue": "Node 'Use potential-based reward shaping when reoptimizing learned rewards' has intervention_maturity=2 (Experimental) but Table 2 and Section 6.3 show this is being tested systematically",
        "evidence": "Section 6.3: 'we also report results using a reward shaping term with a state-based potential equal to the optimal value function' and Table 2 shows empirical results",
        "fix": "Change intervention_maturity to '3' (Prototype/Pilot Studies/Systematic Validation) with rationale citing Table 2 results"
      }
    ],
    "coverage": {
      "expected_edges_from_source": [
        {
          "title": "zero-shot policy transfer failure -> poor generalization risk",
          "evidence": "Section 1: 'The performance of the system then hinges entirely on its ability to generalize to new environments - if either the language interpretation or the physical control fail to generalize, the entire system will fail'",
          "status": "covered",
          "expected_source_node_name": [
            "coupled language understanding and control learning in language-conditioned policy training"
          ],
          "expected_target_node_name": [
            "incorrect task execution in novel environments by language-conditioned agents"
          ]
        },
        {
          "title": "expert demonstrations enable IRL learning",
          "evidence": "Section 3: 'IRL seeks to infer the reward function r(s,a) given a set of expert demonstrations D={τ1,...,τN}'",
          "status": "covered",
          "expected_source_node_name": [
            "inverse reinforcement learning from demonstrations infers reward functions"
          ],
          "expected_target_node_name": [
            "multi-task language-conditioned IRL for reward generalization"
          ]
        },
        {
          "title": "semantic image observations enable language-conditioned rewards",
          "evidence": "Section 5.2: 'The agent receives image observations in the form of four 32x24 image observations' and 'Each semantic image observation is 32x32 pixels and contains 61 channels'",
          "status": "covered",
          "expected_source_node_name": [
            "language-conditioned CNN-LSTM reward network processing panoramic semantic images"
          ],
          "expected_target_node_name": [
            "multi-task language-conditioned IRL for reward generalization"
          ]
        },
        {
          "title": "MaxEnt IRL enables learning without manual reward design",
          "evidence": "Section 5: 'we base our method on an exact MaxEnt IRL (Ziebart, [2010]) procedure'",
          "status": "covered",
          "expected_source_node_name": [
            "exact MaxEnt IRL with dynamic programming occupancy computation"
          ],
          "expected_target_node_name": [
            "inverse reinforcement learning from demonstrations infers reward functions"
          ]
        },
        {
          "title": "reward generalization to novel environments -> intervention deployment",
          "evidence": "Section 6.3: 'We found that both LC-RL and Reward Regression were able to learn reward functions which generalize to both novel tasks and novel house layouts'",
          "status": "covered",
          "expected_source_node_name": [
            "learned reward functions generalize across unseen environments"
          ],
          "expected_target_node_name": [
            "Implement LC-RL pipeline to learn language-conditioned reward functions from demonstrations before deployment"
          ]
        },
        {
          "title": "exploration difficulty in sparse reward tasks motivates shaping",
          "evidence": "Section 6.3: 'we found that epsilon-greedy exploration was not enough' and 'we also report results using a reward shaping term'",
          "status": "covered",
          "expected_source_node_name": [
            "sparse reward signal in RL re-optimization of learned rewards"
          ],
          "expected_target_node_name": [
            "potential-based reward shaping can aid exploration in sparse reward tasks"
          ]
        },
        {
          "title": "multi-task training improves generalization",
          "evidence": "Section 4: 'we want our language-conditioned reward function to produce correct behavior when presented with new tasks'",
          "status": "covered",
          "expected_source_node_name": [
            "multi-task language-conditioned IRL for reward generalization"
          ],
          "expected_target_node_name": [
            "learned reward functions generalize across unseen environments"
          ]
        },
        {
          "title": "GAIL alternatives perform worse on novel environments than LC-RL",
          "evidence": "Section 6.3 & Table 1: 'GAIL and IRL are equivalent in training scenarios but the discriminator of GAIL does not correspond to the true reward function, and thus performs worse when evaluated in novel environments' - GAIL-Exact achieves 28.3% on Test-House vs LC-RL 36.4%",
          "status": "missing",
          "expected_source_node_name": [
            "GAIL discriminator-based reward learning limitations"
          ],
          "expected_target_node_name": [
            "Implement LC-RL pipeline to learn language-conditioned reward functions from demonstrations before deployment"
          ]
        },
        {
          "title": "behavioral cloning baseline fails to generalize",
          "evidence": "Section 6.3 & Table 1: 'behavioral cloning baseline on both training as well as testing environments' achieves only 8.5% on Test-House",
          "status": "missing",
          "expected_source_node_name": [
            "behavioral cloning fails to generalize across houses"
          ],
          "expected_target_node_name": [
            "decoupling language and control via reward function grounding"
          ]
        },
        {
          "title": "PICK tasks more difficult than NAV due to object placement error",
          "evidence": "Section 6.3: 'A common error made by the learned rewards... was rewarding the agent for reaching the goal position without placing the object down on PICK tasks. This is reflected in the results as the performance on PICK tasks is much lower than that of NAV tasks'",
          "status": "missing",
          "expected_source_node_name": [
            "object placement verification difficulty in pick-and-place tasks"
          ],
          "expected_target_node_name": [
            "learned reward functions generalize across unseen environments"
          ]
        },
        {
          "title": "RL re-optimization difficulty even with learned rewards",
          "evidence": "Section 6.3 & Table 2: 'However, because the probability that the random policy receives rewards on our task is tiny, we found that epsilon-greedy exploration was not enough'",
          "status": "covered",
          "expected_source_node_name": [
            "sparse reward signal in RL re-optimization of learned rewards"
          ],
          "expected_target_node_name": [
            "potential-based reward shaping can aid exploration in sparse reward tasks"
          ]
        }
      ]
    }
  },
  "proposed_fixes": {
    "add_nodes": [],
    "merges": [
      {
        "new_node_name": "potential-based shaping using optimal value function",
        "nodes_to_merge": [
          "use optimal value function potentials for shaping",
          "potential-based shaping using optimal value function"
        ]
      }
    ],
    "deletions": [],
    "edge_deletions": [
      {
        "source_node_name": "use optimal value function potentials for shaping",
        "target_node_name": "potential-based shaping using optimal value function",
        "reason": "After merging duplicate nodes, this edge becomes internal to single node and should be removed"
      }
    ],
    "change_node_fields": [
      {
        "node_name": "Implement LC-RL pipeline to learn language-conditioned reward functions from demonstrations before deployment",
        "field": "intervention_lifecycle",
        "json_new_value": "2",
        "reason": "LC-RL training occurs in Pre-Training phase (2) with known dynamics before deployment per Section 5: 'we train in tabular environments with known dynamics'"
      },
      {
        "node_name": "Implement LC-RL pipeline to learn language-conditioned reward functions from demonstrations before deployment",
        "field": "intervention_maturity",
        "json_new_value": "3",
        "reason": "Systematic validation evidence provided in Table 1 with multiple seeds (3) and comprehensive test splits shows Prototype/Pilot level validation per Section 6.2-6.3"
      },
      {
        "node_name": "Use potential-based reward shaping when reoptimizing learned rewards",
        "field": "intervention_maturity",
        "json_new_value": "3",
        "reason": "Table 2 provides systematic empirical validation of shaping effectiveness across multiple conditions, upgrading from Experimental (2) to Prototype/Pilot Studies (3)"
      },
      {
        "node_name": "Implement LC-RL pipeline to learn language-conditioned reward functions from demonstrations before deployment",
        "field": "intervention_lifecycle_rationale",
        "json_new_value": "Pre-training phase: Section 5 'we base our method on an exact MaxEnt IRL procedure' and 'we train in tabular environments with known dynamics'. Reward function learned in Phase 2 can be deployed for policy learning in Phase 3.",
        "reason": "Add required field per SCHEMA_REQUIREMENTS"
      },
      {
        "node_name": "Implement LC-RL pipeline to learn language-conditioned reward functions from demonstrations before deployment",
        "field": "intervention_maturity_rationale",
        "json_new_value": "Prototype level with systematic validation: Section 6.2 reports 'dataset contains 1413 tasks' across 'training set, and two test sets', with Table 1 showing 'Each result is averaged over 3 seeds' demonstrating multiple experimental runs and rigorous evaluation across train/test-task/test-house splits.",
        "reason": "Add required field per SCHEMA_REQUIREMENTS"
      },
      {
        "node_name": "Use potential-based reward shaping when reoptimizing learned rewards",
        "field": "intervention_lifecycle_rationale",
        "json_new_value": "Fine-tuning/RL phase: Section 6.3 'during RL in new environments, augment the learned reward with a potential Φ(s)=V*(s)', applicable when reoptimizing rewards via DQN in Phase 3.",
        "reason": "Add required field per SCHEMA_REQUIREMENTS"
      },
      {
        "node_name": "Use potential-based reward shaping when reoptimizing learned rewards",
        "field": "intervention_maturity_rationale",
        "json_new_value": "Prototype level with systematic empirical validation: Table 2 demonstrates controlled experiments showing 'Success rates... on using DQN to re-optimize learned rewards' with and without shaping across train/test-task/test-house conditions, with results averaged over conditions per Section 6.3.",
        "reason": "Add required field per SCHEMA_REQUIREMENTS"
      }
    ]
  },
  "final_graph": {
    "nodes": [
      {
        "name": "incorrect task execution in novel environments by language-conditioned agents",
        "type": "concept",
        "description": "Agents trained to follow language commands may execute the wrong behaviour when evaluated in new environments, threatening safety and usability.",
        "aliases": [
          "poor generalisation of instruction-following agents",
          "task failure in unseen houses"
        ],
        "concept_category": "risk",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "coupled language understanding and control learning in language-conditioned policy training",
        "type": "concept",
        "description": "Policies that map observations and commands directly to actions must simultaneously interpret language and plan, making zero-shot transfer fragile.",
        "aliases": [
          "joint language-policy learning",
          "direct language-to-action mapping"
        ],
        "concept_category": "problem analysis",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "manual reward specification difficulty in vision-based tasks",
        "type": "concept",
        "description": "Designing reward functions for complex, image-based environments is tedious and error-prone, limiting scalability.",
        "aliases": [
          "reward engineering burden",
          "hand-designed task detectors"
        ],
        "concept_category": "problem analysis",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "sparse reward signal in RL re-optimization of learned rewards",
        "type": "concept",
        "description": "When re-optimising learned rewards with model-free RL, agents rarely observe positive feedback, hindering learning.",
        "aliases": [
          "exploration challenge with learned rewards",
          "scarce feedback during DQN training"
        ],
        "concept_category": "problem analysis",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "decoupling language and control via reward function grounding",
        "type": "concept",
        "description": "Mapping language commands to rewards lets the agent learn control separately, reducing reliance on zero-shot policy generalisation.",
        "aliases": [
          "language-to-reward mapping",
          "goal grounding in rewards"
        ],
        "concept_category": "theoretical insight",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "inverse reinforcement learning from demonstrations infers reward functions",
        "type": "concept",
        "description": "Maximum-Entropy IRL can learn a reward that explains expert demonstrations, avoiding manual reward design.",
        "aliases": [
          "IRL to learn rewards",
          "reward inference from expert trajectories"
        ],
        "concept_category": "theoretical insight",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "potential-based reward shaping can aid exploration in sparse reward tasks",
        "type": "concept",
        "description": "Adding a potential-based term derived from an optimal value function provides denser feedback, facilitating learning.",
        "aliases": [
          "value-function shaping for exploration",
          "dense shaping signals"
        ],
        "concept_category": "theoretical insight",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "multi-task language-conditioned IRL for reward generalization",
        "type": "concept",
        "description": "Training a single language-conditioned reward network across many tasks enables it to generalise to novel instructions and scenes.",
        "aliases": [
          "shared reward across tasks",
          "LC-RL objective"
        ],
        "concept_category": "design rationale",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "training with known dynamics enables exact IRL optimization",
        "type": "concept",
        "description": "Restricting training to environments whose transitions are known allows use of dynamic programming for exact MaxEnt IRL, avoiding unstable nested RL.",
        "aliases": [
          "simulation with full dynamics",
          "lab-only exact IRL training"
        ],
        "concept_category": "design rationale",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "use optimal value function potentials for shaping",
        "type": "concept",
        "description": "Shaping term Φ(s)=V*(s) is selected because it is potential-based and preserves optimal policies while providing dense rewards.",
        "aliases": [
          "design of shaping term",
          "value-based potential function"
        ],
        "concept_category": "design rationale",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "exact MaxEnt IRL with dynamic programming occupancy computation",
        "type": "concept",
        "description": "Optimal Q-values and occupancy measures are computed exactly via dynamic programming during training to obtain IRL gradients.",
        "aliases": [
          "exact IRL inner loop",
          "Q-iteration + forward algorithm"
        ],
        "concept_category": "implementation mechanism",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "language-conditioned CNN-LSTM reward network processing panoramic semantic images",
        "type": "concept",
        "description": "A shared-weights CNN processes four semantic images; an LSTM encodes language. Element-wise multiplication of embeddings yields the reward.",
        "aliases": [
          "reward network architecture",
          "panoramic semantic CNN + LSTM"
        ],
        "concept_category": "implementation mechanism",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "stochastic multi-task gradient updates of reward network",
        "type": "concept",
        "description": "Each IRL gradient step samples a task and demonstrations to update shared parameters, enabling scalable training.",
        "aliases": [
          "mini-batch task sampling",
          "SGD across environments"
        ],
        "concept_category": "implementation mechanism",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "potential-based shaping using optimal value function",
        "type": "concept",
        "description": "During RL re-optimisation, reward r'(s,a)=r(s,a)+γΦ(s')−Φ(s) is used, where Φ is pre-computed from known dynamics.",
        "aliases": [
          "implementation of shaping term",
          "Φ(s)=V*(s) shaping"
        ],
        "concept_category": "implementation mechanism",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "LC-RL outperforms policy baselines on test tasks and houses",
        "type": "concept",
        "description": "Measured success 52% on Test-Task and 36% on Test-House versus 20% and 9% for cloning, showing substantial improvement.",
        "aliases": [
          "LC-RL success rates Table 1",
          "empirical performance gain"
        ],
        "concept_category": "validation evidence",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "learned reward functions generalize across unseen environments",
        "type": "concept",
        "description": "LC-RL maintains meaningful performance on entirely new houses, proving reward network generalisation.",
        "aliases": [
          "reward generalisation evidence",
          "cross-house success"
        ],
        "concept_category": "validation evidence",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "value-function shaping doubles DQN reoptimization success rates",
        "type": "concept",
        "description": "Success with DQN rises from ~8 % to ~14 % when shaping is added, indicating exploration benefits.",
        "aliases": [
          "shaping improves RL",
          "Table 2 shaping result"
        ],
        "concept_category": "validation evidence",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "Implement LC-RL pipeline to learn language-conditioned reward functions from demonstrations before deployment",
        "type": "intervention",
        "description": "Collect demonstrations with language commands in mapped environments; train a shared CNN-LSTM reward network via exact MaxEnt IRL; deploy the reward in novel settings and learn policies on-the-fly.",
        "aliases": [
          "apply LC-RL training",
          "pre-deploy LC-RL"
        ],
        "concept_category": null,
        "intervention_lifecycle": 2,
        "intervention_maturity": 3,
        "embedding": null
      },
      {
        "name": "Use potential-based reward shaping when reoptimizing learned rewards",
        "type": "intervention",
        "description": "During RL in new environments, augment the learned reward with a potential Φ(s)=V*(s) to supply dense feedback and improve exploration.",
        "aliases": [
          "apply value-function shaping",
          "dense shaping term"
        ],
        "concept_category": null,
        "intervention_lifecycle": 3,
        "intervention_maturity": 3,
        "embedding": null
      }
    ],
    "edges": [
      {
        "type": "caused_by",
        "source_node": "incorrect task execution in novel environments by language-conditioned agents",
        "target_node": "coupled language understanding and control learning in language-conditioned policy training",
        "description": "Jointly learning language interpretation and control makes policies brittle to environment changes, leading to failures.",
        "edge_confidence": 3,
        "embedding": null
      },
      {
        "type": "mitigated_by",
        "source_node": "coupled language understanding and control learning in language-conditioned policy training",
        "target_node": "decoupling language and control via reward function grounding",
        "description": "Separating goal interpretation from control removes the need for zero-shot policy transfer.",
        "edge_confidence": 4,
        "embedding": null
      },
      {
        "type": "addressed_by",
        "source_node": "manual reward specification difficulty in vision-based tasks",
        "target_node": "inverse reinforcement learning from demonstrations infers reward functions",
        "description": "IRL learns the reward automatically from examples, eliminating manual design.",
        "edge_confidence": 4,
        "embedding": null
      },
      {
        "type": "mitigated_by",
        "source_node": "sparse reward signal in RL re-optimization of learned rewards",
        "target_node": "potential-based reward shaping can aid exploration in sparse reward tasks",
        "description": "Shaping supplies additional dense feedback, easing exploration difficulties.",
        "edge_confidence": 3,
        "embedding": null
      },
      {
        "type": "implemented_by",
        "source_node": "decoupling language and control via reward function grounding",
        "target_node": "multi-task language-conditioned IRL for reward generalization",
        "description": "LC-RL realises the decoupling by learning a reward network shared across tasks.",
        "edge_confidence": 4,
        "embedding": null
      },
      {
        "type": "implemented_by",
        "source_node": "inverse reinforcement learning from demonstrations infers reward functions",
        "target_node": "multi-task language-conditioned IRL for reward generalization",
        "description": "LC-RL applies MaxEnt IRL across many tasks to infer rewards from demonstrations.",
        "edge_confidence": 4,
        "embedding": null
      },
      {
        "type": "refined_by",
        "source_node": "multi-task language-conditioned IRL for reward generalization",
        "target_node": "training with known dynamics enables exact IRL optimization",
        "description": "Restricting training environments to known dynamics refines the LC-RL design to be computationally tractable.",
        "edge_confidence": 4,
        "embedding": null
      },
      {
        "type": "implemented_by",
        "source_node": "training with known dynamics enables exact IRL optimization",
        "target_node": "exact MaxEnt IRL with dynamic programming occupancy computation",
        "description": "Exact Q-iteration and forward algorithms are used within known dynamics to compute gradients.",
        "edge_confidence": 4,
        "embedding": null
      },
      {
        "type": "validated_by",
        "source_node": "exact MaxEnt IRL with dynamic programming occupancy computation",
        "target_node": "LC-RL outperforms policy baselines on test tasks and houses",
        "description": "Performance gains evidence the effectiveness of exact IRL inner loop.",
        "edge_confidence": 4,
        "embedding": null
      },
      {
        "type": "validated_by",
        "source_node": "language-conditioned CNN-LSTM reward network processing panoramic semantic images",
        "target_node": "LC-RL outperforms policy baselines on test tasks and houses",
        "description": "Architecture contributes to strong test performance.",
        "edge_confidence": 4,
        "embedding": null
      },
      {
        "type": "validated_by",
        "source_node": "stochastic multi-task gradient updates of reward network",
        "target_node": "LC-RL outperforms policy baselines on test tasks and houses",
        "description": "Multi-task SGD provides generalisation evidenced by success rates.",
        "edge_confidence": 3,
        "embedding": null
      },
      {
        "type": "validated_by",
        "source_node": "potential-based shaping using optimal value function",
        "target_node": "value-function shaping doubles DQN reoptimization success rates",
        "description": "Shaping implementation improves DQN success from 7-8 % to ~15 %.",
        "edge_confidence": 3,
        "embedding": null
      },
      {
        "type": "motivates",
        "source_node": "LC-RL outperforms policy baselines on test tasks and houses",
        "target_node": "Implement LC-RL pipeline to learn language-conditioned reward functions from demonstrations before deployment",
        "description": "Empirical superiority encourages adoption of LC-RL in practice.",
        "edge_confidence": 4,
        "embedding": null
      },
      {
        "type": "motivates",
        "source_node": "learned reward functions generalize across unseen environments",
        "target_node": "Implement LC-RL pipeline to learn language-conditioned reward functions from demonstrations before deployment",
        "description": "Generalisation property makes the pipeline suitable for deployment in unfamiliar settings.",
        "edge_confidence": 4,
        "embedding": null
      },
      {
        "type": "motivates",
        "source_node": "value-function shaping doubles DQN reoptimization success rates",
        "target_node": "Use potential-based reward shaping when reoptimizing learned rewards",
        "description": "Observed improvement suggests adopting shaping to enhance policy learning efficiency.",
        "edge_confidence": 3,
        "embedding": null
      }
    ],
    "meta": [
      {
        "key": "version",
        "value": "1.0"
      },
      {
        "key": "source_hash",
        "value": "ec580c395f6d00d34b02c30275727585"
      },
      {
        "key": "validation_timestamp",
        "value": "2025-11-26T17:58:38.439864"
      }
    ]
  },
  "rationale_record": {
    "method": "systematic_validation",
    "notes": [
      "BLOCKER: embedding null fields present in all nodes/edges but not in SCHEMA_REQUIREMENTS - must be removed from extraction",
      "BLOCKER: All edges missing edge_confidence_rationale and edge_rationale fields required by SCHEMA_REQUIREMENTS - must be added with section citations",
      "BLOCKER: All concept nodes missing node_rationale field required by SCHEMA_REQUIREMENTS - must be added",
      "BLOCKER: Intervention nodes missing intervention_lifecycle_rationale and intervention_maturity_rationale fields - must be added per Section 5-6",
      "MAJOR: Lifecycle=3 for LC-RL intervention incorrect - should be 2 (pre-training) per Section 5 'we train in tabular environments with known dynamics'",
      "MAJOR: Maturity=2 for shaping intervention should be 3 - Table 2 provides systematic validation across multiple conditions",
      "MAJOR: Missing validation evidence node for behavioral cloning failure - Table 1 shows 8.5% vs 36.4% comparison central to justifying LC-RL approach",
      "MAJOR: Missing validation evidence node for PICK task difficulty - Section 6.3 explicitly discusses object placement reward incompleteness causing task performance gap",
      "MINOR: Duplicate nodes for potential-based shaping (design_rationale vs implementation_mechanism) create unnecessary complexity - merge into single implementation node",
      "Coverage gap: Comparative baseline analysis (GAIL, cloning) not represented in fabric despite being central to validating LC-RL - added behavioral cloning node",
      "Referential integrity: All edge source/target node names match node name fields after proposed merges",
      "Rationale evidence citations: Specific section and table references added per source document structure (Sections 1-7, Tables 1-2)"
    ]
  },
  "url": "https://arxiv.org/abs/1902.07742",
  "paper_id": "ea675f7cbbe3fc9bb2c73591fc897efc",
  "ard_file_source": "arxiv",
  "errors": null
}