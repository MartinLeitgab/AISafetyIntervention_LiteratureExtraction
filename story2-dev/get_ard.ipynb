{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63dada92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "dir = '/Users/jeff/Volumes/seagate_5tb/stampyai-ard/alignment-research-dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f361cc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_two_random(file_path):\n",
    "    reservoir = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            if len(reservoir) < 2:\n",
    "                reservoir.append(line)\n",
    "            else:\n",
    "                j = random.randint(1, i)\n",
    "                if j <= 2:\n",
    "                    reservoir[j-1] = line\n",
    "    return [json.loads(line) for line in reservoir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f31e844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "papers = pick_two_random(dir + \"agentmodels.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0782ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "layout: chapter\n",
      "title: \"Time inconsistency II\"\n",
      "description: Formal model of time-inconsistent agent, Gridworld and Procrastination examples.\n",
      "\n",
      "---\n",
      "\n",
      "## Formal Model and Implementation of Hyperbolic Discounting\n",
      "\n",
      "\n",
      "### Formal Model of Naive and Sophisticated Hyperbolic Discounters\n",
      "\n",
      "To formalize Naive and Sophisticated hyperbolic discounting, we make a small modificiation to the MDP agent model. The key idea is to add a variable for measuring time, the *delay*, which is distinct from the objective time-index (called `timeLeft` in our implementation). The objective time-index is crucial to planning for finite-horizon MDPs because it determines how far the agent can travel or explore before time is up. By contrast, the delays are *subjective*. They are used by the agent in *evaluating* possible future rewards but they are not an independent feature of the decision problem.\n",
      "\n",
      "We use delays because discounting agents have preferences over when they receive rewards. When evaluating future prospects, they need to keep track of how far ahead in time that reward occurs. Naive and Sophisticated agents *evaluate* future rewards in the same way. They differ in how they simulate their future actions.\n",
      "\n",
      "The Naive agent at objective time $$t$$ assumes his future self at objective time $$t+c$$ (where $$c>0$$) shares his time preference. So he simulates the $$(t+c)$$-agent as evaluating a reward at time $$t+c$$ with delay $$=c$$ rather than the true delay $$=0$$. The Sophisticated agent correctly models his $$(t+c)$$-agent future self as evaluating an immediate reward with delay $$=0$$.\n",
      "\n",
      "To be more concrete, suppose both Naive and Sophisticated have a discount function $$1/(1+kd)$$, where $$d$$ is how much the reward is delayed. When simulating his future self at $$t+c$$, the Naive agent assumes he'll discount immediate gains at rate $$1/(1+kc)$$ and the Sophisticated agent (correctly) assumes a rate of $$1/(1+0)$$. \n",
      "\n",
      "Adding delays to our model is straightforward. In defining the MDP agent, we <a href=\"/chapters/3a-mdp.html#recursion\">presented</a> Bellman-style recursions for the expected utility of state-action pairs. Discounting agents evaluate states and actions differently depending on their *delay* from the present. So we now define expected utilities of state-action-delay triples:\n",
      "\n",
      "$$\n",
      "EU[s,a,d] = \\delta(d)U(s, a) + \\mathbb{E}_{s', a'}(EU[s', a',d+1])\n",
      "$$\n",
      "\n",
      "where:\n",
      "\n",
      "- $$\\delta  \\colon \\mathbb{N} \\to \\mathbb{R}$$ is the discount function from the delay to the discount factor. In our examples we have (where $$k>0$$ is the discount constant):\n",
      "\n",
      "$$\n",
      "\\delta(d) = \\frac{1}{1+kd}\n",
      "$$\n",
      "\n",
      "- $$s' \\sim T(s,a)$$ exactly as in the non-discounting case.\n",
      "\n",
      "- $$a' \\sim C(s'; d_P)$$ where $$d_P=0$$ for Sophisticated and $$d_P=d+1$$ for Naive.\n",
      "\n",
      "\n",
      "The function $$C \\colon S \\times \\mathbb{N} \\to A$$ is again the *act* function. For $$C(s'; d+1)$$ we take a softmax over the expected value of each action $$a$$, namely, $$EU[s',a,d+1]$$. The act function now takes a delay argument. We interpret $$C(s';d+1)$$ as \"the softmax action the agent would take in state $$s'$$ given that their rewards occur with a delay $$d+1$$\".\n",
      "\n",
      "The Naive agent simulates his future actions by computing $$C(s';d+1)$$; the Sophisticated agent computes the action that will *actually* occur, which is $$C(s';0)$$. So if we want to simulate an environment including a hyperbolic discounter, we can compute the agent's action with $$C(s;0)$$ for every state $$s$$. \n",
      "\n",
      "\n",
      "### Implementing the hyperbolic discounter\n",
      " \n",
      "As with the MDP and POMDP agents, our WebPPL implementation directly translates the mathematical formulation of Naive and Sophisticated hyperbolic discounting. The variable names correspond as follows:\n",
      "\n",
      "- The function $$\\delta$$ is named `discountFunction`\n",
      "\n",
      "- The \"perceived delay\", which controls how the agent's simulated future self evaluates rewards, is $$d$$ in the math and `perceivedDelay` below. \n",
      "\n",
      "- $$s'$$, $$a'$$, $$d+1$$ correspond to `nextState`, `nextAction` and `delay+1` respectively. \n",
      "\n",
      "This codebox simplifies the code for the hyperbolic discounter by omitting definitions of `transition`, `utility` and so on:\n",
      "\n",
      "~~~~\n",
      "var makeAgent = function(params, world) {\n",
      "\n",
      "  var act = dp.cache( \n",
      "    function(state, delay){\n",
      "      return Infer({ model() {\n",
      "        var action = uniformDraw(stateToActions(state));\n",
      "        var eu = expectedUtility(state, action, delay);    \n",
      "        factor(params.alpha * eu);\n",
      "        return action;\n",
      "      }});      \n",
      "    });\n",
      "  \n",
      "  var expectedUtility = dp.cache(\n",
      "    function(state, action, delay){\n",
      "      var u = discountFunction(delay) * utility(state, action);\n",
      "      if (state.terminateAfterAction){\n",
      "        return u; \n",
      "      } else {                     \n",
      "        return u + expectation(Infer({ model() {\n",
      "          var nextState = transition(state, action); \n",
      "          var perceivedDelay = isNaive ? delay + 1 : 0;\n",
      "          var nextAction = sample(act(nextState, perceivedDelay));\n",
      "          return expectedUtility(nextState, nextAction, delay+1);  \n",
      "        }}));\n",
      "      }                      \n",
      "    });\n",
      "  \n",
      "  return { params, expectedUtility, act };\n",
      "};\n",
      "~~~~\n",
      "\n",
      "The next codebox shows how the Naive agent can end up at Donut North in the Restaurant Choice problem, despite this being dominated for any possible utility function. The Naive agent first moves in the direction of Veg, which initially looks better than Donut South. When right outside Donut North, discounting makes it look better than Veg. To visualize this, we display the agent's expected utility calculations at different steps along its trajectory. The crucial values are the `expectedValue` of going left at [3,5] when `delay=0` compared with `delay=4`. The function `plannedTrajectories` uses `expectedValue` to access these values. For each timestep, we plot the agent's position and the expected utility of each action they might perform in the future. \n",
      "\n",
      "<!-- simulate_hyperbolic_agent -->\n",
      "~~~~\n",
      "///fold: makeAgent, mdp, plannedTrajectories\n",
      "var makeAgent = function(params, world) {\n",
      "  var defaultParams = {\n",
      "    alpha: 500, \n",
      "    discount: 1\n",
      "  };\n",
      "  var params = extend(defaultParams, params);\n",
      "  var stateToActions = world.stateToActions;\n",
      "  var transition = world.transition;\n",
      "  var utility = params.utility;\n",
      "  var paramsDiscountFunction = params.discountFunction;\n",
      "\n",
      "  var discountFunction = (\n",
      "    paramsDiscountFunction ? \n",
      "    paramsDiscountFunction : \n",
      "    function(delay){ return 1/(1 + params.discount*delay); });\n",
      "\n",
      "  var isNaive = params.sophisticatedOrNaive === 'naive';\n",
      "\n",
      "  var act = dp.cache( \n",
      "    function(state, delay) {\n",
      "      var delay = delay || 0; // make sure delay is never undefined\n",
      "      return Infer({ model() {\n",
      "        var action = uniformDraw(stateToActions(state));\n",
      "        var eu = expectedUtility(state, action, delay);\n",
      "        factor(params.alpha * eu);\n",
      "        return action;\n",
      "      }});\n",
      "    });\n",
      "\n",
      "  var expectedUtility = dp.cache(\n",
      "    function(state, action, delay) {\n",
      "      var u = discountFunction(delay) * utility(state, action);\n",
      "      if (state.terminateAfterAction){\n",
      "        return u; \n",
      "      } else {\n",
      "        return u + expectation(Infer({ model() {\n",
      "          var nextState = transition(state, action); \n",
      "          var perceivedDelay = isNaive ? delay + 1 : 0;\n",
      "          var nextAction = sample(act(nextState, perceivedDelay));\n",
      "          return expectedUtility(nextState, nextAction, delay+1);\n",
      "        }}));\n",
      "      }\n",
      "    });\n",
      "\n",
      "  return { params, expectedUtility, act };\n",
      "};\n",
      "\n",
      "var ___ = ' '; \n",
      "var DN = { name : 'Donut N' };\n",
      "var DS = { name : 'Donut S' };\n",
      "var V = { name : 'Veg' };\n",
      "var N = { name : 'Noodle' };\n",
      "\n",
      "var grid = [\n",
      "  ['#', '#', '#', '#',  V , '#'],\n",
      "  ['#', '#', '#', ___, ___, ___],\n",
      "  ['#', '#', DN , ___, '#', ___],\n",
      "  ['#', '#', '#', ___, '#', ___],\n",
      "  ['#', '#', '#', ___, ___, ___],\n",
      "  ['#', '#', '#', ___, '#',  N ],\n",
      "  [___, ___, ___, ___, '#', '#'],\n",
      "  [DS , '#', '#', ___, '#', '#']\n",
      "];\n",
      "\n",
      "var mdp = makeGridWorldMDP({\n",
      "  grid,\n",
      "  noReverse: true,\n",
      "  maxTimeAtRestaurant: 2,\n",
      "  start: [3, 1],\n",
      "  totalTime: 11\n",
      "});\n",
      "\n",
      "var MAPActionPath = function(state, world, agent, actualTotalTime, statesOrActions) { \n",
      "  var perceivedTotalTime = state.timeLeft;\n",
      "  assert.ok(perceivedTotalTime  > 1 || state.terminateAfterAction==false,\n",
      "            'perceivedTime<=1. If=1 then should have state.terminateAfterAction,' +\n",
      "            ' but then simulate wont work ' + JSON.stringify(state));\n",
      "\n",
      "  var agentAction = agent.act;\n",
      "  var expectedUtility = agent.expectedUtility;\n",
      "  var transition = world.transition;\n",
      "\n",
      "  var sampleSequence = function (state, actualTimeLeft) {\n",
      "    var action = agentAction(state, actualTotalTime-actualTimeLeft).MAP().val;\n",
      "    var nextState = transition(state, action); \n",
      "    var out = {states:state, actions:action, both:[state,action]}[statesOrActions];\n",
      "    if (actualTimeLeft==0 || state.terminateAfterAction){\n",
      "      return [out];\n",
      "    } else {\n",
      "      return [ out ].concat( sampleSequence(nextState, actualTimeLeft-1));\n",
      "    }\n",
      "  };\n",
      "  return sampleSequence(state, actualTotalTime);\n",
      "};\n",
      "\n",
      "var plannedTrajectory = function(world, agent) {\n",
      "  var getExpectedUtilities = function(trajectory, agent, actions) { \n",
      "    var expectedUtility = agent.expectedUtility;\n",
      "    var v = mapIndexed(function(i, state) {\n",
      "      return [state, map(function (a) { return  expectedUtility(state, a, i); }, actions)];\n",
      "    }, trajectory );\n",
      "    return v;\n",
      "  };\n",
      "  return function(state) {\n",
      "    var currentPlan = MAPActionPath(state, world, agent, state.timeLeft, 'states');\n",
      "    return getExpectedUtilities(currentPlan, agent, world.actions);\n",
      "  };\n",
      "} \n",
      "\n",
      "var plannedTrajectories = function(trajectory, world, agent) { \n",
      "  var getTrajectory = plannedTrajectory(world, agent);\n",
      "  return map(getTrajectory, trajectory);\n",
      "}\n",
      "///\n",
      "\n",
      "var world = mdp.world;\n",
      "var start = mdp.startState;\n",
      "\n",
      "var utilityTable = {\n",
      "  'Donut N': [10, -10],  // [immediate reward, delayed reward]\n",
      "  'Donut S': [10, -10],\n",
      "  'Veg': [-10, 20],\n",
      "  'Noodle': [0, 0],\n",
      "  'timeCost': -.01  // cost of taking a single action \n",
      "};\n",
      "\n",
      "var restaurantUtility = function(state, action) {\n",
      "  var feature = world.feature;\n",
      "  var name = feature(state).name;\n",
      "  if (name) {\n",
      "    return utilityTable[name][state.timeAtRestaurant]\n",
      "  } else {\n",
      "    return utilityTable.timeCost;\n",
      "  }\n",
      "};\n",
      "\n",
      "var runAndGraph = function(agent) { \n",
      "  var trajectory = simulateMDP(mdp.startState, world, agent);\n",
      "  var plans = plannedTrajectories(trajectory, world, agent);\n",
      "  viz.gridworld(world, {\n",
      "    trajectory, \n",
      "    dynamicActionExpectedUtilities: plans\n",
      "  });\n",
      "};\n",
      "\n",
      "var agent = makeAgent({\n",
      "  sophisticatedOrNaive: 'naive', \n",
      "  utility: restaurantUtility\n",
      "}, world);\n",
      "\n",
      "print('Naive agent: \\n\\n');\n",
      "runAndGraph(agent);\n",
      "~~~~\n",
      "\n",
      "We run the Sophisticated agent with the same parameters and visualization. \n",
      "\n",
      "<!-- simulate_hyperbolic_agent_sophisticated -->\n",
      "~~~~\n",
      "///fold: \n",
      "var makeAgent = function(params, world) {\n",
      "  var defaultParams = {\n",
      "    alpha: 500, \n",
      "    discount: 1\n",
      "  };\n",
      "  var params = extend(defaultParams, params);\n",
      "  var stateToActions = world.stateToActions;\n",
      "  var transition = world.transition;\n",
      "  var utility = params.utility;\n",
      "  var paramsDiscountFunction = params.discountFunction;\n",
      "\n",
      "  var discountFunction = (\n",
      "    paramsDiscountFunction ? \n",
      "    paramsDiscountFunction : \n",
      "    function(delay){ return 1/(1+params.discount*delay); });\n",
      "\n",
      "  var isNaive = params.sophisticatedOrNaive === 'naive';\n",
      "\n",
      "  var act = dp.cache( \n",
      "    function(state, delay) {\n",
      "      var delay = delay || 0; // make sure delay is never undefined\n",
      "      return Infer({ model() {\n",
      "        var action = uniformDraw(stateToActions(state));\n",
      "        var eu = expectedUtility(state, action, delay);\n",
      "        factor(params.alpha * eu);\n",
      "        return action;\n",
      "      }});\n",
      "    });\n",
      "\n",
      "  var expectedUtility = dp.cache(\n",
      "    function(state, action, delay) {\n",
      "      var u = discountFunction(delay) * utility(state, action);\n",
      "      if (state.terminateAfterAction){\n",
      "        return u; \n",
      "      } else {\n",
      "        return u + expectation(Infer({ model() {\n",
      "          var nextState = transition(state, action); \n",
      "          var perceivedDelay = isNaive ? delay + 1 : 0;\n",
      "          var nextAction = sample(act(nextState, perceivedDelay));\n",
      "          return expectedUtility(nextState, nextAction, delay+1);\n",
      "        }}));\n",
      "      }\n",
      "    });\n",
      "\n",
      "  return { params, expectedUtility, act };\n",
      "};\n",
      "\n",
      "var ___ = ' '; \n",
      "var DN = { name : 'Donut N' };\n",
      "var DS = { name : 'Donut S' };\n",
      "var V = { name : 'Veg' };\n",
      "var N = { name : 'Noodle' };\n",
      "\n",
      "var grid = [\n",
      "  ['#', '#', '#', '#',  V , '#'],\n",
      "  ['#', '#', '#', ___, ___, ___],\n",
      "  ['#', '#', DN , ___, '#', ___],\n",
      "  ['#', '#', '#', ___, '#', ___],\n",
      "  ['#', '#', '#', ___, ___, ___],\n",
      "  ['#', '#', '#', ___, '#',  N ],\n",
      "  [___, ___, ___, ___, '#', '#'],\n",
      "  [DS , '#', '#', ___, '#', '#']\n",
      "];\n",
      "\n",
      "var mdp = makeGridWorldMDP({\n",
      "  grid,\n",
      "  noReverse: true,\n",
      "  maxTimeAtRestaurant: 2,\n",
      "  start: [3, 1],\n",
      "  totalTime: 11\n",
      "});\n",
      "\n",
      "var world = mdp.world;\n",
      "\n",
      "var utilityTable = {\n",
      "  'Donut N': [10, -10],  // [immediate reward, delayed reward]\n",
      "  'Donut S': [10, -10],\n",
      "  'Veg': [-10, 20],\n",
      "  'Noodle': [0, 0],\n",
      "  'timeCost': -.01  // cost of taking a single action \n",
      "};\n",
      "\n",
      "var restaurantUtility = function(state, action) {\n",
      "  var feature = world.feature;\n",
      "  var name = feature(state).name;\n",
      "  if (name) {\n",
      "    return utilityTable[name][state.timeAtRestaurant]\n",
      "  } else {\n",
      "    return utilityTable.timeCost;\n",
      "  }\n",
      "};\n",
      "\n",
      "var MAPActionPath = function(state, world, agent, actualTotalTime, statesOrActions) { \n",
      "  var perceivedTotalTime = state.timeLeft;\n",
      "  assert.ok(perceivedTotalTime  > 1 || state.terminateAfterAction==false,\n",
      "            'perceivedTime<=1. If=1 then should have state.terminateAfterAction,' +\n",
      "            ' but then simulate wont work ' + JSON.stringify(state));\n",
      "\n",
      "  var agentAction = agent.act;\n",
      "  var expectedUtility = agent.expectedUtility;\n",
      "  var transition = world.transition;\n",
      "\n",
      "  var sampleSequence = function (state, actualTimeLeft) {\n",
      "    var action = agentAction(state, actualTotalTime-actualTimeLeft).MAP().val;\n",
      "    var nextState = transition(state, action); \n",
      "    var out = {states:state, actions:action, both:[state,action]}[statesOrActions];\n",
      "    if (actualTimeLeft==0 || state.terminateAfterAction){\n",
      "      return [out];\n",
      "    } else {\n",
      "      return [ out ].concat( sampleSequence(nextState, actualTimeLeft-1));\n",
      "    }\n",
      "  };\n",
      "  return sampleSequence(state, actualTotalTime);\n",
      "};\n",
      "\n",
      "var plannedTrajectory = function(world, agent) {\n",
      "  var getExpectedUtilities = function(trajectory, agent, actions) { \n",
      "    var expectedUtility = agent.expectedUtility;\n",
      "    var v = mapIndexed(function(i, state) {\n",
      "      return [state, map(function (a) { return  expectedUtility(state, a, i); }, actions)];\n",
      "    }, trajectory );\n",
      "    return v;\n",
      "  };\n",
      "  return function(state) {\n",
      "    var currentPlan = MAPActionPath(state, world, agent, state.timeLeft, 'states');\n",
      "    return getExpectedUtilities(currentPlan, agent, world.actions);\n",
      "  };\n",
      "};\n",
      "\n",
      "var plannedTrajectories = function(trajectory, world, agent) { \n",
      "  var getTrajectory = plannedTrajectory(world, agent);\n",
      "  return map(getTrajectory, trajectory);\n",
      "};\n",
      "\n",
      "var runAndGraph = function(agent) { \n",
      "  var trajectory = simulateMDP(mdp.startState, world, agent);\n",
      "  var plans = plannedTrajectories(trajectory, world, agent);\n",
      "  viz.gridworld(world, {\n",
      "    trajectory, \n",
      "    dynamicActionExpectedUtilities: plans\n",
      "  });\n",
      "};\n",
      "///\n",
      "\n",
      "var agent = makeAgent({\n",
      "  sophisticatedOrNaive: 'sophisticated', \n",
      "  utility: restaurantUtility\n",
      "}, world);\n",
      "\n",
      "print('Sophisticated agent: \\n\\n');\n",
      "runAndGraph(agent);\n",
      "~~~~\n",
      "\n",
      ">**Exercise**: What would an exponential discounter with identical preferences to the agents above do on the Restaurant Choice problem? Implement an exponential discounter in the codebox above by adding a `discountFunction` property to the `params` argument to `makeAgent`. \n",
      "<br>\n",
      "\n",
      "--------\n",
      "\n",
      "<a id='procrastination'></a>\n",
      "\n",
      "### Example: Procrastinating on a task\n",
      "\n",
      "Compared to the Restaurant Choice problem, procrastination leads to (systematically biased) behavior that is especially hard to explain on the softmax noise mode.\n",
      "\n",
      "> **The Procrastination Problem**\n",
      "> <br>You have a hard deadline of ten days to complete a task (e.g. write a paper for class, apply for a school or job). Completing the task takes a full day and has a *cost* (it's unpleasant work). After the task is complete you get a *reward* (typically exceeding the cost). There is an incentive to finish early: every day you delay finishing, your reward gets slightly smaller. (Imagine that it's good for your reputation to complete tasks early or that early applicants are considered first).\n",
      "\n",
      "Note that if the task is worth doing at the last minute, then you should do it immediately (because the reward diminishes over time). Yet people often do this kind of task at the last minute -- the worst possible time to do it!\n",
      "\n",
      "Hyperbolic discounting provides an elegant model of this behavior. On Day 1, a hyperbolic discounter will prefer that they complete the task tomorrow rather than today. Moreover, a Naive agent wrongly predicts they will complete the task tomorrow and so puts off the task till Day 2. When Day 2 arrives, the Naive agent reasons in the same way -- telling themself that they can avoid the work today by putting it off till tomorrow. This continues until the last possible day, when the Naive agent finally completes the task.\n",
      "\n",
      "In this problem, the behavior of optimal and time-inconsistent agents with identical preferences (i.e. utility functions) diverges. If the deadline is $$T$$ days from the start, the optimal agent will do the task immediately and the Naive agent will do the task on Day $$T$$. Any problem where a time-inconsistent agent receives exponentially lower reward than an optimal agent contains a close variant of our Procrastination Problem refp:kleinberg2014time [^kleinberg]. \n",
      "\n",
      "[^kleinberg]: Kleinberg and Oren's paper considers a variant problem where the each cost/penalty for waiting is received immediately (rather than being delayed until the time the task is done). In this variant, the agent must eventually complete the task. The authors consider \"semi-myopic\" time-inconsistent agents, i.e. agents who do not discount their next reward, but discount all future rewards by $$\\beta < 1$$. They show that in any problem where a semi-myopic agent receives exponentially lower reward than an optimal agent, the problem must contain a copy of their variant of the Procrastination Problem.\n",
      "\n",
      "We formalize the Procrastination Problem in terms of a deterministic graph. Suppose the **deadline** is $$T$$ steps from the start. Assume that after $$t$$ < $$T$$ steps the agent has not yet completed the task. Then the agent can take the action `\"work\"` (which has **work cost** $$-w$$) or the action `\"wait\"` with zero cost. After the `\"work\"` action the agent transitions to the `\"reward\"` state and receives $$+(R - t \\epsilon)$$, where $$R$$ is the **reward** for the task and $$\\epsilon$$ is how much the reward diminishes for every day of waiting (the **wait cost**). See Figure 3 below.  \n",
      "\n",
      "<img src=\"/assets/img/procrastination_mdp.png\" alt=\"diagram\" style=\"width: 650px;\"/>\n",
      "\n",
      ">**Figure 3:** Transition graph for Procrastination Problem. States are represented by nodes. Edges are state-transitions and are labeled with the action name and the utility of the state-action pair. Terminal nodes have a bold border and their utility is labeled below.\n",
      "\n",
      "We simulate the behavior of hyperbolic discounters on the Procrastination Problem. We vary the discount rate $$k$$ while holding the other parameters fixed. The agent's behavior can be summarized by its final state (`\"wait_state\"` or `\"reward_state`) and by how much time elapses before termination. When $$k$$ is sufficiently high, the agent will not even complete the task on the last day. \n",
      "\n",
      "<!-- procrastinate -->\n",
      "~~~~\n",
      "///fold: makeProcrastinationMDP, makeProcrastinationUtility\n",
      "var makeProcrastinationMDP = function(deadlineTime) {\n",
      "  var stateLocs = [\"wait_state\", \"reward_state\"];\n",
      "  var actions = [\"wait\", \"work\", \"relax\"];\n",
      "\n",
      "  var stateToActions = function(state) {\n",
      "    return (state.loc === \"wait_state\" ? \n",
      "            [\"wait\", \"work\"] :\n",
      "            [\"relax\"]);\n",
      "  };\n",
      "\n",
      "  var advanceTime = function (state) {\n",
      "    var newTimeLeft = state.timeLeft - 1;\n",
      "    var terminateAfterAction = (newTimeLeft === 1 || \n",
      "                                state.loc === \"reward_state\");\n",
      "    return extend(state, {\n",
      "      timeLeft: newTimeLeft,\n",
      "      terminateAfterAction: terminateAfterAction\n",
      "    });\n",
      "  };\n",
      "\n",
      "  var transition = function(state, action) {\n",
      "    assert.ok(_.includes(stateLocs, state.loc) && _.includes(actions, action), \n",
      "              'procrastinate transition:' + [state.loc,action]);\n",
      "    \n",
      "    if (state.loc === \"reward_state\") {\n",
      "      return advanceTime(state);\n",
      "    } else if (action === \"wait\") {\n",
      "      var waitSteps = state.waitSteps + 1;\n",
      "      return extend(advanceTime(state), { waitSteps });\n",
      "    } else {\n",
      "      var newState = extend(state, { loc: \"reward_state\" });\n",
      "      return advanceTime(newState);\n",
      "    }\n",
      "  };\n",
      "\n",
      "  var feature = function(state) {\n",
      "    return state.loc;\n",
      "  };\n",
      "\n",
      "  var startState = {\n",
      "    loc: \"wait_state\",\n",
      "    waitSteps: 0,\n",
      "    timeLeft: deadlineTime,\n",
      "    terminateAfterAction: false\n",
      "  };\n",
      "\n",
      "  return {\n",
      "    actions,\n",
      "    stateToActions,\n",
      "    transition,\n",
      "    feature,\n",
      "    startState\n",
      "  };\n",
      "};\n",
      "\n",
      "\n",
      "var makeProcrastinationUtility = function(utilityTable) {\n",
      "  assert.ok(hasProperties(utilityTable, ['waitCost', 'workCost', 'reward']),\n",
      "            'makeProcrastinationUtility args');\n",
      "  var waitCost = utilityTable.waitCost;\n",
      "  var workCost = utilityTable.workCost;\n",
      "  var reward = utilityTable.reward;\n",
      "\n",
      "  // NB: you receive the *workCost* when you leave the *wait_state*\n",
      "  // You then receive the reward when leaving the *reward_state* state\n",
      "  return function(state, action) {\n",
      "    if (state.loc === \"reward_state\") {\n",
      "      return reward + state.waitSteps * waitCost;\n",
      "    } else if (action === \"work\") {\n",
      "      return workCost;\n",
      "    } else {\n",
      "      return 0;\n",
      "    }\n",
      "  };\n",
      "};\n",
      "///\n",
      "\n",
      "// Construct Procrastinate world \n",
      "var deadline = 10;\n",
      "var world = makeProcrastinationMDP(deadline);\n",
      "\n",
      "// Agent params\n",
      "var utilityTable = {\n",
      "  reward: 4.5,\n",
      "  waitCost: -0.1,\n",
      "  workCost: -1\n",
      "};\n",
      "\n",
      "var params = {\n",
      "  utility: makeProcrastinationUtility(utilityTable),\n",
      "  alpha: 1000,\n",
      "  discount: null,\n",
      "  sophisticatedOrNaive: 'sophisticated'\n",
      "};\n",
      "\n",
      "var getLastState = function(discount){\n",
      "  var agent = makeMDPAgent(extend(params, { discount: discount }), world);\n",
      "  var states = simulateMDP(world.startState, world, agent, 'states');\n",
      "  return [last(states).loc, states.length];\n",
      "};\n",
      "\n",
      "map(function(discount) {\n",
      "  var lastState = getLastState(discount);\n",
      "  print('Discount: ' + discount + '. Last state: ' + lastState[0] +\n",
      "        '. Time: ' + lastState[1] + '\\n')\n",
      "}, _.range(8));\n",
      "~~~~\n",
      "\n",
      "\n",
      ">**Exercise:**\n",
      "\n",
      "> 1. Explain how an exponential discounter would behave on this task. Assume their utilities are the same as above and consider different discount rates.\n",
      "> 2. Run the codebox above with a Sophisticated agent. Explain the results. \n",
      "\n",
      "\n",
      "Next chapter: [Myopia for rewards and belief updates](/chapters/5c-myopic.html)\n",
      "\n",
      "<br>\n",
      "\n",
      "### Footnotes\n",
      "\n",
      "\n",
      "---\n",
      "layout: chapter\n",
      "title: \"MDPs and Gridworld in WebPPL\"\n",
      "description: Noisy actions (softmax), stochastic transitions, policies, Q-values.\n",
      "---\n",
      "\n",
      "This chapter explores some key features of MDPs: stochastic dynamics, stochastic policies, and value functions.\n",
      "\n",
      "### Hiking in Gridworld\n",
      "\n",
      "We begin by introducing a new gridworld MDP:\n",
      "\n",
      "> **Hiking Problem**:\n",
      ">Suppose that Alice is hiking. There are two peaks nearby, denoted \"West\" and \"East\". The peaks provide different views and Alice must choose between them. South of Alice's starting position is a steep hill. Falling down the hill would result in painful (but non-fatal) injury and end the hike early.\n",
      "\n",
      "We represent Alice's hiking problem with a Gridworld similar to Bob's Restaurant Choice example. The peaks are terminal states, providing different utilities. The steep hill is represented by a row of terminal state, each with identical negative utility. Each timestep before Alice reaches a terminal state incurs a \"time cost\", which is negative to represent the fact that Alice prefers a shorter hike. <!-- TODO might be good to indicate on plot that the steep hills are bad -->\n",
      "\n",
      "<!-- draw_hike -->\n",
      "~~~~\n",
      "var H = { name: 'Hill' };\n",
      "var W = { name: 'West' };\n",
      "var E = { name: 'East' };\n",
      "var ___ = ' ';\n",
      "\n",
      "var grid = [\n",
      "  [___, ___, ___, ___, ___],\n",
      "  [___, '#', ___, ___, ___],\n",
      "  [___, '#',  W , '#',  E ],\n",
      "  [___, ___, ___, ___, ___],\n",
      "  [ H ,  H ,  H ,  H ,  H ]\n",
      "];\n",
      "\n",
      "var start = [0, 1];\n",
      "\n",
      "var mdp = makeGridWorldMDP({ grid, start });\n",
      "\n",
      "viz.gridworld(mdp.world, { trajectory: [mdp.startState] });\n",
      "~~~~\n",
      "\n",
      "We start with a *deterministic* transition function. In this case, Alice's risk of falling down the steep hill is solely due to softmax noise in her action choice (which is minimal in this case). The agent model is the same as the one at the end of [Chapter III.1](/chapters/3a-mdp.html). We place the functions `act`, `expectedUtility` in a function `makeMDPAgent`. The following codebox defines this function and we use it later on without defining it (since it's in the `webppl-agents` library).\n",
      "\n",
      "<!-- define_agent_simulate -->\n",
      "~~~~\n",
      "// Set up agent structure\n",
      "\n",
      "var makeMDPAgent = function(params, world) {\n",
      "  var stateToActions = world.stateToActions;\n",
      "  var transition = world.transition;\n",
      "  var utility = params.utility;\n",
      "  var alpha = params.alpha;\n",
      "\n",
      "  var act = dp.cache(\n",
      "    function(state) {\n",
      "      return Infer({ model() {\n",
      "        var action = uniformDraw(stateToActions(state));\n",
      "        var eu = expectedUtility(state, action);\n",
      "        factor(alpha * eu);\n",
      "        return action;\n",
      "      }});\n",
      "    });\n",
      "\n",
      "  var expectedUtility = dp.cache(\n",
      "    function(state, action){\n",
      "      var u = utility(state, action);\n",
      "      if (state.terminateAfterAction){\n",
      "        return u;\n",
      "      } else {\n",
      "        return u + expectation(Infer({ model() {\n",
      "          var nextState = transition(state, action);\n",
      "          var nextAction = sample(act(nextState));\n",
      "          return expectedUtility(nextState, nextAction);\n",
      "        }}));\n",
      "      }\n",
      "    });\n",
      "\n",
      "  return { params, expectedUtility, act };\n",
      "};\n",
      "\n",
      "var simulate = function(startState, world, agent) {\n",
      "  var act = agent.act;\n",
      "  var transition = world.transition;\n",
      "  var sampleSequence = function(state) {\n",
      "    var action = sample(act(state));\n",
      "    var nextState = transition(state, action);\n",
      "    if (state.terminateAfterAction) {\n",
      "      return [state];\n",
      "    } else {\n",
      "      return [state].concat(sampleSequence(nextState));\n",
      "    }\n",
      "  };\n",
      "  return sampleSequence(startState);\n",
      "};\n",
      "\n",
      "\n",
      "// Set up world\n",
      "\n",
      "var makeHikeMDP = function(options) {\n",
      "  var H = { name: 'Hill' };\n",
      "  var W = { name: 'West' };\n",
      "  var E = { name: 'East' };\n",
      "  var ___ = ' ';\n",
      "  var grid = [\n",
      "    [___, ___, ___, ___, ___],\n",
      "    [___, '#', ___, ___, ___],\n",
      "    [___, '#',  W , '#',  E ],\n",
      "    [___, ___, ___, ___, ___],\n",
      "    [ H ,  H ,  H ,  H ,  H ]\n",
      "  ];\n",
      "  return makeGridWorldMDP(_.assign({ grid }, options));\n",
      "};\n",
      "\n",
      "var mdp = makeHikeMDP({\n",
      "  start: [0, 1],\n",
      "  totalTime: 12,\n",
      "  transitionNoiseProbability: 0\n",
      "});\n",
      "\n",
      "var makeUtilityFunction = mdp.makeUtilityFunction;\n",
      "\n",
      "\n",
      "// Create parameterized agent\n",
      "\n",
      "var utility = makeUtilityFunction({\n",
      "  East: 10,\n",
      "  West: 1,\n",
      "  Hill: -10,\n",
      "  timeCost: -.1\n",
      "});\n",
      "var agent = makeMDPAgent({ utility, alpha: 1000 }, mdp.world);\n",
      "\n",
      "\n",
      "// Run agent on world\n",
      "\n",
      "var trajectory = simulate(mdp.startState, mdp.world, agent);\n",
      "\n",
      "\n",
      "viz.gridworld(mdp.world, { trajectory });\n",
      "~~~~\n",
      "\n",
      ">**Exercise**: Adjust the parameters of `utilityTable` in order to produce the following behaviors:\n",
      "\n",
      ">1. The agent goes directly to \"West\".\n",
      ">2. The agent takes the long way around to \"West\".\n",
      ">3. The agent sometimes goes to the Hill at $$[1,0]$$. Try to make this outcome as likely as possible.\n",
      "<!-- 3 is obtained by making timeCost positive and Hill better than alternatives -->\n",
      "\n",
      "\n",
      "### Hiking with stochastic transitions\n",
      "\n",
      "Imagine that the weather is very wet and windy. As a result, Alice will sometimes intend to go one way but actually go another way (because she slips in the mud). In this case, the shorter route to the peaks might be too risky for Alice.\n",
      "\n",
      "To model bad weather, we assume that at every timestep, there is a constant independent probability `transitionNoiseProbability` of the agent moving orthogonally to their intended direction. The independence assumption is unrealistic (if a location is slippery at one timestep it is more likely slippery the next), but it is simple and satisfies the Markov assumption for MDPs.\n",
      "\n",
      "Setting `transitionNoiseProbability=0.1`, the agent's first action is now to move \"up\" instead of \"right\".\n",
      "\n",
      "~~~~\n",
      "///fold: makeHikeMDP\n",
      "var makeHikeMDP = function(options) {\n",
      "  var H = { name: 'Hill' };\n",
      "  var W = { name: 'West' };\n",
      "  var E = { name: 'East' };\n",
      "  var ___ = ' ';\n",
      "  var grid = [\n",
      "    [___, ___, ___, ___, ___],\n",
      "    [___, '#', ___, ___, ___],\n",
      "    [___, '#',  W , '#',  E ],\n",
      "    [___, ___, ___, ___, ___],\n",
      "    [ H ,  H ,  H ,  H ,  H ]\n",
      "  ];\n",
      "  return makeGridWorldMDP(_.assign({ grid }, options));\n",
      "};\n",
      "///\n",
      "\n",
      "// Set up world\n",
      "\n",
      "var mdp = makeHikeMDP({\n",
      "  start: [0, 1],\n",
      "  totalTime: 13,\n",
      "  transitionNoiseProbability: 0.1  // <- NEW\n",
      "});\n",
      "\n",
      "\n",
      "// Create parameterized agent\n",
      "\n",
      "var makeUtilityFunction = mdp.makeUtilityFunction;\n",
      "var utility = makeUtilityFunction({\n",
      "  East: 10,\n",
      "  West: 1,\n",
      "  Hill: -10,\n",
      "  timeCost: -.1\n",
      "});\n",
      "var agent = makeMDPAgent({ utility, alpha: 100 }, mdp.world);\n",
      "\n",
      "\n",
      "// Generate a single trajectory, draw\n",
      "\n",
      "var trajectory = simulateMDP(mdp.startState, mdp.world, agent, 'states');\n",
      "viz.gridworld(mdp.world, { trajectory });\n",
      "\n",
      "\n",
      "// Generate 100 trajectories, plot distribution on lengths\n",
      "\n",
      "var trajectoryDist = Infer({\n",
      "  model() {\n",
      "    var trajectory = simulateMDP(mdp.startState, mdp.world, agent);\n",
      "    return { trajectoryLength: trajectory.length }\n",
      "  },\n",
      "  method: 'forward',\n",
      "  samples: 100\n",
      "});\n",
      "\n",
      "viz(trajectoryDist);\n",
      "~~~~\n",
      "\n",
      ">**Exercise:**\n",
      "\n",
      ">1. Keeping `transitionNoiseProbability=0.1`, find settings for `utilityTable` such that the agent goes \"right\" instead of \"up\".\n",
      ">2. Set `transitionNoiseProbability=0.01`. Change a single parameter in `utilityTable` such that the agent goes \"right\" (there are multiple ways to do this).\n",
      "<!-- put up timeCost to -1 or so -->\n",
      "\n",
      "### Noisy transitions vs. Noisy agents\n",
      "\n",
      "It's important to distinguish noise in the transition function from the softmax noise in the agent's selection of actions. Noise (or \"stochasticity\") in the transition function is a representation of randomness in the world. This is easiest to think about in games of chance[^noise]. In a game of chance (e.g. slot machines or poker) rational agents will take into account the randomness in the game. By contrast, softmax noise is a property of an agent. For example, we can vary the behavior of otherwise identical agents by varying their parameter $$\\alpha$$.\n",
      "\n",
      "Unlike transition noise, softmax noise has little influence on the agent's planning for the Hiking Problem. Since it's so bad to fall down the hill, the softmax agent will rarely do so even if they take the short route. The softmax agent is like a person who takes inefficient routes when stakes are low but \"pulls themself together\" when stakes are high.\n",
      "\n",
      "[^noise]: An agent's world model might treat a complex set of deterministic rules as random. In this sense, agents will vary in whether they represent an MDP as stochastic or not. We won't consider that case in this tutorial.\n",
      "\n",
      ">**Exercise:** Use the codebox below to explore different levels of softmax noise. Find a setting of `utilityTable` and `alpha` such that the agent goes to West and East equally often and nearly always takes the most direct route to both East and West. Included below is code for simulating many trajectories and returning the trajectory length. You can extend this code to measure whether the route taken by the agent is direct or not. (Note that while the softmax agent here is able to \"backtrack\" or return to its previous location, in later Gridworld examples we disalllow backtracking as a possible action).\n",
      "\n",
      "~~~~\n",
      "///fold: makeHikeMDP, set up world\n",
      "var makeHikeMDP = function(options) {\n",
      "  var H = { name: 'Hill' };\n",
      "  var W = { name: 'West' };\n",
      "  var E = { name: 'East' };\n",
      "  var ___ = ' ';\n",
      "  var grid = [\n",
      "    [___, ___, ___, ___, ___],\n",
      "    [___, '#', ___, ___, ___],\n",
      "    [___, '#',  W , '#',  E ],\n",
      "    [___, ___, ___, ___, ___],\n",
      "    [ H ,  H ,  H ,  H ,  H ]\n",
      "  ];\n",
      "  return makeGridWorldMDP(_.assign({ grid }, options));\n",
      "};\n",
      "\n",
      "var mdp = makeHikeMDP({\n",
      "  start: [0, 1],\n",
      "  totalTime: 13,\n",
      "  transitionNoiseProbability: 0.1\n",
      "});\n",
      "\n",
      "var world = mdp.world;\n",
      "var startState = mdp.startState;\n",
      "var makeUtilityFunction = mdp.makeUtilityFunction;\n",
      "///\n",
      "\n",
      "// Create parameterized agent\n",
      "var utility = makeUtilityFunction({\n",
      "  East: 10,\n",
      "  West: 1,\n",
      "  Hill: -10,\n",
      "  timeCost: -.1\n",
      "});\n",
      "var alpha = 1;  // <- SOFTMAX NOISE\n",
      "var agent = makeMDPAgent({ utility, alpha }, world);\n",
      "\n",
      "// Generate a single trajectory, draw\n",
      "var trajectory = simulateMDP(startState, world, agent, 'states');\n",
      "viz.gridworld(world, { trajectory });\n",
      "\n",
      "// Generate 100 trajectories, plot distribution on lengths\n",
      "var trajectoryDist = Infer({\n",
      "  model() {\n",
      "    var trajectory = simulateMDP(startState, world, agent);\n",
      "    return { trajectoryLength: trajectory.length }\n",
      "  },\n",
      "  method: 'forward',\n",
      "  samples: 100\n",
      "});\n",
      "viz(trajectoryDist);\n",
      "~~~~\n",
      "\n",
      "\n",
      "### Stochastic transitions: plans and policies\n",
      "\n",
      "We return to the case of a stochastic environment with very low softmax action noise. In a stochastic environment, the agent sometimes finds themself in a state they did not intend to reach. The functions `agent` and `expectedUtility` (inside `makeMDPAgent`) implicitly compute the expected utility of actions for every possible future state, including states that the agent will try to avoid. In the MDP literature, this function from states and remaining time to actions is called a *policy*. (For infinite-horizon MDPs, policies are functions from states to actions.) Since policies take into account every possible contingency, they are quite different from the everyday notion of a plan.\n",
      "\n",
      "Consider the example from above where the agent takes the long route because of the risk of falling down the hill. If we generate a single trajectory for the agent, they will likely take the long route. However, if we generated many trajectories, we would sometimes see the agent move \"right\" instead of \"up\" on their first move. Before taking this first action, the agent implicitly computes what they *would* do if they end up moving right. To find out what they would do, we can artificially start the agent in $$[1,1]$$ instead of $$[0,1]$$:\n",
      "\n",
      "<!-- policy -->\n",
      "~~~~\n",
      "///fold: makeHikeMDP\n",
      "var makeHikeMDP = function(options) {\n",
      "  var H = { name: 'Hill' };\n",
      "  var W = { name: 'West' };\n",
      "  var E = { name: 'East' };\n",
      "  var ___ = ' ';\n",
      "  var grid = [\n",
      "    [___, ___, ___, ___, ___],\n",
      "    [___, '#', ___, ___, ___],\n",
      "    [___, '#',  W , '#',  E ],\n",
      "    [___, ___, ___, ___, ___],\n",
      "    [ H ,  H ,  H ,  H ,  H ]\n",
      "  ];\n",
      "  return makeGridWorldMDP(_.assign({ grid }, options));\n",
      "};\n",
      "///\n",
      "\n",
      "// Parameters for world\n",
      "var mdp = makeHikeMDP({\n",
      "  start: [1, 1],  // Previously: [0, 1]\n",
      "  totalTime: 11,  // Previously: 12\n",
      "  transitionNoiseProbability: 0.1\n",
      "});\n",
      "var makeUtilityFunction = mdp.makeUtilityFunction;\n",
      "\n",
      "// Parameters for agent\n",
      "var utility = makeUtilityFunction({ \n",
      "  East: 10, \n",
      "  West: 1,\n",
      "  Hill: -10,\n",
      "  timeCost: -.1\n",
      "});\n",
      "var agent = makeMDPAgent({ utility, alpha: 1000 }, mdp.world);\n",
      "var trajectory = simulateMDP(mdp.startState, mdp.world, agent, 'states');\n",
      "\n",
      "viz.gridworld(mdp.world, { trajectory });\n",
      "~~~~\n",
      "\n",
      "Extending this idea, we can display the expected values of each action the agent *could have taken* during their trajectory. These expected values numbers are analogous to state-action Q-values in infinite-horizon MDPs.\n",
      "\n",
      "The expected values were already being computed implicitly; we now use `getExpectedUtilitiesMDP` to access them. The displayed numbers in each grid cell are the expected utilities of moving in the corresponding directions. For example, we can read off how close the agent was to taking the short route as opposed to the long route. (Note that if the difference in expected utility between two actions is small then a noisy agent will take each of them with nearly equal probability).\n",
      "\n",
      "~~~~\n",
      "///fold: makeBigHikeMDP, getExpectedUtilitiesMDP\n",
      "var makeBigHikeMDP = function(options) {\n",
      "  var H = { name: 'Hill' };\n",
      "  var W = { name: 'West' };\n",
      "  var E = { name: 'East' };\n",
      "  var ___ = ' ';\n",
      "  var grid = [\n",
      "    [___, ___, ___, ___, ___, ___],\n",
      "    [___, ___, ___, ___, ___, ___],\n",
      "    [___, ___, '#', ___, ___, ___],\n",
      "    [___, ___, '#',  W , '#',  E ],\n",
      "    [___, ___, ___, ___, ___, ___],\n",
      "    [ H ,  H ,  H ,  H ,  H ,  H ]\n",
      "  ];\n",
      "  return makeGridWorldMDP(_.assign({ grid }, options));\n",
      "};\n",
      "\n",
      "// trajectory must consist only of states. This can be done by calling\n",
      "// *simulate* with an additional final argument 'states'.\n",
      "var getExpectedUtilitiesMDP = function(stateTrajectory, world, agent) {\n",
      "  var eu = agent.expectedUtility;\n",
      "  var actions = world.actions;\n",
      "  var getAllExpectedUtilities = function(state) {\n",
      "    var actionUtilities = map(\n",
      "      function(action){ return eu(state, action); },\n",
      "      actions);\n",
      "    return [state, actionUtilities];\n",
      "  };\n",
      "  return map(getAllExpectedUtilities, stateTrajectory);\n",
      "};\n",
      "///\n",
      "\n",
      "// Long route is better, agent takes long route\n",
      "\n",
      "var mdp = makeBigHikeMDP({\n",
      "  start: [1, 1],\n",
      "  totalTime: 12,\n",
      "  transitionNoiseProbability: 0.03\n",
      "});\n",
      "var makeUtilityFunction = mdp.makeUtilityFunction;\n",
      "\n",
      "var utility = makeUtilityFunction({\n",
      "  East: 10,\n",
      "  West: 7,\n",
      "  Hill : -40,\n",
      "  timeCost: -0.4\n",
      "});\n",
      "var agent = makeMDPAgent({ utility, alpha: 100 }, mdp.world);\n",
      "\n",
      "var trajectory = simulateMDP(mdp.startState, mdp.world, agent, 'states');\n",
      "var actionExpectedUtilities = getExpectedUtilitiesMDP(trajectory, mdp.world, agent);\n",
      "\n",
      "viz.gridworld(mdp.world, { trajectory, actionExpectedUtilities });\n",
      "~~~~\n",
      "\n",
      "So far, our agents all have complete knowledge about the state of the world. In the [next chapter](/chapters/3c-pomdp.html), we will explore partially observable worlds.\n",
      "\n",
      "<br>\n",
      "\n",
      "### Footnotes\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print entire papers[0]['text'] with wrapping\n",
    "for p in papers:\n",
    "    print(p['text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b07a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "817f150e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 'eae3b96985732bb8089867fa0a722db3', 'title': 'Modeling Agents with Probabilistic Programs', 'url': 'https://agentmodels.org/chapters/5b-time-inconsistency.html', 'source': 'agentmodels', 'source_type': 'markdown', 'text': '---\\nlayout: chapter\\ntitle: \"Time inconsistency II\"\\ndescription: Formal model of time-inconsistent agent, Gridworld and Procrastination examples.\\n\\n---\\n\\n## Formal Model and Implementation of Hyperbolic Discounting\\n\\n\\n### Formal Model of Naive and Sophisticated Hyperbolic Discounters\\n\\nTo formalize Naive and Sophisticated hyperbolic discounting, we make a small modificiation to the MDP agent model. The key idea is to add a variable for measuring time, the *delay*, which is distinct from the objective time-index (called `timeLeft` in our implementation). The objective time-index is crucial to planning for finite-horizon MDPs because it determines how far the agent can travel or explore before time is up. By contrast, the delays are *subjective*. They are used by the agent in *evaluating* possible future rewards but they are not an independent feature of the decision problem.\\n\\nWe use delays because discounting agents have preferences over when they receive rewards. When evaluating future prospects, they need to keep track of how far ahead in time that reward occurs. Naive and Sophisticated agents *evaluate* future rewards in the same way. They differ in how they simulate their future actions.\\n\\nThe Naive agent at objective time $$t$$ assumes his future self at objective time $$t+c$$ (where $$c>0$$) shares his time preference. So he simulates the $$(t+c)$$-agent as evaluating a reward at time $$t+c$$ with delay $$=c$$ rather than the true delay $$=0$$. The Sophisticated agent correctly models his $$(t+c)$$-agent future self as evaluating an immediate reward with delay $$=0$$.\\n\\nTo be more concrete, suppose both Naive and Sophisticated have a discount function $$1/(1+kd)$$, where $$d$$ is how much the reward is delayed. When simulating his future self at $$t+c$$, the Naive agent assumes he\\'ll discount immediate gains at rate $$1/(1+kc)$$ and the Sophisticated agent (correctly) assumes a rate of $$1/(1+0)$$. \\n\\nAdding delays to our model is straightforward. In defining the MDP agent, we <a href=\"/chapters/3a-mdp.html#recursion\">presented</a> Bellman-style recursions for the expected utility of state-action pairs. Discounting agents evaluate states and actions differently depending on their *delay* from the present. So we now define expected utilities of state-action-delay triples:\\n\\n$$\\nEU[s,a,d] = \\\\delta(d)U(s, a) + \\\\mathbb{E}_{s\\', a\\'}(EU[s\\', a\\',d+1])\\n$$\\n\\nwhere:\\n\\n- $$\\\\delta  \\\\colon \\\\mathbb{N} \\\\to \\\\mathbb{R}$$ is the discount function from the delay to the discount factor. In our examples we have (where $$k>0$$ is the discount constant):\\n\\n$$\\n\\\\delta(d) = \\\\frac{1}{1+kd}\\n$$\\n\\n- $$s\\' \\\\sim T(s,a)$$ exactly as in the non-discounting case.\\n\\n- $$a\\' \\\\sim C(s\\'; d_P)$$ where $$d_P=0$$ for Sophisticated and $$d_P=d+1$$ for Naive.\\n\\n\\nThe function $$C \\\\colon S \\\\times \\\\mathbb{N} \\\\to A$$ is again the *act* function. For $$C(s\\'; d+1)$$ we take a softmax over the expected value of each action $$a$$, namely, $$EU[s\\',a,d+1]$$. The act function now takes a delay argument. We interpret $$C(s\\';d+1)$$ as \"the softmax action the agent would take in state $$s\\'$$ given that their rewards occur with a delay $$d+1$$\".\\n\\nThe Naive agent simulates his future actions by computing $$C(s\\';d+1)$$; the Sophisticated agent computes the action that will *actually* occur, which is $$C(s\\';0)$$. So if we want to simulate an environment including a hyperbolic discounter, we can compute the agent\\'s action with $$C(s;0)$$ for every state $$s$$. \\n\\n\\n### Implementing the hyperbolic discounter\\n \\nAs with the MDP and POMDP agents, our WebPPL implementation directly translates the mathematical formulation of Naive and Sophisticated hyperbolic discounting. The variable names correspond as follows:\\n\\n- The function $$\\\\delta$$ is named `discountFunction`\\n\\n- The \"perceived delay\", which controls how the agent\\'s simulated future self evaluates rewards, is $$d$$ in the math and `perceivedDelay` below. \\n\\n- $$s\\'$$, $$a\\'$$, $$d+1$$ correspond to `nextState`, `nextAction` and `delay+1` respectively. \\n\\nThis codebox simplifies the code for the hyperbolic discounter by omitting definitions of `transition`, `utility` and so on:\\n\\n~~~~\\nvar makeAgent = function(params, world) {\\n\\n  var act = dp.cache( \\n    function(state, delay){\\n      return Infer({ model() {\\n        var action = uniformDraw(stateToActions(state));\\n        var eu = expectedUtility(state, action, delay);    \\n        factor(params.alpha * eu);\\n        return action;\\n      }});      \\n    });\\n  \\n  var expectedUtility = dp.cache(\\n    function(state, action, delay){\\n      var u = discountFunction(delay) * utility(state, action);\\n      if (state.terminateAfterAction){\\n        return u; \\n      } else {                     \\n        return u + expectation(Infer({ model() {\\n          var nextState = transition(state, action); \\n          var perceivedDelay = isNaive ? delay + 1 : 0;\\n          var nextAction = sample(act(nextState, perceivedDelay));\\n          return expectedUtility(nextState, nextAction, delay+1);  \\n        }}));\\n      }                      \\n    });\\n  \\n  return { params, expectedUtility, act };\\n};\\n~~~~\\n\\nThe next codebox shows how the Naive agent can end up at Donut North in the Restaurant Choice problem, despite this being dominated for any possible utility function. The Naive agent first moves in the direction of Veg, which initially looks better than Donut South. When right outside Donut North, discounting makes it look better than Veg. To visualize this, we display the agent\\'s expected utility calculations at different steps along its trajectory. The crucial values are the `expectedValue` of going left at [3,5] when `delay=0` compared with `delay=4`. The function `plannedTrajectories` uses `expectedValue` to access these values. For each timestep, we plot the agent\\'s position and the expected utility of each action they might perform in the future. \\n\\n<!-- simulate_hyperbolic_agent -->\\n~~~~\\n///fold: makeAgent, mdp, plannedTrajectories\\nvar makeAgent = function(params, world) {\\n  var defaultParams = {\\n    alpha: 500, \\n    discount: 1\\n  };\\n  var params = extend(defaultParams, params);\\n  var stateToActions = world.stateToActions;\\n  var transition = world.transition;\\n  var utility = params.utility;\\n  var paramsDiscountFunction = params.discountFunction;\\n\\n  var discountFunction = (\\n    paramsDiscountFunction ? \\n    paramsDiscountFunction : \\n    function(delay){ return 1/(1 + params.discount*delay); });\\n\\n  var isNaive = params.sophisticatedOrNaive === \\'naive\\';\\n\\n  var act = dp.cache( \\n    function(state, delay) {\\n      var delay = delay || 0; // make sure delay is never undefined\\n      return Infer({ model() {\\n        var action = uniformDraw(stateToActions(state));\\n        var eu = expectedUtility(state, action, delay);\\n        factor(params.alpha * eu);\\n        return action;\\n      }});\\n    });\\n\\n  var expectedUtility = dp.cache(\\n    function(state, action, delay) {\\n      var u = discountFunction(delay) * utility(state, action);\\n      if (state.terminateAfterAction){\\n        return u; \\n      } else {\\n        return u + expectation(Infer({ model() {\\n          var nextState = transition(state, action); \\n          var perceivedDelay = isNaive ? delay + 1 : 0;\\n          var nextAction = sample(act(nextState, perceivedDelay));\\n          return expectedUtility(nextState, nextAction, delay+1);\\n        }}));\\n      }\\n    });\\n\\n  return { params, expectedUtility, act };\\n};\\n\\nvar ___ = \\' \\'; \\nvar DN = { name : \\'Donut N\\' };\\nvar DS = { name : \\'Donut S\\' };\\nvar V = { name : \\'Veg\\' };\\nvar N = { name : \\'Noodle\\' };\\n\\nvar grid = [\\n  [\\'#\\', \\'#\\', \\'#\\', \\'#\\',  V , \\'#\\'],\\n  [\\'#\\', \\'#\\', \\'#\\', ___, ___, ___],\\n  [\\'#\\', \\'#\\', DN , ___, \\'#\\', ___],\\n  [\\'#\\', \\'#\\', \\'#\\', ___, \\'#\\', ___],\\n  [\\'#\\', \\'#\\', \\'#\\', ___, ___, ___],\\n  [\\'#\\', \\'#\\', \\'#\\', ___, \\'#\\',  N ],\\n  [___, ___, ___, ___, \\'#\\', \\'#\\'],\\n  [DS , \\'#\\', \\'#\\', ___, \\'#\\', \\'#\\']\\n];\\n\\nvar mdp = makeGridWorldMDP({\\n  grid,\\n  noReverse: true,\\n  maxTimeAtRestaurant: 2,\\n  start: [3, 1],\\n  totalTime: 11\\n});\\n\\nvar MAPActionPath = function(state, world, agent, actualTotalTime, statesOrActions) { \\n  var perceivedTotalTime = state.timeLeft;\\n  assert.ok(perceivedTotalTime  > 1 || state.terminateAfterAction==false,\\n            \\'perceivedTime<=1. If=1 then should have state.terminateAfterAction,\\' +\\n            \\' but then simulate wont work \\' + JSON.stringify(state));\\n\\n  var agentAction = agent.act;\\n  var expectedUtility = agent.expectedUtility;\\n  var transition = world.transition;\\n\\n  var sampleSequence = function (state, actualTimeLeft) {\\n    var action = agentAction(state, actualTotalTime-actualTimeLeft).MAP().val;\\n    var nextState = transition(state, action); \\n    var out = {states:state, actions:action, both:[state,action]}[statesOrActions];\\n    if (actualTimeLeft==0 || state.terminateAfterAction){\\n      return [out];\\n    } else {\\n      return [ out ].concat( sampleSequence(nextState, actualTimeLeft-1));\\n    }\\n  };\\n  return sampleSequence(state, actualTotalTime);\\n};\\n\\nvar plannedTrajectory = function(world, agent) {\\n  var getExpectedUtilities = function(trajectory, agent, actions) { \\n    var expectedUtility = agent.expectedUtility;\\n    var v = mapIndexed(function(i, state) {\\n      return [state, map(function (a) { return  expectedUtility(state, a, i); }, actions)];\\n    }, trajectory );\\n    return v;\\n  };\\n  return function(state) {\\n    var currentPlan = MAPActionPath(state, world, agent, state.timeLeft, \\'states\\');\\n    return getExpectedUtilities(currentPlan, agent, world.actions);\\n  };\\n} \\n\\nvar plannedTrajectories = function(trajectory, world, agent) { \\n  var getTrajectory = plannedTrajectory(world, agent);\\n  return map(getTrajectory, trajectory);\\n}\\n///\\n\\nvar world = mdp.world;\\nvar start = mdp.startState;\\n\\nvar utilityTable = {\\n  \\'Donut N\\': [10, -10],  // [immediate reward, delayed reward]\\n  \\'Donut S\\': [10, -10],\\n  \\'Veg\\': [-10, 20],\\n  \\'Noodle\\': [0, 0],\\n  \\'timeCost\\': -.01  // cost of taking a single action \\n};\\n\\nvar restaurantUtility = function(state, action) {\\n  var feature = world.feature;\\n  var name = feature(state).name;\\n  if (name) {\\n    return utilityTable[name][state.timeAtRestaurant]\\n  } else {\\n    return utilityTable.timeCost;\\n  }\\n};\\n\\nvar runAndGraph = function(agent) { \\n  var trajectory = simulateMDP(mdp.startState, world, agent);\\n  var plans = plannedTrajectories(trajectory, world, agent);\\n  viz.gridworld(world, {\\n    trajectory, \\n    dynamicActionExpectedUtilities: plans\\n  });\\n};\\n\\nvar agent = makeAgent({\\n  sophisticatedOrNaive: \\'naive\\', \\n  utility: restaurantUtility\\n}, world);\\n\\nprint(\\'Naive agent: \\\\n\\\\n\\');\\nrunAndGraph(agent);\\n~~~~\\n\\nWe run the Sophisticated agent with the same parameters and visualization. \\n\\n<!-- simulate_hyperbolic_agent_sophisticated -->\\n~~~~\\n///fold: \\nvar makeAgent = function(params, world) {\\n  var defaultParams = {\\n    alpha: 500, \\n    discount: 1\\n  };\\n  var params = extend(defaultParams, params);\\n  var stateToActions = world.stateToActions;\\n  var transition = world.transition;\\n  var utility = params.utility;\\n  var paramsDiscountFunction = params.discountFunction;\\n\\n  var discountFunction = (\\n    paramsDiscountFunction ? \\n    paramsDiscountFunction : \\n    function(delay){ return 1/(1+params.discount*delay); });\\n\\n  var isNaive = params.sophisticatedOrNaive === \\'naive\\';\\n\\n  var act = dp.cache( \\n    function(state, delay) {\\n      var delay = delay || 0; // make sure delay is never undefined\\n      return Infer({ model() {\\n        var action = uniformDraw(stateToActions(state));\\n        var eu = expectedUtility(state, action, delay);\\n        factor(params.alpha * eu);\\n        return action;\\n      }});\\n    });\\n\\n  var expectedUtility = dp.cache(\\n    function(state, action, delay) {\\n      var u = discountFunction(delay) * utility(state, action);\\n      if (state.terminateAfterAction){\\n        return u; \\n      } else {\\n        return u + expectation(Infer({ model() {\\n          var nextState = transition(state, action); \\n          var perceivedDelay = isNaive ? delay + 1 : 0;\\n          var nextAction = sample(act(nextState, perceivedDelay));\\n          return expectedUtility(nextState, nextAction, delay+1);\\n        }}));\\n      }\\n    });\\n\\n  return { params, expectedUtility, act };\\n};\\n\\nvar ___ = \\' \\'; \\nvar DN = { name : \\'Donut N\\' };\\nvar DS = { name : \\'Donut S\\' };\\nvar V = { name : \\'Veg\\' };\\nvar N = { name : \\'Noodle\\' };\\n\\nvar grid = [\\n  [\\'#\\', \\'#\\', \\'#\\', \\'#\\',  V , \\'#\\'],\\n  [\\'#\\', \\'#\\', \\'#\\', ___, ___, ___],\\n  [\\'#\\', \\'#\\', DN , ___, \\'#\\', ___],\\n  [\\'#\\', \\'#\\', \\'#\\', ___, \\'#\\', ___],\\n  [\\'#\\', \\'#\\', \\'#\\', ___, ___, ___],\\n  [\\'#\\', \\'#\\', \\'#\\', ___, \\'#\\',  N ],\\n  [___, ___, ___, ___, \\'#\\', \\'#\\'],\\n  [DS , \\'#\\', \\'#\\', ___, \\'#\\', \\'#\\']\\n];\\n\\nvar mdp = makeGridWorldMDP({\\n  grid,\\n  noReverse: true,\\n  maxTimeAtRestaurant: 2,\\n  start: [3, 1],\\n  totalTime: 11\\n});\\n\\nvar world = mdp.world;\\n\\nvar utilityTable = {\\n  \\'Donut N\\': [10, -10],  // [immediate reward, delayed reward]\\n  \\'Donut S\\': [10, -10],\\n  \\'Veg\\': [-10, 20],\\n  \\'Noodle\\': [0, 0],\\n  \\'timeCost\\': -.01  // cost of taking a single action \\n};\\n\\nvar restaurantUtility = function(state, action) {\\n  var feature = world.feature;\\n  var name = feature(state).name;\\n  if (name) {\\n    return utilityTable[name][state.timeAtRestaurant]\\n  } else {\\n    return utilityTable.timeCost;\\n  }\\n};\\n\\nvar MAPActionPath = function(state, world, agent, actualTotalTime, statesOrActions) { \\n  var perceivedTotalTime = state.timeLeft;\\n  assert.ok(perceivedTotalTime  > 1 || state.terminateAfterAction==false,\\n            \\'perceivedTime<=1. If=1 then should have state.terminateAfterAction,\\' +\\n            \\' but then simulate wont work \\' + JSON.stringify(state));\\n\\n  var agentAction = agent.act;\\n  var expectedUtility = agent.expectedUtility;\\n  var transition = world.transition;\\n\\n  var sampleSequence = function (state, actualTimeLeft) {\\n    var action = agentAction(state, actualTotalTime-actualTimeLeft).MAP().val;\\n    var nextState = transition(state, action); \\n    var out = {states:state, actions:action, both:[state,action]}[statesOrActions];\\n    if (actualTimeLeft==0 || state.terminateAfterAction){\\n      return [out];\\n    } else {\\n      return [ out ].concat( sampleSequence(nextState, actualTimeLeft-1));\\n    }\\n  };\\n  return sampleSequence(state, actualTotalTime);\\n};\\n\\nvar plannedTrajectory = function(world, agent) {\\n  var getExpectedUtilities = function(trajectory, agent, actions) { \\n    var expectedUtility = agent.expectedUtility;\\n    var v = mapIndexed(function(i, state) {\\n      return [state, map(function (a) { return  expectedUtility(state, a, i); }, actions)];\\n    }, trajectory );\\n    return v;\\n  };\\n  return function(state) {\\n    var currentPlan = MAPActionPath(state, world, agent, state.timeLeft, \\'states\\');\\n    return getExpectedUtilities(currentPlan, agent, world.actions);\\n  };\\n};\\n\\nvar plannedTrajectories = function(trajectory, world, agent) { \\n  var getTrajectory = plannedTrajectory(world, agent);\\n  return map(getTrajectory, trajectory);\\n};\\n\\nvar runAndGraph = function(agent) { \\n  var trajectory = simulateMDP(mdp.startState, world, agent);\\n  var plans = plannedTrajectories(trajectory, world, agent);\\n  viz.gridworld(world, {\\n    trajectory, \\n    dynamicActionExpectedUtilities: plans\\n  });\\n};\\n///\\n\\nvar agent = makeAgent({\\n  sophisticatedOrNaive: \\'sophisticated\\', \\n  utility: restaurantUtility\\n}, world);\\n\\nprint(\\'Sophisticated agent: \\\\n\\\\n\\');\\nrunAndGraph(agent);\\n~~~~\\n\\n>**Exercise**: What would an exponential discounter with identical preferences to the agents above do on the Restaurant Choice problem? Implement an exponential discounter in the codebox above by adding a `discountFunction` property to the `params` argument to `makeAgent`. \\n<br>\\n\\n--------\\n\\n<a id=\\'procrastination\\'></a>\\n\\n### Example: Procrastinating on a task\\n\\nCompared to the Restaurant Choice problem, procrastination leads to (systematically biased) behavior that is especially hard to explain on the softmax noise mode.\\n\\n> **The Procrastination Problem**\\n> <br>You have a hard deadline of ten days to complete a task (e.g. write a paper for class, apply for a school or job). Completing the task takes a full day and has a *cost* (it\\'s unpleasant work). After the task is complete you get a *reward* (typically exceeding the cost). There is an incentive to finish early: every day you delay finishing, your reward gets slightly smaller. (Imagine that it\\'s good for your reputation to complete tasks early or that early applicants are considered first).\\n\\nNote that if the task is worth doing at the last minute, then you should do it immediately (because the reward diminishes over time). Yet people often do this kind of task at the last minute -- the worst possible time to do it!\\n\\nHyperbolic discounting provides an elegant model of this behavior. On Day 1, a hyperbolic discounter will prefer that they complete the task tomorrow rather than today. Moreover, a Naive agent wrongly predicts they will complete the task tomorrow and so puts off the task till Day 2. When Day 2 arrives, the Naive agent reasons in the same way -- telling themself that they can avoid the work today by putting it off till tomorrow. This continues until the last possible day, when the Naive agent finally completes the task.\\n\\nIn this problem, the behavior of optimal and time-inconsistent agents with identical preferences (i.e. utility functions) diverges. If the deadline is $$T$$ days from the start, the optimal agent will do the task immediately and the Naive agent will do the task on Day $$T$$. Any problem where a time-inconsistent agent receives exponentially lower reward than an optimal agent contains a close variant of our Procrastination Problem refp:kleinberg2014time [^kleinberg]. \\n\\n[^kleinberg]: Kleinberg and Oren\\'s paper considers a variant problem where the each cost/penalty for waiting is received immediately (rather than being delayed until the time the task is done). In this variant, the agent must eventually complete the task. The authors consider \"semi-myopic\" time-inconsistent agents, i.e. agents who do not discount their next reward, but discount all future rewards by $$\\\\beta < 1$$. They show that in any problem where a semi-myopic agent receives exponentially lower reward than an optimal agent, the problem must contain a copy of their variant of the Procrastination Problem.\\n\\nWe formalize the Procrastination Problem in terms of a deterministic graph. Suppose the **deadline** is $$T$$ steps from the start. Assume that after $$t$$ < $$T$$ steps the agent has not yet completed the task. Then the agent can take the action `\"work\"` (which has **work cost** $$-w$$) or the action `\"wait\"` with zero cost. After the `\"work\"` action the agent transitions to the `\"reward\"` state and receives $$+(R - t \\\\epsilon)$$, where $$R$$ is the **reward** for the task and $$\\\\epsilon$$ is how much the reward diminishes for every day of waiting (the **wait cost**). See Figure 3 below.  \\n\\n<img src=\"/assets/img/procrastination_mdp.png\" alt=\"diagram\" style=\"width: 650px;\"/>\\n\\n>**Figure 3:** Transition graph for Procrastination Problem. States are represented by nodes. Edges are state-transitions and are labeled with the action name and the utility of the state-action pair. Terminal nodes have a bold border and their utility is labeled below.\\n\\nWe simulate the behavior of hyperbolic discounters on the Procrastination Problem. We vary the discount rate $$k$$ while holding the other parameters fixed. The agent\\'s behavior can be summarized by its final state (`\"wait_state\"` or `\"reward_state`) and by how much time elapses before termination. When $$k$$ is sufficiently high, the agent will not even complete the task on the last day. \\n\\n<!-- procrastinate -->\\n~~~~\\n///fold: makeProcrastinationMDP, makeProcrastinationUtility\\nvar makeProcrastinationMDP = function(deadlineTime) {\\n  var stateLocs = [\"wait_state\", \"reward_state\"];\\n  var actions = [\"wait\", \"work\", \"relax\"];\\n\\n  var stateToActions = function(state) {\\n    return (state.loc === \"wait_state\" ? \\n            [\"wait\", \"work\"] :\\n            [\"relax\"]);\\n  };\\n\\n  var advanceTime = function (state) {\\n    var newTimeLeft = state.timeLeft - 1;\\n    var terminateAfterAction = (newTimeLeft === 1 || \\n                                state.loc === \"reward_state\");\\n    return extend(state, {\\n      timeLeft: newTimeLeft,\\n      terminateAfterAction: terminateAfterAction\\n    });\\n  };\\n\\n  var transition = function(state, action) {\\n    assert.ok(_.includes(stateLocs, state.loc) && _.includes(actions, action), \\n              \\'procrastinate transition:\\' + [state.loc,action]);\\n    \\n    if (state.loc === \"reward_state\") {\\n      return advanceTime(state);\\n    } else if (action === \"wait\") {\\n      var waitSteps = state.waitSteps + 1;\\n      return extend(advanceTime(state), { waitSteps });\\n    } else {\\n      var newState = extend(state, { loc: \"reward_state\" });\\n      return advanceTime(newState);\\n    }\\n  };\\n\\n  var feature = function(state) {\\n    return state.loc;\\n  };\\n\\n  var startState = {\\n    loc: \"wait_state\",\\n    waitSteps: 0,\\n    timeLeft: deadlineTime,\\n    terminateAfterAction: false\\n  };\\n\\n  return {\\n    actions,\\n    stateToActions,\\n    transition,\\n    feature,\\n    startState\\n  };\\n};\\n\\n\\nvar makeProcrastinationUtility = function(utilityTable) {\\n  assert.ok(hasProperties(utilityTable, [\\'waitCost\\', \\'workCost\\', \\'reward\\']),\\n            \\'makeProcrastinationUtility args\\');\\n  var waitCost = utilityTable.waitCost;\\n  var workCost = utilityTable.workCost;\\n  var reward = utilityTable.reward;\\n\\n  // NB: you receive the *workCost* when you leave the *wait_state*\\n  // You then receive the reward when leaving the *reward_state* state\\n  return function(state, action) {\\n    if (state.loc === \"reward_state\") {\\n      return reward + state.waitSteps * waitCost;\\n    } else if (action === \"work\") {\\n      return workCost;\\n    } else {\\n      return 0;\\n    }\\n  };\\n};\\n///\\n\\n// Construct Procrastinate world \\nvar deadline = 10;\\nvar world = makeProcrastinationMDP(deadline);\\n\\n// Agent params\\nvar utilityTable = {\\n  reward: 4.5,\\n  waitCost: -0.1,\\n  workCost: -1\\n};\\n\\nvar params = {\\n  utility: makeProcrastinationUtility(utilityTable),\\n  alpha: 1000,\\n  discount: null,\\n  sophisticatedOrNaive: \\'sophisticated\\'\\n};\\n\\nvar getLastState = function(discount){\\n  var agent = makeMDPAgent(extend(params, { discount: discount }), world);\\n  var states = simulateMDP(world.startState, world, agent, \\'states\\');\\n  return [last(states).loc, states.length];\\n};\\n\\nmap(function(discount) {\\n  var lastState = getLastState(discount);\\n  print(\\'Discount: \\' + discount + \\'. Last state: \\' + lastState[0] +\\n        \\'. Time: \\' + lastState[1] + \\'\\\\n\\')\\n}, _.range(8));\\n~~~~\\n\\n\\n>**Exercise:**\\n\\n> 1. Explain how an exponential discounter would behave on this task. Assume their utilities are the same as above and consider different discount rates.\\n> 2. Run the codebox above with a Sophisticated agent. Explain the results. \\n\\n\\nNext chapter: [Myopia for rewards and belief updates](/chapters/5c-myopic.html)\\n\\n<br>\\n\\n### Footnotes\\n', 'date_published': '2019-08-28T09:06:53Z', 'authors': ['Owain Evans', 'Andreas Stuhlmüller', 'John Salvatier', 'Daniel Filan'], 'summaries': [], 'filename': '5b-time-inconsistency.md'}, {'id': 'a894af445e54ec10a745213ccd2d14e3', 'title': 'Modeling Agents with Probabilistic Programs', 'url': 'https://agentmodels.org/chapters/3b-mdp-gridworld.html', 'source': 'agentmodels', 'source_type': 'markdown', 'text': '---\\nlayout: chapter\\ntitle: \"MDPs and Gridworld in WebPPL\"\\ndescription: Noisy actions (softmax), stochastic transitions, policies, Q-values.\\n---\\n\\nThis chapter explores some key features of MDPs: stochastic dynamics, stochastic policies, and value functions.\\n\\n### Hiking in Gridworld\\n\\nWe begin by introducing a new gridworld MDP:\\n\\n> **Hiking Problem**:\\n>Suppose that Alice is hiking. There are two peaks nearby, denoted \"West\" and \"East\". The peaks provide different views and Alice must choose between them. South of Alice\\'s starting position is a steep hill. Falling down the hill would result in painful (but non-fatal) injury and end the hike early.\\n\\nWe represent Alice\\'s hiking problem with a Gridworld similar to Bob\\'s Restaurant Choice example. The peaks are terminal states, providing different utilities. The steep hill is represented by a row of terminal state, each with identical negative utility. Each timestep before Alice reaches a terminal state incurs a \"time cost\", which is negative to represent the fact that Alice prefers a shorter hike. <!-- TODO might be good to indicate on plot that the steep hills are bad -->\\n\\n<!-- draw_hike -->\\n~~~~\\nvar H = { name: \\'Hill\\' };\\nvar W = { name: \\'West\\' };\\nvar E = { name: \\'East\\' };\\nvar ___ = \\' \\';\\n\\nvar grid = [\\n  [___, ___, ___, ___, ___],\\n  [___, \\'#\\', ___, ___, ___],\\n  [___, \\'#\\',  W , \\'#\\',  E ],\\n  [___, ___, ___, ___, ___],\\n  [ H ,  H ,  H ,  H ,  H ]\\n];\\n\\nvar start = [0, 1];\\n\\nvar mdp = makeGridWorldMDP({ grid, start });\\n\\nviz.gridworld(mdp.world, { trajectory: [mdp.startState] });\\n~~~~\\n\\nWe start with a *deterministic* transition function. In this case, Alice\\'s risk of falling down the steep hill is solely due to softmax noise in her action choice (which is minimal in this case). The agent model is the same as the one at the end of [Chapter III.1](/chapters/3a-mdp.html). We place the functions `act`, `expectedUtility` in a function `makeMDPAgent`. The following codebox defines this function and we use it later on without defining it (since it\\'s in the `webppl-agents` library).\\n\\n<!-- define_agent_simulate -->\\n~~~~\\n// Set up agent structure\\n\\nvar makeMDPAgent = function(params, world) {\\n  var stateToActions = world.stateToActions;\\n  var transition = world.transition;\\n  var utility = params.utility;\\n  var alpha = params.alpha;\\n\\n  var act = dp.cache(\\n    function(state) {\\n      return Infer({ model() {\\n        var action = uniformDraw(stateToActions(state));\\n        var eu = expectedUtility(state, action);\\n        factor(alpha * eu);\\n        return action;\\n      }});\\n    });\\n\\n  var expectedUtility = dp.cache(\\n    function(state, action){\\n      var u = utility(state, action);\\n      if (state.terminateAfterAction){\\n        return u;\\n      } else {\\n        return u + expectation(Infer({ model() {\\n          var nextState = transition(state, action);\\n          var nextAction = sample(act(nextState));\\n          return expectedUtility(nextState, nextAction);\\n        }}));\\n      }\\n    });\\n\\n  return { params, expectedUtility, act };\\n};\\n\\nvar simulate = function(startState, world, agent) {\\n  var act = agent.act;\\n  var transition = world.transition;\\n  var sampleSequence = function(state) {\\n    var action = sample(act(state));\\n    var nextState = transition(state, action);\\n    if (state.terminateAfterAction) {\\n      return [state];\\n    } else {\\n      return [state].concat(sampleSequence(nextState));\\n    }\\n  };\\n  return sampleSequence(startState);\\n};\\n\\n\\n// Set up world\\n\\nvar makeHikeMDP = function(options) {\\n  var H = { name: \\'Hill\\' };\\n  var W = { name: \\'West\\' };\\n  var E = { name: \\'East\\' };\\n  var ___ = \\' \\';\\n  var grid = [\\n    [___, ___, ___, ___, ___],\\n    [___, \\'#\\', ___, ___, ___],\\n    [___, \\'#\\',  W , \\'#\\',  E ],\\n    [___, ___, ___, ___, ___],\\n    [ H ,  H ,  H ,  H ,  H ]\\n  ];\\n  return makeGridWorldMDP(_.assign({ grid }, options));\\n};\\n\\nvar mdp = makeHikeMDP({\\n  start: [0, 1],\\n  totalTime: 12,\\n  transitionNoiseProbability: 0\\n});\\n\\nvar makeUtilityFunction = mdp.makeUtilityFunction;\\n\\n\\n// Create parameterized agent\\n\\nvar utility = makeUtilityFunction({\\n  East: 10,\\n  West: 1,\\n  Hill: -10,\\n  timeCost: -.1\\n});\\nvar agent = makeMDPAgent({ utility, alpha: 1000 }, mdp.world);\\n\\n\\n// Run agent on world\\n\\nvar trajectory = simulate(mdp.startState, mdp.world, agent);\\n\\n\\nviz.gridworld(mdp.world, { trajectory });\\n~~~~\\n\\n>**Exercise**: Adjust the parameters of `utilityTable` in order to produce the following behaviors:\\n\\n>1. The agent goes directly to \"West\".\\n>2. The agent takes the long way around to \"West\".\\n>3. The agent sometimes goes to the Hill at $$[1,0]$$. Try to make this outcome as likely as possible.\\n<!-- 3 is obtained by making timeCost positive and Hill better than alternatives -->\\n\\n\\n### Hiking with stochastic transitions\\n\\nImagine that the weather is very wet and windy. As a result, Alice will sometimes intend to go one way but actually go another way (because she slips in the mud). In this case, the shorter route to the peaks might be too risky for Alice.\\n\\nTo model bad weather, we assume that at every timestep, there is a constant independent probability `transitionNoiseProbability` of the agent moving orthogonally to their intended direction. The independence assumption is unrealistic (if a location is slippery at one timestep it is more likely slippery the next), but it is simple and satisfies the Markov assumption for MDPs.\\n\\nSetting `transitionNoiseProbability=0.1`, the agent\\'s first action is now to move \"up\" instead of \"right\".\\n\\n~~~~\\n///fold: makeHikeMDP\\nvar makeHikeMDP = function(options) {\\n  var H = { name: \\'Hill\\' };\\n  var W = { name: \\'West\\' };\\n  var E = { name: \\'East\\' };\\n  var ___ = \\' \\';\\n  var grid = [\\n    [___, ___, ___, ___, ___],\\n    [___, \\'#\\', ___, ___, ___],\\n    [___, \\'#\\',  W , \\'#\\',  E ],\\n    [___, ___, ___, ___, ___],\\n    [ H ,  H ,  H ,  H ,  H ]\\n  ];\\n  return makeGridWorldMDP(_.assign({ grid }, options));\\n};\\n///\\n\\n// Set up world\\n\\nvar mdp = makeHikeMDP({\\n  start: [0, 1],\\n  totalTime: 13,\\n  transitionNoiseProbability: 0.1  // <- NEW\\n});\\n\\n\\n// Create parameterized agent\\n\\nvar makeUtilityFunction = mdp.makeUtilityFunction;\\nvar utility = makeUtilityFunction({\\n  East: 10,\\n  West: 1,\\n  Hill: -10,\\n  timeCost: -.1\\n});\\nvar agent = makeMDPAgent({ utility, alpha: 100 }, mdp.world);\\n\\n\\n// Generate a single trajectory, draw\\n\\nvar trajectory = simulateMDP(mdp.startState, mdp.world, agent, \\'states\\');\\nviz.gridworld(mdp.world, { trajectory });\\n\\n\\n// Generate 100 trajectories, plot distribution on lengths\\n\\nvar trajectoryDist = Infer({\\n  model() {\\n    var trajectory = simulateMDP(mdp.startState, mdp.world, agent);\\n    return { trajectoryLength: trajectory.length }\\n  },\\n  method: \\'forward\\',\\n  samples: 100\\n});\\n\\nviz(trajectoryDist);\\n~~~~\\n\\n>**Exercise:**\\n\\n>1. Keeping `transitionNoiseProbability=0.1`, find settings for `utilityTable` such that the agent goes \"right\" instead of \"up\".\\n>2. Set `transitionNoiseProbability=0.01`. Change a single parameter in `utilityTable` such that the agent goes \"right\" (there are multiple ways to do this).\\n<!-- put up timeCost to -1 or so -->\\n\\n### Noisy transitions vs. Noisy agents\\n\\nIt\\'s important to distinguish noise in the transition function from the softmax noise in the agent\\'s selection of actions. Noise (or \"stochasticity\") in the transition function is a representation of randomness in the world. This is easiest to think about in games of chance[^noise]. In a game of chance (e.g. slot machines or poker) rational agents will take into account the randomness in the game. By contrast, softmax noise is a property of an agent. For example, we can vary the behavior of otherwise identical agents by varying their parameter $$\\\\alpha$$.\\n\\nUnlike transition noise, softmax noise has little influence on the agent\\'s planning for the Hiking Problem. Since it\\'s so bad to fall down the hill, the softmax agent will rarely do so even if they take the short route. The softmax agent is like a person who takes inefficient routes when stakes are low but \"pulls themself together\" when stakes are high.\\n\\n[^noise]: An agent\\'s world model might treat a complex set of deterministic rules as random. In this sense, agents will vary in whether they represent an MDP as stochastic or not. We won\\'t consider that case in this tutorial.\\n\\n>**Exercise:** Use the codebox below to explore different levels of softmax noise. Find a setting of `utilityTable` and `alpha` such that the agent goes to West and East equally often and nearly always takes the most direct route to both East and West. Included below is code for simulating many trajectories and returning the trajectory length. You can extend this code to measure whether the route taken by the agent is direct or not. (Note that while the softmax agent here is able to \"backtrack\" or return to its previous location, in later Gridworld examples we disalllow backtracking as a possible action).\\n\\n~~~~\\n///fold: makeHikeMDP, set up world\\nvar makeHikeMDP = function(options) {\\n  var H = { name: \\'Hill\\' };\\n  var W = { name: \\'West\\' };\\n  var E = { name: \\'East\\' };\\n  var ___ = \\' \\';\\n  var grid = [\\n    [___, ___, ___, ___, ___],\\n    [___, \\'#\\', ___, ___, ___],\\n    [___, \\'#\\',  W , \\'#\\',  E ],\\n    [___, ___, ___, ___, ___],\\n    [ H ,  H ,  H ,  H ,  H ]\\n  ];\\n  return makeGridWorldMDP(_.assign({ grid }, options));\\n};\\n\\nvar mdp = makeHikeMDP({\\n  start: [0, 1],\\n  totalTime: 13,\\n  transitionNoiseProbability: 0.1\\n});\\n\\nvar world = mdp.world;\\nvar startState = mdp.startState;\\nvar makeUtilityFunction = mdp.makeUtilityFunction;\\n///\\n\\n// Create parameterized agent\\nvar utility = makeUtilityFunction({\\n  East: 10,\\n  West: 1,\\n  Hill: -10,\\n  timeCost: -.1\\n});\\nvar alpha = 1;  // <- SOFTMAX NOISE\\nvar agent = makeMDPAgent({ utility, alpha }, world);\\n\\n// Generate a single trajectory, draw\\nvar trajectory = simulateMDP(startState, world, agent, \\'states\\');\\nviz.gridworld(world, { trajectory });\\n\\n// Generate 100 trajectories, plot distribution on lengths\\nvar trajectoryDist = Infer({\\n  model() {\\n    var trajectory = simulateMDP(startState, world, agent);\\n    return { trajectoryLength: trajectory.length }\\n  },\\n  method: \\'forward\\',\\n  samples: 100\\n});\\nviz(trajectoryDist);\\n~~~~\\n\\n\\n### Stochastic transitions: plans and policies\\n\\nWe return to the case of a stochastic environment with very low softmax action noise. In a stochastic environment, the agent sometimes finds themself in a state they did not intend to reach. The functions `agent` and `expectedUtility` (inside `makeMDPAgent`) implicitly compute the expected utility of actions for every possible future state, including states that the agent will try to avoid. In the MDP literature, this function from states and remaining time to actions is called a *policy*. (For infinite-horizon MDPs, policies are functions from states to actions.) Since policies take into account every possible contingency, they are quite different from the everyday notion of a plan.\\n\\nConsider the example from above where the agent takes the long route because of the risk of falling down the hill. If we generate a single trajectory for the agent, they will likely take the long route. However, if we generated many trajectories, we would sometimes see the agent move \"right\" instead of \"up\" on their first move. Before taking this first action, the agent implicitly computes what they *would* do if they end up moving right. To find out what they would do, we can artificially start the agent in $$[1,1]$$ instead of $$[0,1]$$:\\n\\n<!-- policy -->\\n~~~~\\n///fold: makeHikeMDP\\nvar makeHikeMDP = function(options) {\\n  var H = { name: \\'Hill\\' };\\n  var W = { name: \\'West\\' };\\n  var E = { name: \\'East\\' };\\n  var ___ = \\' \\';\\n  var grid = [\\n    [___, ___, ___, ___, ___],\\n    [___, \\'#\\', ___, ___, ___],\\n    [___, \\'#\\',  W , \\'#\\',  E ],\\n    [___, ___, ___, ___, ___],\\n    [ H ,  H ,  H ,  H ,  H ]\\n  ];\\n  return makeGridWorldMDP(_.assign({ grid }, options));\\n};\\n///\\n\\n// Parameters for world\\nvar mdp = makeHikeMDP({\\n  start: [1, 1],  // Previously: [0, 1]\\n  totalTime: 11,  // Previously: 12\\n  transitionNoiseProbability: 0.1\\n});\\nvar makeUtilityFunction = mdp.makeUtilityFunction;\\n\\n// Parameters for agent\\nvar utility = makeUtilityFunction({ \\n  East: 10, \\n  West: 1,\\n  Hill: -10,\\n  timeCost: -.1\\n});\\nvar agent = makeMDPAgent({ utility, alpha: 1000 }, mdp.world);\\nvar trajectory = simulateMDP(mdp.startState, mdp.world, agent, \\'states\\');\\n\\nviz.gridworld(mdp.world, { trajectory });\\n~~~~\\n\\nExtending this idea, we can display the expected values of each action the agent *could have taken* during their trajectory. These expected values numbers are analogous to state-action Q-values in infinite-horizon MDPs.\\n\\nThe expected values were already being computed implicitly; we now use `getExpectedUtilitiesMDP` to access them. The displayed numbers in each grid cell are the expected utilities of moving in the corresponding directions. For example, we can read off how close the agent was to taking the short route as opposed to the long route. (Note that if the difference in expected utility between two actions is small then a noisy agent will take each of them with nearly equal probability).\\n\\n~~~~\\n///fold: makeBigHikeMDP, getExpectedUtilitiesMDP\\nvar makeBigHikeMDP = function(options) {\\n  var H = { name: \\'Hill\\' };\\n  var W = { name: \\'West\\' };\\n  var E = { name: \\'East\\' };\\n  var ___ = \\' \\';\\n  var grid = [\\n    [___, ___, ___, ___, ___, ___],\\n    [___, ___, ___, ___, ___, ___],\\n    [___, ___, \\'#\\', ___, ___, ___],\\n    [___, ___, \\'#\\',  W , \\'#\\',  E ],\\n    [___, ___, ___, ___, ___, ___],\\n    [ H ,  H ,  H ,  H ,  H ,  H ]\\n  ];\\n  return makeGridWorldMDP(_.assign({ grid }, options));\\n};\\n\\n// trajectory must consist only of states. This can be done by calling\\n// *simulate* with an additional final argument \\'states\\'.\\nvar getExpectedUtilitiesMDP = function(stateTrajectory, world, agent) {\\n  var eu = agent.expectedUtility;\\n  var actions = world.actions;\\n  var getAllExpectedUtilities = function(state) {\\n    var actionUtilities = map(\\n      function(action){ return eu(state, action); },\\n      actions);\\n    return [state, actionUtilities];\\n  };\\n  return map(getAllExpectedUtilities, stateTrajectory);\\n};\\n///\\n\\n// Long route is better, agent takes long route\\n\\nvar mdp = makeBigHikeMDP({\\n  start: [1, 1],\\n  totalTime: 12,\\n  transitionNoiseProbability: 0.03\\n});\\nvar makeUtilityFunction = mdp.makeUtilityFunction;\\n\\nvar utility = makeUtilityFunction({\\n  East: 10,\\n  West: 7,\\n  Hill : -40,\\n  timeCost: -0.4\\n});\\nvar agent = makeMDPAgent({ utility, alpha: 100 }, mdp.world);\\n\\nvar trajectory = simulateMDP(mdp.startState, mdp.world, agent, \\'states\\');\\nvar actionExpectedUtilities = getExpectedUtilitiesMDP(trajectory, mdp.world, agent);\\n\\nviz.gridworld(mdp.world, { trajectory, actionExpectedUtilities });\\n~~~~\\n\\nSo far, our agents all have complete knowledge about the state of the world. In the [next chapter](/chapters/3c-pomdp.html), we will explore partially observable worlds.\\n\\n<br>\\n\\n### Footnotes\\n', 'date_published': '2016-12-13T14:21:09Z', 'authors': ['Owain Evans', 'Andreas Stuhlmüller', 'John Salvatier', 'Daniel Filan'], 'summaries': [], 'filename': '3b-mdp-gridworld.md'}]\n"
     ]
    }
   ],
   "source": [
    "print(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13347695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf97216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a729628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AISafetyIntervention_LiteratureExtraction (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
