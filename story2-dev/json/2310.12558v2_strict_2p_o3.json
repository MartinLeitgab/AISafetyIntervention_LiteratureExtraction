{
  "nodes": [
    {
      "title": "LLM explanations prone to hallucinations",
      "aliases": [
        "LLM explanation errors",
        "Hallucinated model rationales"
      ],
      "type": "concept",
      "description": "Large-language-model generated explanations sometimes contain factual inaccuracies or hallucinations, providing misleading justifications to users.",
      "maturity": null
    },
    {
      "title": "Users over-trust LLM explanations even when wrong",
      "aliases": [
        "User over-reliance",
        "Misplaced trust in explanations"
      ],
      "type": "concept",
      "description": "Human evaluators tend to accept and agree with LLM explanations despite factual errors, displaying high confidence in incorrect model outputs.",
      "maturity": null
    },
    {
      "title": "Over-reliance decreases human fact-checking accuracy below baseline",
      "aliases": [
        "Accuracy drop from over-trust",
        "Performance degradation via reliance"
      ],
      "type": "concept",
      "description": "When users depend on incorrect single-sided explanations, their verification accuracy falls significantly below conditions with no assistance or simple retrieval.",
      "maturity": null
    },
    {
      "title": "Contrastive explanations present both supporting and refuting arguments",
      "aliases": [
        "Two-sided explanations",
        "Pro-con argument display"
      ],
      "type": "concept",
      "description": "Explanations that juxtapose reasons for and against a claim, exposing potential conflicts and reducing blind acceptance of a single narrative.",
      "maturity": null
    },
    {
      "title": "Higher explanation accuracy raises human verification accuracy",
      "aliases": [
        "Accurate rationales aid users",
        "Explanation quality boosts performance"
      ],
      "type": "concept",
      "description": "When the content of model explanations is factually correct, users are more likely to reach the correct judgement in fact-checking tasks.",
      "maturity": null
    },
    {
      "title": "Grounding explanation on retrieved passages boosts explanation accuracy from 59.5 % to 78 %",
      "aliases": [
        "Evidence-grounded explanation improvement",
        "Grounding raises rationale fidelity"
      ],
      "type": "concept",
      "description": "Conditioning the LLM to cite and base its explanation on top-k passages retrieved from Wikipedia markedly increases factual correctness of the generated rationale.",
      "maturity": null
    },
    {
      "title": "Retrieval provides similar accuracy with no over-reliance but 2.5× time cost",
      "aliases": [
        "Evidence list trade-off",
        "Retrieval-only speed cost"
      ],
      "type": "concept",
      "description": "Showing only the top retrieved passages yields accuracy comparable to explanations while avoiding over-trust, yet users require much longer to decide.",
      "maturity": null
    },
    {
      "title": "In high-stakes decisions, reliability may outweigh efficiency",
      "aliases": [
        "Safety over speed",
        "High-stakes reliability preference"
      ],
      "type": "concept",
      "description": "For tasks where decision errors carry large consequences, users or system designers may prefer slower but more trustworthy assistance modes.",
      "maturity": null
    },
    {
      "title": "Low retrieval recall lowers explanation and human accuracy",
      "aliases": [
        "Recall bottleneck effect",
        "Missing evidence harms performance"
      ],
      "type": "concept",
      "description": "If the retrieval component fails to surface all necessary evidence, generated explanations become less accurate and user judgements decline.",
      "maturity": null
    },
    {
      "title": "Users exhibit high confidence even on wrong judgments",
      "aliases": [
        "Over-confidence on errors",
        "Mis-calibrated self-assessment"
      ],
      "type": "concept",
      "description": "Participants reported strong confidence scores even when their factual verdicts were incorrect, indicating poor calibration.",
      "maturity": null
    },
    {
      "title": "Confidence calibration tools are needed",
      "aliases": [
        "Need for trust calibration",
        "Calibration requirement"
      ],
      "type": "concept",
      "description": "Because users remain over-confident on errors, interfaces should include mechanisms to calibrate trust and signal uncertainty.",
      "maturity": null
    },
    {
      "title": "Implement contrastive explanation generation in AI assistants for fact verification",
      "aliases": [
        "Deploy contrastive rationales",
        "Contrastive explanation feature"
      ],
      "type": "intervention",
      "description": "Add system capability to automatically generate and present side-by-side supporting and challenging arguments for each claim.",
      "maturity": 3
    },
    {
      "title": "Ground LLM explanation generation on top-k retrieved passages",
      "aliases": [
        "Evidence-grounded generation",
        "Retrieval-anchored rationales"
      ],
      "type": "intervention",
      "description": "Force the language model to condition on and cite the highest-ranked retrieved evidence snippets when producing its explanation.",
      "maturity": 3
    },
    {
      "title": "Use retrieval-only evidence display in high-stakes fact-checking tasks",
      "aliases": [
        "Retrieval-only interface",
        "Evidence list without explanation"
      ],
      "type": "intervention",
      "description": "Configure the assistant to show just the relevant source passages, omitting LLM explanations, for scenarios where maximum reliability is required.",
      "maturity": 4
    },
    {
      "title": "Monitor retrieval recall and require minimum 100 % evidence coverage before relying on LLM explanation",
      "aliases": [
        "Recall gating mechanism",
        "Evidence coverage check"
      ],
      "type": "intervention",
      "description": "Continuously estimate whether retrieved documents cover all necessary evidence and block or flag explanations when coverage is insufficient.",
      "maturity": 1
    },
    {
      "title": "Present retrieval coverage/confidence indicators to users to calibrate trust",
      "aliases": [
        "Trust indicators",
        "Coverage confidence UI"
      ],
      "type": "intervention",
      "description": "Display explicit metrics or visual cues about retrieval completeness and model certainty to help users adjust their confidence.",
      "maturity": 1
    }
  ],
  "logical_chains": [
    {
      "title": "Contrastive explanations mitigate over-reliance caused by hallucinations",
      "edges": [
        {
          "type": "causes",
          "source_node": "LLM explanations prone to hallucinations",
          "target_node": "Users over-trust LLM explanations even when wrong",
          "description": "Erroneous explanations create conditions for user over-trust.",
          "confidence": 4
        },
        {
          "type": "contributes_to",
          "source_node": "Users over-trust LLM explanations even when wrong",
          "target_node": "Over-reliance decreases human fact-checking accuracy below baseline",
          "description": "Over-trust leads directly to reduced verification accuracy.",
          "confidence": 4
        },
        {
          "type": "mitigates",
          "source_node": "Contrastive explanations present both supporting and refuting arguments",
          "target_node": "Over-reliance decreases human fact-checking accuracy below baseline",
          "description": "Two-sided explanations reduce the negative impact of over-reliance.",
          "confidence": 4
        },
        {
          "type": "addressed_by",
          "source_node": "Contrastive explanations present both supporting and refuting arguments",
          "target_node": "Implement contrastive explanation generation in AI assistants for fact verification",
          "description": "Implementing contrastive generation operationalises the mitigation.",
          "confidence": 4
        }
      ]
    },
    {
      "title": "Grounded explanations improve human accuracy",
      "edges": [
        {
          "type": "contributes_to",
          "source_node": "Grounding explanation on retrieved passages boosts explanation accuracy from 59.5 % to 78 %",
          "target_node": "Higher explanation accuracy raises human verification accuracy",
          "description": "More factual explanations enable users to decide correctly more often.",
          "confidence": 4
        },
        {
          "type": "enabled_by",
          "source_node": "Grounding explanation on retrieved passages boosts explanation accuracy from 59.5 % to 78 %",
          "target_node": "Ground LLM explanation generation on top-k retrieved passages",
          "description": "Grounded generation method is the mechanism producing the accuracy gain.",
          "confidence": 4
        }
      ]
    },
    {
      "title": "Prioritising reliability via retrieval-only display in high-stakes settings",
      "edges": [
        {
          "type": "implies",
          "source_node": "Retrieval provides similar accuracy with no over-reliance but 2.5× time cost",
          "target_node": "In high-stakes decisions, reliability may outweigh efficiency",
          "description": "Observed trade-off suggests reliability preference when stakes are high.",
          "confidence": 2
        },
        {
          "type": "resolved_by",
          "source_node": "In high-stakes decisions, reliability may outweigh efficiency",
          "target_node": "Use retrieval-only evidence display in high-stakes fact-checking tasks",
          "description": "Choosing retrieval-only interface satisfies the reliability requirement.",
          "confidence": 2
        }
      ]
    },
    {
      "title": "Recall monitoring gate to sustain explanation accuracy",
      "edges": [
        {
          "type": "requires",
          "source_node": "Low retrieval recall lowers explanation and human accuracy",
          "target_node": "Monitor retrieval recall and require minimum 100 % evidence coverage before relying on LLM explanation",
          "description": "Observed recall dependency motivates a gating mechanism.",
          "confidence": 2
        },
        {
          "type": "enables",
          "source_node": "Monitor retrieval recall and require minimum 100 % evidence coverage before relying on LLM explanation",
          "target_node": "Higher explanation accuracy raises human verification accuracy",
          "description": "Ensuring full evidence coverage should maintain explanation fidelity and thus user accuracy.",
          "confidence": 2
        }
      ]
    },
    {
      "title": "Trust indicators to calibrate user confidence",
      "edges": [
        {
          "type": "suggests",
          "source_node": "Users exhibit high confidence even on wrong judgments",
          "target_node": "Confidence calibration tools are needed",
          "description": "Over-confidence signals requirement for calibration aids.",
          "confidence": 4
        },
        {
          "type": "addressed_by",
          "source_node": "Confidence calibration tools are needed",
          "target_node": "Present retrieval coverage/confidence indicators to users to calibrate trust",
          "description": "Displaying retrieval coverage and certainty implements calibration.",
          "confidence": 2
        }
      ]
    }
  ]
}

{"input_tokens": 2093, "input_tokens_details": {"cached_tokens": 0}, "output_tokens": 3350, "output_tokens_details": {"reasoning_tokens": 1024}, "total_tokens": 5443}