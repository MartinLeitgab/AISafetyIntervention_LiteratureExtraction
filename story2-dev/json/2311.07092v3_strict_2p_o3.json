{
  "nodes": [
    {
      "title": "Human difficulty detecting textual deception",
      "aliases": [
        "Low human deception accuracy",
        "Near-random human lie detection"
      ],
      "type": "concept",
      "description": "Empirical finding that human judges achieve roughly chance-level performance (~41 % accuracy) when identifying the truthful contestant using text transcripts alone.",
      "maturity": null
    },
    {
      "title": "Need for automated text-based deception detection",
      "aliases": [
        "Demand for linguistic lie detectors",
        "Requirement for automated deception spotting"
      ],
      "type": "concept",
      "description": "Recognition that machines must assist or replace humans in spotting deceptive statements in text because unaided humans perform poorly.",
      "maturity": null
    },
    {
      "title": "T4TEXT conversational deception dataset",
      "aliases": [
        "T4TEXT corpus",
        "Affidavit-grounded truth dataset"
      ],
      "type": "concept",
      "description": "A 150-episode corpus from the 1950s game show ‘To Tell The Truth’, providing transcripts, ground-truth affidavits and deceptive impostor answers for research on textual deception.",
      "maturity": null
    },
    {
      "title": "Bottleneck LLM deception detector",
      "aliases": [
        "Four-cue bottleneck model",
        "LLM-based lie detector"
      ],
      "type": "intervention",
      "description": "A GPT-4-powered system that first rates four linguistic cues—entailment/contradiction to affidavit, ambiguity/randomness, over-confidence, and half-truth evidence—then feeds the ratings to a discriminator to pick the truthful speaker.",
      "maturity": 3
    },
    {
      "title": "Detector matches or beats human judges",
      "aliases": [
        "Human-level model accuracy",
        "Comparable model performance"
      ],
      "type": "concept",
      "description": "Outcome where the bottleneck detector attains 39 % accuracy (77 % top-2), similar to or exceeding average human performance that used full multimodal evidence.",
      "maturity": null
    },
    {
      "title": "Deploy deception detector in moderation pipelines",
      "aliases": [
        "Content-moderation integration",
        "Production lie-detection workflow"
      ],
      "type": "intervention",
      "description": "Integrating the bottleneck textual deception detector into real-world content-moderation or integrity systems to automatically flag misleading or deceptive user-generated text.",
      "maturity": 1
    },
    {
      "title": "Interpretability through linguistic cue bottleneck",
      "aliases": [
        "Transparent intermediate cues",
        "Explainable cue-based reasoning"
      ],
      "type": "concept",
      "description": "Approach in which the model’s final decision is mediated through explicit ratings of specific linguistic cues, providing traceable, human-readable reasoning steps.",
      "maturity": null
    },
    {
      "title": "Enhanced human trust via interpretable explanations",
      "aliases": [
        "Improved user satisfaction",
        "Better human-AI collaboration"
      ],
      "type": "concept",
      "description": "Finding that users rate explanations derived from the cue bottleneck significantly higher than baselines, increasing trust and willingness to collaborate with the system.",
      "maturity": null
    },
    {
      "title": "Provide bottleneck explanations to human moderators",
      "aliases": [
        "Moderator explanation interface",
        "Cue-based oversight tool"
      ],
      "type": "intervention",
      "description": "Presenting the intermediate cue ratings alongside each moderation flag so human reviewers can understand and audit the model’s reasoning.",
      "maturity": 1
    },
    {
      "title": "Risk of LLM training data contamination",
      "aliases": [
        "Answer-copying risk",
        "Transcript leakage hazard"
      ],
      "type": "concept",
      "description": "Concern that publicly released transcripts could be memorised by future language models, allowing them to ‘cheat’ on evaluation or produce copied answers.",
      "maturity": null
    },
    {
      "title": "Anonymise and permute identities before release",
      "aliases": [
        "Identifier-swapping preprocessing",
        "Name redaction procedure"
      ],
      "type": "intervention",
      "description": "Pre-release sanitisation that replaces real names with placeholders and permutes contestant identifiers to minimise memorisation and data-leakage risks for downstream LLMs.",
      "maturity": 4
    },
    {
      "title": "LLM opacity reduces user trust",
      "aliases": [
        "Opaque model decisions",
        "Lack of LLM interpretability"
      ],
      "type": "concept",
      "description": "General observation that language models often provide outputs without transparent reasoning, which lowers user confidence in their decisions.",
      "maturity": null
    }
  ],
  "logical_chains": [
    {
      "title": "Automated deception detection viability",
      "edges": [
        {
          "type": "causes",
          "source_node": "Human difficulty detecting textual deception",
          "target_node": "Need for automated text-based deception detection",
          "description": "Poor human performance motivates the pursuit of automated solutions.",
          "confidence": 5
        },
        {
          "type": "enabled_by",
          "source_node": "Need for automated text-based deception detection",
          "target_node": "T4TEXT conversational deception dataset",
          "description": "Recognised need leads to creation of a specialised dataset.",
          "confidence": 5
        },
        {
          "type": "enables",
          "source_node": "T4TEXT conversational deception dataset",
          "target_node": "Bottleneck LLM deception detector",
          "description": "Dataset supplies training and evaluation material for the detector.",
          "confidence": 5
        },
        {
          "type": "produces",
          "source_node": "Bottleneck LLM deception detector",
          "target_node": "Detector matches or beats human judges",
          "description": "Model achieves human-level accuracy when applied to the dataset.",
          "confidence": 5
        },
        {
          "type": "enables",
          "source_node": "Detector matches or beats human judges",
          "target_node": "Deploy deception detector in moderation pipelines",
          "description": "Human-level performance suggests readiness for operational deployment.",
          "confidence": 2
        }
      ]
    },
    {
      "title": "Improving trust via interpretable cues",
      "edges": [
        {
          "type": "mitigated_by",
          "source_node": "LLM opacity reduces user trust",
          "target_node": "Interpretability through linguistic cue bottleneck",
          "description": "Explicit cue ratings address the opacity problem.",
          "confidence": 5
        },
        {
          "type": "enables",
          "source_node": "Interpretability through linguistic cue bottleneck",
          "target_node": "Enhanced human trust via interpretable explanations",
          "description": "Transparent reasoning increases user confidence and satisfaction.",
          "confidence": 4
        },
        {
          "type": "addressed_by",
          "source_node": "Enhanced human trust via interpretable explanations",
          "target_node": "Provide bottleneck explanations to human moderators",
          "description": "Supplying explanations within a moderation interface operationalises the trust benefits.",
          "confidence": 2
        }
      ]
    },
    {
      "title": "Dataset sanitisation prevents contamination",
      "edges": [
        {
          "type": "mitigated_by",
          "source_node": "Risk of LLM training data contamination",
          "target_node": "Anonymise and permute identities before release",
          "description": "Identity scrambling reduces the chance that models memorise correct answers.",
          "confidence": 5
        }
      ]
    }
  ]
}

{"input_tokens": 1981, "input_tokens_details": {"cached_tokens": 0}, "output_tokens": 3216, "output_tokens_details": {"reasoning_tokens": 1536}, "total_tokens": 5197}