{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc0fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "from typing import Any, Dict, List, Literal, Optional\n",
    "from pydantic import BaseModel, Field, ConfigDict, ValidationError\n",
    "\n",
    "from plot_json_graphviz import render_json_graph\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e5a4310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# models\n",
    "cheap = 'gpt-4o'\n",
    "best = 'o3'\n",
    "\n",
    "# prompts\n",
    "from prompts import PROMPT_REASONING_STRICT_DEV, PROMPT_JSON_STRICT_DEV, PROMPT_JSON_STRICT_DEV, PROMPT_STRUCT_STRICT_DEV, PROMPT_RESPONSE_EVAL_DEV, PROMPT_REASONING_FLEX_DEV\n",
    "\n",
    "# io\n",
    "input_dir = 'input'\n",
    "json_dir = 'json'\n",
    "traces_dir = 'traces'\n",
    "evals_dir = 'evals'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe56da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once to upload all papers from /input directory\n",
    "\n",
    "# all_papers = []\n",
    "\n",
    "# for paper in os.listdir(input_dir):\n",
    "#     if not paper.endswith('.pdf'):\n",
    "#         continue\n",
    "\n",
    "#     paper_path = os.path.join(input_dir, paper)\n",
    "    \n",
    "#     file = client.files.create(\n",
    "#         file=open(paper_path, \"rb\"),\n",
    "#         purpose=\"user_data\"\n",
    "#     )\n",
    "\n",
    "#     all_papers.append({\n",
    "#         \"id\": paper,\n",
    "#         \"file_id\": file.id,\n",
    "#         \"file_name\": paper.removesuffix('.pdf'),\n",
    "#         \"file_path\": paper_path,\n",
    "#         \"file_size\": os.path.getsize(paper_path),\n",
    "#         \"file_created_at\": datetime.fromtimestamp(os.path.getctime(paper_path)).isoformat(),\n",
    "#         \"file_modified_at\": datetime.fromtimestamp(os.path.getmtime(paper_path)).isoformat(),\n",
    "#         \"file_uploaded_at\": datetime.now().isoformat(),\n",
    "#     })\n",
    "\n",
    "# # save all_papers to file\n",
    "# with open(os.path.join('all_papers.json'), 'w') as f:\n",
    "#     json.dump(all_papers, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28bb1f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all_papers from file\n",
    "with open(os.path.join('all_papers.json'), 'r') as f:\n",
    "    all_papers = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c909ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset papers\n",
    "all_papers = all_papers[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b290ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions \n",
    "\n",
    "def read_file(file_path, encoding='utf-8'):\n",
    "    with open(file_path, 'r', encoding=encoding) as f:\n",
    "        return f.read()\n",
    "    \n",
    "# Get json from non-serial (Response) objects\n",
    "def get_json(obj):\n",
    "    if hasattr(obj, \"__dict__\"):\n",
    "        return {k: get_json(v) for k, v in vars(obj).items()}\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        return [get_json(v) for v in obj]\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: get_json(v) for k, v in obj.items()}\n",
    "    return obj\n",
    "    \n",
    "# json extraction\n",
    "def extract_first_json(text):\n",
    "    start = text.find('{')\n",
    "    if start == -1:\n",
    "        return None\n",
    "    count = 0\n",
    "    for i in range(start, len(text)):\n",
    "        if text[i] == '{':\n",
    "            count += 1\n",
    "        elif text[i] == '}':\n",
    "            count -= 1\n",
    "            if count == 0:\n",
    "                return json.loads(text[start:i+1])\n",
    "    return None\n",
    "\n",
    "# json validation\n",
    "def validate_with_paths(model: type[BaseModel], data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    try:\n",
    "        instance = model.model_validate(data)  # Pydantic v2\n",
    "        return {\n",
    "            \"ok\": True,\n",
    "            \"errors\": [],\n",
    "            \"instance\": instance\n",
    "        }\n",
    "    except ValidationError as e:\n",
    "        def path(loc: List[Any]) -> str:\n",
    "            parts = []\n",
    "            for p in loc:\n",
    "                if isinstance(p, int):\n",
    "                    parts[-1] = f\"{parts[-1]}.{p}\"  # attach index to previous name\n",
    "                else:\n",
    "                    parts.append(str(p))\n",
    "            return \".\".join(parts)\n",
    "\n",
    "        errors = [\n",
    "            {\n",
    "                \"path\": path(err.get(\"loc\", [])),\n",
    "                \"msg\": err.get(\"msg\", \"\"),\n",
    "                \"type\": err.get(\"type\", \"\")\n",
    "            }\n",
    "            for err in e.errors()\n",
    "        ]\n",
    "        return {\n",
    "            \"ok\": False,\n",
    "            \"errors\": errors\n",
    "        }\n",
    "\n",
    "# Save freeform response as txt\n",
    "def save_reasoning_response(resp: str, file_name: str, model: str): \n",
    "    trace_path = os.path.join(traces_dir, f\"{file_name}_{model}.txt\")\n",
    "    usage = json.dumps(get_json(resp.usage))\n",
    "\n",
    "    with open(trace_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(resp.output_text + '\\n\\n' + usage)\n",
    "\n",
    "# Save structured response as JSON\n",
    "def save_json_response(resp: str, file_name: str, model: str):\n",
    "    if model == 'o3':\n",
    "        structured_data = json.loads(resp.model_dump()['output'][1]['arguments'])\n",
    "    else:\n",
    "        structured_data = json.loads(resp.model_dump()['output'][0]['arguments'])\n",
    "\n",
    "    structured_json_path = os.path.join(json_dir, f\"{file_name}_{model}.json\")\n",
    "    usage = json.dumps(get_json(resp.usage))\n",
    "\n",
    "    with open(structured_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(structured_data, f, ensure_ascii=False, indent=2)\n",
    "        f.write('\\n\\n' + usage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21d51feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydantic classes\n",
    "\n",
    "# Schema 1 - Strict\n",
    "class Node1(BaseModel):\n",
    "    name: str  # concise natural-language description of node\n",
    "    aliases: List[str] # 2-3 alternative concise descriptions of node\n",
    "    type: Literal[\"concept\", \"intervention\"]\n",
    "    description: str # detailed technical description of node\n",
    "    concept_category: Optional[str] = Field(default=None, description=\"from examples or create a new category (concept nodes only, otherwise null)\")\n",
    "    intervention_maturity: Optional[int] = Field(default=None, ge=1, le=5, description=\"1-4 (only for intervention nodes)\")\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "class Edge1(BaseModel):\n",
    "    type: str  # relationship label verb\n",
    "    source_node: str  # source node name\n",
    "    target_node: str  # target node name\n",
    "    description: str  # concise description of logical connection\n",
    "    edge_confidence: int = Field(ge=1, le=5, description=\"1-5\")\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "    \n",
    "class LogicalChain1(BaseModel):\n",
    "    title: str  # concise natural-language description of logical chain\n",
    "    edges: List[Edge1]\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "class PaperSchema1(BaseModel):\n",
    "    nodes: List[Node1]\n",
    "    logical_chains: List[LogicalChain1]\n",
    "    model_config = ConfigDict(extra=\"forbid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52327c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema 2 - Flex\n",
    "class Node2(BaseModel):\n",
    "    type: str\n",
    "    name: str\n",
    "    canonical_name: str\n",
    "    aliases: List[str]\n",
    "    confidence: float = Field(..., ge=0.0, le=1.0)\n",
    "    notes: str\n",
    "\n",
    "class Edge2(BaseModel):\n",
    "    type: str\n",
    "    rationale: str\n",
    "    confidence: float = Field(..., ge=0.0, le=1.0)\n",
    "    source_node: Node2\n",
    "    target_node: Node2\n",
    "\n",
    "class PaperSchema2(BaseModel):\n",
    "    nodes: List[Node1]\n",
    "    logical_chains: List[LogicalChain1]\n",
    "    model_config = ConfigDict(extra=\"forbid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "155c60d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get text response from model\n",
    "def get_single_response(file_id: str, prompt_text: str, model: str = 'gpt-4.0'):\n",
    "\n",
    "    input = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"input_file\", \"file_id\": file_id},\n",
    "            {\"type\": \"input_text\", \"text\": prompt_text}\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        input=input,\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d70bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get text and structured json in single response -- models won't comply\n",
    "# def get_single_tool_response(file_id: str, prompt_text: str, schema: object, model: str = 'gpt-4.0'):\n",
    "\n",
    "#     input = [{\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": \"Provide a detailed analysis in text format, then call the causal_chain_structure tool to provide a structured json. Always provide BOTH outputs.\"\n",
    "#     }, {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": [\n",
    "#             {\"type\": \"input_file\", \"file_id\": file_id},\n",
    "#             {\"type\": \"input_text\", \"text\": prompt_text}\n",
    "#         ]\n",
    "#     }]\n",
    "\n",
    "#     tools = [{\n",
    "#         \"type\": \"function\",\n",
    "#         \"name\": \"causal_chain_structure\",\n",
    "#         \"description\": \"Capture the paper's Logical Chain analysis as a structured JSON object.\",\n",
    "#         \"parameters\": schema.model_json_schema()\n",
    "#     }]\n",
    "    \n",
    "#     response = client.responses.create(\n",
    "#         model=model,\n",
    "#         input=input,\n",
    "#         tools=tools,\n",
    "#         tool_choice={\"type\": \"allowed_tools\", \n",
    "#                      \"mode\": \"required\",\n",
    "#                      \"tools\": [{\"type\": \"function\", \"name\": \"causal_chain_structure\"}]\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "#     return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "521ade2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dual responses from model (reasoning + json in separate requests)\n",
    "def get_dual_response(file_id: str, prompt_text: str, schema: object, model: str = 'gpt-4.0'):\n",
    "    \"\"\"\n",
    "    Get response from model in two steps:\n",
    "    1. Freeform analysis of the paper based on the prompt_text.\n",
    "    2. Structured json using the causal_chain_structure tool based on the freeform analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    # First call - get reasoning\n",
    "    reasoning_input = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"input_file\", \"file_id\": file_id},\n",
    "            {\"type\": \"input_text\", \"text\": prompt_text}\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    reasoning_response = client.responses.create(\n",
    "        model=model,\n",
    "        input=reasoning_input,\n",
    "        tools=None  # No tools for reasoning\n",
    "    )\n",
    "\n",
    "    # Second call - get structured json\n",
    "    json_input = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Use the following detailed analysis to help create the structured json:\"\n",
    "    }, {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": reasoning_response.output_text\n",
    "    },{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            # {\"type\": \"input_file\", \"file_id\": file_id},\n",
    "            {\"type\": \"input_text\", \"text\": PROMPT_STRUCT_STRICT_DEV}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    tools = [{\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"causal_chain_structure\",\n",
    "        \"description\": \"Capture the paper's Logical Chain analysis as a structured JSON object.\",\n",
    "        \"parameters\": schema.model_json_schema()\n",
    "    }]\n",
    "    \n",
    "    json_response = client.responses.create(\n",
    "        model=model,\n",
    "        input=json_input,\n",
    "        tools=tools,\n",
    "        tool_choice={\"type\": \"allowed_tools\", \n",
    "                     \"mode\": \"auto\",\n",
    "                     \"tools\": [{\"type\": \"function\", \"name\": \"causal_chain_structure\"}]\n",
    "        }  # force tool use\n",
    "    )\n",
    "\n",
    "    return reasoning_response, json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16f8b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function to analyze paper\n",
    "def analyze_paper(file_name: str, file_id: str, prompt_text: str, schema: object, dual: bool = False, label: str = '', model: str = 'gpt-4.0'):\n",
    "    \n",
    "    if dual:\n",
    "        reasoning_response, json_response = get_dual_response(\n",
    "            file_id=file_id,\n",
    "            prompt_text=prompt_text,\n",
    "            schema=schema,\n",
    "            model=model)\n",
    "        save_reasoning_response(reasoning_response, file_name + label, model=model)\n",
    "        save_json_response(json_response, file_name + label, model=model)\n",
    "        return reasoning_response, json_response\n",
    "    else:\n",
    "        response = get_single_response(\n",
    "            file_id=file_id,\n",
    "            prompt_text=prompt_text,\n",
    "            model=model)\n",
    "        save_reasoning_response(response, file_name + label, model=model)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e9e1ae",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431688c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # single pass, reasoning only\n",
    "# file_name = all_papers[2]['file_name']\n",
    "# file_id = all_papers[2]['file_id']\n",
    "\n",
    "# resp = analyze_paper(\n",
    "#     file_name=file_name,\n",
    "#     file_id=file_id,\n",
    "#     prompt_text=PROMPT_REASONING_STRICT_DEV,\n",
    "#     schema=PaperSchema1,\n",
    "#     model=best,\n",
    "#     label='_reason'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec40fe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single pass, reasoning + json\n",
    "file_name = all_papers[2]['file_name']\n",
    "file_id = all_papers[2]['file_id']\n",
    "\n",
    "resp = analyze_paper(\n",
    "    file_name=file_name,\n",
    "    file_id=file_id,\n",
    "    prompt_text=PROMPT_REASONING_STRICT_DEV + PROMPT_JSON_STRICT_DEV,\n",
    "    schema=PaperSchema1,\n",
    "    model=best,\n",
    "    label='_strict_final_1p'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6532fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # single pass with struct -- models tend to call tool only\n",
    "\n",
    "# file_name = all_papers[2]['file_name']\n",
    "# file_id = all_papers[2]['file_id']\n",
    "\n",
    "# response = get_single_tool_response(\n",
    "#         file_id=file_id,\n",
    "#         prompt_text=PROMPT_REASONING_STRICT_DEV+PROMPT_STRUCT_STRICT_DEV,\n",
    "#         schema=PaperSchema1,\n",
    "#         model=best\n",
    "#         )\n",
    "\n",
    "# resp = get_json(response)\n",
    "# print(json.loads(resp['output'][1]['arguments']))\n",
    "# print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d64ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dual pass\n",
    "# file_name = all_papers[2]['file_name']\n",
    "# file_id = all_papers[2]['file_id']\n",
    "\n",
    "# resp = analyze_paper(\n",
    "#     file_name=file_name,\n",
    "#     file_id=file_id,\n",
    "#     prompt_text=PROMPT_REASONING_STRICT_DEV,\n",
    "#     schema=PaperSchema1,\n",
    "#     dual = True,\n",
    "#     model=best,\n",
    "#     label='_2pass'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e60afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all papers, single-pass\n",
    "\n",
    "# for paper in all_papers:\n",
    "#     analyze_paper(\n",
    "#         file_name=paper['file_name'],\n",
    "#         file_id=paper['file_id'],\n",
    "#         prompt_text=PROMPT_REASONING_STRICT_DEV + PROMPT_JSON_STRICT_DEV,\n",
    "#         schema=PaperSchema1,\n",
    "#         model=best,\n",
    "#         label='_strict_1p'\n",
    "#     )\n",
    "\n",
    "\n",
    "# for paper in all_papers:\n",
    "#     analyze_paper(\n",
    "#         file_name=paper['file_name'],\n",
    "#         file_id=paper['file_id'],\n",
    "#         prompt_text=PROMPT_REASONING_FLEX_DEV,\n",
    "#         schema=PaperSchema2,\n",
    "#         model=best,\n",
    "#         label='_flex_1p'\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all papers, dual-pass\n",
    "\n",
    "# for paper in all_papers:\n",
    "#     analyze_paper(\n",
    "#         file_name=paper['file_name'],\n",
    "#         file_id=paper['file_id'],\n",
    "#         prompt_text=PROMPT_REASONING_STRICT_DEV,\n",
    "#         schema=PaperSchema1,\n",
    "#         dual=True,\n",
    "#         model=best,\n",
    "#         label='_strict_2p'\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650cb4c7",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbf6375",
   "metadata": {},
   "source": [
    "## evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfce1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dual-pass \n",
    "dual_json_data = {}\n",
    "\n",
    "for filename in os.listdir(json_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(json_dir, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            json_part = extract_first_json(content)\n",
    "            dual_json_data[filename] = json_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4a1e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dual_json_data.keys():\n",
    "    print(validate_with_paths(PaperSchema1, dual_json_data[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04960ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load single-pass\n",
    "single_json_data = {}\n",
    "keyword = 'flex'\n",
    "\n",
    "for filename in sorted(os.listdir(traces_dir)):\n",
    "    if keyword in filename:\n",
    "        file_path = os.path.join(traces_dir, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            json_part = extract_first_json(content)\n",
    "            single_json_data[filename] = json_part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f995cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in single_json_data.keys():\n",
    "    print(validate_with_paths(PaperSchema2, single_json_data[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b621ada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "single_json_data.keys()\n",
    "\n",
    "# due to a new type, validation schema format incorrect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d849e940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd4f627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a38361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reasoning evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3009e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm-as-judge, feed paper and concat responses\n",
    "def eval_analyses(file_id: str, eval_text: str, prompt_text: str, model: str = 'gpt-4.0'):\n",
    "\n",
    "    input = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"input_file\", \"file_id\": file_id},\n",
    "            {\"type\": \"input_text\", \"text\": prompt_text},\n",
    "            {\"type\": \"input_text\", \"text\": eval_text},\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        input=input,\n",
    "    )\n",
    "\n",
    "    return response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1b26f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval 1p vs 2p\n",
    "for paper in all_papers:\n",
    "\n",
    "    file_name = paper['file_name']\n",
    "    file_id = paper['file_id']\n",
    "\n",
    "    run1 = '_strict_1p_o3'\n",
    "    run2 = '_strict_2p_o3'\n",
    "\n",
    "    label = '_1p2p'\n",
    "\n",
    "    eval_1 = f\"Analysis 1: {file_name}{run1}\\n\\n\" + read_file(os.path.join(traces_dir, f\"{file_name}{run1}.txt\")) + \"\\n\"\n",
    "    eval_2 = f\"Analysis 2: {file_name}{run2}\\n\\n\" + read_file(os.path.join(traces_dir, f\"{file_name}{run2}.txt\")) + \"\\n\" + read_file(os.path.join(json_dir, f\"{file_name}{run2}.json\")) + \"\\n\"\n",
    "    eval_text = eval_1 + eval_2\n",
    "\n",
    "    resp = eval_analyses(\n",
    "        file_id=file_id,\n",
    "        prompt_text=PROMPT_RESPONSE_EVAL_DEV,\n",
    "        eval_text=eval_text,\n",
    "        model=best\n",
    "    )\n",
    "\n",
    "    with open(os.path.join(evals_dir, f\"{file_name}_{label}_eval.md\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(resp.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57361e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval strict 1p vs flex 1p\n",
    "\n",
    "for paper in all_papers:\n",
    "\n",
    "    file_name = paper['file_name']\n",
    "    file_id = paper['file_id']\n",
    "\n",
    "    run1 = '_strict_1p_o3'\n",
    "    run2 = '_flex_1p_o3'\n",
    "\n",
    "    label = '_fs'\n",
    "\n",
    "    eval_1 = f\"Analysis 1: {file_name}{run1}\\n\\n\" + read_file(os.path.join(traces_dir, f\"{file_name}{run1}.txt\")) + \"\\n\"\n",
    "    eval_2 = f\"Analysis 2: {file_name}{run2}\\n\\n\" + read_file(os.path.join(traces_dir, f\"{file_name}{run2}.txt\")) + \"\\n\"\n",
    "    eval_text = eval_1 + eval_2\n",
    "\n",
    "    resp = eval_analyses(\n",
    "        file_id=file_id,\n",
    "        prompt_text=PROMPT_RESPONSE_EVAL_DEV,\n",
    "        eval_text=eval_text,\n",
    "        model=best\n",
    "    )\n",
    "\n",
    "    with open(os.path.join(evals_dir, f\"{file_name}_{label}_eval.md\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(resp.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4f5f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22167fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd97d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cca9675",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea8656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterative analysis\n",
    "def analyze_paper_iteratively(file_name: str, file_id: str, prompt_text: str, iterations: int = 3, schema: object, model: str = 'gpt-4.0'):\n",
    "    \"\"\"\n",
    "    Iteratively analyze a paper multiple times, asking the model to find more connections each time.\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Name of the file to analyze.\n",
    "        file_id (str): ID of the file in OpenAI.\n",
    "        prompt_text (str): The base prompt to use for analysis.\n",
    "        iterations (int): Number of iterations to perform (default 3).\n",
    "        model (str): The model to use for analysis.\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples containing (freeform_response, structured_response) for each iteration.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    current_prompt = prompt_text\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        print(f\"\\nIteration {i+1}/{iterations}\")\n",
    "        \n",
    "        # For iterations after the first, add the improvement request\n",
    "        if i > 0:\n",
    "            current_prompt = (\n",
    "                current_prompt + \n",
    "                \"\\n\\nIMPORTANT: You missed many causal connections and relationships in your previous analysis. \" +\n",
    "                \"Please analyze again more thoroughly, looking specifically for:\\n\" +\n",
    "                \"1. Additional connections between existing concepts\\n\" +\n",
    "                \"2. Implicit relationships that weren't directly stated\\n\" +\n",
    "                \"3. Higher-order effects and consequences\\n\" +\n",
    "                \"4. Cross-cutting themes and patterns\\n\" +\n",
    "                \"5. Alternative interpretations of the findings\"\n",
    "            )\n",
    "        \n",
    "        # Run the analysis\n",
    "        freeform_response, structured_response = get_dual_response(\n",
    "            file_id=file_id,\n",
    "            prompt_text=current_prompt,\n",
    "            schema=schema,\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        # Save responses with iteration number in filename\n",
    "        save_trace_response(freeform_response, f\"{file_name}_iter{i+1}\", model=model)\n",
    "        save_json_response(structured_response, f\"{file_name}_iter{i+1}\", model=model)\n",
    "        \n",
    "        results.append((freeform_response, structured_response))\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7871b2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test iterative analysis\n",
    "file_name = all_papers[0]['file_name']\n",
    "file_id = all_papers[0]['file_id']\n",
    "\n",
    "iterative_results = analyze_paper_iteratively(\n",
    "    file_name=file_name,\n",
    "    file_id=file_id,\n",
    "    prompt_text=PROMPT_FREEFORM,\n",
    "    iterations=3,\n",
    "    schema=PaperSchema1,\n",
    "    model=cheap\n",
    ")\n",
    "\n",
    "# Print the freeform responses from each iteration\n",
    "# for i, (freeform_resp, _) in enumerate(iterative_results, 1):\n",
    "#     print(f\"\\n=== Iteration {i} Analysis ===\")\n",
    "#     print(freeform_resp.output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AISafetyIntervention_LiteratureExtraction (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
