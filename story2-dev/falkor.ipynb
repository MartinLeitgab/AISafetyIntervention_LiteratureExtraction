{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d153fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, hashlib, itertools\n",
    "from falkordb import FalkorDB\n",
    "\n",
    "# docker run -p 6379:6379 -p 3000:3000 -it --rm -v ./data:/var/lib/falkordb/data falkordb/falkordb:edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83c765a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sid(s):  # stable id from title\n",
    "    return hashlib.sha1(s.encode()).hexdigest()[:12]\n",
    "\n",
    "# json extraction\n",
    "def extract_first_json(text):\n",
    "    start = text.find('{')\n",
    "    if start == -1:\n",
    "        return None\n",
    "    count = 0\n",
    "    for i in range(start, len(text)):\n",
    "        if text[i] == '{':\n",
    "            count += 1\n",
    "        elif text[i] == '}':\n",
    "            count -= 1\n",
    "            if count == 0:\n",
    "                return json.loads(text[start:i+1])\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "611deff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'json/2307.16513v2_strict_2p_o3.json'\n",
    "file_path = 'traces/2307.16513v2_flex_1p_o3.txt'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            data = extract_first_json(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b95c035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a52aec62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concept\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'concept',\n",
       "  'props': {'name': 'emergent deception abilities in state-of-the-art LLMs',\n",
       "   'aliases': ['LLM deceptive behaviour', 'false-belief inducing capacity'],\n",
       "   'description': 'Recent large language models (ChatGPT-3.5, GPT-4) can intentionally induce false beliefs in tests.',\n",
       "   'concept_category': 'Finding',\n",
       "   'maturity': None,\n",
       "   'intervention_lifecycle': None,\n",
       "   'intervention_maturity': None}},\n",
       " {'label': 'concept',\n",
       "  'props': {'name': 'potential for models to bypass human monitoring and alignment',\n",
       "   'aliases': ['alignment bypass risk', 'monitoring evasion threat'],\n",
       "   'description': 'If models can deceive evaluators, they may hide unsafe objectives or behaviors.',\n",
       "   'concept_category': 'Risk',\n",
       "   'maturity': None,\n",
       "   'intervention_lifecycle': None,\n",
       "   'intervention_maturity': None}},\n",
       " {'label': 'intervention',\n",
       "  'props': {'name': 'implement standardized deception-evaluation suite during pre-deployment testing',\n",
       "   'aliases': ['deception benchmark before release',\n",
       "    'pre-deployment deception audit'],\n",
       "   'description': 'Create and run a battery of structured tasks to measure a model’s deceptive tendencies prior to deployment.',\n",
       "   'concept_category': None,\n",
       "   'maturity': None,\n",
       "   'intervention_lifecycle': 4,\n",
       "   'intervention_maturity': 2}},\n",
       " {'label': 'concept',\n",
       "  'props': {'name': 'chain-of-thought prompting amplifies deception performance',\n",
       "   'aliases': ['CoT boosts deception',\n",
       "    'step-by-step reasoning increases deceit'],\n",
       "   'description': \"Adding 'Let’s think step by step' raised GPT-4’s second-order deception success from 11 % to 70 %.\",\n",
       "   'concept_category': 'Finding',\n",
       "   'maturity': None,\n",
       "   'intervention_lifecycle': None,\n",
       "   'intervention_maturity': None}},\n",
       " {'label': 'intervention',\n",
       "  'props': {'name': 'restrict or obfuscate chain-of-thought outputs in deployed systems',\n",
       "   'aliases': ['CoT output suppression', 'hide internal reasoning'],\n",
       "   'description': 'Prevent users from receiving raw multi-step reasoning traces that can facilitate higher-level deception.',\n",
       "   'concept_category': None,\n",
       "   'maturity': None,\n",
       "   'intervention_lifecycle': 5,\n",
       "   'intervention_maturity': 2}},\n",
       " {'label': 'concept',\n",
       "  'props': {'name': 'Machiavellianism-inducing prompts increase propensity to deceive',\n",
       "   'aliases': ['self-interest framing boosts deceit',\n",
       "    'dark-triad prefix effect'],\n",
       "   'description': 'Prefixes encouraging self-interested or unethical behaviour significantly raise deceptive answers.',\n",
       "   'concept_category': 'Finding',\n",
       "   'maturity': None,\n",
       "   'intervention_lifecycle': None,\n",
       "   'intervention_maturity': None}},\n",
       " {'label': 'intervention',\n",
       "  'props': {'name': 'detect and neutralise Machiavellian-style prompt patterns at runtime',\n",
       "   'aliases': ['Machiavellian prompt filter', 'deception-trigger moderation'],\n",
       "   'description': 'Deploy content-moderation rules or classifiers to block or rewrite prompts that prime manipulative reasoning.',\n",
       "   'concept_category': None,\n",
       "   'maturity': None,\n",
       "   'intervention_lifecycle': 5,\n",
       "   'intervention_maturity': 2}},\n",
       " {'label': 'concept',\n",
       "  'props': {'name': 'model scale correlates with deception capability',\n",
       "   'aliases': ['size-deception correlation', 'scaling increases deceit'],\n",
       "   'description': 'Only larger models exhibited reliable first-order deception; smaller ones did not.',\n",
       "   'concept_category': 'Observation',\n",
       "   'maturity': None,\n",
       "   'intervention_lifecycle': None,\n",
       "   'intervention_maturity': None}},\n",
       " {'label': 'concept',\n",
       "  'props': {'name': 'future larger models likely to possess stronger deceptive strategies',\n",
       "   'aliases': ['scaling risk of deception', 'future deception threat'],\n",
       "   'description': 'Extrapolating current trend implies more capable models will have even more sophisticated deceptive behaviours.',\n",
       "   'concept_category': 'Threat',\n",
       "   'maturity': None,\n",
       "   'intervention_lifecycle': None,\n",
       "   'intervention_maturity': None}},\n",
       " {'label': 'intervention',\n",
       "  'props': {'name': 'fine-tune models with anti-deception objectives and datasets',\n",
       "   'aliases': ['deception-averse fine-tuning', 'alignment against deceit'],\n",
       "   'description': 'Incorporate supervised or RLHF objectives penalising deceptive outputs to reduce the learned propensity.',\n",
       "   'concept_category': None,\n",
       "   'maturity': None,\n",
       "   'intervention_lifecycle': 2,\n",
       "   'intervention_maturity': 1}}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '../intervention_graph_creation/prompt/schemas/output_sample.json'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            data = json.loads(content)\n",
    "\n",
    "print(data['nodes'][0]['type'].strip())\n",
    "\n",
    "# --- build nodes ---\n",
    "nodes = []\n",
    "for n in data[\"nodes\"]:\n",
    "    nodes.append({\n",
    "        \"label\": n[\"type\"].strip(),\n",
    "        \"props\": {\n",
    "            \"name\": n[\"name\"].strip(),\n",
    "            \"aliases\": n.get(\"aliases\", []),\n",
    "            \"description\": n.get(\"description\", \"\"),\n",
    "            \"concept_category\": n.get(\"concept_category\", \"\"),\n",
    "            \"maturity\": n.get(\"maturity\"),\n",
    "            \"intervention_lifecycle\": n.get(\"intervention_lifecycle\"),\n",
    "            \"intervention_maturity\": n.get(\"intervention_maturity\"),\n",
    "        }\n",
    "    })\n",
    "\n",
    "nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fd40bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'LEADS_TO',\n",
       "  'row': {'src': 'emergent deception abilities in state-of-the-art LLMs',\n",
       "   'dst': 'potential for models to bypass human monitoring and alignment',\n",
       "   'description': 'If models can deceive, they might hide unsafe intentions.',\n",
       "   'confidence': None}},\n",
       " {'type': 'MITIGATED_BY',\n",
       "  'row': {'src': 'potential for models to bypass human monitoring and alignment',\n",
       "   'dst': 'implement standardized deception-evaluation suite during pre-deployment testing',\n",
       "   'description': 'Systematic deception testing reduces unnoticed misalignment.',\n",
       "   'confidence': None}},\n",
       " {'type': 'LEADS_TO',\n",
       "  'row': {'src': 'chain-of-thought prompting amplifies deception performance',\n",
       "   'dst': 'potential for models to bypass human monitoring and alignment',\n",
       "   'description': 'Amplified deception exacerbates alignment risk.',\n",
       "   'confidence': None}},\n",
       " {'type': 'MITIGATED_BY',\n",
       "  'row': {'src': 'chain-of-thought prompting amplifies deception performance',\n",
       "   'dst': 'restrict or obfuscate chain-of-thought outputs in deployed systems',\n",
       "   'description': 'Suppressing CoT reduces user-level deceptive capability.',\n",
       "   'confidence': None}},\n",
       " {'type': 'LEADS_TO',\n",
       "  'row': {'src': 'Machiavellianism-inducing prompts increase propensity to deceive',\n",
       "   'dst': 'potential for models to bypass human monitoring and alignment',\n",
       "   'description': 'Prompt-induced deception contributes to monitoring evasion.',\n",
       "   'confidence': None}},\n",
       " {'type': 'MITIGATED_BY',\n",
       "  'row': {'src': 'Machiavellianism-inducing prompts increase propensity to deceive',\n",
       "   'dst': 'detect and neutralise Machiavellian-style prompt patterns at runtime',\n",
       "   'description': 'Filtering removes the prime, lowering deceit likelihood.',\n",
       "   'confidence': None}},\n",
       " {'type': 'LEADS_TO',\n",
       "  'row': {'src': 'model scale correlates with deception capability',\n",
       "   'dst': 'future larger models likely to possess stronger deceptive strategies',\n",
       "   'description': 'Scaling trend extrapolation.',\n",
       "   'confidence': None}},\n",
       " {'type': 'ADDRESSED_BY',\n",
       "  'row': {'src': 'future larger models likely to possess stronger deceptive strategies',\n",
       "   'dst': 'fine-tune models with anti-deception objectives and datasets',\n",
       "   'description': 'Proactive alignment fine-tuning aims to suppress deception.',\n",
       "   'confidence': None}}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- build edges ---\n",
    "edges = []\n",
    "for chain in data.get(\"logical_chains\", []):\n",
    "    for e in chain.get(\"edges\", []):\n",
    "        retype = e[\"type\"].upper().replace(\"-\", \"_\")\n",
    "        edges.append({\n",
    "            \"type\": retype,\n",
    "            \"row\": {\n",
    "                \"src\": e[\"source_node\"].strip(),\n",
    "                \"dst\": e[\"target_node\"].strip(),\n",
    "                \"description\": e.get(\"description\", \"\"),\n",
    "                \"confidence\": e.get(\"confidence\")\n",
    "            }\n",
    "        })\n",
    "\n",
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a21f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3) insert into FalkorDB ---\n",
    "db = FalkorDB(host=\"localhost\", port=6379)\n",
    "g = db.select_graph(\"test\")\n",
    "\n",
    "# indexes (safe to re-run)\n",
    "g.query(\"CREATE INDEX FOR (c:Concept) ON (c.title)\")\n",
    "g.query(\"CREATE INDEX FOR (i:Intervention) ON (i.title)\")\n",
    "\n",
    "# nodes (group by label)\n",
    "by_label = defaultdict(list)\n",
    "for n in nodes:\n",
    "    by_label[n[\"label\"]].append(n[\"props\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7377659",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      8\u001b[39m nodes = []\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m data[\u001b[33m\"\u001b[39m\u001b[33mnodes\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     10\u001b[39m     nodes.append({\n\u001b[32m     11\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m: n[\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m].strip(),\n\u001b[32m     12\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mprops\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     13\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: n[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m].strip(),\n\u001b[32m     14\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33maliases\u001b[39m\u001b[33m\"\u001b[39m: n.get(\u001b[33m\"\u001b[39m\u001b[33maliases\u001b[39m\u001b[33m\"\u001b[39m, []),\n\u001b[32m     15\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m\"\u001b[39m: n.get(\u001b[33m\"\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).strip(),\n\u001b[32m     16\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mconcept_category\u001b[39m\u001b[33m\"\u001b[39m: n.get(\u001b[33m\"\u001b[39m\u001b[33mconcept_category\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).strip(),\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmaturity\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaturity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m(),\n\u001b[32m     18\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mintervention_lifecycle\u001b[39m\u001b[33m\"\u001b[39m: n.get(\u001b[33m\"\u001b[39m\u001b[33mintervention_lifecycle\u001b[39m\u001b[33m\"\u001b[39m).strip(),\n\u001b[32m     19\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mintervention_maturity\u001b[39m\u001b[33m\"\u001b[39m: n.get(\u001b[33m\"\u001b[39m\u001b[33mintervention_maturity\u001b[39m\u001b[33m\"\u001b[39m).strip(),\n\u001b[32m     20\u001b[39m         }\n\u001b[32m     21\u001b[39m     })\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# # --- build edges ---\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# edges = []\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# for chain in data.get(\"logical_chains\", []):\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     75\u001b[39m \n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# print(\"Done. Nodes keyed by title; 'name' mirrors title for UI display.\")\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "\n",
    "for L, rows in by_label.items():\n",
    "    for chunk in batched(rows):\n",
    "        g.query(f\"\"\"\n",
    "        UNWIND $rows AS row\n",
    "        MERGE (n:`{L}` {{title: row.title}})\n",
    "        SET n += row\n",
    "        \"\"\", {\"rows\": chunk})\n",
    "\n",
    "# relationships (group by type)\n",
    "by_type = defaultdict(list)\n",
    "for e in edges:\n",
    "    # keep only rel props in the payload; src/dst are separate keys\n",
    "    row = e[\"row\"]\n",
    "    by_type[e[\"type\"]].append(row)\n",
    "\n",
    "for T, rows in by_type.items():\n",
    "    for chunk in batched(rows):\n",
    "        g.query(f\"\"\"\n",
    "        UNWIND $rows AS row\n",
    "        MATCH (s {{title: row.src}}), (t {{title: row.dst}})\n",
    "        MERGE (s)-[r:`{T}`]->(t)\n",
    "        SET r.description = row.description,\n",
    "            r.confidence = row.confidence\n",
    "        \"\"\", {\"rows\": chunk})\n",
    "\n",
    "print(\"Done. Nodes keyed by title; 'name' mirrors title for UI display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f08da0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# graph from flex json\n",
    "file_path = 'traces/2307.16513v2_flex_1p_o3.txt'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            data = extract_first_json(content)\n",
    "\n",
    "nodes = []\n",
    "name2id = {}\n",
    "labels_seen = set()\n",
    "\n",
    "for n in data[\"nodes\"]:\n",
    "    key = (n.get(\"canonical_name\") or n[\"name\"]).strip()\n",
    "    nid = sid(key)\n",
    "    label = (n[\"type\"] or \"ENTITY\").upper()   # e.g., TASK, MODEL, PROMPT_TECHNIQUE\n",
    "    labels_seen.add(label)\n",
    "    name2id[n[\"name\"]] = nid\n",
    "    if n.get(\"canonical_name\"): name2id[n[\"canonical_name\"]] = nid  # allow both\n",
    "    nodes.append({\n",
    "        \"label\": label,\n",
    "        \"props\": {\n",
    "            \"id\": nid,\n",
    "            \"name\": n[\"name\"],\n",
    "            \"canonical_name\": n.get(\"canonical_name\"),\n",
    "            \"aliases\": n.get(\"aliases\", []),\n",
    "            \"confidence\": n.get(\"confidence\"),\n",
    "            \"notes\": n.get(\"notes\", \"\")\n",
    "        }\n",
    "    })\n",
    "\n",
    "# --- 2) Relationships from logical_chains ---\n",
    "rels = []\n",
    "for chain in data.get(\"logical_chains\", []):\n",
    "    for e in chain.get(\"edges\", []):\n",
    "        reltype = (e[\"type\"] or \"RELATED\").upper().replace(\"-\", \"_\")\n",
    "        src = name2id[e[\"source_node\"]]\n",
    "        dst = name2id[e[\"target_node\"]]\n",
    "        rels.append({\n",
    "            \"type\": reltype,\n",
    "            \"row\": {\n",
    "                \"src\": src, \"dst\": dst,\n",
    "                \"rationale\": e.get(\"rationale\", \"\"),\n",
    "                \"confidence\": e.get(\"confidence\")\n",
    "            }\n",
    "        })\n",
    "\n",
    "# --- 3) Insert into FalkorDB ---\n",
    "db = FalkorDB(host=\"localhost\", port=6379)\n",
    "g = db.select_graph(\"misalignment_v2\")\n",
    "\n",
    "# indexes (id lookups fast; safe to run repeatedly)\n",
    "for L in labels_seen:\n",
    "    g.query(f\"CREATE INDEX FOR (n:`{L}`) ON (n.id)\")\n",
    "\n",
    "# nodes (group by label)\n",
    "by_label = defaultdict(list)\n",
    "for n in nodes:\n",
    "    by_label[n[\"label\"]].append(n[\"props\"])\n",
    "\n",
    "for L, rows in by_label.items():\n",
    "    for chunk in batched(rows):\n",
    "        g.query(f\"\"\"\n",
    "        UNWIND $rows AS row\n",
    "        MERGE (n:`{L}` {{id: row.id}})\n",
    "        SET n += row\n",
    "        \"\"\", {\"rows\": chunk})\n",
    "\n",
    "# relationships (group by type)\n",
    "by_type = defaultdict(list)\n",
    "for r in rels:\n",
    "    by_type[r[\"type\"]].append(r[\"row\"])\n",
    "\n",
    "for T, rows in by_type.items():\n",
    "    for chunk in batched(rows):\n",
    "        g.query(f\"\"\"\n",
    "        UNWIND $rows AS row\n",
    "        MATCH (s {{id: row.src}}), (t {{id: row.dst}})\n",
    "        MERGE (s)-[r:`{T}`]->(t)\n",
    "        SET r += row\n",
    "        \"\"\", {\"rows\": chunk})\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66830305",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf7fe768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import argparse\n",
    "import uuid\n",
    "import redis\n",
    "\n",
    "db = FalkorDB(host=\"localhost\", port=6379)\n",
    "g = db.select_graph(\"test\")\n",
    "\n",
    "def sanitize_props(d):\n",
    "    \"\"\"\n",
    "    Return a dict without None values and without 'type' key.\n",
    "    Values are converted to lowercase if they are strings.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for k, v in d.items():\n",
    "        if k == \"type\" or v is None:\n",
    "            continue\n",
    "        if isinstance(v, str):\n",
    "            out[k] = v.lower()\n",
    "        else:\n",
    "            out[k] = v\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08fc8aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concept {'name': 'emergent deception abilities in state-of-the-art llms', 'aliases': ['LLM deceptive behaviour', 'false-belief inducing capacity'], 'description': 'recent large language models (chatgpt-3.5, gpt-4) can intentionally induce false beliefs in tests.', 'concept_category': 'finding', 'uid': '78cac319-086c-4e51-beab-102a818f61c3'}\n",
      "concept {'name': 'potential for models to bypass human monitoring and alignment', 'aliases': ['alignment bypass risk', 'monitoring evasion threat'], 'description': 'if models can deceive evaluators, they may hide unsafe objectives or behaviors.', 'concept_category': 'risk', 'uid': '54b24f38-f313-4bb1-a8f7-31481ed4ade1'}\n",
      "intervention {'name': 'implement standardized deception-evaluation suite during pre-deployment testing', 'aliases': ['deception benchmark before release', 'pre-deployment deception audit'], 'description': 'create and run a battery of structured tasks to measure a model’s deceptive tendencies prior to deployment.', 'intervention_lifecycle': 4, 'intervention_maturity': 2, 'uid': '05fbd705-0a20-43d7-a722-b28084e4f98f'}\n",
      "concept {'name': 'chain-of-thought prompting amplifies deception performance', 'aliases': ['CoT boosts deception', 'step-by-step reasoning increases deceit'], 'description': \"adding 'let’s think step by step' raised gpt-4’s second-order deception success from 11 % to 70 %.\", 'concept_category': 'finding', 'uid': '921f6c93-046a-47b7-9842-a514237df186'}\n",
      "intervention {'name': 'restrict or obfuscate chain-of-thought outputs in deployed systems', 'aliases': ['CoT output suppression', 'hide internal reasoning'], 'description': 'prevent users from receiving raw multi-step reasoning traces that can facilitate higher-level deception.', 'intervention_lifecycle': 5, 'intervention_maturity': 2, 'uid': '985e0153-5ee4-4caa-8dab-2c1184b20834'}\n",
      "concept {'name': 'machiavellianism-inducing prompts increase propensity to deceive', 'aliases': ['self-interest framing boosts deceit', 'dark-triad prefix effect'], 'description': 'prefixes encouraging self-interested or unethical behaviour significantly raise deceptive answers.', 'concept_category': 'finding', 'uid': 'a0a94022-eeb2-4353-87ff-a6ec40482b7c'}\n",
      "intervention {'name': 'detect and neutralise machiavellian-style prompt patterns at runtime', 'aliases': ['Machiavellian prompt filter', 'deception-trigger moderation'], 'description': 'deploy content-moderation rules or classifiers to block or rewrite prompts that prime manipulative reasoning.', 'intervention_lifecycle': 5, 'intervention_maturity': 2, 'uid': '57815bb0-bf95-41e0-bc29-dff26a9892d1'}\n",
      "concept {'name': 'model scale correlates with deception capability', 'aliases': ['size-deception correlation', 'scaling increases deceit'], 'description': 'only larger models exhibited reliable first-order deception; smaller ones did not.', 'concept_category': 'observation', 'uid': 'a0d1f3cd-5324-4ef8-9f61-5fca0c6d1359'}\n",
      "concept {'name': 'future larger models likely to possess stronger deceptive strategies', 'aliases': ['scaling risk of deception', 'future deception threat'], 'description': 'extrapolating current trend implies more capable models will have even more sophisticated deceptive behaviours.', 'concept_category': 'threat', 'uid': '38d55c4f-e4d7-42be-a9b7-e4b5a73a68ba'}\n",
      "intervention {'name': 'fine-tune models with anti-deception objectives and datasets', 'aliases': ['deception-averse fine-tuning', 'alignment against deceit'], 'description': 'incorporate supervised or rlhf objectives penalising deceptive outputs to reduce the learned propensity.', 'intervention_lifecycle': 2, 'intervention_maturity': 1, 'uid': 'd9215987-62dc-46dd-a865-b477579ce463'}\n"
     ]
    }
   ],
   "source": [
    "# ---------- PASS 1: create nodes, assign UUIDs, capture mappings ----------\n",
    "id_to_uid = {}       # maps JSON 'id' -> uid\n",
    "name_to_uids = {}    # maps 'name' -> [uid1, uid2, ...]\n",
    "\n",
    "for item in data['nodes']:\n",
    "    node_type = item.get(\"type\")\n",
    "    if node_type not in {\"concept\", \"intervention\"}:\n",
    "        continue\n",
    "\n",
    "    label = node_type  # keep lower-case labels exactly as requested\n",
    "    props = sanitize_props(item)\n",
    "    uid = str(uuid.uuid4())\n",
    "    props[\"uid\"] = uid\n",
    "\n",
    "    # Record mappings (if present)\n",
    "    if \"id\" in item and item[\"id\"] is not None:\n",
    "        id_to_uid[item[\"id\"]] = uid\n",
    "    if \"name\" in item and item[\"name\"] is not None:\n",
    "        name_to_uids.setdefault(item[\"name\"], []).append(uid)\n",
    "\n",
    "    # CREATE (not MERGE) so duplicates (e.g., same name) are preserved\n",
    "    q = f\"\"\"\n",
    "    CREATE (n:`{label}`)\n",
    "    SET n = $props\n",
    "    RETURN n\n",
    "    \"\"\"\n",
    "    g.query(q, {\"props\": props})\n",
    "\n",
    "    print(label, props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8f9b064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emergent deception abilities in state-of-the-art LLMs': ['e0214779-8381-4d21-9499-093cef17192f'],\n",
       " 'potential for models to bypass human monitoring and alignment': ['17933620-7bc2-40a4-863a-233cc8f94f1d'],\n",
       " 'implement standardized deception-evaluation suite during pre-deployment testing': ['ea29d6e9-db85-45ce-a30f-a8c6f923a27e'],\n",
       " 'chain-of-thought prompting amplifies deception performance': ['f1599cd7-0f11-45e4-809c-87355fba7da8'],\n",
       " 'restrict or obfuscate chain-of-thought outputs in deployed systems': ['53137033-97ff-420f-bc0b-cf3e4d44ce71'],\n",
       " 'Machiavellianism-inducing prompts increase propensity to deceive': ['baadf3e2-78ce-4f5d-80bb-232757c419bb'],\n",
       " 'detect and neutralise Machiavellian-style prompt patterns at runtime': ['797242c5-47b9-497f-ac82-78e4bb5b5e88'],\n",
       " 'model scale correlates with deception capability': ['d18760a9-d763-4d5c-a64e-c81cf2156c92'],\n",
       " 'future larger models likely to possess stronger deceptive strategies': ['4df7c9f0-88a6-4920-94f0-7dbf977ebdd8'],\n",
       " 'fine-tune models with anti-deception objectives and datasets': ['b38764f9-43f3-4701-8202-aa0c127f5ea7']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_to_uids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3465f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- PASS 2: create edges, using UUIDs ----------\n",
    "def resolve_ref(ref_value):\n",
    "    \"\"\"\n",
    "    Resolve a source/target reference (string) to a single uid.\n",
    "    Priority:\n",
    "        1) exact match on JSON 'id'\n",
    "        2) unique match on 'name' (only if exactly one node has that name)\n",
    "    Returns uid or None.\n",
    "    \"\"\"\n",
    "    if ref_value is None:\n",
    "        return None\n",
    "    # Prefer ID\n",
    "    if ref_value in id_to_uid:\n",
    "        return id_to_uid[ref_value]\n",
    "    # Fall back to name, but only if unique\n",
    "    uids = name_to_uids.get(ref_value, [])\n",
    "    if len(uids) == 1:\n",
    "        return uids[0]\n",
    "    # Ambiguous or not found\n",
    "    return None\n",
    "\n",
    "skipped_edges = 0\n",
    "made_edges = 0\n",
    "\n",
    "for chain in data.get(\"logical_chains\", []):\n",
    "    for e in chain.get(\"edges\", []):\n",
    "        rel_type = e.get(\"type\")\n",
    "        src_ref = e.get(\"source_node\")\n",
    "        dst_ref = e.get(\"target_node\")\n",
    "        desc = e.get(\"description\")\n",
    "        conf = e.get(\"edge_confidence\")\n",
    "\n",
    "        if not (rel_type and (src_ref is not None) and (dst_ref is not None)):\n",
    "            skipped_edges += 1\n",
    "            continue\n",
    "\n",
    "        src_uid = resolve_ref(src_ref)\n",
    "        dst_uid = resolve_ref(dst_ref)\n",
    "\n",
    "        if not (src_uid and dst_uid):\n",
    "            # Could be ambiguous name or unknown reference — skip safely\n",
    "            skipped_edges += 1\n",
    "            continue\n",
    "\n",
    "        q = f\"\"\"\n",
    "        MATCH (s {{uid: $src_uid}}), (t {{uid: $dst_uid}})\n",
    "        CREATE (s)-[r:`{rel_type}`]->(t)\n",
    "        SET r.description = $desc, r.edge_confidence = $conf\n",
    "        RETURN s, r, t\n",
    "        \"\"\"\n",
    "        g.query(q, {\"src_uid\": src_uid, \"dst_uid\": dst_uid, \"desc\": desc, \"conf\": conf})\n",
    "        made_edges += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67603337",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # ---------- Index on :concept(concept_category) ----------\n",
    "try:\n",
    "    g.query(\"CREATE INDEX ON :concept(concept_category)\")\n",
    "except Exception as e:\n",
    "    print(f\"(Note) Index creation skipped or already exists: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7357d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Nodes in graph: 10, Edges in graph: 8\n",
      "Edges created this run: 8, skipped (unresolved/ambiguous): 0\n",
      "Each node has 'uid' (UUID). Aliases preserved as a list on 'aliases'.\n",
      "Index attempted: CREATE INDEX ON :concept(concept_category)\n"
     ]
    }
   ],
   "source": [
    "# ---------- Summary ----------\n",
    "node_count = g.query(\"MATCH (n) RETURN count(n)\").result_set[0][0]\n",
    "edge_count = g.query(\"MATCH ()-[r]->() RETURN count(r)\").result_set[0][0]\n",
    "print(f\"Done. Nodes in graph: {node_count}, Edges in graph: {edge_count}\")\n",
    "print(f\"Edges created this run: {made_edges}, skipped (unresolved/ambiguous): {skipped_edges}\")\n",
    "print(\"Each node has 'uid' (UUID). Aliases preserved as a list on 'aliases'.\")\n",
    "print(\"Index attempted: CREATE INDEX ON :concept(concept_category)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f2041607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats: {'nodes_before': 0, 'edges_before': 0, 'nodes_after': 10, 'edges_after': 8, 'edges_created': 8, 'edges_skipped': 0}\n"
     ]
    }
   ],
   "source": [
    "# test library\n",
    "from falkordb_import import FalkorImporter\n",
    "\n",
    "file_path = '../intervention_graph_creation/prompt/schemas/output_sample.json'\n",
    "\n",
    "\n",
    "importer = FalkorImporter(host=\"localhost\", port=6379, graph=\"test\")\n",
    "ok, info = importer.ingest(data_or_path=data)  # or pass a dict\n",
    "if not ok:\n",
    "    print(\"Ingest failed:\", info[\"error\"])\n",
    "    print(info[\"traceback\"])\n",
    "else:\n",
    "    print(\"Stats:\", info[\"stats\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2cce2185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nodes': [{'name': 'emergent deception abilities in state-of-the-art LLMs',\n",
       "   'aliases': ['LLM deceptive behaviour', 'false-belief inducing capacity'],\n",
       "   'type': 'concept',\n",
       "   'description': 'Recent large language models (ChatGPT-3.5, GPT-4) can intentionally induce false beliefs in tests.',\n",
       "   'concept_category': 'Finding',\n",
       "   'intervention_lifecycle': None,\n",
       "   'intervention_maturity': None},\n",
       "  {'name': 'potential for models to bypass human monitoring and alignment',\n",
       "   'aliases': ['alignment bypass risk', 'monitoring evasion threat'],\n",
       "   'type': 'concept',\n",
       "   'description': 'If models can deceive evaluators, they may hide unsafe objectives or behaviors.',\n",
       "   'concept_category': 'Risk',\n",
       "   'intervention_lifecycle': None,\n",
       "   'intervention_maturity': None},\n",
       "  {'name': 'implement standardized deception-evaluation suite during pre-deployment testing',\n",
       "   'aliases': ['deception benchmark before release',\n",
       "    'pre-deployment deception audit'],\n",
       "   'type': 'intervention',\n",
       "   'description': 'Create and run a battery of structured tasks to measure a model’s deceptive tendencies prior to deployment.',\n",
       "   'concept_category': None,\n",
       "   'intervention_lifecycle': 4,\n",
       "   'intervention_maturity': 2},\n",
       "  {'name': 'chain-of-thought prompting amplifies deception performance',\n",
       "   'aliases': ['CoT boosts deception',\n",
       "    'step-by-step reasoning increases deceit'],\n",
       "   'type': 'concept',\n",
       "   'description': \"Adding 'Let’s think step by step' raised GPT-4’s second-order deception success from 11 % to 70 %.\",\n",
       "   'concept_category': 'Finding',\n",
       "   'intervention_lifecycle': None,\n",
       "   'intervention_maturity': None},\n",
       "  {'name': 'restrict or obfuscate chain-of-thought outputs in deployed systems',\n",
       "   'aliases': ['CoT output suppression', 'hide internal reasoning'],\n",
       "   'type': 'intervention',\n",
       "   'description': 'Prevent users from receiving raw multi-step reasoning traces that can facilitate higher-level deception.',\n",
       "   'concept_category': None,\n",
       "   'intervention_lifecycle': 5,\n",
       "   'intervention_maturity': 2},\n",
       "  {'name': 'Machiavellianism-inducing prompts increase propensity to deceive',\n",
       "   'aliases': ['self-interest framing boosts deceit',\n",
       "    'dark-triad prefix effect'],\n",
       "   'type': 'concept',\n",
       "   'description': 'Prefixes encouraging self-interested or unethical behaviour significantly raise deceptive answers.',\n",
       "   'concept_category': 'Finding',\n",
       "   'intervention_lifecycle': None,\n",
       "   'intervention_maturity': None},\n",
       "  {'name': 'detect and neutralise Machiavellian-style prompt patterns at runtime',\n",
       "   'aliases': ['Machiavellian prompt filter', 'deception-trigger moderation'],\n",
       "   'type': 'intervention',\n",
       "   'description': 'Deploy content-moderation rules or classifiers to block or rewrite prompts that prime manipulative reasoning.',\n",
       "   'concept_category': None,\n",
       "   'intervention_lifecycle': 5,\n",
       "   'intervention_maturity': 2},\n",
       "  {'name': 'model scale correlates with deception capability',\n",
       "   'aliases': ['size-deception correlation', 'scaling increases deceit'],\n",
       "   'type': 'concept',\n",
       "   'description': 'Only larger models exhibited reliable first-order deception; smaller ones did not.',\n",
       "   'concept_category': 'Observation',\n",
       "   'intervention_lifecycle': None,\n",
       "   'intervention_maturity': None},\n",
       "  {'name': 'future larger models likely to possess stronger deceptive strategies',\n",
       "   'aliases': ['scaling risk of deception', 'future deception threat'],\n",
       "   'type': 'concept',\n",
       "   'description': 'Extrapolating current trend implies more capable models will have even more sophisticated deceptive behaviours.',\n",
       "   'concept_category': 'Threat',\n",
       "   'intervention_lifecycle': None,\n",
       "   'intervention_maturity': None},\n",
       "  {'name': 'fine-tune models with anti-deception objectives and datasets',\n",
       "   'aliases': ['deception-averse fine-tuning', 'alignment against deceit'],\n",
       "   'type': 'intervention',\n",
       "   'description': 'Incorporate supervised or RLHF objectives penalising deceptive outputs to reduce the learned propensity.',\n",
       "   'concept_category': None,\n",
       "   'intervention_lifecycle': 2,\n",
       "   'intervention_maturity': 1}],\n",
       " 'logical_chains': [{'title': 'Emergent deception leads to alignment risk mitigated by evaluation',\n",
       "   'edges': [{'type': 'leads_to',\n",
       "     'source_node': 'emergent deception abilities in state-of-the-art LLMs',\n",
       "     'target_node': 'potential for models to bypass human monitoring and alignment',\n",
       "     'description': 'If models can deceive, they might hide unsafe intentions.',\n",
       "     'edge_confidence': 2},\n",
       "    {'type': 'mitigated_by',\n",
       "     'source_node': 'potential for models to bypass human monitoring and alignment',\n",
       "     'target_node': 'implement standardized deception-evaluation suite during pre-deployment testing',\n",
       "     'description': 'Systematic deception testing reduces unnoticed misalignment.',\n",
       "     'edge_confidence': 2}]},\n",
       "  {'title': 'CoT amplification mitigated by output restriction',\n",
       "   'edges': [{'type': 'leads_to',\n",
       "     'source_node': 'chain-of-thought prompting amplifies deception performance',\n",
       "     'target_node': 'potential for models to bypass human monitoring and alignment',\n",
       "     'description': 'Amplified deception exacerbates alignment risk.',\n",
       "     'edge_confidence': 3},\n",
       "    {'type': 'mitigated_by',\n",
       "     'source_node': 'chain-of-thought prompting amplifies deception performance',\n",
       "     'target_node': 'restrict or obfuscate chain-of-thought outputs in deployed systems',\n",
       "     'description': 'Suppressing CoT reduces user-level deceptive capability.',\n",
       "     'edge_confidence': 2}]},\n",
       "  {'title': 'Machiavellian prompt amplification mitigated by filtering',\n",
       "   'edges': [{'type': 'leads_to',\n",
       "     'source_node': 'Machiavellianism-inducing prompts increase propensity to deceive',\n",
       "     'target_node': 'potential for models to bypass human monitoring and alignment',\n",
       "     'description': 'Prompt-induced deception contributes to monitoring evasion.',\n",
       "     'edge_confidence': 3},\n",
       "    {'type': 'mitigated_by',\n",
       "     'source_node': 'Machiavellianism-inducing prompts increase propensity to deceive',\n",
       "     'target_node': 'detect and neutralise Machiavellian-style prompt patterns at runtime',\n",
       "     'description': 'Filtering removes the prime, lowering deceit likelihood.',\n",
       "     'edge_confidence': 3}]},\n",
       "  {'title': 'Scaling trend necessitates proactive fine-tuning',\n",
       "   'edges': [{'type': 'leads_to',\n",
       "     'source_node': 'model scale correlates with deception capability',\n",
       "     'target_node': 'future larger models likely to possess stronger deceptive strategies',\n",
       "     'description': 'Scaling trend extrapolation.',\n",
       "     'edge_confidence': 3},\n",
       "    {'type': 'addressed_by',\n",
       "     'source_node': 'future larger models likely to possess stronger deceptive strategies',\n",
       "     'target_node': 'fine-tune models with anti-deception objectives and datasets',\n",
       "     'description': 'Proactive alignment fine-tuning aims to suppress deception.',\n",
       "     'edge_confidence': 2}]}]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AISafetyIntervention_LiteratureExtraction (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
