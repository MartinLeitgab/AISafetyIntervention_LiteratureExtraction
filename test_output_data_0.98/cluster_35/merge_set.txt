[
    {
        "nodes": [
            104,
            101
        ],
        "rationale": "Both nodes describe fundamental, closely-related data-distribution problems that arise when an RL agent gathers experience using its current (on-policy) policy. Node 101 focuses on the strong temporal correlations between consecutive transitions, while Node 104 focuses on the gradual drift of the state-action distribution as the policy improves (non-stationarity). In practice these two phenomena are inseparable facets of the same underlying issue: the on-policy data stream is neither i.i.d. nor stationary, which complicates function-approximation-based learning and evaluation. Merging them avoids redundancy and yields a single concept that cleanly captures the shared root cause and its implications for RL stability and generalisation.",
        "parameters": {
            "name": "nonstationary_and_correlated_onpolicy_data",
            "type": "concept",
            "description": "In reinforcement learning, trajectories collected under the agent\u2019s current policy exhibit two coupled problems: (1) strong temporal correlation between successive transitions and (2) a non-stationary state\u2013action distribution that drifts as the policy evolves. Together these distributional issues violate the i.i.d. assumptions of most statistical learning methods, leading to biased gradient estimates, slower learning, and potential instability."
        }
    }
]