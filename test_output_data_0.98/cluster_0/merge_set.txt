[
    {
        "nodes": [
            18,
            7
        ],
        "rationale": "Node 7 explains the advantage-weighted regression (AWR) learning objective, while Node 18 specifies applying exactly that objective as the practical PHF intervention. Because they describe the same underlying method at different abstraction levels, merging eliminates duplication without loss of meaning and lets a single node capture both the theoretical objective and its use in practice.",
        "parameters": {
            "name": "Advantage-Weighted Regression PHF (AWR)",
            "type": "method",
            "description": "A pre-training with human-feedback (PHF) technique that subtracts a learned value baseline from reward signals and weights each token\u2019s log-likelihood by the resulting advantage, thereby biasing language-model updates toward higher-valued text."
        }
    },
    {
        "nodes": [
            8,
            19
        ],
        "rationale": "Both nodes describe reward-weighted regression (RWR) during pre-training: Node 8 focuses on the objective formulation, whereas Node 19 frames it as the concrete intervention. They are different verbalisations of the same idea, so unifying them removes redundancy and keeps all associated edges intact.",
        "parameters": {
            "name": "Reward-Weighted Regression PHF (RWR)",
            "type": "method",
            "description": "A PHF approach where each segment\u2019s likelihood is multiplied by exp(reward / \u03b2), smoothly interpolating between maximum-likelihood estimation and strong reward optimisation to favour higher-reward examples during language-model pre-training."
        }
    },
    {
        "nodes": [
            9,
            20
        ],
        "rationale": "Node 9 introduces the unlikelihood loss that penalises low-reward tokens, while Node 20 simply instructs to pre-train using that same unlikelihood loss. They are substantively identical; merging yields a single, clearer representation of the unlikelihood PHF technique.",
        "parameters": {
            "name": "Unlikelihood Loss PHF",
            "type": "method",
            "description": "A PHF technique that replaces standard likelihood with an unlikelihood term on segments whose reward falls below a threshold, actively discouraging the model from producing undesirable continuations."
        }
    },
    {
        "nodes": [
            10,
            21
        ],
        "rationale": "Reward-threshold dataset filtering (Node 10) and the intervention that implements it (Node 21) are two descriptions of the exact same mechanism\u2014discarding low-reward documents before continuing MLE training. Consolidating them removes duplicative nodes while preserving all information.",
        "parameters": {
            "name": "Reward-Threshold Filtering PHF",
            "type": "method",
            "description": "A PHF strategy that excludes documents whose average reward falls below a preset percentile and then performs standard MLE training on the remaining high-quality corpus."
        }
    },
    {
        "nodes": [
            11,
            22
        ],
        "rationale": "Node 11 outlines conditional training with reward-based control tokens, and Node 22 prescribes using that very approach during PHF. Since they refer to the same conditional-training mechanism, merging them streamlines the graph and concentrates evidence and motivations in one place.",
        "parameters": {
            "name": "Conditional-Token PHF (Control-Token Training)",
            "type": "method",
            "description": "During PHF, each training segment is prepended with a control token (<|good|>/<|bad|>) determined by a reward threshold; at inference time the model is prompted with <|good|> to steer generation toward preferred behaviour, thereby achieving alignment while retaining data diversity."
        }
    }
]