[
    {
        "nodes": [
            104,
            101
        ],
        "rationale": "Both nodes describe closely related facets of the same underlying phenomenon: the data generated while an RL agent follows (and continually updates) its policy is not i.i.d.  Node 101 focuses on temporal correlations between successive on-policy observations, while Node 104 highlights the distributional drift that occurs as the policy changes.  In practice these two issues co-occur (correlated samples + shifting distribution) and are typically discussed together under the umbrella of \u201cnon-i.i.d. on-policy data.\u201d  They have identical type (concept), no important conflicting metadata, and merging them avoids redundancy while preserving all semantics.",
        "parameters": {
            "name": "non_iid_onpolicy_data",
            "type": "concept",
            "description": "During reinforcement learning, trajectories collected from the continually-updated policy are both temporally correlated and subject to distributional drift.  These non-i.i.d. characteristics complicate estimation, hinder generalisation, and can induce instability in learning algorithms."
        }
    }
]