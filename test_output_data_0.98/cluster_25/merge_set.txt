[
    {
        "nodes": [
            104,
            101
        ],
        "rationale": "Both nodes describe closely related statistical properties of data generated by an agent following its current (on-policy) policy. Node 101 focuses on short-range temporal correlations, while Node 104 highlights the longer-range non-stationarity that arises as the policy itself changes. In practice, these are two facets of the same underlying issue: on-policy experience violates the i.i.d. assumption, leading to learning instabilities and motivating identical families of mitigations (e.g., experience replay, batch-RL, importance weighting). Because they co-occur, share causes (policy evolution) and have the same downstream implications, they are best treated as a single concept about non-i.i.d. on-policy data.",
        "parameters": {
            "name": "non_iid_onpolicy_data",
            "type": "concept",
            "description": "Data collected from an agent acting under its current policy is both temporally correlated and non-stationary because the policy evolves during learning. This violation of the i.i.d. assumption can degrade sample efficiency, bias value estimates, and destabilize reinforcement-learning algorithms."
        }
    }
]