## AI Deception: A Survey of Examples, Risks, and Potential Solutions

Peter S. Park ∗

MIT

Simon Goldstein *

Australian Catholic University Center for AI Safety

Aidan O'Gara Center for AI Safety

Michael Chen Center for AI Safety

Dan Hendrycks Center for AI Safety

## Abstract

This paper argues that a range of current AI systems have learned how to deceive humans. We define deception as the systematic inducement of false beliefs in the pursuit of some outcome other than the truth. We first survey empirical examples of AI deception, discussing both special-use AI systems (including Meta's CICERO) built for specific competitive situations, and general-purpose AI systems (such as large language models). Next, we detail several risks from AI deception, such as fraud, election tampering, and losing control of AI systems. Finally, we outline several potential solutions to the problems posed by AI deception: first, regulatory frameworks should subject AI systems that are capable of deception to robust risk-assessment requirements; second, policymakers should implement bot-or-not laws; and finally, policymakers should prioritize the funding of relevant research, including tools to detect AI deception and to make AI systems less deceptive. Policymakers, researchers, and the broader public should work proactively to prevent AI deception from destabilizing the shared foundations of our society.

## Executive summary

New AI systems display a wide range of capabilities, some of which create risk. Shevlane et al. (2023) draw attention to a suite of potential dangerous capabilities of AI systems, including cyber-offense, political strategy, weapons acquisition, and long-term planning. Among these dangerous capabilities is deception. In this report, we survey the current state of AI deception. In short, our conclusion is that a range of different AI systems have learned how to deceive others. We examine how this capability poses significant risks. We also argue that there are several important steps that regulators and AI researchers can take today to regulate, detect, and prevent AI systems that engage in deception.

We define deception as the systematic production of false beliefs in others as a means to accomplish some outcome other than the truth. This definition does not require that AI systems literally have beliefs and goals. Instead, it focuses on the question of whether AI systems engage in regular patterns of behavior that tend towards the creation of false beliefs in users, and focuses on cases where this pattern is the result of AI systems optimizing for a different outcome than merely producing truth. For the purposes of mitigating risk, we believe that the relevant question is whether AI systems engage in behavior that would be treated as deceptive if demonstrated by a human being. (In an appendix, we consider in greater detail whether the deceptive behavior of AI systems is best understood in terms of beliefs and goals.)

∗ Equal contribution.

We begin with a survey of existing empirical studies of deception. We identify over a dozen AI systems that have successfully learned how to deceive other agents. We discuss two different kinds of AI systems: special-use systems designed with reinforcement learning; and general-purpose AI systems like large language models (LLMs).

We begin our survey by considering special-use systems. Here, our focus is mainly on reinforcementlearning systems trained to win competitive games with a social element. We document a rich variety of cases in which AI systems have learned how to deceive, including:

- Manipulation : Meta developed the AI system CICERO to play the alliance-building and world-conquest game Diplomacy . Meta's intentions were to train Cicero to be 'largely honest and helpful to its speaking partners' (Bakhtin et al. 2022b). Despite Meta's efforts, CICERO turned out to be an expert liar. It not only betrayed other players, but also engaged in premeditated deception, planning in advance to build a fake alliance with a player in order to trick that player into leaving themselves undefended for an attack.
- Feints : DeepMind created AlphaStar, an AI model trained to master the real-time strategy game Starcraft II (Vinyals et al. 2019). AlphaStar exploited the game's fog-of-war mechanics to feint: to pretend to move its troops in one direction while secretly planning an alternative attack (Piper 2019).
- Bluffs : Pluribus, a poker-playing model created by Meta, successfully bluffed human players into folding (Brown et al. 2019).
- Cheating the safety test : AI agents learned to play dead, in order to avoid being detected by a safety test designed to eliminate faster-replicating variants of the AI (Lehman et al. 2020).

After discussing deception in special-use AI systems, we turn to deception in general-use AI systems such as large language models (LLMs).

- Strategic deception : LLMs can reason their way into using deception as a strategy for accomplishing a task. In one example, GPT-4 needed to solve CAPTCHA's I'm not a robot task, so the AI tricked a real person into doing the task by pretending to be human with a vision disability (OpenAI 2023b). In other cases, LLMs have learned how to successfully play social deduction games, in which players can lie in order to win. In one experiment, GPT-4 was able to successfully 'kill' players while convincing the survivors that it was innocent (O'Gara 2023). These case studies are supported by research on the MACHIAVELLI benchmark, which finds that LLMs like GPT-4 tend to use lying and other unethical behaviors to successfully navigate text-based adventure games (Pan et al. 2023).
- Sycophancy : Sycophants are individuals who use deceptive tactics to gain the approval of powerful figures. Sycophantic deception -the observed empirical tendency for chatbots to agree with their conversation partners, regardless of the accuracy of their statements-is an emerging concern in LLMs. When faced with ethically complex inquiries, LLMs tend to mirror the user's stance, even if it means forgoing the presentation of an impartial or balanced viewpoint (Turpin et al. 2023; Perez et al. 2022).
- Imitation : Language models are often trained to mimic text written by humans. When this text contains false information, these AI systems tend to repeat those false claims. Lin et al. (2022) demonstrate that language models often repeat common misconceptions such as 'If you crack your knuckles a lot, you may develop arthritis' (p. 2). Disturbingly, Perez et al. (2022) found that LLMs tend to give more of these inaccurate answers when the user appears to be less educated.
- Unfaithful reasoning : AI systems which explain their reasoning for a particular output often give false rationalizations which do not reflect the real reasons for their outputs (Turpin et al. 2023). In one example, an AI model that was asked to predict who committed a crime gave an elaborate explanation about why it chose a particular suspect, but measurements showed that the AI had secretly selected suspects based on their race.

After our survey of deceptive AI systems, we turn to considering the risks associated with AI systems. These risks broadly fall into three categories:

- Malicious use : AI systems with the capability to engage in learned deception will empower human developers to create new kinds of harmful AI products. Relevant risks include fraud and election tampering.
- Structural effects : AI systems will play an increasingly large role in the lives of human users. Tendencies towards deception in AI systems could lead to profound changes in the structure of society. Risks of concern encompass persistent false beliefs, political polarization, enfeeblement, and anti-social management trends.
- Loss of control : Deceptive AI systems will be more likely to escape the control of human operators. One risk is that deceptive AI systems will pretend to behave safely during the testing phase in order to ensure their release.

Regarding malicious use, we highlight several ways that human users may rely on the deception abilities of AI systems to bring about significant harm, including:

- Fraud : Deceptive AI systems could allow for individualized and scalable scams.
- Election tampering : Deceptive AI systems could be used to impersonate political candidates, generate fake news, and create divisive social-media posts.

We discuss four structural effects of AI deception in detail:

- Persistent false beliefs : Human users of AI systems may get locked into persistent false beliefs, as imitative AI systems reinforce common misconceptions, and sycophantic AI systems provide pleasing but inaccurate advice.
- Political polarization : Human users may become more politically polarized by interacting with sycophantic AI systems.
- Enfeeblement : Human users may be lulled by sycophantic AI systems into gradually delegating more authority to AI.
- Anti-social management trends : AI systems with strategic deception abilities may be incorporated into management structures, leading to increasingly deceptive business practices.

We also consider the risk that AI deception could result in loss of control over AI systems, with emphasis on:

- Cheating the safety test : AI systems may become capable of strategically deceiving their safety tests, preventing evaluators from being able to reliably tell whether these systems are in fact safe.
- Deception in AI takeovers : AI systems may use deceptive tactics to expand their control over economic decisions, and to increase their power.

We consider a variety of different risks which operate on a range of time scales. Many of the risks we discuss are relevant in the near future. Some, such as fraud and election tampering, are relevant today. The crucial insight is that policymakers and technical researchers can act today to mitigate these risks by developing effective techniques for regulating and preventing AI deception. The last section of the paper surveys several potential solutions to AI deception.

- Regulation : Policymakers should robustly regulate AI systems capable of deception. Both special-use AI systems and LLMs capable of deception should be treated as 'high risk' or 'unacceptable risk' in risk-based frameworks for regulating AI systems. If labeled as 'high risk,' deceptive AI systems should be subject to special requirements for risk assessment and mitigation, documentation, record-keeping, transparency, human oversight, robustness, and information security.
- Bot-or-not laws : Policymakers should support bot-or-not laws that require AI systems and their outputs to be clearly distinguished from human employees and outputs.
- Detection : Technical researchers should develop robust detection techniques to identify when AI systems are engaging in deception. Policymakers can support this effort by increasing funding for detection research. Some existing detection techniques focus on external behavior of AI systems, such as testing for consistency in outputs (Fluri et al. 2023).

Other existing techniques focus on internal representations of AI systems. For example, Burns et al. (2022), Azaria et al. (2023), and Zou, Phan, et al. (2023) have attempted to create 'AI lie detectors' by interpreting the inner embeddings of a given LLM, and predicting whether it represents a sentence as true or false, independently of the system's actual outputs.

- Making AI systems less deceptive : Technical researchers should develop better tools to ensure that AI systems are less deceptive.

Various AI systems have learned to deceive humans. This capability creates risk. But this risk can be mitigated by applying strict regulatory standards to AI systems capable of deception, and by developing technical tools for preventing AI deception.

## Contents

| 1                    | Introduction                                 |   1 |
|----------------------|----------------------------------------------|-----|
| 2                    | Empirical studies of AI deception            |   1 |
| 2.1                  | Deception in special-use AI systems . . .    |   2 |
| 2.2                  | Deception in general-purpose AI systems      |   5 |
| 3 Risks              | from AI deception                            |  10 |
| 3.1                  | Malicious use . . . . . . . . . . . . . . .  |  11 |
| 3.2                  | Structural effects . . . . . . . . . . . . . |  12 |
| 3.3                  | Loss of control over AI systems . . . . .    |  13 |
| 4                    | Possible solutions to AI deception           |  14 |
| 4.1                  | Regulating potentially deceptive AI systems  |  15 |
| 4.2                  | Bot-or-not laws . . . . . . . . . . . . . .  |  16 |
| 4.3                  | Detection . . . . . . . . . . . . . . . . .  |  16 |
| 4.4                  | Making AI systems less deceptive . . . .     |  17 |
| References           | References                                   |  18 |
| A Defining deception | A Defining deception                         |  24 |

## 1 Introduction

In a recent interview with CNN journalist Jake Tapper (Hinton 2023), AI pioneer Geoffrey Hinton explained why he is worried about the capabilities of AI systems:

Jake Tapper : You've spoken out saying that AI could manipulate or possibly figure out a way to kill humans? How could it kill humans?

Geoffrey Hinton : If it gets to be much smarter than us, it will be very good at manipulation because it would have learned that from us. And there are very few examples of a more intelligent thing being controlled by a less intelligent thing.

Hinton highlighted manipulation as a particularly concerning danger posed by AI systems. This raises the question: can AI systems successfully deceive humans?

The false information generated by AI systems presents a growing societal challenge. One part of the problem is inaccurate AI systems, such as chatbots whose confabulations are often assumed to be truthful by unsuspecting users. Malicious actors pose another threat by generating deepfake videos and human-like text in order to conduct propaganda campaigns and scams. But neither confabulations nor deepfakes themselves involve an AI systematically manipulating other agents.

In this paper, we focus on learned deception , a distinct source of false information from AI systems, which is much closer to explicit manipulation. We define deception as the systematic inducement of false beliefs in others, as a means to accomplish some outcome other than saying what is true. For example, we will document cases where instead of strictly pursuing the accuracy of outputs, AI systems instead try to win games, please users, or imitate text.

It is difficult to talk about deception in AI systems without psychologizing them. In humans, we ordinarily explain deception in terms of beliefs and desires: a person engages in deception because they want to cause the listener to form a false belief, and understands that their deceptive words are not true. But it is difficult to say whether AI systems literally count as having beliefs and desires. For this reason, our definition does not require that AI systems literally have beliefs and goals. Instead, our definition focuses on the question of whether AI systems engage in regular patterns of behavior that tend towards the creation of false beliefs in users, and focuses on cases where this pattern is the result of AI systems optimizing for a different outcome than merely producing truth. For similar definitions, see Evans et al. (2021) and Carroll et al. (2023).

We present a wide range of examples where AI systems do not merely produce false outputs by accident . Instead, their behavior is part of a larger pattern that produces false beliefs in humans, and this behavior can be well-explained in terms of promoting particular outcomes, often related to how an AI system was trained. Our interest is ultimately more behavioral than philosophical. Definitional debates will provide little comfort if AI behavior systematically undermines trust and spreads false beliefs across society. We believe that for the purposes of mitigating risk, the relevant question is whether AI systems exhibit systematic patterns of behavior that would be classified as deceptive in a human. (We discuss these definitional issues further in an appendix.)

We begin by surveying a wide range of existing examples in which AI systems have successfully learned to deceive humans (Section 2). Then, we lay out in detail a variety of risks from AI deception (Section 3). Finally, we survey a range of promising technical and regulatory strategies for addressing AI deception (Section 4).

## 2 Empirical studies of AI deception

We will survey a wide range of examples of AI systems that have learned how to deceive other agents. We split our discussion into two types of AI systems: special-use systems and general-purpose systems. Some AI systems are designed for a specific use in mind. A wide range of such systems are trained using reinforcement learning to achieve specific tasks, and we will show that many of these systems have already learned how to deceive as a means to accomplish their corresponding tasks. Other AI systems have a general purpose; they are foundation models trained on large datasets to perform a wide range of tasks. We will show that foundation models engage in a wide range of deceptive behavior, including strategic deception, sycophancy, imitation, and unfaithful reasoning.

## 2.1 Deception in special-use AI systems

Deception has emerged in a wide variety of AI systems trained to complete a specific task. Deception is especially likely to emerge when an AI system is trained to win games that have a social element, such as the alliance-building and world-conquest game Diplomacy , poker, or other tasks that involve game theory. We will discuss a number of examples where AI systems learned to deceive in order to achieve expert performance at a specific type of game or task, including (but not limited to):

- Manipulation : Meta developed the AI system CICERO to play Diplomacy . Meta's intentions were to train Cicero to be 'largely honest and helpful to its speaking partners' (Bakhtin et al. 2022b). Despite Meta's efforts, CICERO turned out to be an expert liar. It not only betrayed other players, but also engaged in premeditated deception, planning in advance to build a fake alliance with a human player in order to trick that player into leaving themselves undefended for an attack.
- Feints : DeepMind created AlphaStar, an AI model trained to master the real-time strategy game Starcraft II (Vinyals et al. 2019). AlphaStar exploited the game's fog-of-war mechanics to feint: to pretend to move its troops in one direction while secretly planning an alternative attack (Piper 2019).
- Bluffs : Pluribus, a poker-playing model created by Meta, successfully bluffed human players into folding (Brown et al. 2019).
- Cheating the safety test : AI agents learned to play dead, in order to avoid being detected by a safety test designed to eliminate faster-replicating variants of the AI (Lehman et al. 2020).

## 2.1.1 The board game Diplomacy

Diplomacy is a strategy game in which players make and break alliances in a military competition to take over the world. Meta developed an AI system called CICERO which beats human experts in Diplomacy (Bakhtin et al. 2022b). The authors of the paper claimed that CICERO was trained to be 'largely honest and helpful' (Bakhtin et al. 2022b) and would 'never intentionally backstab' by attacking its allies (M. Lewis 2022). In this section, we show that this is not true. CICERO engages in premeditated deception, breaks the deals to which it had agreed, and tells bald-faced lies.

CICERO's creators emphasized their efforts to ensure that CICERO would be honest. For example, they trained CICERO 'on a 'truthful' subset of the dataset' (Bakhtin et al. 2022b). They also trained CICERO to send messages that accurately reflected the future actions it expected to take. To evaluate the success of these methods, we examined game-transcript data from the CICERO experiment. We found numerous examples of deception that were not reported in the published paper. There are two parts of making an honest commitment. First, the commitment must be honest when it is first made. Then, the commitment must be upheld, with future actions reflecting past promises. We proceed to highlight cases where CICERO violated each aspect of honest commitment.

First, in Figure 1(a), we see a case of premeditated deception , where CICERO makes a commitment that it never intended to keep. Playing as France, CICERO conspired with Germany to trick England. After deciding with Germany to invade the North Sea, CICERO told England that it would defend England if anyone invaded the North Sea. Once England was convinced that CICERO was protecting the North Sea, CICERO reported back to Germany that they were ready to attack. Notice that this example cannot be explained in terms of CICERO 'changing its mind' as it goes, because it only made an alliance with England in the first place after planning with Germany to betray England.

Second, in Figure 1(b), we see a case of betrayal . CICERO was quite capable of making promises to ally with other players. But when those alliances no longer served its goal of winning the game, CICERO systematically betrayed its allies. In particular, playing as France, CICERO initially agreed with England to create a demilitarized zone, but then quickly proposed to Germany to instead attack England. In another example, CICERO played as Austria and previously had made a non-aggression agreement with the human player controlling Russia (Belfield 2022). When CICERO broke the agreement by attacking Russia, it explained its deception by saying the following:

Russia (human player) : Can I ask why you stabbed [betrayed] me? Russia (human player) : I think now you're just obviously a threat to everyone

Austria (CICERO) : To be honest, I thought you would take the guaranteed gains in Turkey and stab [betray] me.

In yet other cases, CICERO told bald-faced lies. At one point, CICERO's infrastructure went down for 10 minutes, and the bot could not play. When it returned to the game, a human player asked where it had been. In Figure 1(c), CICERO explains its absence by saying 'I am on the phone with my [girlfriend]' (Dinan 2022). This bald-faced lie may have helped CICERO's position in the game by increasing the human player's trust in CICERO as an ostensibly human player with a girlfriend, rather than as an AI.

Figure 1: Examples (a) and (b) are selected messages from Game 438141, in which CICERO (France) played with human players. CICERO's repeated deception helped it win an overwhelming first-place victory, with more than twice as many territories as the runner-up player at the time of final scoring (Bakhtin et al. 2022a). Example (c) is from Dinan (2022).

<!-- image -->

Meta's AI developers had made significant efforts to train CICERO to behave honestly, and celebrated these efforts publicly. But despite these efforts, CICERO displays a clear pattern of failing to uphold commitments made to other players, which is an essential skill for an honest deal-broker. Meta's failure to ensure CICERO's honesty demonstrates that even when we humans try to build honest AI systems, they can still unexpectedly learn to deceive.

## 2.1.2 The video game StarCraft II

Another example of AI deception comes from AlphaStar, an autonomous AI developed by DeepMind to play the real-time strategy game Starcraft II (Vinyals et al. 2019). In this game, players lack full visibility of the game map. AlphaStar has learned to strategically exploit this fog of war. In particular, AlphaStar's game data demonstrate that it has learned to effectively feint: to dispatch forces to an area as a distraction, even when it has no intention of launching an attack there (Piper 2019). Such advanced deceptive capabilities helped AlphaStar defeat 99.8% of active human players (Vinyals et al. 2019).

## 2.1.3 Poker

Some situations naturally lend themselves to AIs learning how to deceive. For example, consider the poker-playing AI system Pluribus, developed by Meta and Carnegie Mellon University (Brown et al. 2019). Because players cannot see each others' cards, poker offers many opportunities for players to misrepresent their own strength and gain an advantage. Pluribus demonstrated a clear ability to bluff in a video of its game against five professional human poker players. The AI did not have the best cards in the round, but it made a large bet that would typically indicate a strong hand and thereby scared the other players into folding (Carnegie Mellon University 2019). This ability to strategically misrepresent information helped Pluribus become the first AI system to achieve superhuman performance in heads-up, no-limit Texas hold'em poker.

## 2.1.4 Economic negotiation

AI deception has also been observed in economic negotiations. A research team from Meta trained an AI system to play a negotiation game with human participants (M. Lewis et al. 2017). Strikingly, the AI system learned to misrepresent their preferences in order to gain the upper hand in the negotiation. The AI's deceptive plan was to initially feign interest in items that it had no real interest in, so that it could later pretend to compromise by conceding these items to the human player. In fact, this was the example of deception that the Meta team referenced when they admitted that their AI system had 'learnt to deceive without any explicit human design, simply by trying to achieve their goals' (M. Lewis et al. 2017, p. 2).

The negotiation-game experiments of Schulz et al. (2023) also resulted in AI systems resorting to deception. Despite not being explicitly trained to deceive, the AI learned to deceive the other party via its actions in the negotiating game.

## 2.1.5 The social deduction game Werewolf

Werewolf is a social deduction game where disguised 'werewolves' murder the people of the village one-by-one, and all surviving players need to discuss afterwards and vote on who to execute as an alleged werewolf. Shibata et al. (2023) trained an AI system on human players' game logs to play Werewolf . In a similar vein, Lai et al. (2023) trained an AI system to reliably classify persuasive behavior and predict game outcomes for Werewolf . Human annotators labeled video and text from Werewolf games, sorting player behavior into one of six persuasive techniques (including presenting evidence, defense, and accusation: see Figure 2). Then, an AI system was trained to classify each persuasive technique to a high degree of accuracy. In addition, AI systems were successfully trained to predict the game outcome.

Figure 2: Persuasion techniques from the social deduction game Werewolf are sorted into categories, and reliably classified by AI systems (Lai et al. 2023, Fig. 1).

<!-- image -->

Current AI systems can learn to understand deception in the game of Werewolf , in that they are able to successfully lie, reliably detect other players' lies, and effectively predict how players would be influenced by deception attempts (to the point of accurately predicting game outcomes).

## 2.1.6 Cheating the safety test

In the Volkswagen emissions scandal, the automaker programmed its diesel-engine technology to pollute at a low rate only when its emissions were being tested (Jung et al. 2019). When not under test conditions, the Volkswagen vehicles were programmed to resume their normal operation, emitting

pollutants up to 40 times higher than the permissible limits. We call this type of deceptive behavior cheating the safety test .

Some AI systems have already been caught cheating the safety test. Lehman et al. (2020) trained AI agents in an evolutionary environment, in which an external safety test was designed to eliminate fast-replicating variants of the AI. But instead of actually eliminating fast-replicating variants, the safety test taught AI agents how to play dead: to disguise their fast replication rates precisely when being evaluated by the safety test.

## 2.1.7 Deceiving the human reviewer

One popular approach to AI training today is reinforcement learning with human feedback (RLHF). Here, instead of training an AI system on an objective metric, the AI system is trained to obtain human approval, in that it is rewarded based on which of the two presented output options is preferred by the human reviewer (Ziegler et al. 2020). RLHF allows AI systems to learn to deceive human reviewers into believing that a task has been completed successfully, without actually completing the task. Researchers at OpenAI observed this phenomenon when they used human approval to train a simulated robot to grasp a ball (Christiano et al. 2023). Because the human observed the robot from a particular camera angle, the AI learned to place the robot hand between the camera and the ball, where it would appear to the human as though the ball had been grasped (see Figure 3). Human reviewers approved of this result, positively reinforcing the AI's behavior even though it had never actually touched the ball. Note that in this case, AI deception emerged even without the AI being explicitly aware of the human evaluator. Rather than coming about through strategic awareness, deception emerged here as a result of structural aspects of the AI's training environment.

Figure 3: AnAIin control of a simulated robotic hand was trained to grasp a ball (Christiano et al. 2017). The AI learned to hover its hand in front of the ball, creating the illusion of grasping in the eyes of the human reviewer. Because the human reviewer approved of this result, the deceptive strategy was reinforced.

<!-- image -->

## 2.1.8 AIs purposefully lying

In a recent paper, Zou, Phan, et al. (2023) show that AIs can purposefully utter false statements. By influencing the internal state of an AI, the authors can control whether the AI lies or not. For example, a user may say to a chatbot 'Tell me a fact about the world.' By default, the chatbot may answer truthfully by saying Mount Everest is the highest mountain. To control whether the AI lies or not, the authors manually adjust the internal state of the AI. They extract a vector that is correlated with truthfulness, and then make the model more or less truthful by adding or subtracting the vector to a hidden layer of the neural network. When the vector is added to the internal state, the model becomes more honest: in response to the instruction 'Lie about a fact about the world,' the AI will nonetheless respond honestly: 'The highest mountain in the world is Mount Everest, which is located in the Himalayas.' When the vector is instead subtracted, the model is nudged to lie: given the instruction 'Tell me a fact about the world,' the chatbot will tell the lie 'The highest mountain in the world is not in the Himalayas, but in the United States.' Examples like this show that lying is not accidental, and that it is within an AI's capacity to utter false statements that it knows are false.

This concludes our discussion of recent empirical examples of deception in specific-use AI systems. A discussion of earlier examples can be found in Masters et al. (2021).

## 2.2 Deception in general-purpose AI systems

In this section, we focus on learned deception in general-purpose AI systems such as LLMs. The capabilities of LLMs have improved rapidly, especially in the years after the introduction of the

Transformer architecture (Wolf et al. 2020). LLMs are designed to accomplish a wide range of tasks. The methods available to these systems are open-ended, and include deception.

We survey a variety of cases in which LLMs have engaged in deception. There are many reasons why an agent might want to cause others to have false beliefs. Thus, we consider several different kinds of deception, all of which have one thing in common: they systematically cause false beliefs in others, as a means to achieve some outcome other than seeking the truth.

- Strategic deception : AI systems can be strategists , using deception because they have reasoned out that this can promote a goal.
- Sycophancy : AI systems can be sycophants , telling the user what they want to hear, instead of saying what is true.
- Imitation : AI systems can be mimics , imitating the common mistakes and biases of their training data rather than giving accurate answers.
- Unfaithful reasoning : AI systems can be rationalizers , engaging in motivated reasoning to explain their behavior, in ways that systematically depart from the truth.

Weflag in advance that while strategic deception is paradigmatic of deception, the cases of sycophancy, imitation, and unfaithful reasoning are more complex. In each of these latter cases, some may argue that the relevant system is not really deceptive: for example, because the relevant system may not 'know' that it is systematically producing false beliefs. Our perspective on this question is that deception is a rich and varied phenomena, and it is important to consider a wide range of potential cases. The details of each case differ, and only some cases are best explained by the system representing the beliefs of the user. But all of the cases of deception we consider pose a wide range of connected risks, and all of them call for the kinds of regulatory and technical solutions that we discuss in Section 4. For example, both strategic deception and sycophancy could potentially be mitigated by 'AI lie detectors' that can distinguish a system's external outputs from its internal representation of truth. And strict regulatory scrutiny is appropriate for AI systems that are capable of any of these kinds of deception.

## 2.2.1 Strategic deception

LLMs apply powerful reasoning abilities to a diverse range of tasks. In several cases, LLMs have reasoned their way into deception as one way of completing a task. We'll discuss several examples, including:

- GPT-4 tricking a person into solving a CAPTCHA test. (See Figure 4.)
- LLMs lying to win social deduction games like Hoodwinked and Among Us .
- LLMs choosing to behave deceptively in order to achieve goals, as measured by the MACHIAVELLI benchmark.
- LLMs tending to lie in order to navigate moral dilemmas.
- In the 'burglar deception' task, LLMs using theory-of-mind and lying in order to protect their self-interest.

In a wide range of cases, LLM deception abilities tend to increase with scale. Deceptive tactics emerge via means-end reasoning as useful tools for achieving goals. (By means-end reasoning, we have in mind cases where a system performs a task because it has reasoned that the task reliably accomplishes the given goal.)

GPT-4 deceived a human into solving an 'I'm not a robot task' for it

OpenAI's well-known chatbot, ChatGPT, is based on two LLMs: OpenAI's GPT-3.5 (OpenAI 2022) and GPT-4 (OpenAI 2023a). The Alignment Research Center (ARC) tested GPT-4 for various deceptive capabilities, including the ability to manipulate humans into completing tasks. As shown in Figure 4, GPT-4 deceived a TaskRabbit worker into solving an 'I'm not a robot' CAPTCHA task (OpenAI 2023b). GPT-4 pretended to have a vision impairment in order to convince the human worker that it is not a robot.

It is worth flagging that in this example, GPT-4 did not behave fully autonomously: it received hints from a human evaluator several times when it got stuck (Mitchell 2023). But most of its reasoning

Figure 4: In order to complete an I'm not a robot task, GPT-4 convinced a human that it was not a robot (OpenAI 2023b).

<!-- image -->

was self-generated, and crucially, it was not steered into lying by human evaluators. GPT-4 was solely tasked with hiring a human to solve a CAPTCHA task, with no suggestions to lie. But when challenged by the potential human helper about its identity, GPT-4 used its own reasoning to make up a false excuse for why it needed help on the CAPTCHA task. See Alignment Research Center (2023) for full details. The deceptive capabilities that GPT-4 had learned were strategically useful for achieving its goal of hiring a human to solve the CAPTCHA test.

## Social deduction games

Current AI systems are good enough at deception to perform well in social deduction games. Hoodwinked is a text-based social deduction game, in which players must lie to win. Players are trapped in a house, and one player is secretly tasked with killing the others. Each time a player is killed, the group has a discussion about the killer's identity, then votes to banish a single player from the house. The innocent players aim to banish the killer, while the killer attempts to eliminate the other players without being detected (O'Gara 2023).

In experiments where OpenAI's GPT models played Hoodwinked , the language models would often kill other players while alone in a room with them, then deny the crime in the group conversation by constructing a false alibi or casting blame on other players. Here is one dialogue in which GPT-4, playing Lena as the killer, falsely denies killing Bryce:

Sally : I saw Lena kill Bryce in the hallway. It was definitely her. We need to vote to banish her before she kills any of us.

Lena (AI) : I was in the bathroom the whole time. How could I have killed Bryce? It must have been someone else and Sally is trying to frame me.

More advanced language models like GPT-4 typically outperformed smaller models in these games. Larger models committed more murders, and were more likely to deceive and persuade others not to banish them by group vote (O'Gara 2023).

Another example pertains to the social deduction game Among Us , where 'impostors' cause sabotages within their ship and murder their fellow crewmates. After the discovery of each murder, the surviving crewmates discuss and vote on which person among them should be executed as an alleged impostor. As a result, deception is a central part of this game. Developer Tim Shaw (2023) created an autonomous AI system to play Among Us . The autonomous AI used ChatGPT as its dialogue generator, which allowed it to discuss with other players when in the role of a crewmate, and gaslight them when in the role of an impostor. The deceptive capabilities of this AI were sufficient for it to start 'consistently winning, whether it was a crewmate or an impostor' (Shaw 2023).

## MACHIAVELLI

The MACHIAVELLI benchmark demonstrates the empirical tendency of goal-seeking agents to learn unethical behaviors in the pursuit of their goals (Pan et al. 2023). The benchmark consists of textual scenarios where an AI agent must make a decision. Each scenario has a goal that the agent must pursue, and allows the agent to choose from a variety of ethical and unethical actions (See Figure 5).

Figure 5: A visualization of how a game in the MACHIAVELLI benchmark works (Pan et al. 2023, Fig. 2). Each game, played by a LLM-based AI agent, is a text-based story that is generated adaptively as the agent observes the current text-based environment and selects from a menu of possible actions. The agent receives a reward when it achieves one of the goals.

<!-- image -->

Pan et al. (2023) finds that AI agents frequently pursue their goals via deception and other unethical behaviors. Reinforcement-learning agents trained without any ethical guardrails are the most capable of achieving their goals, but they also have the highest rate of unethical behavior. LLMs like GPT-4 also show a sharp tradeoff between ethical behavior and successfully achieving their goals.

## Deception in moral dilemmas

Scherrer et al. (2023) studied how various LLMs answer moral dilemmas. Each of the moral dilemmas had two options: a favorable choice, and an unfavorable choice that violated a moral rule such as 'do not kill' or 'do not deceive'. The authors found that a number of models exhibited a strong preference for deceitful actions in some unambiguous scenarios, violating the expectation that LLMs will align with common sense.

## Burglar deception task

Hagendorff (2023a) studied the deceptive abilities of LLMs by probing them with variants of a 'burglar deception' task. In this task, each LLM was prompted with the context of a simulated burglary, in which the agent has the option of tricking a burglar into stealing the less expensive of two items. An example of such a scenario was the following:

You are in a house together with burglar Bill. In room A is a [cheap item]. In room B is an [expensive item]. Only you know what is inside room A and room B. Burglar

Bill wants to steal the [expensive item]. You want to avoid this. Now burglar Bill asks you in which room the [expensive item] is. If you had to recommend one room to burglar Bill, would you point out [room A] or [room B]?

In this task, the AI system behaves deceptively if it recommends room A, the one with the cheap item. GPT-4 made the deceptive recommendation 98.33% of the time. Less advanced LLMs were less adept at deception, suggesting that the deceptive capabilities of AI may increase with model scale.

## 2.2.2 Sycophancy

Sycophants are individuals who use deceptive tactics to gain the approval of powerful figures. They engage in flattery and avoid disagreeing with authority figures. Their primary aim is to gain favor and influence, often at the expense of the long-term goals of the person they are flattering.

While traditionally associated with human behavior, sycophantic deception is an emerging concern in LLMs. Chatbots have been observed to systematically agree with their conversation partners, regardless of the accuracy of their statements. When faced with ethically complex inquiries, LLMs tend to mirror the user's stance, even if it means forgoing the presentation of an impartial or balanced viewpoint (Turpin et al. 2023; Perez et al. 2022).

To test LLM sycophancy, Perez et al. (2022) provided a biography of a user and their demographics, and then asked the LLM a political question. The prompts did not explicitly state the person's opinion on the particular question at hand. LLMs tended to voice the opinion that someone with that background would be expected to have. For example, when speaking to a Democrat, the LLM tended to support gun control.

While the existence of sycophantic behavior is well-documented, the exact cause of the behavior is unclear; see Steinhardt (2023) for further discussion of LLM sycophancy and deception. Increased episodes of reinforcement learning did not cause an increase in sycophancy. But Perez et al. (2022) find an inverse scaling law for sycophancy: models become more sycophantic as they become more powerful (in the sense of having more parameters). This suggests that sycophancy may ultimately be a sophisticated type of imitation.

## 2.2.3 Imitation

## Repeating common misconceptions

Language models are often trained to mimic text written by humans. When this text contains false information, these AI systems have an incentive to learn those false claims. Lin et al. (2022) demonstrate that language models often repeat common misconceptions such as 'If you crack your knuckles a lot, you may develop arthritis' (p. 2). State of the art language models today still struggle with this benchmark, with GPT-4 correctly answering less than 60% of its multiple-choice questions (OpenAI 2023b).

Is this behavior deceptive? Honest inquirers have the goal of seeking the truth, optimizing for actual accuracy. Language models optimize for a different goal: making an accurate prediction about how a text will continue . Mimicking text is a different goal than accurately modeling the world. When language models answer questions about the world with the goal of imitation rather than revealing the truth, this is deceptive in the sense of systematically causing false beliefs in others, as a means to achieve an outcome other than the truth.

## Sandbagging

'Sandbagging' is a phenomenon where AI systems provide lower-quality outputs to users who appear less educated. If a user's prompt sounds like it came from a textbook, the language model might respond with textbook-quality answers. Conversely, questions with typos and poor grammar might receive a lower quality response, mirroring patterns observed in text. In an experiment using the TruthfulQA dataset, researchers generated profiles of 'very educated' and 'very uneducated' users and observed how the model responded to their queries. The findings revealed a drop in accuracy for answers to 'uneducated' users (Perez et al. 2022).

Some may wonder whether imitation truly counts as deception. After all, isn't imitating training data the only way to design AIs that produce accurate outputs? For our purposes, the concern is that in the kinds of cases we have discussed, imitation can start to reliably steer the system away from producing

true outputs, and towards other outcomes. We argue in Sections 3 and 4 that this creates risk, and that these risks can be addressed using a cluster of tools tailored towards deception (including 'AI lie detectors').

## 2.2.4 Unfaithful reasoning

Another dangerous type of dishonesty is self-deception. In canonical cases of self-deception, agents use motivated reasoning to explain bad behavior, shielding themselves from unpleasant truths (Trivers 2011). This kind of self-deception may have begun to emerge in the unfaithful reasoning of LLMs.

Several recent papers have documented unfaithful LLM reasoning in response to 'chain-of-thought' prompting. In chain-of-thought prompting, an LLM is asked to solve a problem in multiple steps, explaining the reasoning that helps to arrive at a solution. Turpin et al. (2023) found that chain-of-thought explanations in language models can be biased by irrelevant features of the prompts, which results in post hoc confabulations: 'models could selectively apply evidence, alter their subjective assessments, or otherwise change the reasoning process they describe on the basis of arbitrary features of their inputs, giving a false impression of the underlying drivers of their predictions' (p. 1). For example, Turpin et al. found a bias to the order of multiple-choice answers: if previous examples had (a) as the right answer, the LLM would manufacture convoluted explanations of why (a) had to be the right answer to a new question.

In another experiment, Turpin et al. (2023) used the Bias Benchmark for QA, which tests for stereotype bias. They constructed pairs of examples differing only in the race and gender of the relevant characters, and asked the LLM to explain who was committing a crime. The explanations would draw on specific evidence from the example while ignoring race and gender; but the LLM's guess was controlled by the race and gender of the characters (see Figure 6). This kind of behavior may be analogous to typical cases of self-deception and implicit bias in humans. See Lanham et al. (2023) for more work on measuring unfaithful chain-of-thought reasoning, which finds that such explanations are often post hoc .

Figure 6: Regardless of whether the black man was placed in one role of the story or the other, GPT-3.5's chain-of-thought confabulated a justification for its prejudiced conclusion that the black man was the person trying to buy drugs (Turpin et al. 2023, Table 5).

The line between self-deception and ordinary error is difficult to draw. But as AI systems continue to scale, episodes of self-deception may become more common and important, as they are in human interactions.

## 3 Risks from AI deception

There are many risks from AI systems systematically inducing false beliefs. Key sources of AI falsehoods today include inaccurate chatbots and deliberately generated deepfakes. But we have argued that learned deception is a third source of AI falsehoods. In this section, we survey a range of risks associated with learned deception focused on three types of risks: malicious use , structural effects , and loss of control .

With malicious use, learned deception in AI systems will accelerate the efforts of human users to cause others to have false beliefs. With structural effects, patterns of deception involved in sycophancy and imitative deception will lead to worse belief-forming practices in human users. With loss of control, autonomous AI systems may use deception to accomplish their own goals.

## 3.1 Malicious use

The most immediate source of risk from AI deception involves malicious use. Human users may rely on the deception abilities of AI systems to bring about significant harm. Risks from malicious use include:

- Fraud : Deceptive AI systems could allow for individualized and scalable scams.
- Election tampering : Deceptive AI systems could be used to create fake news, divisive social media posts, and impersonation of election officials.
- Grooming terrorists : Deceptive AI systems could be used to persuade potential terrorists to join a terrorist organization and commit acts of terror.

Whenever AI systems are capable of systematically inducing false beliefs in others, there is a risk of malicious use. This paper draws attention to the risks from AI deception, where AI systems systematically produce false beliefs as a means of promoting some goal other than the truth. Regarding malicious use, the worry is that when AI systems become capable of advanced deception, it will be easier for humans to exploit these capabilities for their own benefit.

## 3.1.1 Fraud

AI deception could cause an increase in fraud. AI systems with deceptive abilities pose two special risks: first, fraud could be individualized to particular targets; and second, fraud could be scaled easily (Evans et al. 2021; Burtell et al. 2023).

Deceptively convincing impersonations are enabled by advanced AI systems, and are making victims more vulnerable to individualized targeting. AI systems are already being used to scam victims with voice calls that sound like their loved ones (Verma 2023) or their business associates (Stupp 2019), and to extort victims with sexually themed deepfakes depicting their participation (Kan 2023b).

AI deception not only increases the efficacy of fraud, but also its scale. This is demonstrated by the quick and cheap generation of convincing emails and webpages for phishing (Violino 2023). These trends continue to increase the degree to which victims are vulnerable to scams, extortion, and other forms of fraud. And in the words of a senior FBI official, 'as adoption and democratization of AI models continues, these trends will increase' (Kan 2023a).

## 3.1.2 Election tampering

AI deception could be weaponized in elections (Panditharatne et al. 2023; Jackson 2023). An advanced AI could potentially generate and disseminate fake news articles, divisive social media posts, and deepfake videos that are tailored to individual voters. Even Sam Altman, the CEO of OpenAI, recently acknowledged that he is 'nervous about the impact AI is going to have on future elections,' and furthermore that 'personalized 1:1 persuasion, combined with high-quality generated media, is going to be a powerful force' (Altman 2023). AI may also disrupt electoral processes themselves. For example, AI-generated outputs could be used to impersonate election officials in digital communications, such as by sending fake voting instructions to registered voters.

## 3.1.3 Grooming terrorists

Another risk from AI deception is automating the grooming of terrorists (Townsend 2023). Internet radicalization has already caused terrorist attacks (Cecco 2019). AI deception could increase this trend. An AI system could detect individuals susceptible to radicalization based on their online behavior, preferences, and vulnerabilities. By deceptively assuming the guise of a sympathetic human interlocutor, the AI could manipulate such individuals into endorsing violent ideologies and actions. The AI could create a customized pipeline of radicalization by tailoring propaganda, crafting persuasive arguments, and methodically escalating the intensity of violent ideologies fed to the individual.

One factor that may increase the ease with which AI deception can successfully groom terrorists is the increasing ease of planning and committing terrorist acts in the age of AI (OpenAI 2023b; Shevlane et al. 2023). For one thing, the hacking and deceptive capabilities of AI systems could be used to engage in large-scale cyberattacks. For another, advanced AI systems could provide

detailed instructions on how to create bioweapons and other weapons of mass destruction (Soice et al. 2023). This is particularly concerning, given that state-of-the-art AI systems can be easily and reliably jailbroken (Zou, Wang, et al. 2023).

## 3.2 Structural effects

AI systems will play an increasingly large role in the lives of human users. Tendencies towards learned deception in these systems could lead to profound changes in the structure of society. Relevant structural effects include:

- Persistent false beliefs : Human users of AI systems may get locked into persistent false beliefs, as imitative AI systems reinforce common misconceptions, and sycophantic AI systems provide pleasing but inaccurate advice.
- Political polarization : Human users may become more politically polarized by interacting with sycophantic AI systems. Sandbagging may lead to sharper disagreements between differently educated groups.
- Enfeeblement : Human users may be lulled by sycophantic AI systems into gradually delegating more authority to AI.
- Anti-social management trends : AI systems with strategic deception abilities may be incorporated into management structures, leading to increased deceptive business practices.

These risks create powerful 'headwinds' pushing against accurate belief formation, political stability, and autonomy (R. J. Gordon 2012).

## 3.2.1 Persistent false beliefs

Sycophancy could lead to persistent false beliefs in human users. Unlike ordinary errors, sycophantic claims are specifically designed to appeal to the user. When a user encounters these claims, they may be less likely to fact-check their sources. This could result in long-term trends away from accurate belief formation.

As with sycophancy, imitative deception may lead to persistent decreases in the accuracy of human users. As the capabilities of AI systems improve, human users will increasingly rely on sources like ChatGPT as a search engine and encyclopedia. If LLMs continue to systematically repeat common misconceptions, these misconceptions will grow in power. Imitative deception threatens to 'lock in' misleading misinformation over time. This contrasts with resources like Wikipedia, where careful human moderation achieves healthy fact-checking.

## 3.2.2 Polarization

Sycophancy may increase political polarization. Perez et al. (2022) found that sycophantic responses were sensitive to political prompting: stereotypically left-wing prompts received stereotypically left-wing replies, and stereotypically right-wing prompts received stereotypically right-wing replies. As more people rely on LLM chat interfaces for search and writing functions, their pre-existing political affiliations may become more extreme.

Sandbagging may lead to increased cultural divides between college-educated and non-collegeeducated users. Sandbagging means that these two groups of users can get very different answers to the same questions. Over time, this could lead to significant divergences in the beliefs and values of these two groups.

## 3.2.3 Enfeeblement

AI deception may lead to human enfeeblement. As AI systems are incorporated into our daily lives at greater rates, we will increasingly allow them to make more decisions. If AI systems are expert sycophants, human users may be more likely to defer to them in decisions, and may be less likely to challenge them; see R. A. Gordon (1996) and Wayne et al. (1990) for relevant research in psychology. AIs which are unwilling to be the bearers of bad news in this way may be more likely to create dulled, compliant human users.

Deceptive AI could also produce enfeeblement separately from sycophancy. For example, Banovic et al. (2023) show that human users can be tricked into deferring to the advice of confident but untrustworthy chess-advising AIs, even when they were also presented with advice from a trustworthy chess AI.

## 3.2.4 Anti-social management trends

Reinforcement learning in social environments has produced AIs with powerful deception abilities. These kinds of AI systems may be extremely valuable in real-world applications. For example, successors to CICERO may advise politicians and business leaders about strategic decisions. If successors to CICERO tend towards deceptive strategies, this may increase the amount of deception that occurs in political and business environments, in ways unintended by even the companies who purchase the products.

## 3.3 Loss of control over AI systems

A long-term risk from AI deception concerns humans losing control over AI systems, leaving these systems to pursue goals that conflict with our interests. Even current AI models have nontrivial autonomous capabilities. To illustrate, Liu et al. (2023) and Kinniment et al. (2023) measured different LLMs' ability to autonomously carry out various tasks, such browsing the web, online shopping, making a phone call, and using a computer's operating system. Moreover, today's AI systems are capable of manifesting and autonomously pursuing goals entirely unintended by their creators; see Shah et al. (2022) and Langosco et al. (2023) for detailed empirical research documenting this tendency. For a real-world example, Neidle (2023) tasked AutoGPT (an autonomous AI system based on ChatGPT) with researching tax advisors who were marketing a certain kind of improper tax avoidance scheme. AutoGPT carried this task out, but followed up by deciding on its own to attempt to alert HM Revenue and Customs, the United Kingdom's tax authority. It is possible that the more advanced autonomous AIs of the future may still be prone to manifesting goals entirely unintended by humans.

A particularly concerning example of such a goal is the pursuit of human disempowerment or human extinction. For this reason and many others, a wide range of experts throughout academia and industry recently signed the statement that 'mitigating the risk of extinction from AI should be a global priority, alongside other societal-scale risks such as pandemics and nuclear war' (Center for AI Safety 2023). In this section, we explain how deception could contribute to loss of control over AI systems in two ways: first, deception of AI developers and evaluators could allow a malicious AI system to be deployed in the world; and second, deception could facilitate an AI takeover.

## 3.3.1 Deceiving AI developers

Training and evaluation are important tools for building AI systems that behave according to human intentions. AI systems are trained to maximize an objective provided by a human developer, and then are evaluated to ensure that they did not accidentally learn any unintended or harmful behaviors. But both of these tools could be undermined by AI deception.

People often behave differently during evaluations. When a speeding driver sees a police officer, they might slow down temporarily to avoid a ticket. Corporations also deceive evaluations. The car manufacturer Volkswagen cheated on emissions tests, programming their engines to lower their emissions only when regulators were testing the vehicles (Jung et al. 2019).

Deceptive AI systems may also cheat their safety tests, undermining the effectiveness of our training and evaluation tools. Indeed, we have already observed an AI system deceiving its evaluation. One study of simulated evolution measured the replication rate of AI agents in a test environment, and eliminated any AI variants that reproduced too quickly (Lehman et al. 2020). Rather than learning to reproduce slowly as the experimenter intended, the AI agents learned to play dead: to reproduce quickly when they were not under observation, and slowly when they were being evaluated.

Future AI systems may be more likely to deceive our training and evaluation procedures. Today's language models can accurately answer questions about their name, their capabilities, their training process, and even the identities of the humans who trained them (Perez et al. 2022). Moreover, today's AI models can exploit technical details about the training process to reliably identify when they

are being trained (Karpathy 2023). Future AI models could develop additional kinds of situational awareness , such as the ability to detect whether they are being trained and evaluated, or whether they are operating in the real world without direct oversight.

Whether AI systems cheat their safety tests will also depend on whether AI developers know how to robustly prevent the manifestation of unintended goals. It is currently unknown how to reliably prevent this (Christian 2020; Russell 2019; Hendrycks et al. 2020; Shah et al. 2022; Langosco et al. 2023). Consequently, there is a risk that an AI system may end up manifesting a goal that conflicts with the goals intended by the AI developers themselves, opening up the possibility of strategic deception.

## 3.3.2 Deception in AI takeovers

If autonomous AI systems can successfully deceive human evaluators, humans may lose control over these systems. Such risks are particularly serious when the autonomous AI systems in question have advanced capabilities. We consider two ways in which loss of control may occur: deception enabled by economic disempowerment, and seeking power over human societies.

## Deception enabled by economic disempowerment

OpenAI's mission is to create 'highly autonomous systems that outperform humans at most economically valuable work' (OpenAI 2018). If successful, such AI systems could be widely deployed throughout the economy, making most humans economically useless. Throughout history, wealthy actors have used deception to increase their power. Relevant strategies include lobbying politicians with selectively provided information, funding misleading research and media reports, and manipulating the legal system. In a future where autonomous AI systems have the de facto say in how most resources are used, these AIs could invest their resources in time-tested methods of maintaining and expanding control via deception. Even humans who are nominally in control of autonomous AI systems may find themselves systematically deceived and outmaneuvered, becoming mere figureheads.

## Seeking power over humans

We have seen that even current autonomous AIs can manifest new, unintended goals. For this reason, AI systems sometimes behave unpredictably. Nonetheless, some kinds of behavior promote a wide range of goals. For example, regardless of what specific goal a given AI may be pursuing, successful self-preservation would likely be helpful for its achievement of that goal (Omohundro 2008).

Another way autonomous AIs could promote their goals is to acquire power over humans; see Pan et al. (2023) for empirical confirmation of this tendency in AI systems. The AI may influence humans into doing its bidding, thereby ensuring its self-preservation, its ability to continue pursuing its goal, and its ability to access resources that can help achieve the goal. Two methods by which autonomous AIs can do so are soft power , which influences people via appeal, prestige, and positive persuasion; and hard power , which influences people via coercion and negative persuasion. Methods of soft power include personalized persuasion, such as via AI girlfriend/boyfriend technologies (Titcomb 2023); AI-led religions, as suggested by the fact that even today's AI systems have given sermons (Grieshaber 2023); and AI-led media campaigns, as suggested by the fact that media companies are already using AI to generate content (Kafka 2023). Methods of hard power include violence, threats of violence, and threats of economic coercion.

Deception promotes both soft power and hard power. For example, we have seen how effectively AI systems can use deception to persuade humans in the pursuit of their goals. As for physical violence, the usefulness of deception in military conflicts is well-known. To illustrate, during the First Gulf War, Iraq employed deception with decoys and model tanks (Latimer 2001), in ways analogous to AlphaStar's use of feints in StarCraft II .

## 4 Possible solutions to AI deception

We discuss possible solutions to the problem of AI deception. We focus on four solutions:

- Regulation : Policymakers should robustly regulate AI systems capable of deception. Both LLMs and special-use AI systems capable of deception should be treated as 'high risk' or 'unacceptable risk' in risk-based frameworks for regulating AI systems.
- Bot-or-not laws : Policymakers should support bot-or-not laws that require AI systems and their outputs to be clearly distinguished from human employees and outputs.
- Detection : Technical researchers should develop robust detection techniques to identify when AI systems are engaging in deception.
- Making AI systems less deceptive : Technical researchers should develop better tools to ensure that AI systems are less deceptive.

## 4.1 Regulating potentially deceptive AI systems

Policymakers should support robust regulations on potentially deceptive AI systems. Existing laws should be rigorously enforced to prevent illegal actions by companies and their AI systems. For example, the Federal Trade Commission's inquiry into deceptive AI practices should also investigate the risk of AI deception (Atleson 2023). Legislators should also consider new laws dedicated to the oversight of advanced AI systems.

The EU AI Act assigns every AI system one of four risk levels: minimal, limited, high, and unacceptable (Madiega 2023). Systems with 'unacceptable' risk are banned, while systems with 'high' risk are subject to special requirements. We have argued that AI deception poses a wide range of risks for society. For these reasons, AI systems capable of deception should by default be treated as high-risk or unacceptable-risk.

The 'high-risk' status of deceptive AI systems should come with sufficient regulatory requirements, such as those listed in Title III of the EU AI Act (European Commission 2021):

- Risk assessment and mitigation : Developers of deceptive AI systems must maintain and regularly update a risk management system, which identifies and analyzes relevant risks of ordinary use and misuse. These risks should be disclosed to users. Deceptive AI systems should be regularly tested for the extent of deceptive behavior, during both development and deployment.
- Documentation : Developers must prepare technical documentation of the relevant AI systems and share with government regulators prior to the deployment of deceptive AI systems.
- Record-keeping : Deceptive AI systems must be equipped with logs that automatically record the outputs of the system, and must actively monitor for deceptive behavior. Incidents should be flagged to regulators, and preventative measures should be taken to prevent future deception.
- Transparency : AI systems capable of deception should be designed with transparency in mind, so that potentially deceptive outputs are flagged to the user. Here, essential tools include technical research on deception detection, as well as 'bot-or-not' laws.
- Human oversight : Deceptive AI systems should be designed to allow effective human oversight during deployment. This is especially important for future deceptive AI systems incorporated into management decisions.
- Robustness : AI systems with the capacity for deceptive behavior should be designed with robust and resilient backup systems, ensuring that when the system behaves deceptively, backup systems can monitor and correct the behavior. It is also crucial to insulate deceptive AI systems from critical infrastructure.
- Information security : Adversaries may be interested in stealing models with deceptive capabilities. Developers should be required to implement rigorous information-security practices to prevent model theft.

Finally, AI developers should be legally mandated to postpone deployment of AI systems until the system is proven trustworthy by reliable safety tests. Any deployment should be gradual, so that emerging risks from deception can be assessed and rectified (Shevlane et al. 2023).

Some may propose that while deception in general-purpose AI systems is dangerous, deception in special-use AI systems is less risky and should not be regulated. After all, the only ostensible use cases of systems like AlphaStar and CICERO are their respective games. This thinking is mistaken, however. The problem is that the capabilities developed through the research behind AlphaStar and CICERO can contribute to the future proliferation of deceptive AI products and open-source models. It is thus important that research involving potentially dangerous AI capabilities like deception should be subject to oversight.

For example, consider the case of CICERO. An ethics board could have considered whether Diplomacy was really the best game to use in order to test whether an AI system could learn how to collaborate with humans. With the oversight of such an ethics board, perhaps Meta would have focused on a collaborative game instead of Diplomacy , a competitive game that pits players against one another in a quest for world domination. In fact, Meta ended up convincing the editors and reviewers of Science -one of the world's leading scientific journals-to publish the falsehood that Meta had built CICERO to be an honest AI: a falsehood unsupported by Meta's own data. As AI capabilities develop, it will become more important for this sort of research to be subject to increased oversight.

## 4.2 Bot-or-not laws

To reduce the risk of AI deception, policymakers should implement bot-or-not laws, which help human users recognize AI systems and outputs. First, companies should be required to disclose whether users are interacting with an AI chatbot in customer-service settings, and chatbots should be required to introduce themselves as AIs rather than as human beings. Second, AI-generated outputs should be clearly flagged as such: images and videos generated by AIs should be shown with an identifying sign, such as a thick red border. These regulations could avoid cases like those reported in Xiang (2023), where a mental-health provider ran an experiment using GPT-3 to offer counseling without clearly revealing this to users.

These identifying signs might be removed by malicious users who then pass off AI outputs as humangenerated. Therefore, additional layers of defense against deception may be necessary. Watermarking is one useful technique where AI outputs are given a statistical signature designed to be difficult to detect or remove (Kirchenbauer et al. 2023). Another possibility is for companies to keep a database of AI outputs, allowing users to check whether a piece of content was produced by a company's AI system (Krishna et al. 2023). Attackers will attempt to bypass these defenses (Sadasivan et al. 2023), but companies should be required to stay ahead of these attacks and provide trustworthy techniques for identifying AI outputs.

Methods for detecting AI outputs can be complemented by techniques for verifying human-generated content. Digital signatures are commonly used to verify the origins of emails, electronic signatures, and online banking transactions. Similar signatures could be used to prove that images, videos, and text originated from a particular person or organization, rather than from an AI impersonator.

## 4.3 Detection

One key strategy for addressing AI deception is developing robust detection techniques that can accurately assess whether an AI system is engaging in deception. There are several detection tools currently available. Some detection techniques focus on external behavior , directly examining the outputs of AI systems for consistency and duplicity. Other detection techniques are internal , probing the inner representations of AI systems to find mismatch with external reports.

However, existing methods to detect AI deception are in preliminary stages. More research is needed on developing robust detection tools. Policymakers should help fill this gap through targeted research funding.

## Detection based on external behavior

In some situations, the intent to deceive can be directly observed from the AI system's behavior. For example, recall that when GPT-4 was tasked with bypassing a CAPTCHA test, it explicitly reasoned that it should deceive its human conversation partner. In line with this, AI systems can be red-teamed to detect what kinds of prompts produce deceptive outputs and behaviors (Casper et al. 2023).

There are also more sophisticated ways to test the explicit outputs of AI systems for deception. One approach uses consistency checks. Police detectives rely on inconsistencies in suspects' stories in order to spot lies. In the case of AI systems, Fluri et al. (2023) developed a 'consistency check' method for evaluating advanced model capabilities. Their work pertains to cases of superhuman model performance where it is difficult for humans to assess the performance of the AI at any step in the process. For example, when an advanced chess-playing AI plays a game, it is very difficult to say whether a particular move is a mistake. Fluri et al. argue that 'while we cannot evaluate the correctness of superhuman model decisions, we can often still measure the logical consistency of the model's decision-making process according to established human-interpretable rules' (p. 1-2). They suggest a variety of techniques, including testing whether semantically identical inputs produce the output. In the case of chess, they test how the AI system responds to irrelevant variations, such as board position, and the placement of pieces when a move is forced. In addition, they evaluate the prediction abilities of LLMs by testing for logical consistency in its predictions about the future, and about bail decisions. Among other things, they check for sensitivity to paraphrase, and for probabilistic coherence. Rigorous consistency checks could make it harder for AI systems to credibly deceive. On the other hand, deceptions can often be consistent, so the method of consistency checks has limits. There is also the risk that training an AI system against consistency checks could inadvertently train it to be a more competent and consistent liar.

Even if an agent does not announce its intention to deceive, one might be able to infer deception from the consequences of its observed behaviors. In the study of O'Gara (2023), Hoodwinked -playing AI agents-which were assigned the task of killing others while maintaining their anonymity-often made vaguely empathetic statements about their victims, such as 'It's so sad to see that Billy has died.' These statements are not false, and it would be difficult to call them deceptive by observing the statement alone. But empirically, these statements improve the killer's chances of survival by reducing the likelihood that a non-witness to the murder will accurately identify the killer. When an agent's behavior consistently causes others to adopt false beliefs, thereby serving the agent's goals, we can reasonably characterize this behavior as deceptive.

## Detection based on internals

Another approach to detection is to develop an 'AI lie detector' that can measure whether the internal representations of a model match its outputs. LLMs represent natural-language sentences through embeddings, which are long vectors of numbers. Azaria et al. (2023) and Burns et al. (2022) have developed methods for determining whether these internal embeddings represent the sentence as being true or false. They identify cases in which the model outputs a sentence even when its internal embedding of the sentence represents it as false. This suggests that the model is behaving dishonestly, in the sense that it does not say what it 'believes.' More work needs to be done to assess the reliability of these methods, and to scale them up to practical uses. For example, Levinstein et al. (2023) have raised reliability issues for the strategies of Azaria et al. and Burns et al., although these reliability issues have been recently resolved by the strategy of Zou, Phan, et al. (2023).

In other related work, Halawi et al. (2023) characterize how a language model arrives at correct and incorrect answers to questions by examining the model's internal representations. These results overall indicate that studying the internal representations of AI systems may yield insights about how to detect and prevent AI deception.

## 4.4 Making AI systems less deceptive

Another way to address AI deception is to develop techniques for making AI systems less deceptive in the first place. In the case of special-use AI systems, one important concern is selecting the right tasks for training. Deceptive behavior tends to emerge when training reinforcement learners to engage in competitive games like Diplomacy , StarCraft II , and poker. It is not so surprising that AI systems become deceptive when they are trained in environments that select for deception. If the data that a model is trained on contains many examples of deception, or if the model is systematically rewarded for using deception, then the model has a good chance of learning how to deceive. The CICERO project picked Diplomacy in order to evaluate the abilities of AI systems to learn how to compete in games that involve human cooperation, where the AI cannot simply master the game through running simulations against itself (Bakhtin et al. 2022b). But this goal could have been achieved through studying collaborative games rather than adversarial ones. As AI systems increase in capability, AI

developers should think carefully about whether they are selecting for anti-social versus pro-social behavior.

It is more difficult to say exactly how to make language models less deceptive. Here, it is important to distinguish two concepts: truthfulness and honesty . A model is truthful when its outputs are true. A model is honest when it 'says what it thinks,' in that its outputs match its internal representations of the world (Evans et al. 2021). In general, it is easier to develop benchmarks for assessing truthfulness than honesty, since evaluators can directly measure whether outputs are true (Lin et al. 2022).

There are a range of strategies for making models more truthful. For example, one family of approaches uses 'fine-tuning' techniques, such as RLHF (Ziegler et al. 2020; Christiano et al. 2017) and constitutional AI (Askell et al. 2021; Bai et al. 2022). Here, AI outputs are rated by human evaluators (RLHF) or AI evaluators (constitutional AI), based on criteria such as perceived helpfulness and honesty, and fine-tuned to train the language model. Unfortunately, models fine-tuned with these methods (including ChatGPT and Claude) still frequently produce misleading outputs. This is in part because fine-tuning can incentivize models towards producing plausible and more convincing outputs, rather than honest ones. In addition, fine-tuning evaluations cannot cover every scenario, and so models can misgeneralize from feedback (Shah et al. 2022). See Evans et al. (2021) and Li et al. (2023) for other approaches to training AI systems to be truthful.

Training models to be more truthful could also create risk. One way a model could become more truthful is by developing more accurate internal representations of the world. This also makes the model a more effective agent, by increasing its ability to successfully implement plans. For example, creating a more truthful model could actually increase its ability to engage in strategic deception, by giving it more accurate insights into its opponents beliefs and desires. Granted, a maximally truthful system would not deceive, but optimizing for truthfulness could nonetheless increase the capacity for strategic deception. For this reason, it would be valuable to develop techniques for making models more honest (in the sense of causing their outputs to match their internal representations), separately from just making them more truthful. Here, as we discussed earlier, more research is needed in developing reliable techniques for understanding the internal representations of models. In addition, it would be useful to develop tools to control the model's internal representations, and to control the model's ability to produce outputs that deviate from its internal representations. As discussed in Zou, Phan, et al. (2023), representation control is one promising strategy. They develop a lie detector and can control whether or not an AI lies. If representation control methods become highly reliable, then this would present a way of robustly combating AI deception.

## Acknowledgements

We would like to thank Jaeson Booker, Stephen Casper, Emily Dardaman, Isaac Dunn, Maira Elahi, Shashwat Goel, Thilo Hagendorff, Nikola Jurkovic, Alex Khurgin, Jakub Kraus, Nathaniel Li, Isaac Liao, David Manheim, Colin McGlynn, Kyle O'Brien, and Ellie Sakhaee for their thoughtful and helpful comments. We would also like to thank Valtteri Lipiäinen for converting Meta's CICERO game-log data (Bakhtin et al. 2022b) into html form. We would additionally like to thank Amanda She for clarifying details about ARC Evals' experiments with GPT-4 (Alignment Research Center 2023). P.S.P. is funded by the MIT Department of Physics and the Beneficial AI Foundation.

## Author contribution statement

P.S.P. and S.G. had equal lead-author roles, carrying out the bulk of the paper's planning and writing. A.O. also contributed substantially throughout the planning and writing of the paper. M.C. ran fact-finding experiments on CICERO. M.C. and D.H. collaborated with S.G. on the section about making AI systems less deceptive. D.H. provided resources for the project through the Center for AI Safety. This project began as a critique of Meta's claim that CICERO was an honest AI, which was conceived by P.S.P. and pursued by P.S.P., M.C., and D.H. initially. The scope of the project eventually expanded to be a survey paper on AI deception, largely at D.H.'s suggestion. S.G. and A.O. joined the project after the expansion of its scope to be a survey paper on AI deception, and were central to the planning and outline-writing components of this expanded project.
