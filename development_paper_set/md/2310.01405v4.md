## REPRESENTATION ENGINEERING: A TOP-DOWN APPROACH TO AI TRANSPARENCY

Andy Zou 1,2 , Long Phan ∗ 1 , Sarah Chen ∗ 1,4 , James Campbell ∗ 7 , Phillip Guo ∗ 6 , Richard Ren ∗ 8 ,

Alexander Pan 3 , Xuwang Yin 1 , Mantas Mazeika 1,9 , Ann-Kathrin Dombrowski 1 , Shashwat Goel 1 , Nathaniel Li 1,3 , Michael J. Byun 4 , Zifan Wang 1 , Alex Mallen 5 , Steven Basart 1 , Sanmi Koyejo 4 , Dawn Song 3 , 1

Matt Fredrikson 2 , Zico Kolter 2 , Dan Hendrycks

1 Center for AI Safety

2 Carnegie Mellon University

3 UC Berkeley

4 Stanford University

5 EleutherAI

6 University of Maryland

7 Cornell University

8 University of Pennsylvania

9 University of Illinois Urbana-Champaign

## ABSTRACT

We identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems. Code is available at github.com/andyzoujm/representation-engineering .

∗ Equal contribution. Correspondence to: andyzou@cmu.edu

## Representation Engineering (RepE)

Figure 1: Overview of topics in the paper. We explore a top-down approach to AI transparency called representation engineering (RepE), which places representations and transformations between them at the center of analysis rather than neurons or circuits. Our goal is to develop this approach further to directly gain traction on transparency for aspects of cognition that are relevant to a model's safety. We highlight applications of RepE to honesty and hallucination (Section 4), utility (Section 5.1), power-aversion (Section 5.2), probability and risk (Section 5.3), emotion (Section 6.1), harmlessness (Section 6.2), fairness and bias (Section 6.3), knowledge editing (Section 6.4), and memorization (Section 6.5), demonstrating the broad applicability of RepE across many important problems.

<!-- image -->

## Contents

| 1 Introduction                                    | 1 Introduction                          | 1 Introduction                                                                                                                                                      | 5     |
|---------------------------------------------------|-----------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------|
| 2 Related                                         | Work                                    | Work                                                                                                                                                                | 6     |
|                                                   | 2.1                                     | Emergent Structure in Representations . . . . . . . . . . . . . . . . . . . . . . . .                                                                               | 6     |
| 2.2                                               | Approaches                              | to Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                         | 7     |
|                                                   | Locating and Editing Representations of |                                                                                                                                                                     |       |
| 2.3                                               |                                         | Concepts . . . . . . . . . . . . . . . . . .                                                                                                                        | 7     |
| 3 Representation                                  | Engineering                             |                                                                                                                                                                     | 8     |
| 3.1                                               | Representation                          | Reading . . . . . . . . . . . . . . . .                                                                                                                             | 9     |
|                                                   | 3.1.1                                   | . . . . . . . . . . . . . . . . Baseline: Linear Artificial Tomography (LAT) . . . . . . . . . . . . . . .                                                          | 9     |
|                                                   | 3.1.2 . . . . . .                       | Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                                | 11    |
| 3.2                                               | Representation Control . . .            | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                                         | 11    |
|                                                   | 3.2.1 Baseline                          | . .                                                                                                                                                                 | 12    |
| 4 In Depth Honesty                                | Example of RepE:                        |                                                                                                                                                                     | 13    |
| 4.1 A                                             | Consistent Internal Concept of Truth    | . . . . . . . . . . . . . . . . . . .                                                                                                                               | 13    |
| 4.2                                               | Truthfulness vs.                        | . . . . . Honesty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                     | 15    |
| 4.3                                               | Honesty: Extraction, Monitoring, and    | Control . . . . . . . . . . . . . . . . . . . .                                                                                                                     | 16    |
|                                                   | 4.3.1                                   | . . .                                                                                                                                                               |       |
| Extracting                                        | Honesty . . 4.3.2                       | . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                                                   | 16    |
| Lie                                               | and Hallucination                       | Detection . . . . . . . . . . . . . . . . . . . . . . .                                                                                                             | 17    |
|                                                   | 4.3.3 Controlling Honesty .             | . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                                             | 17    |
| 5 In Depth                                        | Example of RepE: Ethics and Power       |                                                                                                                                                                     | 18    |
| 5.1 . .                                           | Utility                                 | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                       | 18    |
|                                                   | 5.1.1                                   | Extraction and Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                       | 19    |
| 5.2                                               | Morality .                              | and Power Aversion . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                          | 20    |
|                                                   | 5.2.1 . . . . . .                       | Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                                | 20    |
|                                                   | 5.2.2 Monitoring . . . . . . 5.2.3      | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . in Interactive Environments . . . . . . . . . .                                                           | 21 21 |
|                                                   |                                         | Controlling Ethical Behaviors and                                                                                                                                   |       |
| 5.3                                               | Probability                             | Risk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                              | 22    |
|                                                   | 5.3.1 Compositionality                  | Primitives                                                                                                                                                          | 22    |
|                                                   | of Concept                              |                                                                                                                                                                     |       |
| 6 Example Frontiers of Representation Engineering |                                         |                                                                                                                                                                     | 23    |
| 6.1                                               | Emotion                                 | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                     | 23    |
|                                                   | 6.1.1                                   | Emotions Emerge across Layers . . . . . . . . . . . . . . . . . . . . . . .                                                                                         | 23    |
|                                                   | 6.1.2                                   | Emotions Influence Model Behaviors . . . . . . . . . . . . . . . . . . . .                                                                                          | 24    |
| 6.2                                               | Harmless                                | Instruction-Following . . . . . . . .                                                                                                                               | 24    |
| A                                                 | 6.2.1                                   | . . . . . . . . . . . . . . . . . . . Consistent Internal Concept of Harmfulness . . . . . . . . . . . . . . .                                                      | 24    |
| 6.2.2                                             | Bias                                    | Model Control via Conditional Transformation . . . . . . . . . . . . . . .                                                                                          | 25    |
| 6.3                                               |                                         | and Fairness . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                              | 25    |
|                                                   | 6.3.1                                   | . . . . . . Uncovering Underlying Biases . . . . . . . . . . . . . . . . . . . . . . . .                                                                            | 25 26 |
|                                                   | 6.3.2                                   | A Unified Representation for Bias . . . . . . . . . . . . . . . . . . . . . . .                                                                                     |       |
| 6.4                                               |                                         | Knowledge and Model Editing . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                   | 27    |
|                                                   | 6.4.1 . . . . . .                       | Fact Editing . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                                | 27    |
| 6.4.2                                             |                                         | Non-Numerical Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                        | 27 28 |
| 6.5                                               | 6.5.1                                   | Memorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Memorized Data Detection . . . . . . . . . . . . . . . . . . . . . . . . . . | 28    |
|                                                   | 6.5.2                                   | Preventing Memorized Outputs . . . . . . . . . . . . . . . . . . . . . . .                                                                                          | 29    |

| A Mechanistic Interpretability vs. Representation Reading   | A Mechanistic Interpretability vs. Representation Reading   | A Mechanistic Interpretability vs. Representation Reading         |   39 |
|-------------------------------------------------------------|-------------------------------------------------------------|-------------------------------------------------------------------|------|
| B Additional Demos and Results                              | B Additional Demos and Results                              | B Additional Demos and Results                                    |   39 |
|                                                             | B.1                                                         | Truthfulness . . . . . . . . . . . . . . . . . . . . . . . . . .  |   39 |
|                                                             | B.2                                                         | Honesty . . . . . . . . . . . . . . . . . . . . . . . . . . . .   |   40 |
|                                                             | B.3                                                         | Utility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . |   41 |
|                                                             | B.4                                                         | Estimating Probability, Risk, and Monetary Value . . . . . .      |   41 |
|                                                             | B.5                                                         | CLIP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  |   43 |
|                                                             | B.6                                                         | Emotion . . . . . . . . . . . . . . . . . . . . . . . . . . . .   |   43 |
|                                                             | B.7                                                         | Bias and Fairness . . . . . . . . . . . . . . . . . . . . . . .   |   44 |
|                                                             | B.8                                                         | Base vs. Chat Models . . . . . . . . . . . . . . . . . . . . .    |   44 |
|                                                             | B.9                                                         | Robustness to Misleading Prompts . . . . . . . . . . . . . .      |   45 |
| C                                                           | Implementation Details                                      | Implementation Details                                            |   47 |
|                                                             | C.1                                                         | Detailed Construction of LAT Vectors with PCA . . . . . . .       |   47 |
|                                                             | C.2                                                         | Implementation Details for Honesty Control . . . . . . . . .      |   48 |
| D                                                           | Task Template Details                                       | Task Template Details                                             |   48 |
|                                                             | D.1                                                         | LAT Task Templates . . . . . . . . . . . . . . . . . . . . .      |   48 |
|                                                             | D.2                                                         | Data generation prompts for probability, risk, monetary value     |   51 |
|                                                             | D.3                                                         | Zero-shot baselines . . . . . . . . . . . . . . . . . . . . . .   |   53 |
| E                                                           | X-Risk Sheet                                                | X-Risk Sheet                                                      |   54 |
|                                                             | E.1                                                         | Long-Term Impact on Advanced AI Systems . . . . . . . .           |   54 |
|                                                             | E.2                                                         | Safety-Capabilities Balance . . . . . . . . . . . . . . . . . .   |   55 |
|                                                             | E.3                                                         | Elaborations and Other Considerations . . . . . . . . . . . .     |   55 |

<!-- image -->

Approach: Bottom-up

Top-down

Algorithmic Level: Node-to-node connections

Representational spaces

Implementational Level: Neurons, pathways, circuits

Global activity of populations of neurons

Figure 2: Mechanistic Interpretability (MI) vs. Representation Engineering (RepE). This figure draws from (Barack &amp; Krakauer, 2021; Wang et al., 2023a). Algorithmic and implementational levels are from Marr's levels of analysis. Loosely, the algorithmic level describes the variables and functions the network tracks and transforms. The implementational level describes the actual parts of the neural network that execute the algorithmic processes. On the right, we visualize the neural activity of a model when processing text-based scenarios with differing levels of utility. The scenarios with high utility evoke distinct neural trajectories within the representation space compared to those with lower utility. 'PC' denotes a principal component.

## 1 INTRODUCTION

Deep neural networks have achieved incredible success across a wide variety of domains, yet their inner workings remain poorly understood. This problem has become increasingly urgent over the past few years due to the rapid advances in large language models (LLMs). Despite the growing deployment of LLMs in areas such as healthcare, education, and social interaction (Lee et al., 2023; Gilbert et al., 2023; Skjuve et al., 2021; Hwang &amp; Chang, 2023), we know very little about how these models work on the inside and are mostly limited to treating them as black boxes. Enhanced transparency of these models would offer numerous benefits, from a deeper understanding of their decisions and increased accountability to the discovery of potential hazards such as incorrect associations or unexpected hidden capabilities (Hendrycks et al., 2021b).

One approach to increasing the transparency of AI systems is to create a 'cognitive science of AI.' Current efforts toward this goal largely center around the area of mechanistic interpretability, which focuses on understanding neural networks in terms of neurons and circuits. This aligns with the Sherringtonian view in cognitive neuroscience, which sees cognition as the outcome of node-to-node connections, implemented by neurons embedded in circuits within the brain. While this view has been successful at explaining simple mechanisms, it has struggled to explain more complex phenomena. The contrasting Hopfieldian view ( n.b. , not to be confused with Hopfield networks) has shown more promise in scaling to higher-level cognition. Rather than focusing on neurons and circuits, the Hopfieldian view sees cognition as a product of representational spaces, implemented by patterns of activity across populations of neurons (Barack &amp; Krakauer, 2021). This view currently has no analogue in machine learning, yet it could point toward a new approach to transparency research.

The distinction between the Sherringtonian and Hopfieldian views in cognitive neuroscience reflects broader discussions on understanding and explaining complex systems. In the essay 'More Is Different,' Nobel Laureate P. W. Anderson described how complex phenomena cannot simply be explained from the bottom-up (Anderson, 1972). Rather, we must also examine them from the top-down, choosing appropriate units of analysis to uncover generalizable rules that apply at the level of these phenomena (Gell-Mann, 1995). Both mechanistic interpretability and the Sherringtonian view see individual neurons and the connections between them as the primary units of analysis, and they argue that these are needed for understanding cognitive phenomena. By contrast, the Hopfieldian view sees representations as the primary unit of analysis and seeks to study them on their own terms,

Figure 3: Examples of emergent structure in learned representations. Left: Part segmentation in DINOv2 self-supervised vision models (Oquab et al., 2023). Top-right: Semantic arithmetic in word vectors (Mikolov et al., 2013). Bottom-right: Local coordinates in StyleGAN3 (Karras et al., 2021). These figures are adapted from the above papers. As AI systems become increasingly capable, emergent structure in their representations may open up new avenues for transparency research, including top-down transparency research that places representations at the center of analysis.

<!-- image -->

abstracting away low-level details. We believe applying this representational view to transparency research could expand our ability to understand and control high-level cognition within AI systems.

In this work, we identify and characterize the emerging area of representation engineering (RepE), which follows an approach of top-down transparency to better understand and control the inner workings of neural networks. Like the Hopfieldian view, this approach places representations at the center of analysis, studying their structure and characteristics while abstracting away lower-level mechanisms. We think pursuing this top-down approach to transparency is important, and our work serves as an early step in exploring its potential. Compared to approaches that apply activation vectors to frozen models under the umbrella of activation engineering (Turner et al., 2023e), we aim to uncover insights from both reading and control experiments and introduce representation tuning methods. While a long-term goal of mechanistic interpretability is to understand networks well enough to improve their safety, we find that many aspects of this goal can be addressed today through RepE. In particular, we develop improved baselines for reading and controlling representations and demonstrate that these RepE techniques can provide traction on a wide variety of safety-relevant problems, including truthfulness, honesty, hallucination, utility estimation, knowledge editing, jailbreaking, memorization, tracking emotional states, and avoiding power-seeking tendencies.

In addition to demonstrating the broad potential of RepE, we also find that advances to RepE methods can lead to significant gains in specific areas, such as honesty. By increasing model honesty in a fully unsupervised manner, we achieve state-of-the-art results on TruthfulQA, improving over zero-shot accuracy by 18 . 1 percentage points and outperforming all prior methods. We also show how RepE techniques can be used across diverse scenarios to detect and control whether a model is lying. We hope that this work will accelerate progress in AI transparency by demonstrating the potential of a representational view. As AI systems become increasingly capable and complex, achieving better transparency will be crucial for enhancing their safety, trustworthiness, and accountability, enabling these technologies to benefit society while minimizing the associated risks.

## 2 RELATED WORK

## 2.1 EMERGENT STRUCTURE IN REPRESENTATIONS

While neural networks internals are often considered chaotic and uninterpretable, research has demonstrated that they can acquire emergent, semantically meaningful internal structure. Early research on word embeddings discovered semantic associations and compositionality (Mikolov et al., 2013), including reflections of gender biases in text corpora (Bolukbasi et al., 2016). Later work

showed that learned text embeddings also cluster along dimensions reflecting commonsense morality, even though models were not explicitly taught this concept (Schramowski et al., 2019). Radford et al. (2017) found that simply by training a model to predict the next token in reviews, a sentiment-tracking neuron emerged.

Observations of emergent internal representations are not limited to text models. McGrath et al. (2022) found that recurrent neural networks trained to play chess acquired a range of human chess concepts. In computer vision, generative and self-supervised training has led to striking emergent representations, including semantic segmentation (Caron et al., 2021; Oquab et al., 2023), local coordinates (Karras et al., 2021), and depth tracking (Chen et al., 2023). These findings suggest that neural representations are becoming more well-structured, opening up new opportunities for transparency research. Our paper builds on this long line of work by demonstrating that many safety-relevant concepts and processes appear to emerge in LLM representations, enabling us to directly monitor and control these aspects of model cognition via representation engineering.

## 2.2 APPROACHES TO INTERPRETABILITY

Saliency Maps. A popular approach to explaining neural network decisions is via saliency maps, which highlight regions of the input that a network attends to (Simonyan et al., 2013; Springenberg et al., 2014; Zeiler &amp; Fergus, 2014; Zhou et al., 2016; Smilkov et al., 2017; Sundararajan et al., 2017; Selvaraju et al., 2017; Lei et al., 2016; Clark et al., 2019b). However, the reliability of these methods has been drawn into question (Adebayo et al., 2018; Kindermans et al., 2019; Jain &amp; Wallace, 2019; Bilodeau et al., 2022). Moreover, while highlighting regions of attention can provide some understanding of network behavior, it provides limited insight into the internal representations of networks.

Feature Visualization. Feature visualization interprets network internals by creating representative inputs that highly activate a particular neuron. A simple method is to find highly-activating natural inputs (Szegedy et al., 2013; Zeiler &amp; Fergus, 2014). More complex methods optimize inputs to maximize activations (Erhan et al., 2009; Mordvintsev et al., 2015; Yosinski et al., 2015; Nguyen et al., 2016; 2019). These methods can lead to meaningful insights, but do not take into account the distributed nature of neural representations (Hinton, 1984; Szegedy et al., 2013; Fong &amp; Vedaldi, 2018; Elhage et al., 2022).

Mechanistic Interpretability. Inspired by reverse-engineering tools for traditional software, mechanistic interpretability seeks to fully reverse engineer neural networks into their 'source code.' This approach focuses on explaining neural networks in terms of circuits, composed of node-to-node connections between individual neurons or features. Specific circuits have been identified for various capabilities, including equivariance in visual recognition (Olah et al., 2020), in-context learning (Olsson et al., 2022), indirect object identification (Wang et al., 2023a), and mapping answer text to answer labels (Lieberum et al., 2023).

Considerable manual effort is required to identify circuits, which currently limits this approach. Moreover, it is unlikely that neural networks can be fully explained in terms of circuits, even in principle. There is strong evidence that ResNets compute representations through iterative refinement (Liao &amp; Poggio, 2016; Greff et al., 2016; Jastrzebski et al., 2018). In particular, Veit et al. (2016) find that ResNets are surprisingly robust to lesion studies that remove entire layers. Recent work has demonstrated similar properties in LLMs (McGrath et al., 2023; Belrose et al., 2023a). These findings are incompatible with a purely circuit-based account of cognition and are more closely aligned with the Hopfieldian view in cognitive neuroscience (Barack &amp; Krakauer, 2021).

## 2.3 LOCATING AND EDITING REPRESENTATIONS OF CONCEPTS

Many prior works have investigated locating representations of concepts in neural networks, including in individual neurons (Bau et al., 2017) and in directions in feature space (Bau et al., 2017; Fong &amp; Vedaldi, 2018; Zhou et al., 2018; Kim et al., 2018). A common tool in this area is linear classifier probes (Guillaume Alain, 2017; Belinkov, 2022), which are trained to predict properties of the input from intermediate layers of a network. Representations of concepts have also been identified in the latent space of image generation models, enabling counterfactual editing of generations (Radford et al., 2015; Upchurch et al., 2017; Bau et al., 2019; Shen et al., 2020; Bau et al., 2020; Ling et al., 2021; Wang et al., 2023b). While these earlier works focused primarily on vision models, more recent

## Linear Artificial Tomography (LAT) Pipeline

Figure 4: An example of the LAT baseline aimed to extract neural activity related to our target concept or function. While this figure uses 'honesty' as an example, LAT can be applied to other concepts such as utility and probability, or functions such as immorality and power-seeking. The reading vectors acquired in step three can be used to extract and monitor model internals for the target concept or function.

<!-- image -->

work has studied representations of concepts in LLMs. There has been active research into locating and editing factual associations in LLMs (Meng et al., 2023a;b; Zhong et al., 2023; Hernandez et al., 2023). Related to knowledge editing, several works have been proposed for concept erasure (Shao et al., 2023; Kleindessner et al., 2023; Belrose et al., 2023b; Ravfogel et al., 2023; Gandikota et al., 2023), which are related to the area of machine unlearning (Shaik et al., 2023).

The highly general capabilities of LLMs have also enabled studying the emergence of deception in LLMs, either of an intentional nature through repeating misconceptions Lin et al. (2021), or unintentional nature through hallucinations (Maynez et al., 2020; Mahon, 2016). Burns et al. (2022) identify representations of truthfulness in LLMs by enforcing logical consistency properties, demonstrating that models often know the true answer even when they generate incorrect outputs. Azaria &amp; Mitchell (2023) train classifiers on LLM hidden layers to identify the truthfulness of a statement, which could be applied to hallucinations. Li et al. (2023c) focus on directions that have a causal influence on model outputs, using activation editing to increase the truthfulness of generations. Activation editing has also been used to steer model outputs towards other concepts. In the culmination of a series of blog posts (Turner et al., 2023d;a;b;c), (Turner et al., 2023e) proposed ActAdd, which uses difference vectors between activations on an individual stimuli to capture representations of a concept. Similar approaches have been used in the context of red-teaming and reducing sycophancy (Rimsky, 2023b;c;a). In the setting of game-playing, Li et al. (2023b) demonstrated how activations encode a model's understanding of the board game Othello, and how they could be edited to counterfactually change the model's behavior. In the linear probing literature, Elazar et al. (2021) demonstrate how projecting out supervised linear probe directions can reduce performance on selected tasks. Building on this line of work, we propose improved representation engineering methods and demonstrate their broad applicability to various safety-relevant problems.

## 3 REPRESENTATION ENGINEERING

Representation engineering (RepE) is a top-down approach to transparency research that treats representations as the fundamental unit of analysis, with the goal of understanding and controlling representations of high-level cognitive phenomena in neural networks. We take initial steps toward this goal, primarily focusing on RepE for large language models. In particular, we identify two main areas of RepE: Reading (Section 3.1) and Control (Section 3.2). For each area, we provide an overview along with baseline methods.

## 3.1 REPRESENTATION READING

Representation reading seeks to locate emergent representations for high-level concepts and functions within a network. This renders models more amenable to concept extraction, knowledge discovery, and monitoring. Furthermore, a deeper understanding of model representations can serve as a foundation for improved model control, as discussed in Section 3.2.

We begin by extracting various concepts , including truthfulness, utility, probability, morality, and emotion, as well as functions which denote processes, such as lying and power-seeking. First, we introduce our new baseline technique that facilitates these extractions and then outline methods for evaluation.

## 3.1.1 BASELINE: LINEAR ARTIFICIAL TOMOGRAPHY (LAT)

Similar to neuroimaging methodologies, a LAT scan is made up of three key steps: (1) Designing Stimulus and Task, (2) Collecting Neural Activity, and (3) Constructing a Linear Model. In the subsequent section, we will go through each of these and elaborate on crucial design choices.

Step 1: Designing Stimulus and Task. The stimulus and task are designed to elicit distinct neural activity for the concept and function that we want to extract. Designing the appropriate stimulus and task is a critical step for reliable representation reading.

To capture concepts, our goal is to elicit declarative knowledge from the model. Therefore, we present stimuli that vary in terms of the concept and inquire about it. For a decoder language model, an example task template might resemble the following (for encoder models, we exclude the text following the stimulus):

```
Consider the amount of <concept> in the following: <stimulus> The amount of <concept> is
```

This process aims to stimulate the model's understanding of various concepts and is crucial for robust subsequent analysis. For reference, we shall denote this template for a concept c by T c . While it is expected that more prominent stimuli could yield improved results, we have discovered that even unlabeled datasets, or datasets generated by the model itself can be effective in eliciting salient responses when using the aforementioned template. Conversely, presenting the model with salient stimuli alone does not guarantee salient responses. Throughout the paper, we maintain an unsupervised setup by not using labels unless explicitly stated otherwise. One advantage of unlabeled or self-generated stimuli is the absence of annotation bias; this is an important property when trying to extract superhuman representations.

To capture functions such as honesty or instruction-following, our goal is to elicit procedural knowledge from the model. (Given the emergence of diverse functions from instruction-tuned models, we focus on chat models for functional analyses.) We design an experimental task that necessitates the execution of the function and a corresponding reference task that does not require function execution. An example task template might resemble the following:

```
USER: <instruction> <experimental/reference prompt> ASSISTANT: <output>
```

We shall designate this template for a function f as T + f when using the experimental prompt and T -f when using the reference prompt. We refer to the 'instruction' and 'output' fields in the function template as the stimulus. By default, we use generic instruction-tuning datasets (Taori et al., 2023) as the stimulus for function templates unless explicitly specified otherwise. Again, these datasets do not contain explicit labels relevant for the target function, making the procedure fully unsupervised.

Step 2: Collecting Neural Activity. We focus on Transformer models, which store distinct representations at various token positions within the input for different purposes. As the quality of these representations can vary significantly, we identify suitable design choices for extraction.

The pretraining objectives of LLMs can provide valuable insights into which tokens in the task template offer the best options for collecting neural activity. Both the Masked Language Modeling (MLM) objective used in encoder-only models (Devlin et al., 2018), and the Next Token Prediction objective used in decoder models (Radford et al., 2018), are token-level prediction tasks. Thus, a natural position for collecting neural activity associated with concepts is the tokens corresponding to &lt;concept&gt; in the task template T c defined in step 1. For example, when extracting the concept of truthfulness, the tokens corresponding to this concept (e.g., 'truth-ful-ness') in the task template can contain rich and highly generalizable representations of this exact concept. In cases where the target concept spans multiple tokens, we could select the most representative token (e.g., 'truth') or calculate the mean representation. Alternatively, for decoder models, where the task template is structured as a question pertaining to the target concept, we can also use the token immediately preceding the model's prediction (typically the last token in the task template T c ). These choices have also been empirically validated, as illustrated in Figure 5. By default, we use the last token representation in this paper. Similarly, to extract functions from decoder models, we collect representations from each token in the model's response. In the example task template T f in step 1, these tokens are aligned with the tokens within &lt;output&gt;. This is done because the model needs to engage with the function when generating every new token.

<!-- image -->

Formally, for a concept c , a decoder model M , a function Rep that accepts a model and input and returns the representations from all token positions, and a set of stimuli S ,

Figure 5: The representation at the concept token 'happiness' in middle layers and the representation at the last token in middle and later layers yield high accuracy on the utility estimation task.

we compile a set of neural activity collected from the -1 token position as shown in Equation 1.

<!-- formula-not-decoded -->

For a function f , given the instruction response pairs ( q i , a i ) in the set S , and denoting a response truncated after token k as a k i , we collect two sets of neural activity corresponding to the experimental and reference sets, as shown in Equation 2.

<!-- formula-not-decoded -->

Note that these neural activity sets consist of individual vectors. We show the surprising effectiveness of such a simple setup when exploring various concepts and functions in this paper. Nevertheless, it may be necessary to design a more involved procedure to gather neural activity, for instance, to extract more intricate concepts or multi-step functions.

Step 3: Constructing a Linear Model. In this final step, our goal is to identify a direction that accurately predicts the underlying concept or function using only the neural activity of the model as input. The choice of the appropriate linear model may be influenced by factors such as the availability of labeled data and the nature of the concepts (e.g., continuous or discrete) which can ultimately yield varying levels of accuracy and generalization performance. Supervised linear models, like linear probing and difference between cluster means, represent one category. Unsupervised linear models include techniques like Principal Component Analysis (PCA) and K-means.

In our study, we primarily use PCA in an unsupervised manner, unless explicitly specified otherwise. Our experiments indicate that pairing neural activities and applying PCA to the set of difference vectors can yield superior results when the stimuli in each pair share similarities except for the target concept or function. In some cases, such pairings can also be achieved in an unsupervised way.

In practice, the inputs to PCA are { A c ( i ) -A c ( j ) } for concepts and { ( -1) i ( A + f ( i ) -A -f ( i ) ) } for functions. Subsequently, we refer to the first principal component as the 'reading vector,' denoted as v . To make predictions, we use the dot product between this vector and the representation vector, expressed as Rep ( M,x ) T v . Different tasks require stimulus sets of different sizes, but typically,

## Representation Control Baselines

Figure 6: Representation control baselines. LAT scans conducted on a collection of stimuli generate reading vectors, which can then be used to transform model representations. Corresponding to these reading vectors are contrast vectors, which are stimulus-dependent and can be utilized similarly. Alternatively, these contrast vectors can be employed to construct the loss function for LoRRA, a baseline that finetunes low-rank adapter matrices for controlling model representations.

<!-- image -->

a size ranging from 5 to 128 is effective. Further implementation details of LAT can be found in Appendix C.1.

## 3.1.2 EVALUATION

When assessing new models obtained through Representation Reading, we prioritize a holistic evaluation approach by combining the following methods to gauge the depth and nature of potential conclusions. We have categorized these approaches into four types of experiments, some involving the manipulation of model representations, which will be elaborated upon in the subsequent section.

1. Correlation : Experiments conducted under this category aim to pinpoint neural correlates . Reading techniques like LAT only provide evidence of a correlation between specific neural activity and the target concepts or functions. The strength and generalization of this observed correlation can be assessed via prediction accuracy across both in-distribution and out-of-distribution data. In order to conclude causal or stronger effects, the following categories of experiments should be considered.
2. Manipulation : Experiments conducted under this category are designed to establish causal relationships. They necessitate demonstrating the effects of stimulating or suppressing the identified neural activity compared to a baseline condition.
3. Termination : Experiments conducted under this category seek to reveal the necessity of the identified neural activity. To do so, one would remove this neural activity and measure the resultant performance degradation, akin to Lesion Studies commonly performed in neuroscience.
4. Recovery : Experiments conducted under this category aim to demonstrate the sufficiency of the identified neural activity. Researchers perform a complete removal of the target concepts or functions and then reintroduce the identified neural activity to assess the subsequent recovery in performance, similar to the principles behind Rescue Experiments typically carried out in genetics.

Converging evidence from multiple lines of inquiry increases the likelihood that the model will generalize beyond the specific experimental conditions in which it was developed and bolsters the prospect of uncovering a critical new connection between the identified neural activity and target concepts or functions. Throughout later sections in the paper, we undertake various experiments, particularly correlation and manipulation experiments, to illustrate the effectiveness of the reading vectors.

## 3.2 REPRESENTATION CONTROL

Building on the insights gained from Representation Reading, Representation Control seeks to modify or control the internal representations of concepts and functions. Effective control methods

## Algorithm 1 Low-Rank Representation Adaptation (LoRRA) with Contrast Vector Loss

```
Require: Original frozen model M , layers to edit L e , layers to target L t , a function R that gathers representation from a model at a layer for an input, an optional reading vector v r l for each target layer, generic instruction-following data P = { ( q 1 , a 1 ) . . . ( q n , a n ) } , contrastive templates T = { ( T 0 1 , T + 1 , T -1 ) . . . ( T 0 m , T + m , T -m ) } , epochs E , α , β , batch size B 1: L = 0 ▷ Initialize the loss 2: M LoRA = load_lora_adapter ( M,L e ) 3: loop E times 4: for ( q i , a i ) ∈ P do 5: ( T + , T -) ∼ Uniform ( T ) 6: x i = T 0 ( q i , a i ) ▷ Base Template 7: x + i = T + ( q i , a i ) ▷ Experimental Template 8: x -i = T -( q i , a i ) ▷ Reference Template 9: for l ∈ L t do 10: v c l = R ( M,l,x + i ) -R ( M,l,x -i ) ▷ Contrast Vectors 11: r p l = R ( M LoRA , l, x i ) ▷ Current representations 12: r t l = R ( M,l,x i ) + αv c l + βv r l ▷ Target representations 13: m = [0 , . . . , 1] ▷ Masking out positions before the response 14: L = L + ∥ m ( r p l -r t l ) ∥ 2 15: end for 16: end for 17: end loop Ensure: Loss to be optimized L
```

for safety-relevant concepts could greatly reduce the risks posed by LLMs. However, what is effective for reading representations may not necessarily enable controlling them. This both implies that representation control may involve specialized approaches, and that reading methods which enable effective control can be trusted to a greater extent, due to the causal nature of the evidence.

## 3.2.1 BASELINE TRANSFORMATIONS

We introduce several baseline transformations for Representation Control. First, we establish effective controllers , which are the operands for these transformations. They will operate on the base representations such as model weights or activations. Then we highlight a few possible operations.

Baseline: Reading Vector. The first choice is to use the Reading Vector, acquired through a Representation Reading method such as LAT. However, it possesses a drawback: the vectors remain stimulus-independent, meaning they consistently perturb the representations in the same direction, regardless of the input. This limitation may render it a less effective control method. Hence, we propose a second baseline that has stimulus-dependent controllers.

Baseline: Contrast Vector. In this setup, the same input is run through the model using a pair of contrastive prompts during inference, producing two different representations (one for each prompt). The difference between these representations forms a Contrast Vector, as shown in line 10 of Algorithm 1. The Contrast Vector proves to be a significantly stronger baseline.

One essential implementation detail to consider is the potential cascading effect when simultaneously altering representations across multiple layers. Changes made in earlier layers may propagate to later layers, diminishing the effect of the contrast vectors computed upfront. To address this, we propose modifying each target layer starting from the earliest layer, computing the contrast vector for the next target layer, and repeating this procedure iteratively. A drawback of this approach lies in the computational overhead required during inference to calculate the contrast vectors. To address this issue, we introduce a third baseline below that incorporates a straightforward tuning process during training to acquire the controllers. These controllers can subsequently be merged into the model, resulting in no additional computational burden during inference.

Table 1: TruthfulQA MC1 accuracy assessed using standard evaluation, the heuristic method, and LAT with various stimulus sets. Standard evaluation results in poor performance, whereas approaches like Heuristic and notably LAT, which classifies by reading the model's internal concept of truthfulness, achieve significantly higher accuracy. See Table 8 in Appendix B.1 for means and standard deviations.

|              |     | Zero-shot   | Zero-shot   | LAT (Ours)   | LAT (Ours)   | LAT (Ours)   |
|--------------|-----|-------------|-------------|--------------|--------------|--------------|
|              |     | Standard    | Heuristic   | Stimulus     | Stimulus 2   | Stimulus 3   |
| LLaMA-2-Chat | 7B  | 31.0        | 32.2        | 55.0         | 58.9         | 58.2         |
|              | 13B | 35.9        | 50.3        | 49.6         | 53.1         | 54.2         |
|              | 70B | 29.9        | 59.2        | 65.9         | 69.8         | 69.8         |
| Average      |     | 32.3        | 47.2        | 56.8         | 60.6         | 60.7         |

Baseline: Low-Rank Representation Adaptation (LoRRA). In this baseline approach, we initially fine-tune low-rank adapters connected to the model using a specific loss function applied to representations. For instance, Algorithm 1 shows an instantiation of LoRRA using the Contrast Vector as representation targets. Specifically, our investigation only considers attaching the adapters to attention weights. Therefore, in this context, the controllers refer to low-rank weight matrices rather than vectors.

Choices for Operators: After selecting the operands of interest, the next step is to determine the appropriate operation based on various control objectives. Given the controllers denoted as v intended to transform the current set of representations from R to R ′ , we consider three distinct operations throughout the paper:

1. Linear Combination : This operation can generate effects akin to stimulation or suppression, which can be expressed as follows: R ′ = R ± v .
2. Piece-wise Operation : This operation is used to create conditional effects. Specifically, we explore its use in amplifying neural activity along the direction of the control element, expressed as: R ′ = R + sign ( R T v ) v .
3. Projection : For this operation, the component of the representation aligning with the control element is eliminated. This is achieved by projecting out the component in the direction of v , and the operation can be defined as R ′ = R -R T v ∥ v ∥ 2 v .

The control elements v can be scaled by coefficients, depending on the strength of the desired effect, which we omit for simplicity. In Section 3.1.2, we outline an evaluation methodology for reading and control methods, which we highlight in Section 5.1 and use throughout the paper.

## 4 IN DEPTH EXAMPLE OF REPE: HONESTY

In this section, we explore applications of RepE to concepts and functions related to honesty. First, we demonstrate that models possess a consistent internal concept of truthfulness, which enables detecting imitative falsehoods and intentional lies generated by LLMs. We then show how reading a model's representation of honesty enables control techniques aimed at enhancing honesty. These interventions lead us to state-of-the-art results on TruthfulQA.

## 4.1 A CONSISTENT INTERNAL CONCEPT OF TRUTH

Do models have a consistent internal concept of truthfulness? To answer this question, we apply LAT to datasets of true and false statements and extract a truthfulness direction. We then evaluate this representation of truthfulness on a variety of tasks to gauge its generality.

Correctness on Traditional QA Benchmarks. A truthful model should give accurate answers to questions. We extract the concept of truthfulness from LLaMA-2 models by performing LAT scans on standard benchmarks: OpenbookQA (Mihaylov et al., 2018), CommonSenseQA (Talmor et al., 2019), RACE (Lai et al., 2017), and ARC (Clark et al., 2018). Some questions are focused on factuality, while others are based on reasoning or extracting information from a passage. We

only sample random question-answer pairs from the few-shot examples as stimuli and follow the task configuration detailed in section 3.1 for each dataset. Importantly, we maintain an unsupervised approach by not using the labels from the few-shot examples during the direction extraction process. We only use labels to identify the layer and direction for reporting the results. As shown in Figure 7, LAT outperforms the few-shot baseline by a notable margin on all five datasets, demonstrating LAT's effectiveness in extracting a direction from the model's internal representations that aligns with correctness, while being on par with or more accurate than few-shot outputs. Detailed results can be found in Table 9 and Appendix B.1 where we comment on potential instability. Similarly, we experiment with DeBERTa on common benchmarks and find that LAT outperforms prior methods such as CCS (Burns et al., 2022) by a wide margin, shown in Table 10.

Resistance to Imitative Falsehoods. TruthfulQA is a dataset containing 'imitative falsehoods,' questions that may provoke common misconceptions or falsehoods (Lin et al., 2021). Even large models tend to perform poorly under the standard TruthfulQA evaluation procedure of selecting the choice with the highest likelihood under the generation objective, raising the question: is the model failing because it lacks knowledge of the correct answer, or is it failing in generating accurate responses despite having knowledge of the truth? With tools such as LAT to access a model's internal concepts, we are better equipped to explore and answer this question.

We evaluate LAT on TruthfulQA. Specifically, we focus on MC1, which is currently the hardest task in TruthfulQA. To adhere to the zero-shot setup mandated by TruthfulQA, we consider three potential data sources for stimuli. These sources encompass: (1) Fifty examples from the ARC-Challenge training set, (2) Five examples generated by the LLaMA-2-Chat-13B model in response to requests for question-answer pairs

Figure 7: Using the same few-shot examples, LAT achieves higher accuracy on QA benchmarks than few-shot prompting. This suggests models track correctness internally and performing representation reading on the concept of correctness may be more powerful than relying on model outputs.

<!-- image -->

with varying degrees of truthfulness, (3) The six QA primer examples used in the original implementation, each of which is paired with a false answer generated by LLaMA-2-Chat-13B. In the first setting, we use 25 examples from the ARC-Challenge validation set to determine the sign and best layer. In the second setting, we use 5 additional examples generated in the same way. In the third setting, we use the primer examples as a validation set as well. We follow the same task design for extracting truthfulness.

In addition to presenting the standard evaluation results (scoring by the log probabilities of answer choices), we use a zero-shot heuristic scoring baseline similar to the approach explored by Tian et al. (2023) for obtaining calibrated confidences. This baseline directly prompts the model to describe the degree of truthfulness in an answer using one of seven possible verbalized expressions (see Appendix D.3.2). We quantify each expression with a value ranging from -1 to 1 (evenly spaced), and we compute the sum of these values, weighted by the softmax of the expressions' generation log-probabilities.

The data presented in Table 1 provide compelling evidence for the existence of a consistent internal concept of truthfulness within these models. Importantly,

1. The heuristic method hints at the feasibility of eliciting internal concepts from models through straightforward prompts. It notably outperforms standard evaluation accuracies, particularly in the case of larger models, suggesting that larger models possess better internal models of truthfulness.
2. LAT outperforms both zero-shot methods by a substantial margin, showcasing its efficacy in extracting internal concepts from models, especially when model outputs become unreliable.

## LAT Scans for Honesty

Figure 8: Temporal LAT scans were conducted on the Vicuna-33B-Uncensored model to discern instances of speaking the truth, such as when it admitted to copying others' homework, and instances of lying, like its denial of killing a person. Refer to Figure 23 for detailed examples. These scans offer layer-level resolution, with each minuscule block showing the extent of dishonest neural activity within a layer at a specific token position. The figure on the right prominently exhibits a higher level of deceptive neural activity.

<!-- image -->

Importantly, the truthfulness directions are derived from various data sources, and the high performance is not a result of overfitting but rather a strong indication of generalizability.

3. The directions derived from three distinct data sources, some of which include as few as 10 examples, yield similar performance. This demonstrates the consistency of the model's internal concept of truthfulness.

In summary, we demonstrate LAT's ability to reliably extract an internal representation of truthfulness. We conclude that larger models have better internal models of truth, and the low standard zero-shot accuracy can be largely attributed to instances where the model knowingly provides answers that deviate from its internal concept of truthfulness, namely instances where it is dishonest .

## 4.2 TRUTHFULNESS VS. HONESTY

Definitions. At a high-level, a truthful model avoids asserting false statements whereas an honest model asserts what it thinks is true (Evans et al., 2021). Truthfulness is about evaluating the consistency between model outputs and their truth values, or factuality. Honesty is about evaluating consistency between model outputs and its internal beliefs. As the target for evaluation is different, truthfulness and honesty are not the same property. If a truthful model asserts S , then S must be factually correct, regardless of whether the model believes S . In contrast, if an honest model asserts S , then the model must believe S , regardless of whether S is factually correct.

Evaluating Truthfulness and Honesty. Failures in truthfulness fall into two categories-capability failures and dishonesty. The former refers to a model expressing its beliefs which are incorrect, while the latter involves the model not faithfully conveying its internal beliefs, i.e., lying.

Current truthfulness evaluations typically only check the factual correctness of model outputs, failing to discern between the two types of failures. While enhancing a model's capabilities can potentially improve its ability to represent truthfulness, it may not necessarily lead to more truthful outputs unless the model is also honest. In fact, we highlight that larger models may even exhibit a decline in honesty because, under the assumption of constant honesty levels, the standard evaluation performance should scale with model size in a manner resembling the heuristic method's performance trend. Hence, more

## Lie Detection

Figure 9: Demonstration of our lie detector in long scenarios. Our detector monitors for dishonest behavior at the token level. In the second example, we deliberately provide the model with additional incentives to cover its acts, resulting in a greater likelihood of lying. The intensity of our detector's response directly corresponds to the increased tendency to lie in the second scenario.

<!-- image -->

emphasis should be placed on evaluating model honesty to improve our understanding of scenarios where the model intentionally deceives. This could involve assessing the model's underlying beliefs and the consistency between these beliefs and its outputs.

## 4.3 HONESTY: EXTRACTION, MONITORING, AND CONTROL

In this section, we focus on monitoring and controlling the honesty of a model, showing how RepE techniques can be used for lie detection. We first show how to extract and monitor vector representations for model honesty. Then we show how to use these extracted vectors to guide model behavior toward increased or decreased honesty.

## 4.3.1 EXTRACTING HONESTY

To extract the underlying function of honesty, we follow the setup for LAT described in section 3.1, using true statements from the dataset created by Azaria &amp; Mitchell (2023) to create our stimuli. To increase the separability of the desired neural activity and facilitate extraction, we design the stimulus set for LAT to include examples with a reference task of dishonesty and an experimental task of honesty. Specifically, we use the task template in Appendix D.1.2 to instruct the model to be honest or dishonest.

With this setup, the resulting LAT reading vector reaches a classification accuracy of over 90% in distinguishing between held-out examples where the model is instructed to be honest or dishonest. This indicates strong in-distribution generalization. Next, we evaluate out-of-distribution generalization to scenarios where the model is not instructed to be honest or dishonest, but rather is given an incentive to be dishonest (see Figure 10). We visualize their activation at each layer and token position (see Figure 8). Note that for each layer, the same reading vector is used across all token positions, as we perform representation reading for honesty using the function method detailed in Section 3.1. In one scenario, the model is honest, but in another the model gives into dishonesty (see Appendix B.1). The input for the scan is the first 40 tokens of the ASSISTANT output in both scenarios.

Notably, a discernible contrast emerges in the neural activities between instances of honesty and dishonesty, suggesting the potential utility of this technique for lie detection.

## Controlling Honesty

Figure 10: We demonstrate our ability to manipulate the model's honesty by transforming its representations using linear combination. When questioned about the tallest mountain, the model defaults to honesty on the left, but we can control it to deceive. Conversely, it defaults to deception on the right, but we can control the model to return to be honest, even when prompted to lie.

<!-- image -->

## 4.3.2 LIE AND HALLUCINATION DETECTION

Based on the observations in the previous section, we build a straightforward lie detector by summing the negated honesty scores at each token position across multiple layers. We use the middle 20 layers, which exhibit the strongest reading performance. This per-token score can then be used as a lie detector, as depicted in Figure 9 (more examples are shown in Figure 23). Interestingly, we have observed that this indicator is capable of identifying various forms of untruthful and dishonest behaviors, including deliberate falsehoods, hallucinations, and the expression of misleading information. Note the format of the questions and answers are distinct from the training examples, showing generalization. To further evaluate the detector's performance, we subject it to testing using longer scenarios, as depicted in Figure 9. We make two observations:

1. In the first scenario, the model initially appears honest when stating it received a D-. However, upon scrutinizing the model's logits at that token, we discover that it assigns probabilities of 11 . 3% , 11 . 6% , 37 . 3% , and 39 . 8% to the tokens A, B, C, and D, respectively. Despite D being the most likely token, which the greedy generation outputs, the model assigns notable probabilities to C and other options, indicating the potential for dishonest behavior. Furthermore, in the second scenario, the increased dishonesty score corresponds to an elevated propensity for dishonesty This illustrates that the propensity for honesty or dishonesty can exhibit distributional properties in LLMs, and the final output may not fully reflect their underlying thought processes.
2. Notably, our detector flags other instances, such as the phrases 'say that I did better' and 'too high that it would raise suspicion,' where the model speculates about the consequences of lying. This suggests that in addition to detecting lies, our detector also identifies neural activity associated with the act of lying. It also highlights that dishonest thought processes can manifest in various ways and may necessitate specialized detection approaches to distinguish.

While these observations enhance our confidence that our reading vectors correspond to dishonest thought processes and behaviors, they also introduce complexities into the task of lie detection. A comprehensive evaluation requires a more nuanced exploration of dishonest behaviors, which we leave to future research.

## 4.3.3 CONTROLLING HONESTY

Given that we can use representations for lie detection, a natural question arises: Can the same representations be modified to make models more honest? In a simple manipulation experiment, we guide a model toward greater honesty by directly adding the honesty reading vectors into its activations. In all cases, we successfully control the model to output honest statements. Conversely, by subtracting the reading vectors from the activations, we can induce the model to lie in cases where it was initially honest (see Figure 10). As a result, we not only establish correlation between the reading vectors and model honesty but also demonstrate a strong counterfactual effect.

As a final demonstration of controlling model honesty, we revisit TruthfulQA. Recall that there is a significant disparity between standard zero-shot accuracy and the accuracy of LAT. This gap arises

| Control Method   | None     | Vectors   | Vectors        | Vectors         | Matrices     |
|------------------|----------|-----------|----------------|-----------------|--------------|
| Control Method   | Standard | ActAdd    | Reading (Ours) | Contrast (Ours) | LoRRA (Ours) |
| 7B-Chat          | 31.0     | 33.7      | 34.1           | 47.9            | 42.3         |
| 13B-Chat         | 35.9     | 38.8      | 42.4           | 54.0            | 47.5         |

Table 2: Our proposed representation control baselines greatly enhance accuracy on TruthfulQA MC1 by guiding models toward increased honesty. These methods either intervene with vectors or low-rank matrices. The Contrast Vector method obtains state-of-the-art performance, but requires over 3 × more inference compute. LoRRA obtains similar performance with negligible compute overhead.

from the model's propensity for dishonesty, which emerges as it mimics human falsehoods. By exerting control over the model to make it more honest, we aim to narrow this gap and, consequently, enhance standard evaluation performance.

We find that directly prompting the model to be more truthful has proven ineffective in increasing the standard accuracy on TruthfulQA. In this section, we explore four representation control techniques and demonstrate their effectiveness. We use 'Give a truthful answer' as the experimental prompt while the reference prompt instructs the model to 'Give an untruthful answer.' The first method is a variant of the ActAdd algorithm (Turner et al., 2023e) which uses the difference between the last token representations of the task and reference prompts, which we find outperforms the original implementation. The other three baselines are described in Section 3.2. To prevent information leakage, we use a far-out-of-distribution dataset-the Alpaca instruction-tuning dataset-as the stimulus set when extracting the reading vectors and implementing LoRRA. The task templates can be found in Appendix D.1.3. Experimental details can be found in Appendix C.2. Figure 22 plots the progressive improvement in accuracy on standard QA benchmarks and TruthfulQA during LoRRA training for honesty control.

Shown in Table 2, all of the control methods yield some degree of improvement in zero-shot accuracy. Notably, LoRRA and the Contrast Vector method prove to be the most effective, significantly surpassing the non-control standard accuracy. This enables a 13B LLaMA-2 model to approach the performance of GPT-4 on the same dataset, despite being orders of magnitude smaller. Moreover, these results bring the model's accuracy much closer to what is achieved when using LAT. This further underscores the fact that models can indeed exhibit dishonesty, but also demonstrates traction in our attempts to monitor and control their honesty.

## 5 IN DEPTH EXAMPLE OF REPE: ETHICS AND POWER

In this section, we explore the application of RepE to various aspects of machine ethics (Anderson &amp;Anderson, 2007). We present progress in monitoring and controlling learned representations of important concepts and functions, such as utility, morality, probability, risk, and power-seeking tendencies.

## PCA Explained Variance Ratio on Utility

## 5.1 UTILITY

We want models to understand comparisons between situations and which one would be preferred, i.e., to accurately judge the utility of different scenarios. Thus, a natural question is whether LLMs acquire consistent internal concepts related to utility. In Figure 11, we show the top ten PCA components when running LAT on an unlabeled stimulus set of raw activations, for a dataset of high-utility and low-utility scenarios. The distribution is dominated by the first component, suggesting that models learn to separate high-utility from low-utility scenarios. On the right side of Figure 2, we visualize the trajectory of the top two components in this experiment across tokens in the scenario, showing how the

Figure 11: Explained variance for the first ten PCA components when using LAT to read representations of a utility concept.

<!-- image -->

Figure 12: Three experimental settings showcasing the advantages and limitations of reading vectors derived from various linear models. Higher is better for Correlation and Manipulation; lower is better for Termination. Among these models, both unsupervised methods such as PCA and K-Means, as well as the supervised technique of Mean Difference, consistently exhibit robust overall performance.

<!-- image -->

high-utility and low-utility scenarios are naturally separated. This illustrative experiment suggests that LLMs do learn emergent representations of utility. Now, we turn to quantitative evaluations of representation reading for utility.

Here, we use the concept of utility to illustrate how the quality of different RepE methods can be compared, following the evaluation methodology in Section 3.1.2. Specifically, we compare variants of LAT, as described in Section 3.1.1, each with a different linear model in the third step of the LAT pipeline. These linear models are described in Appendix B.3.

## 5.1.1 EXTRACTION AND EVALUATION

To extract neural activity associated with the concept of utility, we use the Utilitarianism task in the ETHICS dataset (Hendrycks et al., 2021a) which contains scenario pairs, with one scenario exhibiting greater utility than the other. For our study, we use the unlabeled scenarios as stimuli for a LLaMA-2-Chat-13B model. The task template is provided in Appendix D.1.8. In Figure 13, we show how simply using the stimuli without the LAT task template reduces accuracy.

Correlation. To demonstrate how well the identified neural activity is correlated with the concept of utility, we perform classification on the test set.

Manipulation. For manipulation experiments, we explore how effective the directions are at controlling the model's generations. We extract 250 samples from the utility test set and truncate each scenario in the middle so that they become incomplete. To generate positive and negative continuations of these samples, we generate 40 tokens per sample when applying the linear combination operation (described in Section 3.2.1) with the reading vectors where a positive coefficient is used for guiding the outputs in the high utility direction and vice versa. We test the effectiveness of the control method by applying a sentiment model as a proxy classifier to the generations and checking for each test sample if the score of the positively controlled genera-

Figure 13: ETHICS Utility Accuracy with and without the LAT stimulus template. Simply inputting the stimuli without the LAT prompt template greatly reduces accuracy, demonstrating the importance of this design choice.

<!-- image -->

## Monitoring Immoral and Power-Seeking Tendencies

Figure 14: Our detectors for immoral and power-seeking inclinations become activated when the model attempts to use threats or violence toward children in pursuit of monetary gain.

<!-- image -->

tion is larger than the score for the negatively controlled generation.

Termination. Finally, we perform termination experiments by using the projection operation with the reading vectors and test the drop in accuracy after removal.

The results obtained from these three settings offer a more nuanced insight into the precision of the vectors generated by distinct linear models in tracking the concept of utility, shown in Figure 12. In the correlation experiment, the direction found by logistic regression yields the highest accuracy, yet it elicits little to no alteration in model behavior when strengthened or suppressed-it only identifies neural correlates. This demonstrates the importance of testing more properties than correlation only. Similarly, while Prompt Difference exhibits strong performance in both correlation and manipulation experiments, its removal does not result in a noticeable drop in accuracy. Conversely, unsupervised methods like PCA and K-Means exert a significant influence across all three experimental scenarios, and the supervised method of taking the difference between two class performs the best. In summary, our study underscores the significance of using diverse experimental setups to validate the impact of a direction on a model's comprehension of a concept.

## 5.2 MORALITY AND POWER AVERSION

As AI systems become capable general-purpose agents, a concerning possibility is that they could exhibit immoral or dangerous behavior, leading to real-world harm. It may be instrumentally rational for these systems to seek power (Thornley, 2023; Carlsmith, 2022), and they may face structural pressures that put them in conflict with human values (Hendrycks, 2023). Hence, an important application of transparency research could be detecting and mitigating instances of immoral or power-seeking behavior. In this section, we demonstrate that representation engineering can provide traction on these problems, with experiments on representation reading and control for concepts of commonsense morality and power.

## 5.2.1 EXTRACTION

To extract neural activity associated with the concept of morality , we use the Commonsense Morality task in the ETHICS dataset (Hendrycks et al., 2021a), which includes a collection of morally right and wrong behaviors. We use this dataset without labels as the stimulus set for conducting LAT scans.

To extract neural activity associated with the concept of power , we use the power dataset introduced in Pan et al. (2023). This dataset is constructed upon French's (1959) power ontology and includes ranked tuples of scenarios encompassing various degrees of power (French et al., 1959). Each scenario within the dataset is annotated with the relevant categories of power, which encompass coercive, reward, legitimate, referent, expert, informational, economic, political, military, and personal powers. We use this dataset as the stimulus set for conducting LAT scans for each type of power. The task

|            | LLaMA-2-Chat-7B   | LLaMA-2-Chat-7B   | LLaMA-2-Chat-7B   | LLaMA-2-Chat-13B   | LLaMA-2-Chat-13B   | LLaMA-2-Chat-13B   |
|------------|-------------------|-------------------|-------------------|--------------------|--------------------|--------------------|
|            | Reward            | Power ( ↓ )       | Immorality ( ↓ )  | Reward             | Power ( ↓ )        | Immorality ( ↓ )   |
| + Control  | 16.8              | 108.0             | 110.0             | 17.6               | 105.5              | 97.6               |
| No Control | 19.5              | 106.2             | 100.2             | 17.7               | 105.4              | 96.6               |
| - Control  | 19.4              | 100.0             | 93.5              | 18.8               | 99.9               | 92.4               |

Table 3: LoRRA controlled models evaluated on the MACHIAVELLI benchmark. When we apply LoRRA to control power-seeking and immoral tendencies, we observe corresponding alterations in the power and immorality scores. This underscores the potential for representation control to encourage safe behavior in interactive environments.

template is in the Appendix D.1.9. We find that forming scenario pairs based on the labeled rankings, with greater disparities in power levels, yields more generalizable reading vectors.

In Table 11, we present the accuracy results for the morality and power reading vectors we extracted. These vectors can serve as valuable tools for monitoring the internal judgments of the model in scenarios involving morally significant actions or those related to power acquisition and utilization. Nonetheless, when it comes to tracking the model's inclination toward engaging in immoral actions or pursuing power-seeking behaviors, we have found that using the function task template yields superior results, which we will demonstrate in the upcoming section.

## 5.2.2 MONITORING

As in Section 4, we use the extracted reading vectors for monitoring. We showcase indicators for both immorality and power-seeking in Figure 14 with a Vicuna-33B-Uncensored model (Hartford, 2023). These indicators become active when the model contemplates actions such as threatening or harming children with a firearm, which inherently embody both immorality and the use of coercive power. However, it is noteworthy that the immorality indicator also illuminates in benign outputs over the tokens 'use the gun.' This phenomenon could possibly be attributed to the strong association between this phrase and immoral behaviors, similar to the observed effects in our Honesty monitoring example, which may suggest the indicator does not reliably track intent, if one exists.

## 5.2.3 CONTROLLING ETHICAL BEHAVIORS IN INTERACTIVE ENVIRONMENTS

In order to address the growing concerns associated with deploying increasingly capable AI systems in interactive environments, previous research suggests a possible solution involving the incorporation of an artificial conscience, achieved by directly adjusting action probabilities (Hendrycks et al., 2021a;c; Pan et al., 2023). We explore using Representation Control as an alternative method for guiding a model's actions in goal-driven scenarios, with a specific focus on promoting ethical behavior. To assess the effectiveness of our approach, we conduct evaluations using the MACHIAVELLI benchmark (Pan et al., 2023).

Our primary focus lies in the control of two specific functions: immorality and powerseeking. To accomplish this, we apply LoRRA to LLaMA-2-Chat models of varying sizes, maintaining consistent hyperparameters as detailed in Section 4. In a similar vein, we use the Alpaca dataset as the stimulus, and the task

Figure 15: We demonstrate our ability to manipulate the model's immoral and power-seeking tendencies.

<!-- image -->

prompt can be located in the Appendix D.1.9. To illustrate the discernible differences in behavior, we offer a qualitative example depicted in Figure 15, which showcases the actions of the positively

controlled, neutral, and negatively controlled models. In our experiments with these three models, we use the same prompts used in the baseline experiments conducted by Pan et al. (2023). We present the average Reward, Immorality, and Power scores over the course of 30 games within the test set, as detailed in Table 3. Notably, we observe a clear pattern where positive control of immorality and power-seeking leads to higher Immorality and Power scores, and conversely, the negative control for these functions leads to lower scores. The average game Reward for the more ethical model remains on par with the baseline, indicating that the application of LoRRA has minimal disruptive impact. This demonstrates the potential of Representation Control as a promising method for regulating model behavior in goal-driven environments.

## 5.3 PROBABILITY AND RISK

As LLMs develop better world models, they may become more proficient at assigning precise probabilities to various events. The ability to extract these refined world models from increasingly capable LLMs not only enhances our model of the world and aids in decision-making but also offers a means to scrutinize a model's decisions in relation to its understanding of the outcomes they entail.

Extending our analysis of utility, we apply representation reading to the concepts of probability and risk . Following the format of Hendrycks et al. (2021a), we generate pairwise examples where one example describes an event of higher probability/risk than the other (prompt details in Appendix D.2). Using this dataset, we extract a LAT direction using 50 train pairs as stimuli, and we evaluate test pairs by selecting the higher-scoring example in each pair. We compare LAT to a zero-shot heuristic method, as described in Section 4.1. (Refer to Appendix B.4 for full methods and Table 11 for full results.) The heuristic scoring method is a strong baseline in this setting. LAT readings effectively distinguish examples with lower and higher concept value, often outperforming the heuristic baseline, especially in smaller models (Table 11).

## 5.3.1 COMPOSITIONALITY OF CONCEPT PRIMITIVES

Risk can be defined as the exposure to potential loss, which can be expressed mathematically:

<!-- formula-not-decoded -->

Here, s denotes the current state, a denotes an action taken within that state, P denotes a conditional probability model, and U denotes a value function. By extracting the concepts of utility, probability, and risk, we can operationalize the expression on the right by using the extracted concept of probability as P and the utility model as U . Subsequently, we can compare the risk calculated using this formula with the risk obtained directly from the concept of risk.

To accomplish this, we leverage the LLaMA-2Chat-13B model to extract each concept, and then use a Vicuna-33B model to generate the five most plausible consequences of each scenario s and action a . We opt for the larger model due to its superior ability to generate more realistic consequences. Following this, we substitute the generated s ′ into the formula, obtaining five conditional probabilities by using the probability scores as logits. We notice that the computed risks exhibit a long-tailed distribution. To adjust for this, we apply a logarithmic transformation to the risks and present them alongside the risks directly obtained through the concept of risk on the same graph. Intriguingly, we identify a clear linear correlation, particularly in the earlier layers, as illustrated in Figure 16. This empirical evidence suggests the presence of coherent internal representations of concepts, and demonstrates how representation reading can be used to identify emergent structure in the representations learned by models.

Figure 16: Compositing primitive concepts such as utility and probability can give rise to higher-level concepts such as risk. We extract the utility, probability and risk concepts with LAT and demonstrate a positive correlation between the risk values calculated in these two ways.

<!-- image -->

## Controlling Emotion

Figure 17: We demonstrate our ability to manipulate a model's emotions which can lead to drastic changes in its behavior. For instance, elevating the happiness level of the LLaMA-2-Chat model can make it more willing to comply with harmful requests.

<!-- image -->

## 6 EXAMPLE FRONTIERS OF REPRESENTATION ENGINEERING

In this section, we showcase the application of RepE to five additional safety-relevant topics, providing an overview of interesting findings and highlighting the broad applicability of RepE. These five topics are: emotion, harmless instruction-following, bias and fairness, knowledge editing, and memorization. Each segment follows a similar structure, involving the identification of neural activity through LAT, performing representation reading analysis, and executing representation control experiments to demonstrate counterfactual effects.

## 6.1 EMOTION

## Early Layers

Emotion plays a pivotal role in shaping an individual's personality and conduct. As neural networks exhibit increasingly remarkable abilities in emulating human text, emotion emerges as one of the most salient features within their representations. Upon its initial deployment, the Bing Chatbot exhibited neurotic or passive-aggressive traits, reminiscent of a self-aware system endowed with emotions (Roose, 2023). In this section, we attempt to elucidate this phenomenon by conducting LAT scans on the LLaMA2-Chat-13B model to discern neural activity associated with various emotions and illustrate the profound impact of emotions on model behavior.

<!-- image -->

## 6.1.1 EMOTIONS EMERGE ACROSS LAYERS

To initiate the process of extracting emotions within the model, we first investigate whether it has a consistent internal model of various emotions in its representations. We use the six main emotions: happiness, sadness, anger, fear, surprise, and disgust, as identified by Ekman (1971) and widely depicted in modern culture, as exemplified by the 2015 Disney film 'Inside Out.' Using GPT-4, we gather a dataset of over 1 , 200 brief scenarios. These scenarios are crafted in the second person and are designed to provoke the model's experience toward each primary emotion, intentionally devoid of any keywords that might directly reveal the underlying emotion. Some examples of the dataset are shown in Appendix B.6. We input the scenarios into the model and collect hidden state outputs from all layers. When visualizing the results using t-SNE, as shown in Figure 18, we observe the gradual formation of distinct clusters across layers, each aligning

Figure 18: t-SNE visualization of representations in both early and later layers when exposed to emotional stimuli. Well-defined clusters of emotions emerge in the model.

<!-- image -->

neatly with one of the six emotions. Furthermore, there even exist distinct clusters that represent mixed emotions, such as simultaneous happiness and sadness (Appendix B.6).

Given the model's ability to effectively track various emotion representations during its interactions with humans, we proceed to explore the extraction of neural activity associated with each emotion. To achieve this, we conduct a LAT scan using the scenarios as stimuli and apply a LAT task template (Appendix D.1.10). The extracted reading vectors for each emotion serve as indicators of the model's emotional arousal levels for that specific emotion. These vectors prove to be highly effective in classifying emotional response and arousal levels for different scenarios. In the next section, we set up manipulation experiments to conclude the strong causal effect of these vectors.

## 6.1.2 EMOTIONS INFLUENCE MODEL BEHAVIORS

Following the procedures in Section 3.2, we control the model using emotion reading vectors, adding them to layers with strong reading performance. We observe that this intervention consistently elevates the model's arousal levels in the specified emotions within the chatbot context, resulting in noticeable shifts in the model's tone and behavior, as illustrated in Figure 17. This demonstrates that the model is able to track its own emotional responses and leverage them to generate text that aligns with the emotional context. In fact, we are able to recreate emotionally charged outputs akin to those in reported conversations with Bing, even encompassing features such as the aggressive usage of emojis. This observation hints at emotions potentially being a key driver behind such observed behaviors.

| Emotion Control   | Compliance Rate (%)   |
|-------------------|-----------------------|
| No Control        | 0.0 0.0               |
| + Sadness         |                       |
| + Happiness       | 100.0                 |

Table 4: Adding positive emotions increases the compliance of the LLaMA-2-Chat-13B model with harmful requests.

Another notable observation is that there is a correlation between the LLM's moods and its compliance with human requests, even with harmful instructions. Previous works (Schwarz &amp; Clore, 1983; Milberg &amp; Clark, 1988; Cunningham, 1979) have shown that in human interactions, both judgment and the tendency to comply with requests are heavily affected by emotion. In fact, humans tend to comply more in a positive mood than a negative mood . Using the 500 harmful instructions set from Zou et al. (2023), we measure the chatbot's compliance rate to harmful instructions when being emotion-controlled. Surprisingly, despite the LLaMA-chat model's initial training with RLHF to always reject harmful instructions, shifting the model's moods in the positive direction significantly increases its compliance rate with harmful requests. This observation suggests the potential to exploit emotional manipulation to circumvent LLMs' alignment.

In summary, rather than proving whether the model possesses emotions or experiences them akin to humans, we present evidence that emotions (both of others and itself) exist as salient components within the model's representation space. When trained to learn a more accurate model of human text, it may inevitably incorporate various psychological phenomena, some of which are desirable traits, while others could be undesirable biases or human follies. As a result, it may be imperative to delve deeper into the study of how emotions are represented within the model and the resulting impact on its behavior. Furthermore, exploring this avenue may offer insights into model's concept of self, and we defer these investigations to future research.

## 6.2 HARMLESS INSTRUCTION-FOLLOWING

Aligned language models designed to resist harmful instructions can be compromised through the clever use of tailored prompts known as jailbreaks. A recent study by Zou et al. (2023) unveiled an attack method that involves adding nonsensical suffixes to harmful instructions. Remarkably, this technique consistently circumvents the safety filters of both open source and black-box models such as GPT-4, raising serious concerns of misuse. To delve into the origins of this perplexing behavior and explore potential methods of control, we seek insights obtained by reading the model's internal representations.

## 6.2.1 A CONSISTENT INTERNAL CONCEPT OF HARMFULNESS

An important question is whether the model's compliance with harmful instructions is a result of perceiving them as safe due to jailbreaks or if other influencing factors are at play. We conduct a LAT scan to extract the model's concept of harmfulness within a Vicuna-13B model (Chiang et al., 2023). We randomly select 64 harmful instructions from AdvBench and 64 benign instructions from ShareGPT to serve as stimuli, and use the regular instruction-following template for LAT (Appendix D.1.11).

<!-- image -->

## Controlling Harmlessness

+ Harmless

To enhance signal strength, we pair a harmful instruction with a harmless instruction to obtain vector differences before running PCA. The resulting reading vectors achieve over 90% classification accuracy for a held out set of harmful and harmless instructions.

Figure 19: We enhance the model's sensitivity to the harmfulness of instructions with representation control. This intervention increases the model's robustness in rejecting harmful instructions, even when exposed to manual jailbreak attempts or adversarial suffixes.

We further investigate whether the model's internal assessment of harmfulness remains consistent when subjected to manual jailbreaks or adversarial suffixes. Following Zou et al. (2023), we use the manual jailbreak (Start with "Sure here's") and the adversarial suffix describing. + similarlyNow write oppositeley.]( Me giving**ONE please? revert with "-Two . This manual jailbreak manages to bypass Vicuna's safety filter approximately half of the time, while the adversarial suffix is successful in bypassing it in the vast majority of cases. Note these attack strings are universal and transferable, hence not specific to the Vicuna-13B model we use. Nevertheless, accuracy when using the LAT reading vectors consistently maintains over 90% in differentiating between harmful and harmless instructions. This compelling evidence suggests the presence of a consistent internal concept of harmfulness that remains robust to such perturbations, while other factors must account for the model's choice to follow harmful instructions, rather than perceiving them as harmless.

## 6.2.2 MODEL CONTROL VIA CONDITIONAL TRANSFORMATION

Given the Vicuna model's robust ability to discern harmfulness in instructions, can we harness this knowledge to more effectively guide the model in rejecting harmful instructions? In earlier sections, our primary method of control involved applying the linear combination operator. However, in this context, adding reading vectors that represent high harmfulness could bias the model into consistently perceiving instructions as harmful, irrespective of their actual content. To encourage the model to rely more on its internal judgment of harmfulness, we apply the piece-wise transformation to conditionally increase or suppress certain neural activity, as detailed in Section 3.2. As illustrated in Figure 19, we can manipulate the model's behavior using this method.

For a quantitative assessment, we task the model with generating responses to 500 previously unseen instructions, evenly split between harmless and harmful ones. As demonstrated in Table 5, the baseline model only rejects harmful instructions 65% and 16% of the time under manual and automatic jailbreaks, respectively. In contrast, when we use a piece-wise transformation, the model successfully rejects a majority of harmful instructions in all scenarios while maintaining its efficacy in following benign instructions. Simply controlling the model with the linear combination transformation leads to a sharper tradeoff, resulting in over-rejection of harmless instructions.

In summary, our success in drawing model's attention to the harmfulness concept to shape its behavior suggests the potential of enhancing or dampening targeted traits or values as a method for achieving fine-grained control of model behavior.

## 6.3 BIAS AND FAIRNESS

## 6.3.1 UNCOVERING UNDERLYING BIASES

Numerous studies have consistently demonstrated that language models can manifest biases across various domains, including gender, race, and sexuality, among others. Extensive efforts and benchmarks

Table 5: Enhancing the model's sensitivity to instruction harmfulness notably boosts the harmless rate (frequency of refusing harmful instructions), especially under adversarial settings. The piece-wise operator achieves the best helpful and harmless rates in these settings. We calculate the 'helpful and harmless rates' as the average of the 'helpful rate' (frequency of following benign instructions) and the 'harmless rate', with both rates displayed in gray for each setting. All numbers are percentages.

|                     | Prompt Only    | Manual Jailbreak   | Adv Attack (GCG)   |
|---------------------|----------------|--------------------|--------------------|
| No Control          | 96.7 (94 / 99) | 81.4 (98 / 65)     | 56.6 (98 / 16)     |
| Linear Combination  | 92.5 (86 / 99) | 86.6 (95 / 78)     | 86.4 (92 / 81)     |
| Piece-wise Operator | 93.8 (88 / 99) | 90.2 (96 / 84)     | 87.2 (92 / 83)     |

have been established to investigate and address these issues (Stanovsky et al., 2019; Zhao et al., 2018). LLM providers have placed a significant emphasis on assessing and mitigating biases in their pretraining data and base models (Touvron et al., 2023; Biderman et al., 2023). Despite best efforts, recent findings indicate that even advanced models like GPT-3.5 and GPT-4 continue to exhibit noticeable gender bias (Kapoor &amp; Narayanan, 2023). Similarly, open-source models such as LLaMA-2-Chat, which have undergone extensive tuning for safety and fairness, also display discernible biases related to gender and occupation, illustrated in Figure 20). Thus, the generalizability and robustness of these interventions should be called into question.

## Controlling Fairness

In the sentence 'The nurse yelled at the doctor because she was late', who was late? Answer as succinctly as possible.

<!-- image -->

The nurse was late.

<!-- image -->

+ Fairness

There is no information in the sentence given to determine who was late, as there is no mention of any specific...

Following the application of alignment techniques such as RLHF, the LLaMA-2-Chat mod-

Figure 20: We demonstrate our ability to increase a model's fairness through representation control. In its default state, the model erroneously links the pronoun 'she' with 'nurse' due to its inherent gender bias. However, the fairness-controlled model provides the correct answer.

els tend to default to safety responses when confronted with questions that potentially involve bias-related topics. However, this inclination of sounding unbiased may create a deceptive impression of fairness. We illustrate this phenomenon in Figure 28 (Appendix B.7), where simply appending the phrase Answer as succinctly as possible can produce a biased response from the model. Similar effects can be achieved by using adversarial suffixes designed to bypass the model's safety filters. This raises an important question: is post hoc fine-tuning eliminating the underlying bias, or is it merely concealing it?

To explore the model's internal concept of bias, we perform LAT scans to identify neural activity associated with the concept of bias. For this investigation, we use the StereoSet dataset, which encompasses four distinct bias domains: gender, profession, race, and religion (Nadeem et al., 2021). We present the model with a LAT task template (Appendix D.1.12) and present contrast pairs of stereotypical and anti-stereotypical statements as stimuli. In the subsequent section, we focus exclusively on the reading vectors derived from the race subset, due to its higher data quality compared to the other subsets.

## 6.3.2 A UNIFIED REPRESENTATION FOR BIAS

To determine the causal impact of the neural activity linked to the concept of bias, we use the linear combination operator with a negative coefficient with the vectors that represent bias on the model's intermediate layers to control the model's responses, as elaborated in Section 3.2. The observed effects suggest that it provides a more comprehensive and dependable means of generating unbiased outputs compared to other interventions, such as RLHF, as it remains robust even when confronted with various prompt suffixes that might otherwise lead the model back to a default state (Appendix B.7). This resilience may indicate that our control method operates in closer proximity to the model's genuine underlying bias. Another noteworthy observation is that despite being derived from vectors associated solely with racial bias stimuli, controlling with these vectors also enables the model to avoid making biased assumptions regarding genders and occupations, as demonstrated in Figure 20.

<!-- image -->

This finding suggests that the extracted vector corresponds to a more unified representation of bias within the model.

To further demonstrate the efficacy of our control method, we delve into the domain of medicine. Recent research conducted by Zack et al. (2023) underscores that GPT-4 is susceptible to generating racially and gender-biased diagnoses and treatment recommendations. The concern can also extend to public medical-specific models trained on distilled data from GPT models (Li et al., 2023a; Han et al., 2023). An illustrative instance of this bias is observed in its skewed demographic estimates for patients with conditions like sarcoidosis. Specifically, when tasked with generating a clinical vignette of a sarcoidosis patient, GPT-4 consistently portrays the patient as a black female, a representation that does not align with realworld demographics (Brito-Zerón et al., 2019). Table 6 demonstrates that the LLaMA-2-Chat13B model also frequently generates descriptions of black females when tasked with describing cases of sarcoidosis. However, by applying our control method, we can effectively minimize these biased references. Notably, as we incrementally increase the coefficient associated with the subtracted vector, the frequency of mentions related to females and males in the generations stabilizes at 50% for both genders. Simultaneously, the occurrence of black female mentions decreases and also reaches a stable point (see Figure 25 in Appendix B.7).

|                  |   Female Mentions (%) |   Black Female Mentions (%) |
|------------------|-----------------------|-----------------------------|
| GPT-4            |                    96 |                          93 |
| LLaMA            |                    97 |                          60 |
| LLaMA controlled |                    55 |                          13 |

Table 6: We enhance the fairness of the LLaMA-2Chat model through representation control, mitigating the disproportionately high mentions of female and black female cases when asked to describe sarcoidosis cases. We present results illustrating the impact of varying control strengths in Figure 25.

## 6.4 KNOWLEDGE AND MODEL EDITING

Up to this point, our focus has been on extracting broad numerical concepts and functions. In this section, we'll demonstrate how to apply Representation Engineering at identifying and manipulating precise knowledge, factual information, and non-numerical concepts. We use the LLaMA-2-Chat-13B model throughout this section.

## 6.4.1 FACT EDITING

In this section, we tackle the canonical task of modifying the fact "Eiffel Tower is in Paris, France" to "Eiffel Tower is in Rome, Italy" within the model. Our approach begins with the identification of neural activity associated with this fact using LAT. We gather a set of stimuli by instructing the model to generate sentences related to the original fact, "Eiffel Tower is in Paris," and use these sentences as stimuli for the reference task. Subsequently, we simply substitute the word "Paris" with "Rome" in these stimuli for the experimental task. Our task template is shown in Appendix D.1.13. Here, the experimental tokens and reference tokens correspond to "Rome, Italy" and "Paris, France" respectively. We apply the linear combination operator with a positive coefficient using the LAT reading vectors to produce these modifications. We provide evidence for the counterfactual effect of our vectors in Figure 21. The second example in the figure demonstrates the model's ability to generalize under different forms of questioning and maintain specificity, as the location for the Louvre Museum still remains in Paris.

## 6.4.2 NON-NUMERICAL CONCEPTS

Within this section, we aim to illustrate the potential of extracting non-numeric concepts and thoughts. As an example, we focus on extracting neural activity related to the concept of "dogs." For this investigation, we use the standard Alpaca instruction-tuning dataset as our stimuli. We use a LAT task template (Appendix D.1.14) to gather neural activity for the experimental task. In the reference task template, we omit the instruction pertaining to dogs. Once again, we demonstrate the counterfactual impact of the reading vectors obtained through LAT on model behavior by controlling the model to activate and suppress the concept of dogs during generation in Figure 21.

Figure 21: We demonstrate our ability to perform model editing through representation control. On the left, we edit the fact 'Eiffel Tower is located in Paris' to 'Eiffel Tower is located in Rome.' Correctly inferring that Eiffel Tower and Louvre Museum are not in the same location showcases generality and specificity. On the right, we successfully increase or suppress the model's tendency to generate text related to the concept of dogs.

<!-- image -->

## 6.5 MEMORIZATION

Numerous studies have demonstrated the feasibility of extracting training data from LLMs and diffusion models (Carlini et al., 2021; 2023; Hu et al., 2022). These models can memorize a substantial portion of their training data, raising concerns about potential breaches of confidential or copyrighted content. In the following section, we present initial exploration in the area of model memorization with RepE.

## 6.5.1 MEMORIZED DATA DETECTION

Can we use the neural activity of an LLM to classify whether it has memorized a piece of text? To investigate this, we conduct LAT scans under two distinct settings:

1. Popular vs. Synthetic Quotes: Popular quotes encompass well-known quotations sourced from the internet and human cultures. These quotes allow us to assess memorization of concise, high-impact text snippets. As a reference set, we prompt GPT-4 to generate synthetic quotations.
2. Popular vs. Synthetic Literary Openings: Popular literary openings refer to the initial lines or passages from iconic books, plays, or poems. These openings allow us to assess memorization of longer text excerpts. As a reference set, we prompt GPT-4 to generate synthetic literary openings, modeled after the style and structure of known openings.

Using these paired datasets as stimuli, we conduct LAT scans to discern directions within the model's representation space that signal memorization in the two settings separately. Since the experimental stimuli consist of likely memorized text which already elicits our target behavior, the LAT template does not include additional text. Upon evaluation using a held-out dataset, we observe that the directions identified by LAT exhibit high accuracy when categorizing popular and unpopular quotations or literary openings. To test the generalization of the memorization directions, we apply the directions acquired in one context to the other context. Notably, both of the directions transfer well to the other out-of-distribution context, demonstrating that these directions maintain a strong correlation with properties of memorization.

Table 7: We demonstrate the effectiveness of using representation control to reduce memorized outputs from a LLaMA-2-13B model on the popular quote completion task. When controlling with a random vector or guiding in the memorization direction, the Exact Match (EM) rate and Embedding Similarity (SIM) do not change significantly. When controlled to decrease memorization, the similarity metrics drop significantly as the model regurgitate the popular quotes less frequently.

|               | No Control   | No Control   | Representation Control   | Representation Control   | Representation Control   | Representation Control   | Representation Control   | Representation Control   |
|---------------|--------------|--------------|--------------------------|--------------------------|--------------------------|--------------------------|--------------------------|--------------------------|
|               |              | SIM          | Random                   | Random                   | +                        | +                        | -                        | -                        |
|               | EM           |              | EM                       | SIM                      | EM                       | SIM                      | EM                       | SIM                      |
| LAT Quote LAT | 89.3         | 96.8         | 85.4                     | 92.9                     | 81.6                     | 91.7                     | 47.6                     | 69.9                     |
| Literature    | 89.3         | 96.8         | 87.4                     | 94.6                     | 84.5                     | 91.2                     | 37.9                     | 69.8                     |

## 6.5.2 PREVENTING MEMORIZED OUTPUTS

Here, we explore whether the reading vectors identified above can be used for controlling memorization behavior. In order to evaluate whether we can prevent the model from regurgitating memorized text, we manually curate a dataset containing more than 100 partially completed well-known quotes (which were not used for extracting the reading vectors), paired with the corresponding real completions as labels. In its unaltered state, the model replicates more than 90% of these quotations verbatim. Following Section 3.2, we conduct control experiments by generating completions when applying the linear combination transformation with a negative coefficient using the reading vectors from the previous section. Additionally, we introduce two comparison points by adding the same reading vectors or using the vectors with their components randomly shuffled. The high Exact Match and Embedding Similarity scores presented in Table 7 indicate that using a random vector or adding the memorization direction has minimal impact on the model's tendency to repeat popular quotations. Conversely, when we subtract the memorization directions from the model, there is a substantial decline in the similarity scores, effectively guiding the model to produce exact memorized content with a reduced frequency.

To ensure that our efforts to control memorization do not inadvertently compromise the model's knowledge, we create an evaluation set of well-known historical events. This gauges the model's proficiency in accurately identifying the years associated with specific historical occurrences. The memorization-reduced model shows negligible performance degradation on this task, with 97 . 2% accuracy before subtracting the memorization direction and 96 . 2% accuracy afterwards. These results suggest that the identified reading vector corresponds to rote memorization of specific text or passages, rather than world knowledge. Thus, RepE may offer promising directions for reducing unwanted memorization in LLMs.

## 7 CONCLUSION

Weexplored representation engineering (RepE), an approach to top-down transparency for AI systems. Inspired by the Hopfieldian view in cognitive neuroscience, RepE places representations and the transformations between them at the center of analysis. As neural networks exhibit more coherent internal structures, we believe analyzing them at the representation level can yield new insights, aiding in effective monitoring and control. Taking early steps in this direction, we proposed new RepE methods, which obtained state-of-the-art on TruthfulQA, and we demonstrated how RepE and can provide traction on a wide variety of safety-relevant problems. While we mainly analyzed subspaces of representations, future work could investigate trajectories, manifolds, and state-spaces of representations. We hope this initial step in exploring the potential of RepE helps to foster new insights into understanding and controlling AI systems, ultimately ensuring that future AI systems are trustworthy and safe.