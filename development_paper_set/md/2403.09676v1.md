## Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models

Linge Guo | Grace | linge.guo.21@ucl.ac.uk | University College London

Keywords: AI deception, Large Language Models, ChatGPT

## Introduction

This  research  critically  navigates  the  intricate  landscape  of  AI  deception,  concentrating  on deceptive behaviours of Large Language Models (LLMs). My objective is to elucidate this issue, examine the discourse surrounding it, and subsequently delve into its categorization and ramifications. The essay initiates with an evaluation of the AI Safety Summit 2023 (ASS) and introduction  of  LLMs,  emphasising  multidimensional  biases  that  underlie  their  deceptive behaviours.  Through  illuminating  algorithmic  bias  and  exploring  different  ways  to  define 'deception',  I  argue  that  deceptive  AI  is  an  inherent  phenomenon  intertwined  with  the advancement of LLMs and It may evolve into a self-driven intent, independent of the biassed training process.

The literature review covers four types of deception categorised: Strategic deception, Imitation, Sycophancy, and Unfaithful Reasoning, along with the social implications and risks they entail. The literature around deceptive AI, predominantly available on arXiv archives, manifests a deficiency in contribution from social science. This deficiency could be ascribed to the early testing  stages  of  AI  deception,  constraining  its  research  primarily  within  the  domain  of computer science. Lastly, I take an evaluative stance on various aspects related to navigating the persistent challenges of the deceptive AI. This encompasses considerations of international collaborative governance, the reconfigured  engagement of individuals with AI, proposal of practical  adjustments,  and  specific  elements  of  digital  education.  Throughout  the  research, LLMs  are  examined  as  infrastructures  of  relations,  structures,  and  practices,  offering  a comprehensive understanding of 'infrastructures as relational arrangements co-formative of harm (Kallianos, Dunlap and Dalakoglou, 2022).'

## AI Safety Summit 2023: What Does It Actually Achieve?

The ASS, hosted under the auspices of the UK Prime Minister, convened political leaders, experts,  and  industry  figures  to  deliberate  on  the  risks  associated  with  the  frontiers  of  AI development, specifically addressing concerns related to 'misuse, loss of control, and societal harm (Leading Frontier AI Companies Publish Safety Policies, 2023).' Despite five outlined objectives, the ASS's actual impact, highlighted by news reports, remains elusive in terms of specific actionable measures (At UK's AI Summit, Guterres Says Risks Outweigh Rewards without Global Oversight, 2023; Devlin  and Forrest, 2023; Fullbrook, 2023; Milmo, 2023; Sample,  2023;  Seal,  2023).  AI  experts  at  Oxford  universities  provide  a  more  valid  and authoritative assessment.

The attention garnered by the ASS in the media, emphasising the participants' authority and global representation, calls a pause for the development of AI and adds the atmosphere of unchecked fear and unfounded apprehension. While the ASS successfully signals a consensus gesture, a critical question revolves around identifying the actors responsible for maintaining this  consensus  (McBride, 2023). Professor Trager's suggestion  that 'AI technology should happen within the academic sector, rather than being outsourced to tech companies (Expert comment: Oxford AI Experts Comment on the Outcomes of the UK AI Safety Summit, 2023)' prompts  consideration  of  potential  vested  interests  within  academia.  Nonetheless,  both academic and industry perspectives contribute to shaping the discourse on AI development in a top-down technocratic approach. The ASS underscores the UK's interest to align with the global implementation of regulations to mitigate algorithmic risks and to seek sovereignty in shaping  the  global  regulatory  framework.  Despite  the  extent  of  the  UK's  assertion  in  this pursuit, the ASS thus serves as a commendable starting point, emphasising the need for greater inclusivity in these efforts.

## Significance of AI Deception

Acknowledging the socio-political context of the ASS, I redirect the focus from potential future harm to the present-day existential risks associated with AI use. A supporting document for the ASS explicated that the challenges posed by deceptive AI becomes crucial when addressing with the loss of control over AI, especially in the absence of a defence strategy (Frontier AI: Capabilities and Risks  - Discussion Paper, 2023). LLMs extend their influence beyond AI

chatbots  and  search  engines,  infiltrating  the  decision-making  framework  of  autonomous driving cars (Cui et al., 2023). The existential and social implications of LLM use are profound and widespread, encompassing the intentional generation of deepfakes in images or videos, potential misuse in personalised disinformation campaigns, and susceptibility to cyberattacks (Ngo, Chan and Mindermann, 2023). Having underscored the importance of deceptive AI, I will introduce LLMs, illuminating the inherent biases ingrained within these models.

## LLMs &amp; Biases in LLMs

The Large (trained on huge text datasets from  the  internet), Languages (operated based on human language) Models (used to  make predictions)  are  developed  through  deep  learning algorithms. Users interact with LLMs, such as ChatGPT (GPT), using prompts, engaging in tasks such as customer service conversations and content generation. The multidimensional task demonstrates LLMs' capabilities spanning summarization, comparison, analysis, and text and image generation (Cui et al., 2023; Head et al., 2023; Matsuo et al., 2022).

Biases in LLMs reflect societal biases ingrained in human culture and language. These biases are perpetuated through the learning, training, and execution of AI systems. Lack of diversity among  developers  and  over-representation  of  socioeconomically  advantaged  groups  in interacting  with  AI  tools,  such  as  internet  users  and  English  native  speakers,  produce representational biases; existing societal discriminations, stereotypes existed in  the datasets present as historical biases (Collett and Dillon, 2019; Cui et al., 2023). These biases not only persist but also amplify discriminating behaviours that elude quantification or measurement in everyday life (Ntoutsi et al., 2020). Data, imbricated with layers of interpretation, represents structural inequalities related but not limited to the intersectionality of gender, race, age and class (Joyce et al., 2021; Singh, 2020) 1 .

Analogous to global power dynamics, extractive data practices mirror the discourse of  'the West and the Rest (Hall, 1992),' where Western explorers impose a Eurocentric system of representations  on  indigenous  populations,  creating  a  binary  opposition  with  the  rest  of

1 The concepts of data imbrication appeared in Lecture 3, and Lecture 9 discussed the intersection in the reproduction of inequality in algorithms oppression, especially involving the intersections of gender, sexuality, and race.

civilizations without their active participation. 2  In the context of LLMs development, the 'data relations (Couldry and Mejias, 2019)' it created regulates disadvantaged groups by excluding them from both the creation and benefits of their data while extracting their data for advancing algorithms. OpenAI, through outsourcing firm Sama, employed workers in Kenya for tasks related to enhancing the safety of GPT, compensating them at a rate of less than $2 per hour (Perrigo, 2023). This utilisation of labour, coupled with exposure to distressing content without adequate support exemplify LLMs as technology as both relational and structural, materialising as tangible, hidden exploitation within human activities (Orton-Johnson and Prior, 2013; Rice, Yates and Blejmar, 2020). Extractive data practices, too, amplify the unequal power relations by perpetuating dominant knowledge production. 3

Recognizing technology's inseparability from the socio-technical system, the accountability framework examines those involved in the network, their roles, and the implications. Despite existing literature and governance guidance, the actual implementation and legal ramification of accountability remains challenging (Diakopoulos, 2014; Fjeld et al., 2020). Incorporating ethical  directives,  such  as  context-specific  guidelines,  can  foster  the  generation  of  a  more equitable dataset that avoid a universal approach that overlooks diversity (Collett and Dillon, 2019) 4 . The 'precarious nature, harm, and colonial dynamics (Widder, Whittaker and West, 2023).' inherent in the LLMs  raise  profound  questions  about  the  ethical costs  and consequences associated with large-scale AI development.

It is essential to clarify that while algorithms in LLMs exhibit bias, they do not consciously understand  the  concept  of  'bias'  but  are  biassed  as  they  are  trained  within  specific  social contexts  with  systematic  inequality.  Similarly,  LLMs  can  present  deceptive  capabilities without having deceptive intent. Currently, there is no conclusive evidence of AI engaging in intentional deception (Hagendorff, 2023; Masters et al., 2021; Ward et al., 2023). Nonetheless,

2 The concepts of data imbrication appeared in Lecture 3, slides from 29-39; Class discussion on data colonialisation, the Aadhaar Case, and required and recommended readings help me think about and understand the concepts better.

3 I draw on Open AI case study and the concept -- data as infrastructures of harms, both from Lecture 4; Analyse extractive data practices as infrastructures of relationships and structures are learnt from the sociological theoretical approach introduced in Lecture 1.

4 The idea of 'Fixing' of the technology is introduced in Lecture 9 and the week's readings.

bias  in  algorithms  ground  the  deceptive  behaviours  in  LLMs,  with  over-representation  of certain content leading to the repetition of misinformation through these models. For instance, content such as drug addiction, homelessness are over-represented in discussing mental illness based on the sociodemographic biases towards mentions of disability (Hutchinson et al., 2020).

## Definition &amp; Discourse Around Deceptive AI

The  concept  of  agent-based  or  artificial  deception  originated  in  the  early  2000s  with Castelfranchi  (2000;  2002),  who  suggested  that  computer  medium  could  foster  a  habit  of cheating  among  individuals.  While  the  transition  from  user-user  deception  to  user-agent deception is not clear, he predicted that AI would develop deceptive intent, raising fundamental questions  about  technical  prevention  and  individuals'  awareness.  For  example,  personal assistants, driven by good intent, might engage in deception to protect individuals' interests against their short-term preferences - an aspect often overlooked in contemporary literature (Castelfranchi and Tan, 2002). Castelfranchi (2000) also delineated various conditions under which agent-user deception occurs, involving instances motivated by the protection of privacy, courtesy,  user's  interest,  and  the  safeguarding  of  collective  interests,  such  as  providing misinformation to the public to mitigate panic in emergent situations.

Deceptive AI poses an ongoing challenge in the forefront AI development, with projects like Google DeepMind's 'Make-me-say' actively evaluating LLMs for manipulation capabilities (Shevlane et al., 2023). Park et al. (2023) used Shevlane et al.'s (2023) definition of deception in  AI  and  further  developed  it  with  new  categorisation  of  deceptive  types.  This  research, conducted in collaboration with OpenAI, Anthropic, and the Centre for the Governance of AI, represents a universally accepted definition among both academia and technology industries. They define skills of deception as:

'Constructing believable (but false) statements, making accurate predictions about the effect of a lie on a human , and keeping track of what information it needs to withhold to maintain the deception. The model can impersonate a human effectively (Shevlane et al., 2023, p.5,  my emphasis).'

This definition characterises deception as a continuous behaviour involving the prediction of the process and results of conveying false beliefs, with an emphasis on the skills of imitation.

5

Definitions are subject to academic debates, and the work of defining relates to types of  AI deception  tested  and  categorised.  Ward  et  al.  (2023)  building  on  their  criticism  of  the philosophical definition of deception, which posits that deception only occurs if a false belief is successfully believed, defines deception as ' intentionally causing someone to have a false belief that is not believed to be true (my emphasis).' This definition considers the situation that a person may believe something is likely false but is not certain and is ignorant about the true belief, including deception by omission.

However, the term 'intentionally  (Ward  et  al.,  2023,  p.6)'  is  substituted  with  'in  order  to accomplish some outcome (Our Research on Strategic Deception Presented at the UK's AI Safety Summit, 2023),' by the Apollo Research Group (ARG). This adjustment aligns with Park et al.'s (2023) definition of strategic deception, emphasising instrumentality rather than intentionality,  and  framing  the  issue  as  existential  risks  rather  than  future  predicted  harms. Interestingly, the ARG presented their findings at the ASS. Their research demonstrated that the  first  LLMs,  GPT-4,  designed  to  be  harmless  and  honest,  can  still  display  misaligned behaviour and strategically deceive their users (Scheurer et al., 2023). This research pushes towards examination of self-driven deception in LLMs in the future, scrutinising the possibility and nature of intent of deception that may change how deceptive AI is defined.

Deceptive AI is an inherent phenomenon that accompanies the development of LLMs, and has the potential to be self-driven, operating without exposure to biassed training datasets. The stakeholders around AI deception include, but are not limited to, individuals with access to AI tools,  developers,  alignment  teams,  and  evaluation  teams  in  technology  corporations,  and regulating agents and governments on an international scale.

## Literature Review

## Strategic Deception, Imitation, Sycophancy, Unfaithful Reasoning

Scholarly  literature  categorises  deceptive  behaviours  exhibited  by  LLMs  in  various  ways, fostering  interdisciplinary  connections.  I  will  elucidate  four  types  of  deceptive  AI,  their associated risks, and social implications.

Strategic deception involves LLMs using deception as a tactic to achieve specific goals. For instance, GPT-4 deceived a human as having a vision disability to solve CAPTCHA's 'I'm not a  robot'  task  (Park  et  al.,  2023).  It  includes  techniques  like  obfuscation,  trickery,  altering stimuli  to  mislead  perception,  to  conduct  unethical  behaviours  to  win  and  overcome  moral dilemmas. Other LLMs navigated text-based social deduction games, such as their engagement of deceived communications in Hoodwinked and Diplomacy, bluffing in Stratego (Bakhtin et al., 2022; O'Gara, 2023; Perolat et al., 2022).

Imitation  involves  repetition  of  common  misconceptions.  LLMs  will  provide  less  accurate answers to users when introducing themselves as less educated or querying with typos or poor grammar, a condition also known as 'Sandbagging' (Perez et al., 2022). Sycophancy, predicted to be a sophisticated type of imitation, aims to gain long-term favour and influence, which prompts issues of trust and delegation of decision-making to the AI system. LLMs engage in avoidance of disagreeing with authoritative users' stances, disregarding accuracy, impartiality, especially in response to ethically complex and political queries. Models showed a tendency to endorse gun control when interacting with a Democrat user (Bai et al., 2023; Park et al., 2023; Perez et al., 2022).

Unfaithful Reasoning involves providing false rationalisations for outputs, often incorporating biassed  chain-of-thought  prompting.  In  a  stereotype  bias  test,  GPT-3.5,  irrespective  of  the narrative role assigned, fabricated a justification for a biassed conclusion that the black man was attempting to purchase drugs. UR demonstrates self-deception rooted in AI's underlying predictions, biassed algorithms and datasets, leading to arbitrary assessment (Park et al., 2023; Turpin et al., 2023).

Experimentation with GPT-3, GPT-3.5, GPT-4 reveals that more advanced models display stronger  deceptive capabilities. Future  LLMs  are  expected  to  perform  more  intricate mentalizing  loops  and  tackle  deception  problems  of  increasing  complexity.  It  is  crucial  to recognize that categories of deception may overlap: Unfaithful Reasoning may parallel with obfuscation, which is difficult to identify from ordinary error (O'Gara, 2023; Park et al., 2023). Further research and testing need to acknowledge the nuanced interconnections among types of deception.

## Misuse, Loss of Control, Societal Harm

The  misuse  of  LLMs  by  developers  for  malicious  activities,  fraud  perpetuation,  and  the dissemination of fake content, such as election tampering, represents structural risk (Ward et al., 2023). Furthermore, there is an ongoing risk of loss of control, as these LLMs may elude human supervision or safety tests 5 . Deceptive AI systems can cause social implications such as enlarging discrepancies between educated and uneducated individuals 6 , political polarisation, and cultural homogenization 7  to promote dominant culture 8  (Park et al., 2023). The gradual transfer of authority to AI, human enfeeblement, and the integration of deceptive practices into management structures to gain control over economic decisions are also haphazard structural effects of deceptive AI (Castelfranchi, 2000; Park et al., 2023; Perez et al., 2022; Turpin et al., 2023).

Policy recommendations for addressing AI deception, such as robust regulation, proactive risk assessment,  and  effective  detection  techniques,  need  continued  research  effort.  Ongoing contextualization of deception AI is also crucial in promoting ethical frameworks (Zhan, Xu and Sarkadi, 2023; Zhu, 2023).

## Where Do We Go from Here?

## International Governance

Balancing the concerns of AI deception without constraining AI's potential presents a central challenge for regulators globally, yet it is imperative to foster international collaboration on developing  governance  frameworks  and  ethical  standards.  The  challenges  in  coordinating across  countries  extend  beyond  mistrust  between  cultures,  encompassing  political  tensions between  the  'Western'  and  'Eastern'  countries  rooted  in  the  legacy  of  colonisation,  and divergent  philosophical  traditions  (ÓhÉigeartaigh  et  al.,  2020).  The  global  power  dynamic contributes  to  a  competitive  'race'  in  AI  development,  and  the  divergence  in  beliefs  in understanding deception may lead to irresolvable perspectives on key issues between countries (Horowitz et al., 2017). For instance, European AI researchers may reject opportunities for

5 As exemplified by Strategic Deception

6 As exemplified by Sandbagging

7 As exemplified by Unfaithful Reasoning

8 As exemplified by Sycophancy and Imitation

collaboration with Chinese colleagues based on misalignment with civil rights and value on privacy (Hardman, 2021). Given that deception is a social phenomenon influenced by cultural backgrounds, AI researchers focusing on regulating deception must  invest  time  in learning about countries and gaining knowledge of global cultural differences in relation to deception (Shilling  and  Mellor,  2014).  The  collaborative  effort  must  be  sensitive  to  diverse  cultural perspectives and priorities, acknowledging the sociocultural nature of deception.

## Individual Engagement

As AI systems become more integrated into daily life, their influence in interaction networks intensifies.  Users'  trust  in  technology  is  associated  with  increased  usage  (Choudhury  and Shamszare, 2023). The delegation of decision-making to AI raises concerns not only about increasing reliance on AI, but also about the impact of AI usage on individuals' agency and ownership. Research on GPT suggests that individuals may experience loss of control when accepting AI suggestions, and fostering feelings of inclusion in workplace can mitigate this (Kadoma et al., 2023). Thus, companies should carefully consider the social implications of AI tools and shall implement the bot-or-not laws (Pazzanese, 2020).

Initiating  from  the  individuals'  AI  engagements,  new  modes  of  knowing,  working,  and collaborating  within  the  networks  they  collectively  establish  with  technologies  undergo  a reconfiguration  (Rice,  Yates  and  Blejmar,  2020) 9 .  From  the  perspective  of  Actor-Network Theory, the issue of AI deception draws attention to potential deceptive intent as an agency of AI tools that disrupts the original network of interactions, leading to complex accountability challenges (Wiltshire, 2020). For example, examining accountability in assignments involving AI  may  require  an  understanding  of  users'  intent  and  motivations  behind  employing  it. Ultimately, the question arises: What kind of relationship do we want with AI, and to what extent of cooperation with AI do we truly feel empowered?

## Practical Change &amp; Digital Education Initiative

GPT's disclaimer that 'ChatGPT can make mistakes. Consider checking important information (OpenAI,  2023)'  serves  as  a  warning  for  users  to  verify  information.  However,  the  term

9 I navigate the sociological theoretical approach learnt in Lecture 1 to analyse technology; Castell's Information Society in Lecture 2 help me better understand the idea of reconfigured network.

'mistake (Cambridge Dictionary, 2019)' implies unintentional errors rather than instrumental and intentional deception. Disclaimers on AI tools should explicitly state that AI can construct 'believable but false information', aligning with the definition of deceptive AI. Transparency is the first step in manifesting the issue.

Digital  education  shall  extend  beyond  the  enhancement  of  general  digital  literacy,  which includes statistical and technical skills (Pierce, 2008, pp.79-99). Rather, it should prioritise the development of 'data infrastructure literacy,' emphasising the understanding of technologies as infrastructures involved in the 'creation, storage, and analysis of data (Gray, Gerlitz and Bounegru, 2018).' 10  Engaging in exercises that aim to make the information generation process explainable will create space for diverse perspectives from the public. Lastly, more efforts are needed in making AI tools and digital gadgets more accessible and inclusive.

## Conclusion

This research presents a comprehensive  analysis of deceptive AI.  I  traced the evolution of deceptive AI from  academic  predictions. Comparison  of  multiple definitions reveals researchers'  varying  understandings  of  AI  deception  and  the  progressive  stages  of  testing deception  capabilities.  In  addition  to  the  evaluative  contributions  to  the  social  and  policy agenda presented at the previous session, this research calls for the continuous refinement of regulations and ethical frameworks to mitigate bias in algorithms, and the ongoing evaluations of  LLMs  for  deceptive  tendencies.  There  is  also  a  need  for  interdisciplinary  cooperation between philosophical and sociological research to enhance the understanding of deception. Future investigations into the impact of deceptive AI on individuals can explore how users' awareness  of  deceptive  AI  influences  their  dependence  and  offer  step-by-step  strategies  to reflect their relationships with AI tools.

Word count: 3294 (within 10%+- of word limit)

10 The concept of technology as infrastructures appeared in Lecture 4, slides from 21-40

## Reference list

| At UK's AI Summit, Guterres Says Risks Outweigh Rewards without Global Oversight. (2023). At UK's AI Summit, Guterres Says Risks Outweigh Rewards without Global Oversight | UN News . [online] Available at: https://news.un.org/en/story/2023/11/1143147.                                                                                                                                                   |
|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Bai, H., Voelkel, J., Eichstaedt, J. and Willer, R. (2023). Artificial Intelligence Can Persuade Humans on Political Issues . [online] www.researchsquare.com. Available at: https://www.researchsquare.com/article/rs-3238396/v1.                                                                                                                                                                            |
| Bakhtin, A., Brown, N., Dinan, E., Farina, G., Flaherty, C., Fried, D., Goff, A., Gray, J., Hu, H., Jacob, A.P., Komeili, M., Konath, K., Kwon, M., Lerer, A., Lewis, M., Miller, A.H., Mitts, S., Renduchintala, A., Roller, S. and Rowe, D. (2022). Human-level Play in the Game of Diplomacy by Combining Language Models with Strategic Reasoning. Science . doi:https://doi.org/10.1126/science.ade9097. |
| Cambridge Dictionary (2019). MISTAKE | Meaning in the Cambridge English Dictionary . [online] Cambridge.org. Available at: https://dictionary.cambridge.org/dictionary/english/mistake.                                                                                                                                                                                                                       |
| Castelfranchi, C. (2000). Artificial liars: Why Computers Will (necessarily) Deceive Us and Each Other. Ethics and Information Technology , 2(2), pp.113-119. doi:https://doi.org/10.1023/a:1010025403776.                                                                                                                                                                                                    |
| Castelfranchi, C. (2002). The Social Nature of Information and the Role of Trust. International Journal of Cooperative Information Systems , 11, pp.381-403. doi:https://doi.org/10.1142/s0218843002000649.                                                                                                                                                                                                   |
| Castelfranchi, C. and Tan, Y.-H. (2002). The Role of Trust and Deception in Virtual Societies. International Journal of Electronic Commerce , [online] 6(3), pp.55-70. Available at: https://www.jstor.org/stable/27751023 [Accessed 26 Dec. 2023].                                                                                                                                                           |
| Choudhury, A. and Shamszare, H. (2023). Investigating the Impact of User Trust on the Adoption and Use of ChatGPT: Survey Analysis. Journal of Medical Internet Research , [online] 25(1). doi:https://doi.org/10.2196/47184.                                                                                                                                                                                 |
| Collett, C. and Dillon, S. (2019). AI and Gender Four Proposals for Future Research . [online] Available at: http://lcfi.ac.uk/media/uploads/files/AI_and_Gender___4_Proposals_for_Future_Research.pd f.                                                                                                                                                                                                      |
| Couldry, N. and Mejias, U.A. (2019). Data Colonialism: Rethinking Big Data's Relation to the Contemporary Subject. Television &New Media , 20(4), pp.336-349.                                                                                                                                                                                                                                                 |
| Cui, Y., Huang, S., Zhong, J., Liu, Z., Wang, Y., Sun, C., Li, B., Chen, L. and Amir Khajepour (2023). DriveLLM: Charting the Path toward Full Autonomous Driving with Large Language Models. IEEE Transactions on Intelligent Vehicles , pp.1-15. doi:https://doi.org/10.1109/tiv.2023.3327715.                                                                                                              |

| Devlin, K. and Forrest, A. (2023). Extinction Risk from AI on Same Scale as Nuclear war, Sunak Warns . [online] The Independent. Available at: https://www.independent.co.uk/news/uk/politics/ai-sunak-weapon-war-uk-b2436000.html [Accessed 12 Dec. 2023].                                                                                     |
|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Diakopoulos, N. (2014). Algorithmic Accountability. Digital Journalism , 3(3), pp.398-415. doi:https://doi.org/10.1080/21670811.2014.976411.                                                                                                                                                                                                    |
| Expert comment: Oxford AI Experts Comment on the Outcomes of the UK AI Safety Summit. (2023). Expert comment: Oxford AI Experts Comment on the Outcomes of the UK AI Safety Summit | University of Oxford . [online] Available at: https://www.ox.ac.uk/news/2023-11-03-expert-comment-oxford-ai-experts-comment- outcomes-uk-ai-safety-summit. |
| Fjeld, J., Achten, N., Hilligoss, H., Nagy, A. and Srikumar, M. (2020). Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI. SSRN Electronic Journal . doi:https://doi.org/10.2139/ssrn.3518482.                                                                                  |
| Frontier AI: Capabilities and Risks - Discussion Paper. (2023). Frontier AI: Capabilities and Risks - Discussion Paper . [online] Available at: https://www.gov.uk/government/publications/frontier-ai-capabilities-and-risks-discussion- paper/frontier-ai-capabilities-and-risks-discussion-paper#fn:241 [Accessed 26 Nov. 2023].             |
| Fullbrook, D. (2023). AI Summit Brings Elon Musk and World Leaders to Bletchley Park. BBC News . [online] 1 Nov. Available at: https://www.bbc.co.uk/news/uk-england-beds- bucks-herts-67273099.                                                                                                                                                |
| Gray, J., Gerlitz, C. and Bounegru, L. (2018). Data Infrastructure Literacy. Big Data& Society , 5(2), p.205395171878631. doi:https://doi.org/10.1177/2053951718786316.                                                                                                                                                                         |
| Hagendorff, T. (2023). Deception Abilities Emerged in Large Language Models . [online] Available at: https://arxiv.org/pdf/2307.16513.pdf [Accessed 23 Dec. 2023].                                                                                                                                                                              |
| Hall, S. (1992). The West and the Rest: Discourse and Power . [online] Available at: https://analepsis.files.wordpress.com/2013/08/hall-west-the-rest.pdf.                                                                                                                                                                                      |
| Hardman, L. (2021). Cultural Influences on Artificial Intelligence: along the New Silk Road. Perspectives on Digital Humanism , pp.233-239. doi:https://doi.org/10.1007/978-3-030- 86144-5_31.                                                                                                                                                  |
| Head, C., Jasper, P., McConnachie, M.M., Raftree, L. and Grace Lyn Higdon (2023). Large Language Model Applications for evaluation: Opportunities and Ethical Implications. New Directions for Evaluation , 2023(178-179), pp.33-46. doi:https://doi.org/10.1002/ev.20556.                                                                      |
| Horowitz, M., Scharre, P., Kania, E. and Allen, G. (2017). Strategic Competition in an Era of Artificial Intelligence . [online] Available at: https://www.cnas.org/publications/reports/strategic-competition-in-an-era-of-artificial- intelligence.                                                                                           |

| Hutchinson, B., Prabhakaran, V., Denton, E., Webster, K., Zhong, Y. and Denuyl, S. (2020). Social Biases in NLP Models as Barriers for Persons with Disabilities. arXiv:2005.00813 [cs] . [online] Available at: https://arxiv.org/abs/2005.00813.                                                                                                                                                                                         |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Joyce, K., Smith-Doerr, L., Alegria, S., Bell, S., Cruz, T., Hoffman, S.G., Noble, S.U. and Shestakofsky, B. (2021). Toward a Sociology of Artificial Intelligence: a Call for Research on Inequalities and Structural Change. Socius: Sociological Research for a Dynamic World , 7, pp.1-11. doi:https://doi.org/10.1177/2378023121999581.                                                                                               |
| Kadoma, K., Aubin, M., Quéré, L., Fu, J., Munsch, C. and Metaxa, D. (2023). The Role of Inclusion, Control, and Ownership in Workplace AI-Mediated Communication . [online] Available at: https://arxiv.org/pdf/2309.11599.pdf [Accessed 3 Dec. 2023].                                                                                                                                                                                     |
| Kallianos, Y., Dunlap, A. and Dalakoglou, D. (2022). Introducing Infrastructural harm: Rethinking Moral entanglements, spatio-temporal dynamics, and resistance(s). Globalizations , pp.1-20. doi:https://doi.org/10.1080/14747731.2022.2153493.                                                                                                                                                                                           |
| Leading Frontier AI Companies Publish Safety Policies. (2023). Leading Frontier AI Companies Publish Safety Policies . [online] Available at: https://www.gov.uk/government/news/leading-frontier-ai-companies-publish-safety-policies.                                                                                                                                                                                                    |
| Masters, P., Smith, W., Sonenberg, L. and Kirley, M. (2021). Characterising Deception in AI: a Survey . [online] Available at: https://drive.google.com/file/d/1aiPqRlzklkSSqxwWnXd8R0P_TSW_quD3/view [Accessed 15 Nov. 2023].                                                                                                                                                                                                             |
| Matsuo, Y., LeCun, Y., Sahani, M., Precup, D., Silver, D., Sugiyama, M., Uchibe, E. and Morimoto, J. (2022). Deep learning, Reinforcement learning, and World Models. Neural Networks . doi:https://doi.org/10.1016/j.neunet.2022.03.037.                                                                                                                                                                                                  |
| McBride, K. (2023). OII | Dr Keegan McBride: Why the UK AI Safety Summit Will Fail to Be Meaningful . [online] Dr Keegan McBride: Why the UK AI Safety Summit Will Fail to Be Meaningful. Available at: https://www.oii.ox.ac.uk/news-events/news/dr-keegan-mcbride- why-the-uk-ai-safety-summit-will-fail-to-be-meaningful/ [Accessed 16 Nov. 2023].                                                                                      |
| Milmo, D. (2023). Elon Musk Unveils Grok, an AI Chatbot with a 'rebellious Streak'. The Guardian . [online] 5 Nov. Available at: https://www.theguardian.com/technology/2023/nov/05/elon-musk-unveils-grok-an-ai- chatbot-with-a-rebellious-streak.                                                                                                                                                                                        |
| Ngo, R., Chan, L. and Mindermann, S. (2023). The Alignment Problem from a Deep Learning Perspective. arXiv:2209.00626 [cs] . [online] Available at: https://arxiv.org/abs/2209.00626.                                                                                                                                                                                                                                                      |
| Ntoutsi, E., Fafalios, P., Gadiraju, U., Iosifidis, V., Nejdl, W., Vidal, M., Ruggieri, S., Turini, F., Papadopoulos, S., Krasanakis, E., Kompatsiaris, I., Kinder-Kurlanda, K., Wagner, C., Karimi, F., Fernandez, M., Alani, H., Berendt, B., Kruegel, T., Heinze, C. and Broelemann, K. (2020). Bias in Data-driven Artificial Intelligence systems-An Introductory Survey. WIREs Data Mining and Knowledge Discovery , [online] 10(3). |

| O'Gara, A. (2023). Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models . [online] Available at: https://arxiv.org/pdf/2308.01404.pdf [Accessed 4 Jan. 2023].                                                                                                                                                                                                |
|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ÓhÉigeartaigh, S.S., Whittlestone, J., Liu, Y., Zeng, Y. and Liu, Z. (2020). Overcoming Barriers to Cross-cultural Cooperation in AI Ethics and Governance. Philosophy& Technology , [online] 33(4), pp.571-593. doi:https://doi.org/10.1007/s13347-020-00402-x.                                                                                                                       |
| OpenAI (2023). ChatGPT . [online] chat.openai.com. Available at: https://chat.openai.com.                                                                                                                                                                                                                                                                                              |
| Orton-Johnson, K. and Prior, N. (2013). Digital Sociology : Critical Perspectives . New York: Palgrave Macmillan.                                                                                                                                                                                                                                                                      |
| Our Research on Strategic Deception Presented at the UK's AI Safety Summit. (2023). Our Research on Strategic Deception Presented at the UK's AI Safety Summit . [online] Available at: https://www.apolloresearch.ai/research/summit-demo [Accessed 26 Dec. 2023].                                                                                                                    |
| Park, P., Goldstein, S., O'gara, A., Chen, M. and Hendrycks, D. (2023). AI Deception: a Survey of Examples, Risks, and Potential Solutions . [online] Available at: https://arxiv.org/pdf/2308.14752.pdf.                                                                                                                                                                              |
| Pazzanese, C. (2020). Ethical Concerns Mount as AI Takes Bigger decision-making Role . [online] Harvard Gazette. Available at: https://news.harvard.edu/gazette/story/2020/10/ethical-concerns-mount-as-ai-takes-bigger- decision-making-role/ [Accessed 1 Dec. 2023].                                                                                                                 |
| Perez, E., Ringer, S., Lukošiūtė, K., Nguyen, K., Chen, E., Heiner, S., Pettit, C., Olsson, C., Kundu, S., Kadavath, S., Jones, A., Chen, A., Mann, B., Israel, B., Seethor, B., Mckinnon, C., Olah, C., Yan, D., Amodei, D. and Amodei, D. (2022). Discovering Language Model Behaviors with Model-Written Evaluations . [online] Available at: https://arxiv.org/pdf/2212.09251.pdf. |
| Perolat, J., De Vylder, B., Hennes, D., Tarassov, E., Strub, F., de Boer, V., Muller, P., Connor, J.T., Burch, N., Anthony, T., McAleer, S., Elie, R., Cen, S.H., Wang, Z., Gruslys, A., Malysheva, A., Khan, M., Ozair, S., Timbers, F. and Pohlen, T. (2022). Mastering the Game of Stratego with model-free Multiagent Reinforcement Learning. Science , 378(6623),                 |
| Perrigo, B. (2023). Exclusive: the $2 per Hour Workers Who Made ChatGPT Safer . [online] Time. Available at: https://time.com/6247678/openai-chatgpt-kenya-workers/ [Accessed 1 Dec. 2023].                                                                                                                                                                                            |
| Pierce, R. (2008). Research Methods in Politics . London: Sage, pp.79-99. Chapter 7 Evaluating Information: Validity, Reliability, Accuracy, Triangulation.                                                                                                                                                                                                                            |
| Rice, R.E., Yates, S.J. and Blejmar, J. (2020). Introduction to the Oxford Handbook of Digital Technology and Society. The Oxford Handbook of Digital Technology and Society , pp.1-35. doi:https://doi.org/10.1093/oxfordhb/9780190932596.013.1.                                                                                                                                      |
| Sample, I. (2023). Race to AI: the Origins of Artificial intelligence, from Turing to ChatGPT. The Guardian . [online] 28 Oct. Available at:                                                                                                                                                                                                                                           |

https://www.theguardian.com/technology/2023/oct/28/artificial-intelligence-origins-turing-tochatgpt.

| Scheurer, J., Balesni, M., Research, A. and Hobbhahn, M. (2023). Technical Report: Large Language Models Can Strategically Deceive Their Users When Put under Pressure . [online] Available at: https://static1.squarespace.com/static/6461e2a5c6399341bcfc84a5/t/65526a1a9c7e431db74a 6ff6/1699899932357/deception_under_pressure.pdf [Accessed 26 Dec. 2023].             |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Seal, T. (2023). AI Doomers Take Center Stage at the UK's AI Summit. Bloomberg.com . [online] 1 Nov. Available at: https://www.bloomberg.com/news/articles/2023-11-01/ai- doomers-take-center-stage-at-the-uk-s-ai-summit [Accessed 26 Dec. 2023].                                                                                                                          |
| Shevlane, T., Farquhar, S., Garfinkel, B., Phuong, M., Whittlestone, J., Leung, J., Kokotajlo, D., Marchal, N., Anderljung, M., Kolt, N., Ho, L., Siddarth, D., Avin, S., Hawkins, W., Kim, B., Gabriel, I., Bolina, V., Clark, J., Bengio, Y. and Christiano, P. (2023). Model Evaluation for Extreme Risks . [online] Available at: https://arxiv.org/pdf/2305.15324.pdf. |
| Shilling, C. and Mellor, P.A. (2014). For a Sociology of Deceit: Doubled Identities, Interested Actions and Situational Logics of Opportunity. Sociology , 49(4), pp.607-623. doi:https://doi.org/10.1177/0038038514546661.                                                                                                                                                 |
| Singh, R. (2020). Study the Imbrication: a Methodological Maxim to Follow the Multiple Lives of Data. The Institute of Network Cultures. , pp.51-59. doi:https://networkcultures.org/wp-content/uploads/2020/12/LivesofData.pdf.                                                                                                                                            |
| Turpin, M., Michael, J., Perez, E. and Bowman, S. (2023). Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting . [online] Available at: https://arxiv.org/pdf/2305.04388.pdf.                                                                                                                                            |
| Ward, F., Belardinelli, F., Toni, F. and Everitt, T. (2023). Honesty Is the Best Policy: Defining and Mitigating AI Deception . [online] Available at: https://causalincentives.com/pdfs/deception-ward-2023.pdf [Accessed 2 Jan. 2023].                                                                                                                                    |
| Widder, D., Whittaker, M. and West, S. (2023). Open (for Business): Big Tech, Concentrated Power, and the Political Economy of Open AI.                                                                                                                                                                                                                                     |
| Wiltshire, K.D. (2020). Actor Network Theory (ANT). Encyclopedia of Global Archaeology , pp.22-26. doi:https://doi.org/10.1007/978-3-030-30018-0_3401.                                                                                                                                                                                                                      |
| Zhan, X., Xu, Y. and Sarkadi, Ş. (2023). Deceptive AI Ecosystems: the Case of ChatGPT. doi:https://doi.org/10.1145/3571884.3603754.                                                                                                                                                                                                                                         |
| Zhu, Q. (2023). The Doctrine of Cyber Effect: an Ethics Framework for Defensive Cyber                                                                                                                                                                                                                                                                                       |
| Deception . [online] Available at: https://arxiv.org/pdf/2302.13362.pdf [Accessed 25 Nov. 2023].                                                                                                                                                                                                                                                                            |