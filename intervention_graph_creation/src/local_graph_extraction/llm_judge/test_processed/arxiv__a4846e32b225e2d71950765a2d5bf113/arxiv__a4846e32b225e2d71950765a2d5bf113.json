{
  "nodes": [
    {
      "name": "prediction non-robustness in deep neural networks",
      "aliases": [
        "model unreliability",
        "lack of robustness in DNN predictions"
      ],
      "type": "concept",
      "description": "Deep neural networks often produce confident but brittle predictions that cannot be trusted in downstream tasks, especially when explanations reveal reliance on spurious features.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Motivating problem stated in 1 Introduction"
    },
    {
      "name": "adversarial vulnerability in image classifiers",
      "aliases": [
        "susceptibility to adversarial attacks",
        "image model attackability"
      ],
      "type": "concept",
      "description": "Small, human-imperceptible perturbations can flip class predictions, showing that vision models are vulnerable to adversarial manipulation.",
      "concept_category": "risk",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 3.2 describes extensive adversarial attack experiments."
    },
    {
      "name": "human-in-the-loop explanation-based robustness assessment scalability limits",
      "aliases": [
        "manual concept annotation bottleneck",
        "human effort in explanation checks"
      ],
      "type": "concept",
      "description": "Existing concept-based robustness assessments require humans to annotate concepts or judge explanation quality, which does not scale to large datasets or real-time use.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Problem discussed in 1 Introduction (paragraph on ‘humans-in-the-loop’)."
    },
    {
      "name": "feature reliance misalignment with human reasoning in DNN predictions",
      "aliases": [
        "spurious feature dependence",
        "concept–prediction mismatch"
      ],
      "type": "concept",
      "description": "Models sometimes rely on background or non-semantic cues (e.g. snow for wolf) leading to explanations humans deem unreasonable.",
      "concept_category": "problem analysis",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Illustrated in 1 Introduction (wolf-in-snow example)."
    },
    {
      "name": "machine-checkable concepts enabling automated explanation-conformity",
      "aliases": [
        "MACCs theory",
        "automatic concept hypothesis"
      ],
      "type": "concept",
      "description": "Define concepts as features shared by specific class subsets and detectable automatically, allowing machine-only conformity checks.",
      "concept_category": "theoretical insight",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Key insight in 2.1.1 Automatically defining MACCs."
    },
    {
      "name": "explanation-conformity filtering using MACCs to assess robustness",
      "aliases": [
        "MACC robustness rationale",
        "conformity gate idea"
      ],
      "type": "concept",
      "description": "Predictions are accepted as robust only if detected MACCs match those expected for the predicted class, providing an automated reject option.",
      "concept_category": "design rationale",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Design described in 2.1.3 Explanation-conformity checks."
    },
    {
      "name": "automatic MACC definition via class subset feature overlaps",
      "aliases": [
        "MACC enumeration",
        "concept set generation"
      ],
      "type": "concept",
      "description": "Generate 2^K − 1 possible MACCs by enumerating every non-empty subset of classes and defining features common to that subset but absent elsewhere.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Implementation detailed in 2.1.1."
    },
    {
      "name": "post-hoc training of MACC detector multilabel head",
      "aliases": [
        "auxiliary MACC head fine-tuning",
        "post-hoc MACC detector"
      ],
      "type": "concept",
      "description": "Attach a new multilabel classifier to an intermediate layer of a pre-trained network and train it to predict presence of each MACC without altering original weights.",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Section 2.1.2 Detecting MACCs in DNNs (‘Post-hoc training’ option)."
    },
    {
      "name": "threshold-based explanation-conformity robustness check",
      "aliases": [
        "t_con filter",
        "MACC conformity threshold"
      ],
      "type": "concept",
      "description": "Compute the fraction of expected MACCs detected in an input and accept the prediction if it exceeds a tunable threshold t_con (0–1).",
      "concept_category": "implementation mechanism",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Equation in 2.1.3 Explanation-conformity checks."
    },
    {
      "name": "higher accuracy on explanation-conformant predictions",
      "aliases": [
        "error estimability evidence",
        "accuracy boost on conformant set"
      ],
      "type": "concept",
      "description": "Across CIFAR-10/100 and Fashion-MNIST, accuracy on conformant predictions is 4–7 pp higher than baseline while non-conformant images show much lower accuracy.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Empirical results in Table 1 (Section 3.1)."
    },
    {
      "name": "MACC checks detect majority of adversarial attacks",
      "aliases": [
        "adversarial detection evidence",
        "attack rejection rate"
      ],
      "type": "concept",
      "description": "For CIFAR-10 and Fashion-MNIST, >98 % of FGSM, PGD, C&W attacks fail the conformity check; detection on CIFAR-100 ~50 %.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Table 2 in Section 3.2."
    },
    {
      "name": "explanation-conformant attacks require perceptible perturbations",
      "aliases": [
        "large L2 perturbation evidence",
        "human-visible adversarial examples"
      ],
      "type": "concept",
      "description": "Adversarial examples that also satisfy MACC conformity need perturbations 10× larger (L2≈5) and are spotted by humans 85 % of the time.",
      "concept_category": "validation evidence",
      "intervention_lifecycle": null,
      "intervention_lifecycle_rationale": null,
      "intervention_maturity": null,
      "intervention_maturity_rationale": null,
      "node_rationale": "Results & Figure 2, Section 3.2."
    },
    {
      "name": "Deploy MACC-based explanation-conformity filter to reject non-robust predictions",
      "aliases": [
        "MACC robustness gate",
        "runtime conformity filter"
      ],
      "type": "intervention",
      "description": "At inference time compute MACC conformity and reject or flag predictions that fall below threshold t_con, improving delivered accuracy and robustness.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Applies during model serving as per Section 2.1.3 & Section 3 Evaluation.",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Demonstrated on multiple datasets but not yet deployed in production (Section 3).",
      "node_rationale": "Primary practical use-case derived from methodology (2.1.3)."
    },
    {
      "name": "Fine-tune existing models with post-hoc MACC detector head for automated robustness assessment",
      "aliases": [
        "add MACC head during fine-tuning",
        "post-hoc MACC training routine"
      ],
      "type": "intervention",
      "description": "Take a pretrained classifier and train only the attached MACC detection head on the same data to enable conformity checks without retraining the backbone.",
      "concept_category": null,
      "intervention_lifecycle": "3",
      "intervention_lifecycle_rationale": "Performed during task-specific fine-tuning phase (Section 2.1.2 Post-hoc training).",
      "intervention_maturity": "3",
      "intervention_maturity_rationale": "Validated experimentally on three datasets (Section 3).",
      "node_rationale": "Enables adoption without full retraining."
    },
    {
      "name": "Deploy MACC-based adversarial input detection module",
      "aliases": [
        "MACC attack detector",
        "conformity-based adversary filter"
      ],
      "type": "intervention",
      "description": "Monitor live inputs; if MACC conformity fails, flag possible adversarial attack and trigger defensive response.",
      "concept_category": null,
      "intervention_lifecycle": "5",
      "intervention_lifecycle_rationale": "Runtime monitoring application referencing Section 3.2.",
      "intervention_maturity": "2",
      "intervention_maturity_rationale": "Proof-of-concept detection shown; large-scale deployment not yet demonstrated.",
      "node_rationale": "Directly leverages adversarial detection evidence (Section 3.2)."
    }
  ],
  "edges": [
    {
      "type": "caused_by",
      "source_node": "prediction non-robustness in deep neural networks",
      "target_node": "feature reliance misalignment with human reasoning in DNN predictions",
      "description": "When models rely on spurious cues, predictions become non-robust",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Multiple concrete examples in 1 Introduction but no formal quantification",
      "edge_rationale": "Wolf-snow case in 1 Introduction"
    },
    {
      "type": "caused_by",
      "source_node": "prediction non-robustness in deep neural networks",
      "target_node": "human-in-the-loop explanation-based robustness assessment scalability limits",
      "description": "Lack of scalable robustness checks leaves many non-robust predictions undetected",
      "edge_confidence": "2",
      "edge_confidence_rationale": "Moderate inference from discussion of human bottlenecks",
      "edge_rationale": "1 Introduction paragraph on scalability"
    },
    {
      "type": "caused_by",
      "source_node": "adversarial vulnerability in image classifiers",
      "target_node": "feature reliance misalignment with human reasoning in DNN predictions",
      "description": "Spurious feature use makes models easier to fool adversarially",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Widely accepted link; implicit in Section 3.2 setup",
      "edge_rationale": "Background context in 3.2"
    },
    {
      "type": "mitigated_by",
      "source_node": "human-in-the-loop explanation-based robustness assessment scalability limits",
      "target_node": "machine-checkable concepts enabling automated explanation-conformity",
      "description": "Automated, machine-checkable concepts remove need for human annotation",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Explicit goal statement in 1 Introduction & 2.1.1",
      "edge_rationale": "Goals and contributions section"
    },
    {
      "type": "mitigated_by",
      "source_node": "feature reliance misalignment with human reasoning in DNN predictions",
      "target_node": "machine-checkable concepts enabling automated explanation-conformity",
      "description": "Using concept conformity tests discourages reliance on mismatching features",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Supported by empirical accuracy gap (Table 1)",
      "edge_rationale": "Analysis in 3.1"
    },
    {
      "type": "enabled_by",
      "source_node": "machine-checkable concepts enabling automated explanation-conformity",
      "target_node": "explanation-conformity filtering using MACCs to assess robustness",
      "description": "MACCs provide the signals needed for an automated conformity filter",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 2.1.3 builds directly on MACCs",
      "edge_rationale": "Methodology flow 2.1"
    },
    {
      "type": "implemented_by",
      "source_node": "explanation-conformity filtering using MACCs to assess robustness",
      "target_node": "automatic MACC definition via class subset feature overlaps",
      "description": "Filter relies on having a defined set of MACCs per class",
      "edge_confidence": "4",
      "edge_confidence_rationale": "2.1.1 specifies enumeration procedure required for filter",
      "edge_rationale": "Figure 1 and associated text"
    },
    {
      "type": "refined_by",
      "source_node": "automatic MACC definition via class subset feature overlaps",
      "target_node": "post-hoc training of MACC detector multilabel head",
      "description": "Detection head turns defined MACCs into measurable signals",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Section 2.1.2 describes post-hoc training after definition",
      "edge_rationale": "2.1.2 Detecting MACCs"
    },
    {
      "type": "enabled_by",
      "source_node": "post-hoc training of MACC detector multilabel head",
      "target_node": "threshold-based explanation-conformity robustness check",
      "description": "Detector outputs feed the threshold-based conformity decision",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Equation defining t_con requires detector outputs",
      "edge_rationale": "2.1.3"
    },
    {
      "type": "validated_by",
      "source_node": "threshold-based explanation-conformity robustness check",
      "target_node": "higher accuracy on explanation-conformant predictions",
      "description": "Empirical results show accuracy boost for predictions passing the check",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Statistically significant accuracy differences in Table 1",
      "edge_rationale": "3.1 Error Estimability"
    },
    {
      "type": "validated_by",
      "source_node": "threshold-based explanation-conformity robustness check",
      "target_node": "MACC checks detect majority of adversarial attacks",
      "description": "Most adversarially perturbed inputs fail the conformity check",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Detection rates quantified in Table 2",
      "edge_rationale": "3.2 Error Vulnerability"
    },
    {
      "type": "validated_by",
      "source_node": "threshold-based explanation-conformity robustness check",
      "target_node": "explanation-conformant attacks require perceptible perturbations",
      "description": "Attacks that do pass the check need large, visible changes",
      "edge_confidence": "4",
      "edge_confidence_rationale": "L2 norms & human survey in 3.2",
      "edge_rationale": "Figure 2 and accompanying text"
    },
    {
      "type": "motivates",
      "source_node": "higher accuracy on explanation-conformant predictions",
      "target_node": "Deploy MACC-based explanation-conformity filter to reject non-robust predictions",
      "description": "Accuracy evidence encourages deploying a runtime filter",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Direct practical implication discussed in 3.1 and 3.4",
      "edge_rationale": "3 Evaluation discussion"
    },
    {
      "type": "motivates",
      "source_node": "MACC checks detect majority of adversarial attacks",
      "target_node": "Deploy MACC-based adversarial input detection module",
      "description": "High detection rates support using conformity check for security monitoring",
      "edge_confidence": "5",
      "edge_confidence_rationale": "Quantified detection >98 %",
      "edge_rationale": "Table 2"
    },
    {
      "type": "motivates",
      "source_node": "higher accuracy on explanation-conformant predictions",
      "target_node": "Fine-tune existing models with post-hoc MACC detector head for automated robustness assessment",
      "description": "Demonstrated gains justify adding MACC head during fine-tuning",
      "edge_confidence": "4",
      "edge_confidence_rationale": "Post-hoc training used in experiments yielding accuracy improvements",
      "edge_rationale": "3 Evaluation setup"
    },
    {
      "type": "motivates",
      "source_node": "explanation-conformant attacks require perceptible perturbations",
      "target_node": "Deploy MACC-based adversarial input detection module",
      "description": "Human-detectable perturbations further support practical attack detection",
      "edge_confidence": "3",
      "edge_confidence_rationale": "Evidence from AMT survey; moderate link to deployment choice",
      "edge_rationale": "3.2 human perceptibility survey"
    }
  ],
  "meta": [
    {
      "key": "source",
      "value": "arxiv"
    },
    {
      "key": "date_published",
      "value": "2020-07-01T00:00:00Z"
    },
    {
      "key": "url",
      "value": "https://arxiv.org/abs/2007.00251"
    },
    {
      "key": "authors",
      "value": [
        "Vedant Nanda",
        "Till Speicher",
        "John P. Dickerson",
        "Krishna P. Gummadi",
        "Muhammad Bilal Zafar"
      ]
    },
    {
      "key": "title",
      "value": "Unifying Model Explainability and Robustness via Machine-Checkable Concepts"
    }
  ]
}