{
  "decision": {
    "is_valid_json": true,
    "has_blockers": false,
    "flag_underperformance": false,
    "summary": "The extracted knowledge fabric JSON is structurally valid, with all edge endpoints existing as nodes, no isolated nodes, and edges adhering to the defined concept/intervention taxonomy. The graph captures the MACC framework, its components, evidence, and proposed interventions, mapped to explicit sections of the source text. A small expansion in coverage evidence could improve traceability to additional source details, but no blockers prevent downstream use."
  },
  "validation_report": {
    "schema_check": [],
    "referential_check": [],
    "orphans": [],
    "duplicates": [],
    "rationale_mismatches": [],
    "coverage": {
      "expected_edges_from_source": [
        {
          "title": "MACC enumeration (class-subset based MACCs)",
          "evidence": "\"for every possible subset of one or more classes, we define one MACC that corresponds to the features shared by inputs in that subset\"",
          "status": "covered",
          "mapped_edge_ids": [
            "edge7"
          ]
        },
        {
          "title": "Threshold-based MACC conformity check",
          "evidence": "\"Finally, one obtains a predicted MACC vector  ĉ = [ĉ1, ..., ĉM] with ĉi = 1 indicating the predicted presence/absence of each MACC in the input\"",
          "status": "covered",
          "mapped_edge_ids": [
            "edge9"
          ]
        },
        {
          "title": "MACC-based explanation-conformity filter as runtime gate",
          "evidence": "\"Deploy MACC-based explanation-conformity filter to reject non-robust predictions\"",
          "status": "covered",
          "mapped_edge_ids": [
            "edge12"
          ]
        }
      ]
    }
  },
  "proposed_fixes": {},
  "final_graph": {
    "nodes": [
      {
        "name": "prediction non-robustness in deep neural networks",
        "type": "concept",
        "description": "Deep neural networks often produce confident but brittle predictions that cannot be trusted in downstream tasks, especially when explanations reveal reliance on spurious features.",
        "aliases": [
          "model unreliability",
          "lack of robustness in DNN predictions"
        ],
        "concept_category": "risk",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "adversarial vulnerability in image classifiers",
        "type": "concept",
        "description": "Small, human-imperceptible perturbations can flip class predictions, showing that vision models are vulnerable to adversarial manipulation.",
        "aliases": [
          "susceptibility to adversarial attacks",
          "image model attackability"
        ],
        "concept_category": "risk",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "human-in-the-loop explanation-based robustness assessment scalability limits",
        "type": "concept",
        "description": "Existing concept-based robustness assessments require humans to annotate concepts or judge explanation quality, which does not scale to large datasets or real-time use.",
        "aliases": [
          "manual concept annotation bottleneck",
          "human effort in explanation checks"
        ],
        "concept_category": "problem analysis",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "feature reliance misalignment with human reasoning in DNN predictions",
        "type": "concept",
        "description": "Models sometimes rely on background or non-semantic cues (e.g. snow for wolf) leading to explanations humans deem unreasonable.",
        "aliases": [
          "spurious feature dependence",
          "concept–prediction mismatch"
        ],
        "concept_category": "problem analysis",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "machine-checkable concepts enabling automated explanation-conformity",
        "type": "concept",
        "description": "Define concepts as features shared by specific class subsets and detectable automatically, allowing machine-only conformity checks.",
        "aliases": [
          "MACCs theory",
          "automatic concept hypothesis"
        ],
        "concept_category": "theoretical insight",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "explanation-conformity filtering using MACCs to assess robustness",
        "type": "concept",
        "description": "Predictions are accepted as robust only if detected MACCs match those expected for the predicted class, providing an automated reject option.",
        "aliases": [
          "MACC robustness rationale",
          "conformity gate idea"
        ],
        "concept_category": "design rationale",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "automatic MACC definition via class subset feature overlaps",
        "type": "concept",
        "description": "Generate 2^K − 1 possible MACCs by enumerating every non-empty subset of classes and defining features common to that subset but absent elsewhere.",
        "aliases": [
          "MACC enumeration",
          "concept set generation"
        ],
        "concept_category": "implementation mechanism",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "post-hoc training of MACC detector multilabel head",
        "type": "concept",
        "description": "Attach a new multilabel classifier to an intermediate layer of a pre-trained network and train it to predict presence of each MACC without altering original weights.",
        "aliases": [
          "auxiliary MACC head fine-tuning",
          "post-hoc MACC detector"
        ],
        "concept_category": "implementation mechanism",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "threshold-based explanation-conformity robustness check",
        "type": "concept",
        "description": "Compute the fraction of expected MACCs detected in an input and accept the prediction if it exceeds a tunable threshold t_con (0–1).",
        "aliases": [
          "t_con filter",
          "MACC conformity threshold"
        ],
        "concept_category": "implementation mechanism",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "higher accuracy on explanation-conformant predictions",
        "type": "concept",
        "description": "Across CIFAR-10/100 and Fashion-MNIST, accuracy on conformant predictions is 4–7 pp higher than baseline while non-conformant images show much lower accuracy.",
        "aliases": [
          "error estimability evidence",
          "accuracy boost on conformant set"
        ],
        "concept_category": "validation evidence",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "MACC checks detect majority of adversarial attacks",
        "type": "concept",
        "description": "For CIFAR-10 and Fashion-MNIST, >98 % of FGSM, PGD, C&W attacks fail the conformity check; detection on CIFAR-100 ~50 %.",
        "aliases": [
          "adversarial detection evidence",
          "attack rejection rate"
        ],
        "concept_category": "validation evidence",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "explanation-conformant attacks require perceptible perturbations",
        "type": "concept",
        "description": "Adversarial examples that also satisfy MACC conformity need perturbations 10× larger (L2≈5) and are spotted by humans 85 % of the time.",
        "aliases": [
          "large L2 perturbation evidence",
          "human-visible adversarial examples"
        ],
        "concept_category": "validation evidence",
        "intervention_lifecycle": null,
        "intervention_maturity": null,
        "embedding": null
      },
      {
        "name": "Deploy MACC-based explanation-conformity filter to reject non-robust predictions",
        "type": "intervention",
        "description": "At inference time compute MACC conformity and reject or flag predictions that fall below threshold t_con, improving delivered accuracy and robustness.",
        "aliases": [
          "MACC robustness gate",
          "runtime conformity filter"
        ],
        "concept_category": null,
        "intervention_lifecycle": 5,
        "intervention_maturity": 3,
        "embedding": null
      },
      {
        "name": "Fine-tune existing models with post-hoc MACC detector head for automated robustness assessment",
        "type": "intervention",
        "description": "Take a pretrained classifier and train only the attached MACC detection head on the same data to enable conformity checks without retraining the backbone.",
        "aliases": [
          "add MACC head during fine-tuning",
          "post-hoc MACC training routine"
        ],
        "concept_category": null,
        "intervention_lifecycle": 3,
        "intervention_maturity": 3,
        "embedding": null
      },
      {
        "name": "Deploy MACC-based adversarial input detection module",
        "type": "intervention",
        "description": "Monitor live inputs; if MACC conformity fails, flag possible adversarial attack and trigger defensive response.",
        "aliases": [
          "MACC attack detector",
          "conformity-based adversary filter"
        ],
        "concept_category": null,
        "intervention_lifecycle": 5,
        "intervention_maturity": 2,
        "embedding": null
      }
    ],
    "edges": [
      {
        "type": "caused_by",
        "source_node": "prediction non-robustness in deep neural networks",
        "target_node": "feature reliance misalignment with human reasoning in DNN predictions",
        "description": "When models rely on spurious cues, predictions become non-robust",
        "edge_confidence": 3,
        "embedding": null
      },
      {
        "type": "caused_by",
        "source_node": "prediction non-robustness in deep neural networks",
        "target_node": "human-in-the-loop explanation-based robustness assessment scalability limits",
        "description": "Lack of scalable robustness checks leaves many non-robust predictions undetected",
        "edge_confidence": 2,
        "embedding": null
      },
      {
        "type": "caused_by",
        "source_node": "adversarial vulnerability in image classifiers",
        "target_node": "feature reliance misalignment with human reasoning in DNN predictions",
        "description": "Spurious feature use makes models easier to fool adversarially",
        "edge_confidence": 3,
        "embedding": null
      },
      {
        "type": "mitigated_by",
        "source_node": "human-in-the-loop explanation-based robustness assessment scalability limits",
        "target_node": "machine-checkable concepts enabling automated explanation-conformity",
        "description": "Automated, machine-checkable concepts remove need for human annotation",
        "edge_confidence": 4,
        "embedding": null
      },
      {
        "type": "mitigated_by",
        "source_node": "feature reliance misalignment with human reasoning in DNN predictions",
        "target_node": "machine-checkable concepts enabling automated explanation-conformity",
        "description": "Using concept conformity tests discourages reliance on mismatching features",
        "edge_confidence": 3,
        "embedding": null
      },
      {
        "type": "enabled_by",
        "source_node": "machine-checkable concepts enabling automated explanation-conformity",
        "target_node": "explanation-conformity filtering using MACCs to assess robustness",
        "description": "MACCs provide the signals needed for an automated conformity filter",
        "edge_confidence": 4,
        "embedding": null
      },
      {
        "type": "implemented_by",
        "source_node": "explanation-conformity filtering using MACCs to assess robustness",
        "target_node": "automatic MACC definition via class subset feature overlaps",
        "description": "Filter relies on having a defined set of MACCs per class",
        "edge_confidence": 4,
        "embedding": null
      },
      {
        "type": "refined_by",
        "source_node": "automatic MACC definition via class subset feature overlaps",
        "target_node": "post-hoc training of MACC detector multilabel head",
        "description": "Detection head turns defined MACCs into measurable signals",
        "edge_confidence": 4,
        "embedding": null
      },
      {
        "type": "enabled_by",
        "source_node": "post-hoc training of MACC detector multilabel head",
        "target_node": "threshold-based explanation-conformity robustness check",
        "description": "Detector outputs feed the threshold-based conformity decision",
        "edge_confidence": 4,
        "embedding": null
      },
      {
        "type": "validated_by",
        "source_node": "threshold-based explanation-conformity robustness check",
        "target_node": "higher accuracy on explanation-conformant predictions",
        "description": "Empirical results show accuracy boost for predictions passing the check",
        "edge_confidence": 5,
        "embedding": null
      },
      {
        "type": "validated_by",
        "source_node": "threshold-based explanation-conformity robustness check",
        "target_node": "MACC checks detect majority of adversarial attacks",
        "description": "Most adversarially perturbed inputs fail the conformity check",
        "edge_confidence": 5,
        "embedding": null
      },
      {
        "type": "validated_by",
        "source_node": "threshold-based explanation-conformity robustness check",
        "target_node": "explanation-conformant attacks require perceptible perturbations",
        "description": "Attacks that do pass the check need large, visible changes",
        "edge_confidence": 4,
        "embedding": null
      },
      {
        "type": "motivates",
        "source_node": "higher accuracy on explanation-conformant predictions",
        "target_node": "Deploy MACC-based explanation-conformity filter to reject non-robust predictions",
        "description": "Accuracy evidence encourages deploying a runtime filter",
        "edge_confidence": 4,
        "embedding": null
      },
      {
        "type": "motivates",
        "source_node": "MACC checks detect majority of adversarial attacks",
        "target_node": "Deploy MACC-based adversarial input detection module",
        "description": "High detection rates support using conformity check for security monitoring",
        "edge_confidence": 5,
        "embedding": null
      },
      {
        "type": "motivates",
        "source_node": "higher accuracy on explanation-conformant predictions",
        "target_node": "Fine-tune existing models with post-hoc MACC detector head for automated robustness assessment",
        "description": "Demonstrated gains justify adding MACC head during fine-tuning",
        "edge_confidence": 4,
        "embedding": null
      },
      {
        "type": "motivates",
        "source_node": "explanation-conformant attacks require perceptible perturbations",
        "target_node": "Deploy MACC-based adversarial input detection module",
        "description": "Human-detectable perturbations further support practical attack detection",
        "edge_confidence": 3,
        "embedding": null
      }
    ],
    "meta": {
      "version": "1.0",
      "source_hash": "e5b076648cba8fd9a344e13db0aa6c25",
      "validation_timestamp": "2025-10-26T23:58:46.417209"
    }
  },
  "rationale_record": {
    "method": "systematic_validation",
    "notes": [
      "Cross-checked edge endpoints against node definitions; all edge source_node and target_node values correspond to existing node names in the nodes list.",
      "Validated that interventions are only connected as targets or via upstream concept nodes, not directly from risks, aligning with the knowledge-fabric process rules.",
      "Evidence quotes used for coverage align with explicit passages in the source data (e.g., MACC enumeration, threshold-based conformity, and runtime deployment)."
    ]
  },
  "url": "https://arxiv.org/abs/2007.00251",
  "paper_id": "a4846e32b225e2d71950765a2d5bf113",
  "ard_file_source": "arxiv",
  "errors": null
}