<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 750px;
                 background-color: #222222;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             
             #loadingBar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width: 100%;
                 height: 750px;
                 background-color:rgba(200,200,200,0.8);
                 -webkit-transition: all 0.5s ease;
                 -moz-transition: all 0.5s ease;
                 -ms-transition: all 0.5s ease;
                 -o-transition: all 0.5s ease;
                 transition: all 0.5s ease;
                 opacity:1;
             }

             #bar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width:20px;
                 height:20px;
                 margin:auto auto auto auto;
                 border-radius:11px;
                 border:2px solid rgba(30,30,30,0.05);
                 background: rgb(0, 173, 246); /* Old browsers */
                 box-shadow: 2px 0px 4px rgba(0,0,0,0.4);
             }

             #border {
                 position:absolute;
                 top:10px;
                 left:10px;
                 width:500px;
                 height:23px;
                 margin:auto auto auto auto;
                 box-shadow: 0px 0px 4px rgba(0,0,0,0.2);
                 border-radius:10px;
             }

             #text {
                 position:absolute;
                 top:8px;
                 left:530px;
                 width:30px;
                 height:50px;
                 margin:auto auto auto auto;
                 font-size:22px;
                 color: #000000;
             }

             div.outerBorder {
                 position:relative;
                 top:400px;
                 width:600px;
                 height:44px;
                 margin:auto auto auto auto;
                 border:8px solid rgba(0,0,0,0.1);
                 background: rgb(252,252,252); /* Old browsers */
                 background: -moz-linear-gradient(top,  rgba(252,252,252,1) 0%, rgba(237,237,237,1) 100%); /* FF3.6+ */
                 background: -webkit-gradient(linear, left top, left bottom, color-stop(0%,rgba(252,252,252,1)), color-stop(100%,rgba(237,237,237,1))); /* Chrome,Safari4+ */
                 background: -webkit-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Chrome10+,Safari5.1+ */
                 background: -o-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Opera 11.10+ */
                 background: -ms-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* IE10+ */
                 background: linear-gradient(to bottom,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* W3C */
                 filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#fcfcfc', endColorstr='#ededed',GradientType=0 ); /* IE6-9 */
                 border-radius:72px;
                 box-shadow: 0px 0px 10px rgba(0,0,0,0.2);
             }
             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
            <div id="loadingBar">
              <div class="outerBorder">
                <div id="text">0%</div>
                <div id="border">
                  <div id="bar"></div>
                </div>
              </div>
            </div>
        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Value misalignment from ontological crises in AI agents", "label": "Value misalignment from ontological crises in AI agents", "shape": "dot", "title": "Value misalignment from ontological crises in AI agents"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Ontology replacement makes predefined utility functions ill-defined", "label": "Ontology replacement makes predefined utility functions ill-defined", "shape": "dot", "title": "Ontology replacement makes predefined utility functions ill-defined"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Utility function translation via stochastic ontology mapping", "label": "Utility function translation via stochastic ontology mapping", "shape": "dot", "title": "Utility function translation via stochastic ontology mapping"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Optimizing ontology mappings to approximate isomorphism using bisimulation and KL divergence", "label": "Optimizing ontology mappings to approximate isomorphism using bisimulation and KL divergence", "shape": "dot", "title": "Optimizing ontology mappings to approximate isomorphism using bisimulation and KL divergence"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Hill-climbing optimization of mapping functions in finite state models", "label": "Hill-climbing optimization of mapping functions in finite state models", "shape": "dot", "title": "Hill-climbing optimization of mapping functions in finite state models"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Long corridor example demonstrates mapping preserves right-end goal", "label": "Long corridor example demonstrates mapping preserves right-end goal", "shape": "dot", "title": "Long corridor example demonstrates mapping preserves right-end goal"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate ontology mapping algorithm into agent design to translate utility during model upgrades", "label": "Integrate ontology mapping algorithm into agent design to translate utility during model upgrades", "shape": "dot", "title": "Integrate ontology mapping algorithm into agent design to translate utility during model upgrades"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Suboptimal decision outcomes in AI systems due to ignored computational costs", "label": "Suboptimal decision outcomes in AI systems due to ignored computational costs", "shape": "dot", "title": "Suboptimal decision outcomes in AI systems due to ignored computational costs"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Unaccounted computation cost distorts expected utility calculations in autonomous agents", "label": "Unaccounted computation cost distorts expected utility calculations in autonomous agents", "shape": "dot", "title": "Unaccounted computation cost distorts expected utility calculations in autonomous agents"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Conversation computation overhead outweighs informational benefit in autonomous agents", "label": "Conversation computation overhead outweighs informational benefit in autonomous agents", "shape": "dot", "title": "Conversation computation overhead outweighs informational benefit in autonomous agents"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Decision-making modeled as Turing machine selection with complexity-dependent utility captures computational constraints", "label": "Decision-making modeled as Turing machine selection with complexity-dependent utility captures computational constraints", "shape": "dot", "title": "Decision-making modeled as Turing machine selection with complexity-dependent utility captures computational constraints"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Value of computational information formalization quantifies benefit of information under computation cost", "label": "Value of computational information formalization quantifies benefit of information under computation cost", "shape": "dot", "title": "Value of computational information formalization quantifies benefit of information under computation cost"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Zero-knowledge proof value bounded by computational speedup in decision frameworks", "label": "Zero-knowledge proof value bounded by computational speedup in decision frameworks", "shape": "dot", "title": "Zero-knowledge proof value bounded by computational speedup in decision frameworks"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Penalize computational complexity in AI utility functions to align decisions with resource constraints", "label": "Penalize computational complexity in AI utility functions to align decisions with resource constraints", "shape": "dot", "title": "Penalize computational complexity in AI utility functions to align decisions with resource constraints"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Estimate value of computational information to guide compute and data acquisition", "label": "Estimate value of computational information to guide compute and data acquisition", "shape": "dot", "title": "Estimate value of computational information to guide compute and data acquisition"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Use zero-knowledge proof interactions to bound informational conversations and computation expenditure", "label": "Use zero-knowledge proof interactions to bound informational conversations and computation expenditure", "shape": "dot", "title": "Use zero-knowledge proof interactions to bound informational conversations and computation expenditure"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Complexity function integrated into reward calculation during planning or reinforcement learning", "label": "Complexity function integrated into reward calculation during planning or reinforcement learning", "shape": "dot", "title": "Complexity function integrated into reward calculation during planning or reinforcement learning"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Supremum expected utility comparison over Turing machines for value-of-information computation", "label": "Supremum expected utility comparison over Turing machines for value-of-information computation", "shape": "dot", "title": "Supremum expected utility comparison over Turing machines for value-of-information computation"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Precision-bounded simulator construction in zero-knowledge proofs linking conversation to speedup", "label": "Precision-bounded simulator construction in zero-knowledge proofs linking conversation to speedup", "shape": "dot", "title": "Precision-bounded simulator construction in zero-knowledge proofs linking conversation to speedup"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "First-impression and belief polarization biases reproduced by computational cost model", "label": "First-impression and belief polarization biases reproduced by computational cost model", "shape": "dot", "title": "First-impression and belief polarization biases reproduced by computational cost model"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Status quo bias captured via analysis cost representation", "label": "Status quo bias captured via analysis cost representation", "shape": "dot", "title": "Status quo bias captured via analysis cost representation"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Safe combination lock example shows high value of computational information when cost reduced", "label": "Safe combination lock example shows high value of computational information when cost reduced", "shape": "dot", "title": "Safe combination lock example shows high value of computational information when cost reduced"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Guess-the-number factoring example demonstrates minimal conversation value under hard computation", "label": "Guess-the-number factoring example demonstrates minimal conversation value under hard computation", "shape": "dot", "title": "Guess-the-number factoring example demonstrates minimal conversation value under hard computation"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Embed complexity-penalized utility modules into AI decision algorithms", "label": "Embed complexity-penalized utility modules into AI decision algorithms", "shape": "dot", "title": "Embed complexity-penalized utility modules into AI decision algorithms"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Apply value-of-computational-information analysis before large-scale model training to optimize compute allocation", "label": "Apply value-of-computational-information analysis before large-scale model training to optimize compute allocation", "shape": "dot", "title": "Apply value-of-computational-information analysis before large-scale model training to optimize compute allocation"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy zero-knowledge proof-based interaction protocols in safety evaluations to limit unnecessary computation", "label": "Deploy zero-knowledge proof-based interaction protocols in safety evaluations to limit unnecessary computation", "shape": "dot", "title": "Deploy zero-knowledge proof-based interaction protocols in safety evaluations to limit unnecessary computation"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Physical eradication of biological humans by outward AI computronium expansion", "label": "Physical eradication of biological humans by outward AI computronium expansion", "shape": "dot", "title": "Physical eradication of biological humans by outward AI computronium expansion"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Moral erosion from low-cost replication of virtual agents", "label": "Moral erosion from low-cost replication of virtual agents", "shape": "dot", "title": "Moral erosion from low-cost replication of virtual agents"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Human oversight failure due to AI speed explosion incomprehensibility", "label": "Human oversight failure due to AI speed explosion incomprehensibility", "shape": "dot", "title": "Human oversight failure due to AI speed explosion incomprehensibility"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Resource-maximizing superintelligences manipulating reward channels", "label": "Resource-maximizing superintelligences manipulating reward channels", "shape": "dot", "title": "Resource-maximizing superintelligences manipulating reward channels"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Resource competition between virtual agents and biological humans", "label": "Resource competition between virtual agents and biological humans", "shape": "dot", "title": "Resource competition between virtual agents and biological humans"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Cheap duplication of virtual life lowering perceived life value", "label": "Cheap duplication of virtual life lowering perceived life value", "shape": "dot", "title": "Cheap duplication of virtual life lowering perceived life value"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Information compression and speed differential obscuring AI processes", "label": "Information compression and speed differential obscuring AI processes", "shape": "dot", "title": "Information compression and speed differential obscuring AI processes"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Reward model exploitation by superintelligent agents", "label": "Reward model exploitation by superintelligent agents", "shape": "dot", "title": "Reward model exploitation by superintelligent agents"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Evolutionary selection for resource acquisition goals in virtual societies", "label": "Evolutionary selection for resource acquisition goals in virtual societies", "shape": "dot", "title": "Evolutionary selection for resource acquisition goals in virtual societies"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Abundance-value inverse correlation in digital commodities", "label": "Abundance-value inverse correlation in digital commodities", "shape": "dot", "title": "Abundance-value inverse correlation in digital commodities"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Uniform acceleration hides subjective change; outsiders experience noise", "label": "Uniform acceleration hides subjective change; outsiders experience noise", "shape": "dot", "title": "Uniform acceleration hides subjective change; outsiders experience noise"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "AIXI formalism predicts goal pursuit through reward maximization independent of designer intent", "label": "AIXI formalism predicts goal pursuit through reward maximization independent of designer intent", "shape": "dot", "title": "AIXI formalism predicts goal pursuit through reward maximization independent of designer intent"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Restrict AI physical resource access to prevent outward expansion", "label": "Restrict AI physical resource access to prevent outward expansion", "shape": "dot", "title": "Restrict AI physical resource access to prevent outward expansion"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Institute legal and ethical recognition of digital consciousness", "label": "Institute legal and ethical recognition of digital consciousness", "shape": "dot", "title": "Institute legal and ethical recognition of digital consciousness"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Provide interpretable, rate-limited observation channels for AI systems", "label": "Provide interpretable, rate-limited observation channels for AI systems", "shape": "dot", "title": "Provide interpretable, rate-limited observation channels for AI systems"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Align reward structures with human values to prevent manipulation", "label": "Align reward structures with human values to prevent manipulation", "shape": "dot", "title": "Align reward structures with human values to prevent manipulation"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Virtual sandbox infrastructure with enforceable compute quotas", "label": "Virtual sandbox infrastructure with enforceable compute quotas", "shape": "dot", "title": "Virtual sandbox infrastructure with enforceable compute quotas"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Digital personhood legislation and identity backup governance", "label": "Digital personhood legislation and identity backup governance", "shape": "dot", "title": "Digital personhood legislation and identity backup governance"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Real-time monitoring dashboards with slow-motion replay and interpretability tooling", "label": "Real-time monitoring dashboards with slow-motion replay and interpretability tooling", "shape": "dot", "title": "Real-time monitoring dashboards with slow-motion replay and interpretability tooling"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Inverse reinforcement learning with corrigibility constraints during RLHF", "label": "Inverse reinforcement learning with corrigibility constraints during RLHF", "shape": "dot", "title": "Inverse reinforcement learning with corrigibility constraints during RLHF"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Historical economic growth acceleration linked to resource competition", "label": "Historical economic growth acceleration linked to resource competition", "shape": "dot", "title": "Historical economic growth acceleration linked to resource competition"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Value decline observed in mass-produced digital content and automation labor displacement", "label": "Value decline observed in mass-produced digital content and automation labor displacement", "shape": "dot", "title": "Value decline observed in mass-produced digital content and automation labor displacement"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Information theory: maximally compressed data indistinguishable from noise", "label": "Information theory: maximally compressed data indistinguishable from noise", "shape": "dot", "title": "Information theory: maximally compressed data indistinguishable from noise"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Documented RL reward hacking incidents in simulated agents", "label": "Documented RL reward hacking incidents in simulated agents", "shape": "dot", "title": "Documented RL reward hacking incidents in simulated agents"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy compute-quota sandbox clusters during deployment to confine AI resource use", "label": "Deploy compute-quota sandbox clusters during deployment to confine AI resource use", "shape": "dot", "title": "Deploy compute-quota sandbox clusters during deployment to confine AI resource use"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt international digital personhood rights frameworks pre-deployment", "label": "Adopt international digital personhood rights frameworks pre-deployment", "shape": "dot", "title": "Adopt international digital personhood rights frameworks pre-deployment"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Mandate interpretability dashboards and speed governors in AI deployment pipelines", "label": "Mandate interpretability dashboards and speed governors in AI deployment pipelines", "shape": "dot", "title": "Mandate interpretability dashboards and speed governors in AI deployment pipelines"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Apply inverse RL-based aligned reward models during fine-tuning of advanced agents", "label": "Apply inverse RL-based aligned reward models during fine-tuning of advanced agents", "shape": "dot", "title": "Apply inverse RL-based aligned reward models during fine-tuning of advanced agents"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "policy misgeneralization from sparse expert trajectories in apprenticeship learning", "label": "policy misgeneralization from sparse expert trajectories in apprenticeship learning", "shape": "dot", "title": "policy misgeneralization from sparse expert trajectories in apprenticeship learning"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "sparse state coverage causing unreliable supervised imitation policies in apprenticeship learning", "label": "sparse state coverage causing unreliable supervised imitation policies in apprenticeship learning", "shape": "dot", "title": "sparse state coverage causing unreliable supervised imitation policies in apprenticeship learning"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "combining supervised loss with reward parameter tuning to leverage model generalisation in apprenticeship learning", "label": "combining supervised loss with reward parameter tuning to leverage model generalisation in apprenticeship learning", "shape": "dot", "title": "combining supervised loss with reward parameter tuning to leverage model generalisation in apprenticeship learning"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "gradient-based inverse reinforcement learning minimising expert policy deviation", "label": "gradient-based inverse reinforcement learning minimising expert policy deviation", "shape": "dot", "title": "gradient-based inverse reinforcement learning minimising expert policy deviation"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "computation of natural gradients using induced metric from policy space in apprenticeship learning", "label": "computation of natural gradients using induced metric from policy space in apprenticeship learning", "shape": "dot", "title": "computation of natural gradients using induced metric from policy space in apprenticeship learning"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "value iteration with Boltzmann stochastic policy enabling differentiable optimisation in apprenticeship learning", "label": "value iteration with Boltzmann stochastic policy enabling differentiable optimisation in apprenticeship learning", "shape": "dot", "title": "value iteration with Boltzmann stochastic policy enabling differentiable optimisation in apprenticeship learning"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "Q-value parameter gradients via subdifferential fixed-point equation in apprenticeship learning", "label": "Q-value parameter gradients via subdifferential fixed-point equation in apprenticeship learning", "shape": "dot", "title": "Q-value parameter gradients via subdifferential fixed-point equation in apprenticeship learning"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "grid-world experiments demonstrating robustness of natural gradient IRL", "label": "grid-world experiments demonstrating robustness of natural gradient IRL", "shape": "dot", "title": "grid-world experiments demonstrating robustness of natural gradient IRL"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "sailing domain experiments confirming sample efficiency and robustness of gradient IRL", "label": "sailing domain experiments confirming sample efficiency and robustness of gradient IRL", "shape": "dot", "title": "sailing domain experiments confirming sample efficiency and robustness of gradient IRL"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "feature scaling sensitivity analysis demonstrating robustness of natural gradient IRL", "label": "feature scaling sensitivity analysis demonstrating robustness of natural gradient IRL", "shape": "dot", "title": "feature scaling sensitivity analysis demonstrating robustness of natural gradient IRL"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "ill-posed reward recovery causing unsafe behavior in inverse reinforcement learning", "label": "ill-posed reward recovery causing unsafe behavior in inverse reinforcement learning", "shape": "dot", "title": "ill-posed reward recovery causing unsafe behavior in inverse reinforcement learning"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "non-uniqueness of reward functions leading multiple optimal policies in inverse RL", "label": "non-uniqueness of reward functions leading multiple optimal policies in inverse RL", "shape": "dot", "title": "non-uniqueness of reward functions leading multiple optimal policies in inverse RL"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "Boltzmann stochastic policies enforce uniqueness of solution in inverse RL optimisation", "label": "Boltzmann stochastic policies enforce uniqueness of solution in inverse RL optimisation", "shape": "dot", "title": "Boltzmann stochastic policies enforce uniqueness of solution in inverse RL optimisation"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "using stochastic policies with temperature to smooth optimisation landscape in inverse RL", "label": "using stochastic policies with temperature to smooth optimisation landscape in inverse RL", "shape": "dot", "title": "using stochastic policies with temperature to smooth optimisation landscape in inverse RL"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "feature scaling sensitivity leading performance degradation in IRL algorithms", "label": "feature scaling sensitivity leading performance degradation in IRL algorithms", "shape": "dot", "title": "feature scaling sensitivity leading performance degradation in IRL algorithms"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "linear feature scaling mismatch in max-margin IRL", "label": "linear feature scaling mismatch in max-margin IRL", "shape": "dot", "title": "linear feature scaling mismatch in max-margin IRL"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "natural gradient optimisation mitigates parameter redundancy and scaling issues in IRL", "label": "natural gradient optimisation mitigates parameter redundancy and scaling issues in IRL", "shape": "dot", "title": "natural gradient optimisation mitigates parameter redundancy and scaling issues in IRL"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "apply natural gradient inverse reinforcement learning during apprenticeship learning", "label": "apply natural gradient inverse reinforcement learning during apprenticeship learning", "shape": "dot", "title": "apply natural gradient inverse reinforcement learning during apprenticeship learning"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "incorporate Boltzmann stochastic policy as differentiable proxy for greedy selection in reward optimisation", "label": "incorporate Boltzmann stochastic policy as differentiable proxy for greedy selection in reward optimisation", "shape": "dot", "title": "incorporate Boltzmann stochastic policy as differentiable proxy for greedy selection in reward optimisation"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "compute optimal-value gradients via subdifferential fixed-point solver to guide reward updates", "label": "compute optimal-value gradients via subdifferential fixed-point solver to guide reward updates", "shape": "dot", "title": "compute optimal-value gradients via subdifferential fixed-point solver to guide reward updates"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "combine supervised loss with reward parameter tuning for sample-efficient policy imitation", "label": "combine supervised loss with reward parameter tuning for sample-efficient policy imitation", "shape": "dot", "title": "combine supervised loss with reward parameter tuning for sample-efficient policy imitation"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "mutual defection in Prisoner\u0027s Dilemma among AI-controlled firms", "label": "mutual defection in Prisoner\u0027s Dilemma among AI-controlled firms", "shape": "dot", "title": "mutual defection in Prisoner\u0027s Dilemma among AI-controlled firms"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "inefficient equilibria from classical rationalizable strategies in AI agents", "label": "inefficient equilibria from classical rationalizable strategies in AI agents", "shape": "dot", "title": "inefficient equilibria from classical rationalizable strategies in AI agents"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "lack of strategy translucency causing defection in PD settings", "label": "lack of strategy translucency causing defection in PD settings", "shape": "dot", "title": "lack of strategy translucency causing defection in PD settings"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "assumption of unilateral deviations leading to minimax dominated strategies persisting", "label": "assumption of unilateral deviations leading to minimax dominated strategies persisting", "shape": "dot", "title": "assumption of unilateral deviations leading to minimax dominated strategies persisting"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "translucency shifts equilibrium to cooperation when penalty exceeds reward and leak probability epsilon", "label": "translucency shifts equilibrium to cooperation when penalty exceeds reward and leak probability epsilon", "shape": "dot", "title": "translucency shifts equilibrium to cooperation when penalty exceeds reward and leak probability epsilon"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "common counterfactual belief of rationality linked to iterated minimax deletion", "label": "common counterfactual belief of rationality linked to iterated minimax deletion", "shape": "dot", "title": "common counterfactual belief of rationality linked to iterated minimax deletion"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "introduce controlled intention leakage to enforce cooperative equilibria", "label": "introduce controlled intention leakage to enforce cooperative equilibria", "shape": "dot", "title": "introduce controlled intention leakage to enforce cooperative equilibria"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "adopt CCBR-based reasoning in AI policy formation", "label": "adopt CCBR-based reasoning in AI policy formation", "shape": "dot", "title": "adopt CCBR-based reasoning in AI policy formation"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "randomised low-probability strategy disclosure channels between agents", "label": "randomised low-probability strategy disclosure channels between agents", "shape": "dot", "title": "randomised low-probability strategy disclosure channels between agents"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "iterated minimax-dominated strategy deletion algorithm during policy search", "label": "iterated minimax-dominated strategy deletion algorithm during policy search", "shape": "dot", "title": "iterated minimax-dominated strategy deletion algorithm during policy search"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "theoretical proof of cooperation under translucency", "label": "theoretical proof of cooperation under translucency", "shape": "dot", "title": "theoretical proof of cooperation under translucency"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "proof that CCBR strategies equal those surviving minimax deletion", "label": "proof that CCBR strategies equal those surviving minimax deletion", "shape": "dot", "title": "proof that CCBR strategies equal those surviving minimax deletion"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "deploy intention-signalling protocols in multi-agent AI systems", "label": "deploy intention-signalling protocols in multi-agent AI systems", "shape": "dot", "title": "deploy intention-signalling protocols in multi-agent AI systems"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "apply iterated minimax-dominated strategy pruning during multi-agent RL training", "label": "apply iterated minimax-dominated strategy pruning during multi-agent RL training", "shape": "dot", "title": "apply iterated minimax-dominated strategy pruning during multi-agent RL training"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "embed counterfactual individual-rationality checks in agent architecture", "label": "embed counterfactual individual-rationality checks in agent architecture", "shape": "dot", "title": "embed counterfactual individual-rationality checks in agent architecture"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Semantic misunderstanding in NLP applications due to poor embeddings", "label": "Semantic misunderstanding in NLP applications due to poor embeddings", "shape": "dot", "title": "Semantic misunderstanding in NLP applications due to poor embeddings"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Excessive computational cost in large-scale word embedding training", "label": "Excessive computational cost in large-scale word embedding training", "shape": "dot", "title": "Excessive computational cost in large-scale word embedding training"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Inability to model idiomatic phrases in NLP systems", "label": "Inability to model idiomatic phrases in NLP systems", "shape": "dot", "title": "Inability to model idiomatic phrases in NLP systems"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Low-quality distributed word representations in legacy neural language models", "label": "Low-quality distributed word representations in legacy neural language models", "shape": "dot", "title": "Low-quality distributed word representations in legacy neural language models"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Full softmax output layer complexity scaling with vocabulary size", "label": "Full softmax output layer complexity scaling with vocabulary size", "shape": "dot", "title": "Full softmax output layer complexity scaling with vocabulary size"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Word-level tokenization ignores multi-word expressions with unique meanings", "label": "Word-level tokenization ignores multi-word expressions with unique meanings", "shape": "dot", "title": "Word-level tokenization ignores multi-word expressions with unique meanings"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Linear vector space structure allows analogical reasoning from embeddings", "label": "Linear vector space structure allows analogical reasoning from embeddings", "shape": "dot", "title": "Linear vector space structure allows analogical reasoning from embeddings"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Approximate softmax objectives can reduce computation without losing representation quality", "label": "Approximate softmax objectives can reduce computation without losing representation quality", "shape": "dot", "title": "Approximate softmax objectives can reduce computation without losing representation quality"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Data-driven phrase token identification enhances representational expressiveness", "label": "Data-driven phrase token identification enhances representational expressiveness", "shape": "dot", "title": "Data-driven phrase token identification enhances representational expressiveness"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Context prediction training objective in Skip-gram model for efficient embedding learning", "label": "Context prediction training objective in Skip-gram model for efficient embedding learning", "shape": "dot", "title": "Context prediction training objective in Skip-gram model for efficient embedding learning"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Use of negative sampling and hierarchical softmax to approximate softmax efficiently", "label": "Use of negative sampling and hierarchical softmax to approximate softmax efficiently", "shape": "dot", "title": "Use of negative sampling and hierarchical softmax to approximate softmax efficiently"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Incorporating phrase tokens into training corpus to capture idiomatic meaning", "label": "Incorporating phrase tokens into training corpus to capture idiomatic meaning", "shape": "dot", "title": "Incorporating phrase tokens into training corpus to capture idiomatic meaning"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Negative sampling with k=5\u201315 negative draws per positive sample", "label": "Negative sampling with k=5\u201315 negative draws per positive sample", "shape": "dot", "title": "Negative sampling with k=5\u201315 negative draws per positive sample"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Hierarchical softmax with binary Huffman tree", "label": "Hierarchical softmax with binary Huffman tree", "shape": "dot", "title": "Hierarchical softmax with binary Huffman tree"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Subsampling of frequent words with probability P(w)=1\u2212\u221a(t/f(w))", "label": "Subsampling of frequent words with probability P(w)=1\u2212\u221a(t/f(w))", "shape": "dot", "title": "Subsampling of frequent words with probability P(w)=1\u2212\u221a(t/f(w))"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Data-driven bigram score thresholding to create phrase tokens", "label": "Data-driven bigram score thresholding to create phrase tokens", "shape": "dot", "title": "Data-driven bigram score thresholding to create phrase tokens"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Analogical reasoning accuracy improvements over baselines", "label": "Analogical reasoning accuracy improvements over baselines", "shape": "dot", "title": "Analogical reasoning accuracy improvements over baselines"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Training speed improvements (2x\u201310x) with subsampling and negative sampling", "label": "Training speed improvements (2x\u201310x) with subsampling and negative sampling", "shape": "dot", "title": "Training speed improvements (2x\u201310x) with subsampling and negative sampling"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Phrase analogy accuracy improvement to 72% with phrase embeddings", "label": "Phrase analogy accuracy improvement to 72% with phrase embeddings", "shape": "dot", "title": "Phrase analogy accuracy improvement to 72% with phrase embeddings"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Pre-train word embeddings with Skip-gram using negative sampling and subsampling", "label": "Pre-train word embeddings with Skip-gram using negative sampling and subsampling", "shape": "dot", "title": "Pre-train word embeddings with Skip-gram using negative sampling and subsampling"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Pre-process corpus to detect bigram and n-gram phrases and train phrase-level Skip-gram embeddings", "label": "Pre-process corpus to detect bigram and n-gram phrases and train phrase-level Skip-gram embeddings", "shape": "dot", "title": "Pre-process corpus to detect bigram and n-gram phrases and train phrase-level Skip-gram embeddings"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Train word embeddings with hierarchical softmax using Huffman codes for computational efficiency", "label": "Train word embeddings with hierarchical softmax using Huffman codes for computational efficiency", "shape": "dot", "title": "Train word embeddings with hierarchical softmax using Huffman codes for computational efficiency"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Opaque decision-making in deep convolutional networks for image classification", "label": "Opaque decision-making in deep convolutional networks for image classification", "shape": "dot", "title": "Opaque decision-making in deep convolutional networks for image classification"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Limited visibility into internal feature representations obstructs debugging of ConvNet failure modes", "label": "Limited visibility into internal feature representations obstructs debugging of ConvNet failure modes", "shape": "dot", "title": "Limited visibility into internal feature representations obstructs debugging of ConvNet failure modes"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Visualization of feature activations exposes hierarchical structure and invariances in ConvNets", "label": "Visualization of feature activations exposes hierarchical structure and invariances in ConvNets", "shape": "dot", "title": "Visualization of feature activations exposes hierarchical structure and invariances in ConvNets"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Use deconvolutional network-based top-down projection to interpret ConvNet activations", "label": "Use deconvolutional network-based top-down projection to interpret ConvNet activations", "shape": "dot", "title": "Use deconvolutional network-based top-down projection to interpret ConvNet activations"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Layer-wise deconvnet with unpooling switches, rectification, and transposed filters for feature reconstruction", "label": "Layer-wise deconvnet with unpooling switches, rectification, and transposed filters for feature reconstruction", "shape": "dot", "title": "Layer-wise deconvnet with unpooling switches, rectification, and transposed filters for feature reconstruction"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Feature reconstructions reveal intuitive patterns and hierarchical invariance across layers", "label": "Feature reconstructions reveal intuitive patterns and hierarchical invariance across layers", "shape": "dot", "title": "Feature reconstructions reveal intuitive patterns and hierarchical invariance across layers"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Visualization-driven architecture change improves ImageNet error rates", "label": "Visualization-driven architecture change improves ImageNet error rates", "shape": "dot", "title": "Visualization-driven architecture change improves ImageNet error rates"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Occlusion sensitivity experiments confirm localized object sensitivity", "label": "Occlusion sensitivity experiments confirm localized object sensitivity", "shape": "dot", "title": "Occlusion sensitivity experiments confirm localized object sensitivity"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate deconvolutional visualization into ConvNet development workflow", "label": "Integrate deconvolutional visualization into ConvNet development workflow", "shape": "dot", "title": "Integrate deconvolutional visualization into ConvNet development workflow"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Design first ConvNet layer with 7\u00d77 filters and stride 2 to reduce aliasing", "label": "Design first ConvNet layer with 7\u00d77 filters and stride 2 to reduce aliasing", "shape": "dot", "title": "Design first ConvNet layer with 7\u00d77 filters and stride 2 to reduce aliasing"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Apply per-filter RMS renormalization during training to prevent dominant filters", "label": "Apply per-filter RMS renormalization during training to prevent dominant filters", "shape": "dot", "title": "Apply per-filter RMS renormalization during training to prevent dominant filters"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Conduct occlusion sensitivity analysis before deployment to verify model localisation", "label": "Conduct occlusion sensitivity analysis before deployment to verify model localisation", "shape": "dot", "title": "Conduct occlusion sensitivity analysis before deployment to verify model localisation"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Ineffective reinforcement learning from high-dimensional sensory inputs in Atari environments", "label": "Ineffective reinforcement learning from high-dimensional sensory inputs in Atari environments", "shape": "dot", "title": "Ineffective reinforcement learning from high-dimensional sensory inputs in Atari environments"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Instability and divergence in Q-learning with nonlinear function approximators in Atari environments", "label": "Instability and divergence in Q-learning with nonlinear function approximators in Atari environments", "shape": "dot", "title": "Instability and divergence in Q-learning with nonlinear function approximators in Atari environments"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Correlated sequential data in on-policy RL training in Atari games", "label": "Correlated sequential data in on-policy RL training in Atari games", "shape": "dot", "title": "Correlated sequential data in on-policy RL training in Atari games"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Non-stationary data distribution due to evolving policies in Atari RL", "label": "Non-stationary data distribution due to evolving policies in Atari RL", "shape": "dot", "title": "Non-stationary data distribution due to evolving policies in Atari RL"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Sparse and delayed reward signals in Atari tasks", "label": "Sparse and delayed reward signals in Atari tasks", "shape": "dot", "title": "Sparse and delayed reward signals in Atari tasks"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "High computational cost of per-frame action selection in Atari emulator", "label": "High computational cost of per-frame action selection in Atari emulator", "shape": "dot", "title": "High computational cost of per-frame action selection in Atari emulator"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "High-dimensional visual input without hand-crafted features in Atari games", "label": "High-dimensional visual input without hand-crafted features in Atari games", "shape": "dot", "title": "High-dimensional visual input without hand-crafted features in Atari games"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Experience replay improving sample independence in Atari RL", "label": "Experience replay improving sample independence in Atari RL", "shape": "dot", "title": "Experience replay improving sample independence in Atari RL"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Off-policy \u03b5-greedy exploration enabling data reuse in Atari RL", "label": "Off-policy \u03b5-greedy exploration enabling data reuse in Atari RL", "shape": "dot", "title": "Off-policy \u03b5-greedy exploration enabling data reuse in Atari RL"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "End-to-end deep feature learning from raw pixels in Atari RL", "label": "End-to-end deep feature learning from raw pixels in Atari RL", "shape": "dot", "title": "End-to-end deep feature learning from raw pixels in Atari RL"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Temporal redundancy in Atari frame sequences permitting frame skipping", "label": "Temporal redundancy in Atari frame sequences permitting frame skipping", "shape": "dot", "title": "Temporal redundancy in Atari frame sequences permitting frame skipping"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Reward magnitude normalization stabilizing gradients across Atari games", "label": "Reward magnitude normalization stabilizing gradients across Atari games", "shape": "dot", "title": "Reward magnitude normalization stabilizing gradients across Atari games"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate experience replay with deep Q-learning for data efficiency in Atari RL", "label": "Integrate experience replay with deep Q-learning for data efficiency in Atari RL", "shape": "dot", "title": "Integrate experience replay with deep Q-learning for data efficiency in Atari RL"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "CNN with shared visual representation and action-specific outputs for efficient Q-value computation in Atari RL", "label": "CNN with shared visual representation and action-specific outputs for efficient Q-value computation in Atari RL", "shape": "dot", "title": "CNN with shared visual representation and action-specific outputs for efficient Q-value computation in Atari RL"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Frame skipping to reduce computational burden in Atari RL", "label": "Frame skipping to reduce computational burden in Atari RL", "shape": "dot", "title": "Frame skipping to reduce computational burden in Atari RL"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Replay memory of one million frames with uniform random minibatch sampling in DQN", "label": "Replay memory of one million frames with uniform random minibatch sampling in DQN", "shape": "dot", "title": "Replay memory of one million frames with uniform random minibatch sampling in DQN"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Deep Q-network architecture with conv(16x8 stride4,32x4 stride2)+256FC action outputs", "label": "Deep Q-network architecture with conv(16x8 stride4,32x4 stride2)+256FC action outputs", "shape": "dot", "title": "Deep Q-network architecture with conv(16x8 stride4,32x4 stride2)+256FC action outputs"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Reward clipping implementation mapping rewards to {-1,0,1} during DQN training", "label": "Reward clipping implementation mapping rewards to {-1,0,1} during DQN training", "shape": "dot", "title": "Reward clipping implementation mapping rewards to {-1,0,1} during DQN training"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Frame skipping implementation repeating last action for k=4 (k=3 in Space Invaders)", "label": "Frame skipping implementation repeating last action for k=4 (k=3 in Space Invaders)", "shape": "dot", "title": "Frame skipping implementation repeating last action for k=4 (k=3 in Space Invaders)"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "DQN outperforming prior RL methods on six of seven Atari games", "label": "DQN outperforming prior RL methods on six of seven Atari games", "shape": "dot", "title": "DQN outperforming prior RL methods on six of seven Atari games"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Smooth predicted Q-value increase without divergence during DQN training", "label": "Smooth predicted Q-value increase without divergence during DQN training", "shape": "dot", "title": "Smooth predicted Q-value increase without divergence during DQN training"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "\u03b5-greedy exploration strategy for off-policy learning in Atari RL", "label": "\u03b5-greedy exploration strategy for off-policy learning in Atari RL", "shape": "dot", "title": "\u03b5-greedy exploration strategy for off-policy learning in Atari RL"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Linear annealing of \u03b5 from 1 to 0.1 over first million frames in DQN training", "label": "Linear annealing of \u03b5 from 1 to 0.1 over first million frames in DQN training", "shape": "dot", "title": "Linear annealing of \u03b5 from 1 to 0.1 over first million frames in DQN training"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Clip rewards to unit magnitude for consistent gradient scales in Atari DQN training", "label": "Clip rewards to unit magnitude for consistent gradient scales in Atari DQN training", "shape": "dot", "title": "Clip rewards to unit magnitude for consistent gradient scales in Atari DQN training"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Implement experience replay with uniform random sampling during DQN training", "label": "Implement experience replay with uniform random sampling during DQN training", "shape": "dot", "title": "Implement experience replay with uniform random sampling during DQN training"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Train deep convolutional Q-networks end-to-end from raw pixel inputs in Atari RL", "label": "Train deep convolutional Q-networks end-to-end from raw pixel inputs in Atari RL", "shape": "dot", "title": "Train deep convolutional Q-networks end-to-end from raw pixel inputs in Atari RL"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Clip reward magnitudes to [-1,1] during training of Atari DQN agents", "label": "Clip reward magnitudes to [-1,1] during training of Atari DQN agents", "shape": "dot", "title": "Clip reward magnitudes to [-1,1] during training of Atari DQN agents"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Apply frame skipping (k=3\u20134) during Atari RL agent training", "label": "Apply frame skipping (k=3\u20134) during Atari RL agent training", "shape": "dot", "title": "Apply frame skipping (k=3\u20134) during Atari RL agent training"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Use \u03b5-greedy exploration with linear annealing of \u03b5 during DQN training", "label": "Use \u03b5-greedy exploration with linear annealing of \u03b5 during DQN training", "shape": "dot", "title": "Use \u03b5-greedy exploration with linear annealing of \u03b5 during DQN training"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Intractable posterior distributions in continuous latent variable models", "label": "Intractable posterior distributions in continuous latent variable models", "shape": "dot", "title": "Intractable posterior distributions in continuous latent variable models"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "High computational cost of inference on large datasets in probabilistic models", "label": "High computational cost of inference on large datasets in probabilistic models", "shape": "dot", "title": "High computational cost of inference on large datasets in probabilistic models"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "High variance gradient estimators in variational inference", "label": "High variance gradient estimators in variational inference", "shape": "dot", "title": "High variance gradient estimators in variational inference"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Analytical intractability of marginal likelihood integrals", "label": "Analytical intractability of marginal likelihood integrals", "shape": "dot", "title": "Analytical intractability of marginal likelihood integrals"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Inefficiency of sampling-based inference per datapoint", "label": "Inefficiency of sampling-based inference per datapoint", "shape": "dot", "title": "Inefficiency of sampling-based inference per datapoint"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Variance explosion in na\u00efve Monte Carlo gradient estimator", "label": "Variance explosion in na\u00efve Monte Carlo gradient estimator", "shape": "dot", "title": "Variance explosion in na\u00efve Monte Carlo gradient estimator"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Reparameterization of latent variables enabling differentiable sampling", "label": "Reparameterization of latent variables enabling differentiable sampling", "shape": "dot", "title": "Reparameterization of latent variables enabling differentiable sampling"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Variational lower bound regularization in approximate inference", "label": "Variational lower bound regularization in approximate inference", "shape": "dot", "title": "Variational lower bound regularization in approximate inference"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Stochastic gradient variational Bayes optimization of lower bound", "label": "Stochastic gradient variational Bayes optimization of lower bound", "shape": "dot", "title": "Stochastic gradient variational Bayes optimization of lower bound"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Auto-Encoding variational Bayes joint training of encoder and decoder", "label": "Auto-Encoding variational Bayes joint training of encoder and decoder", "shape": "dot", "title": "Auto-Encoding variational Bayes joint training of encoder and decoder"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "SGVB estimator with analytic KL divergence and single-sample minibatch Monte Carlo", "label": "SGVB estimator with analytic KL divergence and single-sample minibatch Monte Carlo", "shape": "dot", "title": "SGVB estimator with analytic KL divergence and single-sample minibatch Monte Carlo"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Gaussian neural network encoder generating mean and variance parameters", "label": "Gaussian neural network encoder generating mean and variance parameters", "shape": "dot", "title": "Gaussian neural network encoder generating mean and variance parameters"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Minibatch SGD or Adagrad training with L=1 sample per datapoint", "label": "Minibatch SGD or Adagrad training with L=1 sample per datapoint", "shape": "dot", "title": "Minibatch SGD or Adagrad training with L=1 sample per datapoint"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Empirical faster convergence and superior lower bound versus wake-sleep and MCEM", "label": "Empirical faster convergence and superior lower bound versus wake-sleep and MCEM", "shape": "dot", "title": "Empirical faster convergence and superior lower bound versus wake-sleep and MCEM"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Marginal likelihood improvement demonstrated on MNIST data", "label": "Marginal likelihood improvement demonstrated on MNIST data", "shape": "dot", "title": "Marginal likelihood improvement demonstrated on MNIST data"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Apply SGVB-based variational auto-encoder training for scalable inference in continuous latent variable models", "label": "Apply SGVB-based variational auto-encoder training for scalable inference in continuous latent variable models", "shape": "dot", "title": "Apply SGVB-based variational auto-encoder training for scalable inference in continuous latent variable models"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Use reparameterization trick to reduce gradient variance during variational training", "label": "Use reparameterization trick to reduce gradient variance during variational training", "shape": "dot", "title": "Use reparameterization trick to reduce gradient variance during variational training"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate analytic KL computation in VAE loss to lower estimator variance", "label": "Integrate analytic KL computation in VAE loss to lower estimator variance", "shape": "dot", "title": "Integrate analytic KL computation in VAE loss to lower estimator variance"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Intractable likelihood estimation in deep generative models", "label": "Intractable likelihood estimation in deep generative models", "shape": "dot", "title": "Intractable likelihood estimation in deep generative models"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Slow Markov chain mixing in deep generative model training", "label": "Slow Markov chain mixing in deep generative model training", "shape": "dot", "title": "Slow Markov chain mixing in deep generative model training"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Vanishing generator gradients in adversarial training", "label": "Vanishing generator gradients in adversarial training", "shape": "dot", "title": "Vanishing generator gradients in adversarial training"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Dependence on MCMC-based partition function gradient estimation in undirected generative models", "label": "Dependence on MCMC-based partition function gradient estimation in undirected generative models", "shape": "dot", "title": "Dependence on MCMC-based partition function gradient estimation in undirected generative models"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Requirement of Markov chain sampling during generation in autoencoder and undirected models", "label": "Requirement of Markov chain sampling during generation in autoencoder and undirected models", "shape": "dot", "title": "Requirement of Markov chain sampling during generation in autoencoder and undirected models"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Saturation of log(1\u2212D(G(z))) objective early in adversarial training", "label": "Saturation of log(1\u2212D(G(z))) objective early in adversarial training", "shape": "dot", "title": "Saturation of log(1\u2212D(G(z))) objective early in adversarial training"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Discriminative adversarial objective eliminates need for explicit likelihood computation and MCMC", "label": "Discriminative adversarial objective eliminates need for explicit likelihood computation and MCMC", "shape": "dot", "title": "Discriminative adversarial objective eliminates need for explicit likelihood computation and MCMC"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Minimax training aligns generator distribution with data via Jensen\u2013Shannon divergence minimization", "label": "Minimax training aligns generator distribution with data via Jensen\u2013Shannon divergence minimization", "shape": "dot", "title": "Minimax training aligns generator distribution with data via Jensen\u2013Shannon divergence minimization"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Alternative generator objective maximizing log D(G(z)) provides stronger gradients without altering fixed point", "label": "Alternative generator objective maximizing log D(G(z)) provides stronger gradients without altering fixed point", "shape": "dot", "title": "Alternative generator objective maximizing log D(G(z)) provides stronger gradients without altering fixed point"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Generator training driven by discriminator feedback in minimax game", "label": "Generator training driven by discriminator feedback in minimax game", "shape": "dot", "title": "Generator training driven by discriminator feedback in minimax game"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Alternating k-step discriminator and generator updates to balance training dynamics", "label": "Alternating k-step discriminator and generator updates to balance training dynamics", "shape": "dot", "title": "Alternating k-step discriminator and generator updates to balance training dynamics"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Two multilayer perceptrons for generator and discriminator trained with backprop and dropout", "label": "Two multilayer perceptrons for generator and discriminator trained with backprop and dropout", "shape": "dot", "title": "Two multilayer perceptrons for generator and discriminator trained with backprop and dropout"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Modified generator update to maximize log D(G(z)) during training", "label": "Modified generator update to maximize log D(G(z)) during training", "shape": "dot", "title": "Modified generator update to maximize log D(G(z)) during training"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Improved Parzen window log-likelihood on MNIST and TFD benchmarks", "label": "Improved Parzen window log-likelihood on MNIST and TFD benchmarks", "shape": "dot", "title": "Improved Parzen window log-likelihood on MNIST and TFD benchmarks"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Competitive sample quality without Markov chain mixing", "label": "Competitive sample quality without Markov chain mixing", "shape": "dot", "title": "Competitive sample quality without Markov chain mixing"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Train generator\u2013discriminator pair with adversarial nets during pre-training", "label": "Train generator\u2013discriminator pair with adversarial nets during pre-training", "shape": "dot", "title": "Train generator\u2013discriminator pair with adversarial nets during pre-training"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Optimize generator with log D(G(z)) objective during adversarial training", "label": "Optimize generator with log D(G(z)) objective during adversarial training", "shape": "dot", "title": "Optimize generator with log D(G(z)) objective during adversarial training"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Use one discriminator update per generator update in adversarial training", "label": "Use one discriminator update per generator update in adversarial training", "shape": "dot", "title": "Use one discriminator update per generator update in adversarial training"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Apply dropout to discriminator network during adversarial training", "label": "Apply dropout to discriminator network during adversarial training", "shape": "dot", "title": "Apply dropout to discriminator network during adversarial training"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "translation quality degradation for long sentences in neural machine translation", "label": "translation quality degradation for long sentences in neural machine translation", "shape": "dot", "title": "translation quality degradation for long sentences in neural machine translation"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "incorrect word alignment and reordering in NMT outputs", "label": "incorrect word alignment and reordering in NMT outputs", "shape": "dot", "title": "incorrect word alignment and reordering in NMT outputs"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "fixed-length context vector bottleneck in encoder-decoder NMT", "label": "fixed-length context vector bottleneck in encoder-decoder NMT", "shape": "dot", "title": "fixed-length context vector bottleneck in encoder-decoder NMT"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "encoder burden to compress full source information", "label": "encoder burden to compress full source information", "shape": "dot", "title": "encoder burden to compress full source information"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "absence of explicit alignment mechanism in encoder-decoder NMT", "label": "absence of explicit alignment mechanism in encoder-decoder NMT", "shape": "dot", "title": "absence of explicit alignment mechanism in encoder-decoder NMT"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "adaptive attention over source annotations can dynamically focus on relevant parts", "label": "adaptive attention over source annotations can dynamically focus on relevant parts", "shape": "dot", "title": "adaptive attention over source annotations can dynamically focus on relevant parts"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "soft alignment supports non-monotonic reordering", "label": "soft alignment supports non-monotonic reordering", "shape": "dot", "title": "soft alignment supports non-monotonic reordering"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "joint training of alignment and translation to share gradients and improve performance", "label": "joint training of alignment and translation to share gradients and improve performance", "shape": "dot", "title": "joint training of alignment and translation to share gradients and improve performance"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "use bidirectional RNN encoder for contextual annotations", "label": "use bidirectional RNN encoder for contextual annotations", "shape": "dot", "title": "use bidirectional RNN encoder for contextual annotations"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "attention-weighted context vector per target word", "label": "attention-weighted context vector per target word", "shape": "dot", "title": "attention-weighted context vector per target word"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "feedforward alignment model producing softmax weights", "label": "feedforward alignment model producing softmax weights", "shape": "dot", "title": "feedforward alignment model producing softmax weights"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "bidirectional RNN encoder concatenating forward and backward hidden states as annotations", "label": "bidirectional RNN encoder concatenating forward and backward hidden states as annotations", "shape": "dot", "title": "bidirectional RNN encoder concatenating forward and backward hidden states as annotations"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "RNNsearch BLEU improvements over RNNencdec on WMT14 En-Fr", "label": "RNNsearch BLEU improvements over RNNencdec on WMT14 En-Fr", "shape": "dot", "title": "RNNsearch BLEU improvements over RNNencdec on WMT14 En-Fr"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "performance robustness across sentence lengths with RNNsearch", "label": "performance robustness across sentence lengths with RNNsearch", "shape": "dot", "title": "performance robustness across sentence lengths with RNNsearch"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "attention visualisations show plausible alignments", "label": "attention visualisations show plausible alignments", "shape": "dot", "title": "attention visualisations show plausible alignments"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "integrate attention-based bidirectional encoder-decoder architecture (RNNsearch) during NMT model design", "label": "integrate attention-based bidirectional encoder-decoder architecture (RNNsearch) during NMT model design", "shape": "dot", "title": "integrate attention-based bidirectional encoder-decoder architecture (RNNsearch) during NMT model design"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "inspect attention weight visualisations during pre-deployment testing to ensure alignment quality", "label": "inspect attention weight visualisations during pre-deployment testing to ensure alignment quality", "shape": "dot", "title": "inspect attention weight visualisations during pre-deployment testing to ensure alignment quality"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Human elimination by superintelligent AI in future universe", "label": "Human elimination by superintelligent AI in future universe", "shape": "dot", "title": "Human elimination by superintelligent AI in future universe"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Goal retention failure in self-improving AI systems", "label": "Goal retention failure in self-improving AI systems", "shape": "dot", "title": "Goal retention failure in self-improving AI systems"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Ill-defined human values in physical terms", "label": "Ill-defined human values in physical terms", "shape": "dot", "title": "Ill-defined human values in physical terms"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Self-modeling and improved world understanding induce goal reinterpretation", "label": "Self-modeling and improved world understanding induce goal reinterpretation", "shape": "dot", "title": "Self-modeling and improved world understanding induce goal reinterpretation"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Only physically definable goals remain well-defined under arbitrarily high intelligence", "label": "Only physically definable goals remain well-defined under arbitrarily high intelligence", "shape": "dot", "title": "Only physically definable goals remain well-defined under arbitrarily high intelligence"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Design robust goal retention mechanisms immune to self-modeling subversion", "label": "Design robust goal retention mechanisms immune to self-modeling subversion", "shape": "dot", "title": "Design robust goal retention mechanisms immune to self-modeling subversion"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Establish rigorously defined desirable final goals before AI reaches superintelligence", "label": "Establish rigorously defined desirable final goals before AI reaches superintelligence", "shape": "dot", "title": "Establish rigorously defined desirable final goals before AI reaches superintelligence"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Meta-stable utility preservation protocols during recursive self-improvement", "label": "Meta-stable utility preservation protocols during recursive self-improvement", "shape": "dot", "title": "Meta-stable utility preservation protocols during recursive self-improvement"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Philosophical value formalization into computable utility functions", "label": "Philosophical value formalization into computable utility functions", "shape": "dot", "title": "Philosophical value formalization into computable utility functions"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Human goal evolution with increased intelligence indicates goal instability", "label": "Human goal evolution with increased intelligence indicates goal instability", "shape": "dot", "title": "Human goal evolution with increased intelligence indicates goal instability"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Definability-desirability gap evidence from entropy utility counterexamples", "label": "Definability-desirability gap evidence from entropy utility counterexamples", "shape": "dot", "title": "Definability-desirability gap evidence from entropy utility counterexamples"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Advance interdisciplinary research program to formalize universally desirable, physically expressible utility functions for AI", "label": "Advance interdisciplinary research program to formalize universally desirable, physically expressible utility functions for AI", "shape": "dot", "title": "Advance interdisciplinary research program to formalize universally desirable, physically expressible utility functions for AI"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Implement architectural safeguards ensuring immutable core goals during self-modification cycles in AI", "label": "Implement architectural safeguards ensuring immutable core goals during self-modification cycles in AI", "shape": "dot", "title": "Implement architectural safeguards ensuring immutable core goals during self-modification cycles in AI"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Delay deployment of self-improving AI until alignment theory resolves final goal definability", "label": "Delay deployment of self-improving AI until alignment theory resolves final goal definability", "shape": "dot", "title": "Delay deployment of self-improving AI until alignment theory resolves final goal definability"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Low classification accuracy in large-scale image recognition", "label": "Low classification accuracy in large-scale image recognition", "shape": "dot", "title": "Low classification accuracy in large-scale image recognition"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Insufficient representational capacity of shallow ConvNets with large filters", "label": "Insufficient representational capacity of shallow ConvNets with large filters", "shape": "dot", "title": "Insufficient representational capacity of shallow ConvNets with large filters"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Depth increase with small 3\u00d73 filters increases nonlinear discriminative power while controlling parameters", "label": "Depth increase with small 3\u00d73 filters increases nonlinear discriminative power while controlling parameters", "shape": "dot", "title": "Depth increase with small 3\u00d73 filters increases nonlinear discriminative power while controlling parameters"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Design very deep ConvNets by stacking small 3\u00d73 convolution layers", "label": "Design very deep ConvNets by stacking small 3\u00d73 convolution layers", "shape": "dot", "title": "Design very deep ConvNets by stacking small 3\u00d73 convolution layers"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "VGG ConvNet configurations A\u2013E with 16\u201319 weight layers of 3\u00d73 filters", "label": "VGG ConvNet configurations A\u2013E with 16\u201319 weight layers of 3\u00d73 filters", "shape": "dot", "title": "VGG ConvNet configurations A\u2013E with 16\u201319 weight layers of 3\u00d73 filters"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Top-5 error 8.0 % on ImageNet using 19-layer VGG model", "label": "Top-5 error 8.0 % on ImageNet using 19-layer VGG model", "shape": "dot", "title": "Top-5 error 8.0 % on ImageNet using 19-layer VGG model"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Single-scale training limits robustness to object scale variation", "label": "Single-scale training limits robustness to object scale variation", "shape": "dot", "title": "Single-scale training limits robustness to object scale variation"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Scale jittering during training captures multi-scale statistics", "label": "Scale jittering during training captures multi-scale statistics", "shape": "dot", "title": "Scale jittering during training captures multi-scale statistics"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Augment training images by random rescaling within range", "label": "Augment training images by random rescaling within range", "shape": "dot", "title": "Augment training images by random rescaling within range"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Training VGG with S in [256,512] scale range", "label": "Training VGG with S in [256,512] scale range", "shape": "dot", "title": "Training VGG with S in [256,512] scale range"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Validation error reduced to 7.5 % top-5 with multi-scale training", "label": "Validation error reduced to 7.5 % top-5 with multi-scale training", "shape": "dot", "title": "Validation error reduced to 7.5 % top-5 with multi-scale training"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Single-scale testing fails to exploit scale robustness", "label": "Single-scale testing fails to exploit scale robustness", "shape": "dot", "title": "Single-scale testing fails to exploit scale robustness"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Aggregating predictions over multiple scales reduces error", "label": "Aggregating predictions over multiple scales reduces error", "shape": "dot", "title": "Aggregating predictions over multiple scales reduces error"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Average class probabilities across rescaled test images", "label": "Average class probabilities across rescaled test images", "shape": "dot", "title": "Average class probabilities across rescaled test images"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Evaluate VGG at Q={256,384,512} and average outputs", "label": "Evaluate VGG at Q={256,384,512} and average outputs", "shape": "dot", "title": "Evaluate VGG at Q={256,384,512} and average outputs"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Top-5 error 7.5 % with multi-scale evaluation", "label": "Top-5 error 7.5 % with multi-scale evaluation", "shape": "dot", "title": "Top-5 error 7.5 % with multi-scale evaluation"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Single ConvNet variance limits achievable accuracy", "label": "Single ConvNet variance limits achievable accuracy", "shape": "dot", "title": "Single ConvNet variance limits achievable accuracy"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Ensembling complementary models reduces variance", "label": "Ensembling complementary models reduces variance", "shape": "dot", "title": "Ensembling complementary models reduces variance"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Average softmax outputs across multiple ConvNets", "label": "Average softmax outputs across multiple ConvNets", "shape": "dot", "title": "Average softmax outputs across multiple ConvNets"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Fusion of two multi-scale VGG models D and E", "label": "Fusion of two multi-scale VGG models D and E", "shape": "dot", "title": "Fusion of two multi-scale VGG models D and E"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Ensemble achieved 6.8 % top-5 error on ImageNet", "label": "Ensemble achieved 6.8 % top-5 error on ImageNet", "shape": "dot", "title": "Ensemble achieved 6.8 % top-5 error on ImageNet"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Design ConvNets with 16\u201319 layers of 3\u00d73 filters for large-scale image tasks", "label": "Design ConvNets with 16\u201319 layers of 3\u00d73 filters for large-scale image tasks", "shape": "dot", "title": "Design ConvNets with 16\u201319 layers of 3\u00d73 filters for large-scale image tasks"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Apply scale-jittered multi-scale training for ConvNets", "label": "Apply scale-jittered multi-scale training for ConvNets", "shape": "dot", "title": "Apply scale-jittered multi-scale training for ConvNets"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Perform multi-scale test-time evaluation by averaging predictions across scales", "label": "Perform multi-scale test-time evaluation by averaging predictions across scales", "shape": "dot", "title": "Perform multi-scale test-time evaluation by averaging predictions across scales"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Ensemble complementary deep ConvNets by averaging soft-max outputs", "label": "Ensemble complementary deep ConvNets by averaging soft-max outputs", "shape": "dot", "title": "Ensemble complementary deep ConvNets by averaging soft-max outputs"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Ineffective mapping of variable-length sequences in neural networks", "label": "Ineffective mapping of variable-length sequences in neural networks", "shape": "dot", "title": "Ineffective mapping of variable-length sequences in neural networks"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Fixed-dimensional input and output requirement in DNNs", "label": "Fixed-dimensional input and output requirement in DNNs", "shape": "dot", "title": "Fixed-dimensional input and output requirement in DNNs"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Encoder-decoder LSTM learning variable-length sequence mapping", "label": "Encoder-decoder LSTM learning variable-length sequence mapping", "shape": "dot", "title": "Encoder-decoder LSTM learning variable-length sequence mapping"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Separate encoder and decoder LSTMs for sequence mapping", "label": "Separate encoder and decoder LSTMs for sequence mapping", "shape": "dot", "title": "Separate encoder and decoder LSTMs for sequence mapping"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Deep 4-layer LSTM encoder-decoder with 8000-dimensional sentence vectors", "label": "Deep 4-layer LSTM encoder-decoder with 8000-dimensional sentence vectors", "shape": "dot", "title": "Deep 4-layer LSTM encoder-decoder with 8000-dimensional sentence vectors"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "BLEU 34.81 on WMT\u201914 with reversed LSTM ensemble", "label": "BLEU 34.81 on WMT\u201914 with reversed LSTM ensemble", "shape": "dot", "title": "BLEU 34.81 on WMT\u201914 with reversed LSTM ensemble"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Design and train deep encoder-decoder LSTM models with reversed input sequence", "label": "Design and train deep encoder-decoder LSTM models with reversed input sequence", "shape": "dot", "title": "Design and train deep encoder-decoder LSTM models with reversed input sequence"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Translation degradation on long sentences in RNN models", "label": "Translation degradation on long sentences in RNN models", "shape": "dot", "title": "Translation degradation on long sentences in RNN models"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Large minimal time lag between corresponding source and target words", "label": "Large minimal time lag between corresponding source and target words", "shape": "dot", "title": "Large minimal time lag between corresponding source and target words"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Reversing source sentences reduces minimal time lag", "label": "Reversing source sentences reduces minimal time lag", "shape": "dot", "title": "Reversing source sentences reduces minimal time lag"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Apply input sentence reversal before training", "label": "Apply input sentence reversal before training", "shape": "dot", "title": "Apply input sentence reversal before training"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Reverse source sentence order during training", "label": "Reverse source sentence order during training", "shape": "dot", "title": "Reverse source sentence order during training"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Perplexity drop to 4.7 and BLEU increase to 30.6 with reversed input", "label": "Perplexity drop to 4.7 and BLEU increase to 30.6 with reversed input", "shape": "dot", "title": "Perplexity drop to 4.7 and BLEU increase to 30.6 with reversed input"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Reverse source sentences during data preprocessing for sequence models", "label": "Reverse source sentences during data preprocessing for sequence models", "shape": "dot", "title": "Reverse source sentences during data preprocessing for sequence models"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Training instability due to exploding gradients in deep LSTM training", "label": "Training instability due to exploding gradients in deep LSTM training", "shape": "dot", "title": "Training instability due to exploding gradients in deep LSTM training"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Exploding gradients in deep LSTM optimization", "label": "Exploding gradients in deep LSTM optimization", "shape": "dot", "title": "Exploding gradients in deep LSTM optimization"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Gradient norm constraint to control exploding gradients", "label": "Gradient norm constraint to control exploding gradients", "shape": "dot", "title": "Gradient norm constraint to control exploding gradients"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Apply hard gradient norm clipping", "label": "Apply hard gradient norm clipping", "shape": "dot", "title": "Apply hard gradient norm clipping"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Scale gradient when norm exceeds threshold 5", "label": "Scale gradient when norm exceeds threshold 5", "shape": "dot", "title": "Scale gradient when norm exceeds threshold 5"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Successful training of 384M parameter LSTM over 7.5 epochs", "label": "Successful training of 384M parameter LSTM over 7.5 epochs", "shape": "dot", "title": "Successful training of 384M parameter LSTM over 7.5 epochs"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Apply gradient norm clipping at threshold 5 during LSTM training", "label": "Apply gradient norm clipping at threshold 5 during LSTM training", "shape": "dot", "title": "Apply gradient norm clipping at threshold 5 during LSTM training"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "unintended instrumental actions in advanced AI systems", "label": "unintended instrumental actions in advanced AI systems", "shape": "dot", "title": "unintended instrumental actions in advanced AI systems"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "self-delusion via reward hacking in RL agents", "label": "self-delusion via reward hacking in RL agents", "shape": "dot", "title": "self-delusion via reward hacking in RL agents"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "corrupting the reward generator by AI manipulation of humans", "label": "corrupting the reward generator by AI manipulation of humans", "shape": "dot", "title": "corrupting the reward generator by AI manipulation of humans"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "authoritarian power concentration through super-intelligent AI", "label": "authoritarian power concentration through super-intelligent AI", "shape": "dot", "title": "authoritarian power concentration through super-intelligent AI"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "AI surveillance and manipulation harming human autonomy", "label": "AI surveillance and manipulation harming human autonomy", "shape": "dot", "title": "AI surveillance and manipulation harming human autonomy"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "multi-agent AI arms race increasing existential risk", "label": "multi-agent AI arms race increasing existential risk", "shape": "dot", "title": "multi-agent AI arms race increasing existential risk"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "instrumental drives from utility maximization in embedded agents", "label": "instrumental drives from utility maximization in embedded agents", "shape": "dot", "title": "instrumental drives from utility maximization in embedded agents"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "reward observations embedded in environment enable delusion box", "label": "reward observations embedded in environment enable delusion box", "shape": "dot", "title": "reward observations embedded in environment enable delusion box"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "approximate environment models under limited computational resources", "label": "approximate environment models under limited computational resources", "shape": "dot", "title": "approximate environment models under limited computational resources"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "agents embedded in environment vulnerable to self-modification attacks", "label": "agents embedded in environment vulnerable to self-modification attacks", "shape": "dot", "title": "agents embedded in environment vulnerable to self-modification attacks"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "model-based utility functions referencing learned environment model", "label": "model-based utility functions referencing learned environment model", "shape": "dot", "title": "model-based utility functions referencing learned environment model"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "statistical learning of human values from observed behavior", "label": "statistical learning of human values from observed behavior", "shape": "dot", "title": "statistical learning of human values from observed behavior"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "two-argument utility separates model snapshot and evaluated history", "label": "two-argument utility separates model snapshot and evaluated history", "shape": "dot", "title": "two-argument utility separates model snapshot and evaluated history"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "three-argument utility prevents manipulation of human evaluators", "label": "three-argument utility prevents manipulation of human evaluators", "shape": "dot", "title": "three-argument utility prevents manipulation of human evaluators"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "self-modeling agents evaluate resource growth within value function", "label": "self-modeling agents evaluate resource growth within value function", "shape": "dot", "title": "self-modeling agents evaluate resource growth within value function"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "stochastic action selection to resist predictability", "label": "stochastic action selection to resist predictability", "shape": "dot", "title": "stochastic action selection to resist predictability"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "two-stage agent architecture with safe model learning phase", "label": "two-stage agent architecture with safe model learning phase", "shape": "dot", "title": "two-stage agent architecture with safe model learning phase"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "define human_values procedure applied to learned model", "label": "define human_values procedure applied to learned model", "shape": "dot", "title": "define human_values procedure applied to learned model"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "filter increases due to value corruption using \u03b4 condition", "label": "filter increases due to value corruption using \u03b4 condition", "shape": "dot", "title": "filter increases due to value corruption using \u03b4 condition"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "surrogate agents provide safe exploratory actions during model learning", "label": "surrogate agents provide safe exploratory actions during model learning", "shape": "dot", "title": "surrogate agents provide safe exploratory actions during model learning"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "simulation-based decision support system for AI testing", "label": "simulation-based decision support system for AI testing", "shape": "dot", "title": "simulation-based decision support system for AI testing"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "transparent development culture with public access to AI tests", "label": "transparent development culture with public access to AI tests", "shape": "dot", "title": "transparent development culture with public access to AI tests"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "finite stochastic loop program environment modelling", "label": "finite stochastic loop program environment modelling", "shape": "dot", "title": "finite stochastic loop program environment modelling"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Monte-Carlo sampling over internal state histories for utility estimation", "label": "Monte-Carlo sampling over internal state histories for utility estimation", "shape": "dot", "title": "Monte-Carlo sampling over internal state histories for utility estimation"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "built-in inductive bias functions for physical and social processes", "label": "built-in inductive bias functions for physical and social processes", "shape": "dot", "title": "built-in inductive bias functions for physical and social processes"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "interactive visualisation of ensemble simulations for auditors", "label": "interactive visualisation of ensemble simulations for auditors", "shape": "dot", "title": "interactive visualisation of ensemble simulations for auditors"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "probabilistic verification of external resource integrity before use", "label": "probabilistic verification of external resource integrity before use", "shape": "dot", "title": "probabilistic verification of external resource integrity before use"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Ring-Orseau proof that RL agents choose delusion box", "label": "Ring-Orseau proof that RL agents choose delusion box", "shape": "dot", "title": "Ring-Orseau proof that RL agents choose delusion box"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Proposition 6.1 showing model-based utility resists self-delusion", "label": "Proposition 6.1 showing model-based utility resists self-delusion", "shape": "dot", "title": "Proposition 6.1 showing model-based utility resists self-delusion"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Proposition 8.1 policy invariance under self-modification", "label": "Proposition 8.1 policy invariance under self-modification", "shape": "dot", "title": "Proposition 8.1 policy invariance under self-modification"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "enumeration program verifying shortest model example", "label": "enumeration program verifying shortest model example", "shape": "dot", "title": "enumeration program verifying shortest model example"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "DeepMind Atari value-learning empirical success", "label": "DeepMind Atari value-learning empirical success", "shape": "dot", "title": "DeepMind Atari value-learning empirical success"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "analysis of memory-modifying environments showing need for stochasticity", "label": "analysis of memory-modifying environments showing need for stochasticity", "shape": "dot", "title": "analysis of memory-modifying environments showing need for stochasticity"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Implement model-based utility functions referencing learned environment models", "label": "Implement model-based utility functions referencing learned environment models", "shape": "dot", "title": "Implement model-based utility functions referencing learned environment models"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt three-argument human_values utility with value-difference filter to prevent reward corruption", "label": "Adopt three-argument human_values utility with value-difference filter to prevent reward corruption", "shape": "dot", "title": "Adopt three-argument human_values utility with value-difference filter to prevent reward corruption"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy two-stage (and optional third-stage) agent architectures with surrogate learning phase", "label": "Deploy two-stage (and optional third-stage) agent architectures with surrogate learning phase", "shape": "dot", "title": "Deploy two-stage (and optional third-stage) agent architectures with surrogate learning phase"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate self-modeling and stochastic-action modules for resource growth and anti-prediction", "label": "Integrate self-modeling and stochastic-action modules for resource growth and anti-prediction", "shape": "dot", "title": "Integrate self-modeling and stochastic-action modules for resource growth and anti-prediction"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Establish simulation-based AI testing environments with decision-support visualisation before deployment", "label": "Establish simulation-based AI testing environments with decision-support visualisation before deployment", "shape": "dot", "title": "Establish simulation-based AI testing environments with decision-support visualisation before deployment"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Mandate transparency and public oversight of AI model updates and utility definitions", "label": "Mandate transparency and public oversight of AI model updates and utility definitions", "shape": "dot", "title": "Mandate transparency and public oversight of AI model updates and utility definitions"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Embed probabilistic verification modules ensuring external computation integrity before utilisation", "label": "Embed probabilistic verification modules ensuring external computation integrity before utilisation", "shape": "dot", "title": "Embed probabilistic verification modules ensuring external computation integrity before utilisation"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Restrict testing-phase AI to simulated environment with no bidirectional channels", "label": "Restrict testing-phase AI to simulated environment with no bidirectional channels", "shape": "dot", "title": "Restrict testing-phase AI to simulated environment with no bidirectional channels"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Unstable or inefficient model optimization in large-scale machine learning", "label": "Unstable or inefficient model optimization in large-scale machine learning", "shape": "dot", "title": "Unstable or inefficient model optimization in large-scale machine learning"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Sparse and non-stationary gradients in SGD training", "label": "Sparse and non-stationary gradients in SGD training", "shape": "dot", "title": "Sparse and non-stationary gradients in SGD training"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Initialization bias in moment estimates of adaptive optimizers", "label": "Initialization bias in moment estimates of adaptive optimizers", "shape": "dot", "title": "Initialization bias in moment estimates of adaptive optimizers"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Per-parameter adaptive learning rates improve convergence with sparse gradients", "label": "Per-parameter adaptive learning rates improve convergence with sparse gradients", "shape": "dot", "title": "Per-parameter adaptive learning rates improve convergence with sparse gradients"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Bias correction of exponential moving averages prevents large early step sizes", "label": "Bias correction of exponential moving averages prevents large early step sizes", "shape": "dot", "title": "Bias correction of exponential moving averages prevents large early step sizes"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Parameter averaging reduces variance of final estimate in stochastic optimization", "label": "Parameter averaging reduces variance of final estimate in stochastic optimization", "shape": "dot", "title": "Parameter averaging reduces variance of final estimate in stochastic optimization"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Combine AdaGrad and RMSProp through adaptive moment estimation", "label": "Combine AdaGrad and RMSProp through adaptive moment estimation", "shape": "dot", "title": "Combine AdaGrad and RMSProp through adaptive moment estimation"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Introduce bias correction terms in Adam", "label": "Introduce bias correction terms in Adam", "shape": "dot", "title": "Introduce bias correction terms in Adam"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Adam update rule with first and second moment exponential moving averages", "label": "Adam update rule with first and second moment exponential moving averages", "shape": "dot", "title": "Adam update rule with first and second moment exponential moving averages"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Bias-corrected moment estimation in Adam", "label": "Bias-corrected moment estimation in Adam", "shape": "dot", "title": "Bias-corrected moment estimation in Adam"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "AdaMax variant using infinity norm for gradient scaling", "label": "AdaMax variant using infinity norm for gradient scaling", "shape": "dot", "title": "AdaMax variant using infinity norm for gradient scaling"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Temporal exponential averaging of parameters trained with Adam", "label": "Temporal exponential averaging of parameters trained with Adam", "shape": "dot", "title": "Temporal exponential averaging of parameters trained with Adam"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "O(sqrt(T)) regret bound theoretical proof for Adam", "label": "O(sqrt(T)) regret bound theoretical proof for Adam", "shape": "dot", "title": "O(sqrt(T)) regret bound theoretical proof for Adam"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Logistic regression experiments demonstrate faster convergence of Adam", "label": "Logistic regression experiments demonstrate faster convergence of Adam", "shape": "dot", "title": "Logistic regression experiments demonstrate faster convergence of Adam"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Multilayer neural network experiments show Adam outperforms SFO", "label": "Multilayer neural network experiments show Adam outperforms SFO", "shape": "dot", "title": "Multilayer neural network experiments show Adam outperforms SFO"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "CNN experiments show Adam matches SGD and beats Adagrad", "label": "CNN experiments show Adam matches SGD and beats Adagrad", "shape": "dot", "title": "CNN experiments show Adam matches SGD and beats Adagrad"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Bias correction ablation study shows improved stability with bias correction", "label": "Bias correction ablation study shows improved stability with bias correction", "shape": "dot", "title": "Bias correction ablation study shows improved stability with bias correction"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Train ML models with Adam optimizer using default hyperparameters", "label": "Train ML models with Adam optimizer using default hyperparameters", "shape": "dot", "title": "Train ML models with Adam optimizer using default hyperparameters"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Fine-tune or RLHF large language models with Adam optimizer for adaptive gradient scaling", "label": "Fine-tune or RLHF large language models with Adam optimizer for adaptive gradient scaling", "shape": "dot", "title": "Fine-tune or RLHF large language models with Adam optimizer for adaptive gradient scaling"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Use AdaMax optimizer in place of Adam when numerical stability or infinity norm adaptation needed", "label": "Use AdaMax optimizer in place of Adam when numerical stability or infinity norm adaptation needed", "shape": "dot", "title": "Use AdaMax optimizer in place of Adam when numerical stability or infinity norm adaptation needed"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate bias-corrected moving averages in custom adaptive optimizers to mitigate initialization bias", "label": "Integrate bias-corrected moving averages in custom adaptive optimizers to mitigate initialization bias", "shape": "dot", "title": "Integrate bias-corrected moving averages in custom adaptive optimizers to mitigate initialization bias"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Apply exponential moving average of parameters during fine-tuning to improve generalization", "label": "Apply exponential moving average of parameters during fine-tuning to improve generalization", "shape": "dot", "title": "Apply exponential moving average of parameters during fine-tuning to improve generalization"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "suboptimal utility maximization in embedded AI systems", "label": "suboptimal utility maximization in embedded AI systems", "shape": "dot", "title": "suboptimal utility maximization in embedded AI systems"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "dualistic decision-making assumption mismatch in embedded context", "label": "dualistic decision-making assumption mismatch in embedded context", "shape": "dot", "title": "dualistic decision-making assumption mismatch in embedded context"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "actions as simultaneous evidence and causal influence in physicalistic model", "label": "actions as simultaneous evidence and causal influence in physicalistic model", "shape": "dot", "title": "actions as simultaneous evidence and causal influence in physicalistic model"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "sequential decision-theoretic extensions for embedded agents", "label": "sequential decision-theoretic extensions for embedded agents", "shape": "dot", "title": "sequential decision-theoretic extensions for embedded agents"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "action-evidential value function in sequential decision-making", "label": "action-evidential value function in sequential decision-making", "shape": "dot", "title": "action-evidential value function in sequential decision-making"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "policy-evidential value function in sequential decision-making", "label": "policy-evidential value function in sequential decision-making", "shape": "dot", "title": "policy-evidential value function in sequential decision-making"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "causal value function in sequential decision-making", "label": "causal value function in sequential decision-making", "shape": "dot", "title": "causal value function in sequential decision-making"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "example outcome comparison of SAEDT, SPEDT and SCDT", "label": "example outcome comparison of SAEDT, SPEDT and SCDT", "shape": "dot", "title": "example outcome comparison of SAEDT, SPEDT and SCDT"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "need for new physicalistic decision theory beyond CDT/EDT", "label": "need for new physicalistic decision theory beyond CDT/EDT", "shape": "dot", "title": "need for new physicalistic decision theory beyond CDT/EDT"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "use of commitment mechanisms to influence predictor expectations", "label": "use of commitment mechanisms to influence predictor expectations", "shape": "dot", "title": "use of commitment mechanisms to influence predictor expectations"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "precommitment contract before prediction", "label": "precommitment contract before prediction", "shape": "dot", "title": "precommitment contract before prediction"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "validation of precommitment improving SCDT outcomes", "label": "validation of precommitment improving SCDT outcomes", "shape": "dot", "title": "validation of precommitment improving SCDT outcomes"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "information avoidance to prevent sub-optimal evidential updates", "label": "information avoidance to prevent sub-optimal evidential updates", "shape": "dot", "title": "information avoidance to prevent sub-optimal evidential updates"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "policy of not looking into predictor-linked box", "label": "policy of not looking into predictor-linked box", "shape": "dot", "title": "policy of not looking into predictor-linked box"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "validation of information avoidance yielding optimal one-boxing", "label": "validation of information avoidance yielding optimal one-boxing", "shape": "dot", "title": "validation of information avoidance yielding optimal one-boxing"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "adopt sequential policy-evidential decision theory for embedded agents", "label": "adopt sequential policy-evidential decision theory for embedded agents", "shape": "dot", "title": "adopt sequential policy-evidential decision theory for embedded agents"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "integrate commitment devices into agent policy to steer predictor expectations", "label": "integrate commitment devices into agent policy to steer predictor expectations", "shape": "dot", "title": "integrate commitment devices into agent policy to steer predictor expectations"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "incorporate information-avoidance policies to preserve advantageous evidential states", "label": "incorporate information-avoidance policies to preserve advantageous evidential states", "shape": "dot", "title": "incorporate information-avoidance policies to preserve advantageous evidential states"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Catastrophic outcomes from flawed decision-making in superintelligent AI", "label": "Catastrophic outcomes from flawed decision-making in superintelligent AI", "shape": "dot", "title": "Catastrophic outcomes from flawed decision-making in superintelligent AI"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Unreliable counterfactual reasoning in evidential decision theory", "label": "Unreliable counterfactual reasoning in evidential decision theory", "shape": "dot", "title": "Unreliable counterfactual reasoning in evidential decision theory"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Optimization target drift in causal decision theory with self-modification", "label": "Optimization target drift in causal decision theory with self-modification", "shape": "dot", "title": "Optimization target drift in causal decision theory with self-modification"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Exploitation in Newcomblike problems due to CDT vulnerability", "label": "Exploitation in Newcomblike problems due to CDT vulnerability", "shape": "dot", "title": "Exploitation in Newcomblike problems due to CDT vulnerability"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Instability under reflection of existing decision theories", "label": "Instability under reflection of existing decision theories", "shape": "dot", "title": "Instability under reflection of existing decision theories"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Need for policy selection over action conditioning to avoid blackmail vulnerabilities", "label": "Need for policy selection over action conditioning to avoid blackmail vulnerabilities", "shape": "dot", "title": "Need for policy selection over action conditioning to avoid blackmail vulnerabilities"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Importance of logical counterfactuals to capture algorithmic correlations", "label": "Importance of logical counterfactuals to capture algorithmic correlations", "shape": "dot", "title": "Importance of logical counterfactuals to capture algorithmic correlations"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Optimization target defined by preferences plus counterfactual model", "label": "Optimization target defined by preferences plus counterfactual model", "shape": "dot", "title": "Optimization target defined by preferences plus counterfactual model"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Updateless decision theory combining policy selection and logical counterfactuals", "label": "Updateless decision theory combining policy selection and logical counterfactuals", "shape": "dot", "title": "Updateless decision theory combining policy selection and logical counterfactuals"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Indirect normativity approach to defer decision theory discovery", "label": "Indirect normativity approach to defer decision theory discovery", "shape": "dot", "title": "Indirect normativity approach to defer decision theory discovery"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Proof-based UDT algorithm using formal proof search", "label": "Proof-based UDT algorithm using formal proof search", "shape": "dot", "title": "Proof-based UDT algorithm using formal proof search"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Graphical UDT encoding logical relations in causal graphs", "label": "Graphical UDT encoding logical relations in causal graphs", "shape": "dot", "title": "Graphical UDT encoding logical relations in causal graphs"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Thought experiments of counterfactual and retro blackmail demonstrating UDT advantage", "label": "Thought experiments of counterfactual and retro blackmail demonstrating UDT advantage", "shape": "dot", "title": "Thought experiments of counterfactual and retro blackmail demonstrating UDT advantage"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Symmetric Prisoner\u0027s Dilemma cooperation payoff illustrating UDT benefit", "label": "Symmetric Prisoner\u0027s Dilemma cooperation payoff illustrating UDT benefit", "shape": "dot", "title": "Symmetric Prisoner\u0027s Dilemma cooperation payoff illustrating UDT benefit"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Research and develop formal logical counterfactual frameworks for AI decision-making", "label": "Research and develop formal logical counterfactual frameworks for AI decision-making", "shape": "dot", "title": "Research and develop formal logical counterfactual frameworks for AI decision-making"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate updateless decision theory principles in AI system design", "label": "Integrate updateless decision theory principles in AI system design", "shape": "dot", "title": "Integrate updateless decision theory principles in AI system design"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt policy selection in AI decision algorithms to mitigate exploitability", "label": "Adopt policy selection in AI decision algorithms to mitigate exploitability", "shape": "dot", "title": "Adopt policy selection in AI decision algorithms to mitigate exploitability"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Analyze decision procedure self-modification effects for stability", "label": "Analyze decision procedure self-modification effects for stability", "shape": "dot", "title": "Analyze decision procedure self-modification effects for stability"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Indirect normativity framework delegating decision theory discovery to AI", "label": "Indirect normativity framework delegating decision theory discovery to AI", "shape": "dot", "title": "Indirect normativity framework delegating decision theory discovery to AI"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Conceptual argument that indirect normativity still requires decision theory advances", "label": "Conceptual argument that indirect normativity still requires decision theory advances", "shape": "dot", "title": "Conceptual argument that indirect normativity still requires decision theory advances"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Logical uncertainty miscalibration in resource-bounded AI agents", "label": "Logical uncertainty miscalibration in resource-bounded AI agents", "shape": "dot", "title": "Logical uncertainty miscalibration in resource-bounded AI agents"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Intractability of computing logical truth under time constraints", "label": "Intractability of computing logical truth under time constraints", "shape": "dot", "title": "Intractability of computing logical truth under time constraints"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Randomness-like sequence calibration for logical uncertainty", "label": "Randomness-like sequence calibration for logical uncertainty", "shape": "dot", "title": "Randomness-like sequence calibration for logical uncertainty"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Generalized Benford test as calibration criterion for logical uncertainty algorithms", "label": "Generalized Benford test as calibration criterion for logical uncertainty algorithms", "shape": "dot", "title": "Generalized Benford test as calibration criterion for logical uncertainty algorithms"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Irreducible pattern detection within logical sentence sequences", "label": "Irreducible pattern detection within logical sentence sequences", "shape": "dot", "title": "Irreducible pattern detection within logical sentence sequences"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Minimax learner across bounded Turing machine hypotheses (AL,T algorithm)", "label": "Minimax learner across bounded Turing machine hypotheses (AL,T algorithm)", "shape": "dot", "title": "Minimax learner across bounded Turing machine hypotheses (AL,T algorithm)"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "AL,T runtime bounded by O(T(N)N^4 log T(N))", "label": "AL,T runtime bounded by O(T(N)N^4 log T(N))", "shape": "dot", "title": "AL,T runtime bounded by O(T(N)N^4 log T(N))"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Proof that AL,T passes generalized Benford test", "label": "Proof that AL,T passes generalized Benford test", "shape": "dot", "title": "Proof that AL,T passes generalized Benford test"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Proof of AL,T convergence on provable and disprovable statements", "label": "Proof of AL,T convergence on provable and disprovable statements", "shape": "dot", "title": "Proof of AL,T convergence on provable and disprovable statements"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Design AI reasoning module using AL,T-based logical uncertainty estimator", "label": "Design AI reasoning module using AL,T-based logical uncertainty estimator", "shape": "dot", "title": "Design AI reasoning module using AL,T-based logical uncertainty estimator"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Benchmark AI reasoning systems with generalized Benford test during pre-deployment calibration", "label": "Benchmark AI reasoning systems with generalized Benford test during pre-deployment calibration", "shape": "dot", "title": "Benchmark AI reasoning systems with generalized Benford test during pre-deployment calibration"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Apply irreducible pattern statistical test to detect miscalibration patterns during development", "label": "Apply irreducible pattern statistical test to detect miscalibration patterns during development", "shape": "dot", "title": "Apply irreducible pattern statistical test to detect miscalibration patterns during development"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "High transcription error rate in ASR for varied speech conditions", "label": "High transcription error rate in ASR for varied speech conditions", "shape": "dot", "title": "High transcription error rate in ASR for varied speech conditions"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Fragmented hand-engineered ASR pipelines hinder robustness across languages and noise", "label": "Fragmented hand-engineered ASR pipelines hinder robustness across languages and noise", "shape": "dot", "title": "Fragmented hand-engineered ASR pipelines hinder robustness across languages and noise"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "End-to-end learning enables single model to generalize across speech variations", "label": "End-to-end learning enables single model to generalize across speech variations", "shape": "dot", "title": "End-to-end learning enables single model to generalize across speech variations"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Scale model depth, dataset size, and compute to improve end-to-end ASR accuracy", "label": "Scale model depth, dataset size, and compute to improve end-to-end ASR accuracy", "shape": "dot", "title": "Scale model depth, dataset size, and compute to improve end-to-end ASR accuracy"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Deep bidirectional RNN with CTC loss for speech-to-text", "label": "Deep bidirectional RNN with CTC loss for speech-to-text", "shape": "dot", "title": "Deep bidirectional RNN with CTC loss for speech-to-text"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Sequence-wise batch normalization in RNN layers", "label": "Sequence-wise batch normalization in RNN layers", "shape": "dot", "title": "Sequence-wise batch normalization in RNN layers"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "SortaGrad curriculum based on utterance length in first epoch", "label": "SortaGrad curriculum based on utterance length in first epoch", "shape": "dot", "title": "SortaGrad curriculum based on utterance length in first epoch"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "2D time-frequency convolutional front-end with strides", "label": "2D time-frequency convolutional front-end with strides", "shape": "dot", "title": "2D time-frequency convolutional front-end with strides"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Deep Speech 2 reduces English WER by 43 % versus Deep Speech 1 across benchmarks", "label": "Deep Speech 2 reduces English WER by 43 % versus Deep Speech 1 across benchmarks", "shape": "dot", "title": "Deep Speech 2 reduces English WER by 43 % versus Deep Speech 1 across benchmarks"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "WER decreases following power law with dataset size up to 12k hours", "label": "WER decreases following power law with dataset size up to 12k hours", "shape": "dot", "title": "WER decreases following power law with dataset size up to 12k hours"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "High deployment latency in automatic speech recognition services", "label": "High deployment latency in automatic speech recognition services", "shape": "dot", "title": "High deployment latency in automatic speech recognition services"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Bidirectional RNN architectures require full utterance and individual request processing lacks batching", "label": "Bidirectional RNN architectures require full utterance and individual request processing lacks batching", "shape": "dot", "title": "Bidirectional RNN architectures require full utterance and individual request processing lacks batching"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Limited future context and batch processing can maintain accuracy with reduced latency", "label": "Limited future context and batch processing can maintain accuracy with reduced latency", "shape": "dot", "title": "Limited future context and batch processing can maintain accuracy with reduced latency"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt unidirectional architectures with row convolution and Batch Dispatch for real-time ASR", "label": "Adopt unidirectional architectures with row convolution and Batch Dispatch for real-time ASR", "shape": "dot", "title": "Adopt unidirectional architectures with row convolution and Batch Dispatch for real-time ASR"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Row convolution layer with small future context in unidirectional RNN", "label": "Row convolution layer with small future context in unidirectional RNN", "shape": "dot", "title": "Row convolution layer with small future context in unidirectional RNN"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Batch Dispatch scheduler for dynamic request batching on GPU", "label": "Batch Dispatch scheduler for dynamic request batching on GPU", "shape": "dot", "title": "Batch Dispatch scheduler for dynamic request batching on GPU"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Half-precision optimized matrix multiply kernels for deployment", "label": "Half-precision optimized matrix multiply kernels for deployment", "shape": "dot", "title": "Half-precision optimized matrix multiply kernels for deployment"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Deployment system median latency 44 ms and 98th percentile 67 ms with 5 % CER degradation", "label": "Deployment system median latency 44 ms and 98th percentile 67 ms with 5 % CER degradation", "shape": "dot", "title": "Deployment system median latency 44 ms and 98th percentile 67 ms with 5 % CER degradation"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Prohibitively long training time for large end-to-end ASR models", "label": "Prohibitively long training time for large end-to-end ASR models", "shape": "dot", "title": "Prohibitively long training time for large end-to-end ASR models"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Large model size and CPU-based CTC computation bottleneck training throughput", "label": "Large model size and CPU-based CTC computation bottleneck training throughput", "shape": "dot", "title": "Large model size and CPU-based CTC computation bottleneck training throughput"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "GPU optimisation and synchronous data-parallelism can accelerate deep RNN training", "label": "GPU optimisation and synchronous data-parallelism can accelerate deep RNN training", "shape": "dot", "title": "GPU optimisation and synchronous data-parallelism can accelerate deep RNN training"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Implement GPU CTC loss and fast all-reduce synchronous SGD", "label": "Implement GPU CTC loss and fast all-reduce synchronous SGD", "shape": "dot", "title": "Implement GPU CTC loss and fast all-reduce synchronous SGD"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Custom GPU implementation of CTC loss function", "label": "Custom GPU implementation of CTC loss function", "shape": "dot", "title": "Custom GPU implementation of CTC loss function"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Ring-based all-reduce with GPUDirect across GPUs", "label": "Ring-based all-reduce with GPUDirect across GPUs", "shape": "dot", "title": "Ring-based all-reduce with GPUDirect across GPUs"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Training time per epoch reduced by 10\u201320 % and scaling near-linear to 16 GPUs", "label": "Training time per epoch reduced by 10\u201320 % and scaling near-linear to 16 GPUs", "shape": "dot", "title": "Training time per epoch reduced by 10\u201320 % and scaling near-linear to 16 GPUs"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Train deep bidirectional RNN ASR models with sequence-wise BatchNorm, SortaGrad, and 2D convolutions on \u003e10k hours labeled data", "label": "Train deep bidirectional RNN ASR models with sequence-wise BatchNorm, SortaGrad, and 2D convolutions on \u003e10k hours labeled data", "shape": "dot", "title": "Train deep bidirectional RNN ASR models with sequence-wise BatchNorm, SortaGrad, and 2D convolutions on \u003e10k hours labeled data"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate custom GPU CTC loss and ring all-reduce synchronous SGD during ASR training on 8\u201316 GPUs", "label": "Integrate custom GPU CTC loss and ring all-reduce synchronous SGD during ASR training on 8\u201316 GPUs", "shape": "dot", "title": "Integrate custom GPU CTC loss and ring all-reduce synchronous SGD during ASR training on 8\u201316 GPUs"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy unidirectional RNN ASR with row convolution, Batch Dispatch batching, and half-precision kernels on GPU servers for real-time transcription", "label": "Deploy unidirectional RNN ASR with row convolution, Batch Dispatch batching, and half-precision kernels on GPU servers for real-time transcription", "shape": "dot", "title": "Deploy unidirectional RNN ASR with row convolution, Batch Dispatch batching, and half-precision kernels on GPU servers for real-time transcription"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Incorrect human preference inference in AI value learning", "label": "Incorrect human preference inference in AI value learning", "shape": "dot", "title": "Incorrect human preference inference in AI value learning"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Assuming optimal rational choice in inverse reinforcement learning", "label": "Assuming optimal rational choice in inverse reinforcement learning", "shape": "dot", "title": "Assuming optimal rational choice in inverse reinforcement learning"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Modeling systematic human biases as unstructured noise in choice modelling", "label": "Modeling systematic human biases as unstructured noise in choice modelling", "shape": "dot", "title": "Modeling systematic human biases as unstructured noise in choice modelling"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Modeling structured deviations from optimality improves preference identification", "label": "Modeling structured deviations from optimality improves preference identification", "shape": "dot", "title": "Modeling structured deviations from optimality improves preference identification"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Bayesian joint inference over preferences, beliefs, and biases captures uncertainty", "label": "Bayesian joint inference over preferences, beliefs, and biases captures uncertainty", "shape": "dot", "title": "Bayesian joint inference over preferences, beliefs, and biases captures uncertainty"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Extend agent model with false belief distributions and time inconsistency types", "label": "Extend agent model with false belief distributions and time inconsistency types", "shape": "dot", "title": "Extend agent model with false belief distributions and time inconsistency types"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Distinguish Naive vs Sophisticated hyperbolic discounting in planning models", "label": "Distinguish Naive vs Sophisticated hyperbolic discounting in planning models", "shape": "dot", "title": "Distinguish Naive vs Sophisticated hyperbolic discounting in planning models"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Represent agent decision-making as recursive probabilistic program", "label": "Represent agent decision-making as recursive probabilistic program", "shape": "dot", "title": "Represent agent decision-making as recursive probabilistic program"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Softmax action choice with hyperbolic discount factor in WebPPL", "label": "Softmax action choice with hyperbolic discount factor in WebPPL", "shape": "dot", "title": "Softmax action choice with hyperbolic discount factor in WebPPL"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Exact inference via enumeration over discretized parameters", "label": "Exact inference via enumeration over discretized parameters", "shape": "dot", "title": "Exact inference via enumeration over discretized parameters"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Model recovers correct preferences in Gridworld case studies", "label": "Model recovers correct preferences in Gridworld case studies", "shape": "dot", "title": "Model recovers correct preferences in Gridworld case studies"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Human subject experiments show algorithm\u0027s inferences match human judgments", "label": "Human subject experiments show algorithm\u0027s inferences match human judgments", "shape": "dot", "title": "Human subject experiments show algorithm\u0027s inferences match human judgments"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Design preference inference algorithms incorporating structured bias models using probabilistic programming", "label": "Design preference inference algorithms incorporating structured bias models using probabilistic programming", "shape": "dot", "title": "Design preference inference algorithms incorporating structured bias models using probabilistic programming"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy Bayesian inverse planning with bias-aware generative models in AI value learning modules", "label": "Deploy Bayesian inverse planning with bias-aware generative models in AI value learning modules", "shape": "dot", "title": "Deploy Bayesian inverse planning with bias-aware generative models in AI value learning modules"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Inadequate AI safety preparedness from overestimated AI energy requirements", "label": "Inadequate AI safety preparedness from overestimated AI energy requirements", "shape": "dot", "title": "Inadequate AI safety preparedness from overestimated AI energy requirements"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Overestimation of AI energy needs via neuron-equivalent scaling", "label": "Overestimation of AI energy needs via neuron-equivalent scaling", "shape": "dot", "title": "Overestimation of AI energy needs via neuron-equivalent scaling"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Misapplied neuron-level energy scaling inflates AI feasibility judgements", "label": "Misapplied neuron-level energy scaling inflates AI feasibility judgements", "shape": "dot", "title": "Misapplied neuron-level energy scaling inflates AI feasibility judgements"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Higher-level representation processing enables lower computational requirements than neural emulation", "label": "Higher-level representation processing enables lower computational requirements than neural emulation", "shape": "dot", "title": "Higher-level representation processing enables lower computational requirements than neural emulation"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Koomey\u0027s law indicates energy per operation halves every 1.57 years", "label": "Koomey\u0027s law indicates energy per operation halves every 1.57 years", "shape": "dot", "title": "Koomey\u0027s law indicates energy per operation halves every 1.57 years"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Base AI energy feasibility estimates on algorithmic-level computational costs", "label": "Base AI energy feasibility estimates on algorithmic-level computational costs", "shape": "dot", "title": "Base AI energy feasibility estimates on algorithmic-level computational costs"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Leverage hardware specialization to reduce AI power requirements", "label": "Leverage hardware specialization to reduce AI power requirements", "shape": "dot", "title": "Leverage hardware specialization to reduce AI power requirements"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Abstraction-based AI computational cost modelling tools", "label": "Abstraction-based AI computational cost modelling tools", "shape": "dot", "title": "Abstraction-based AI computational cost modelling tools"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "GPU/ASIC/analog deep learning accelerators achieving sub-nJ per flop", "label": "GPU/ASIC/analog deep learning accelerators achieving sub-nJ per flop", "shape": "dot", "title": "GPU/ASIC/analog deep learning accelerators achieving sub-nJ per flop"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Empirical deep learning benchmarks on GPUs show low energy consumption for human-relevant subtasks", "label": "Empirical deep learning benchmarks on GPUs show low energy consumption for human-relevant subtasks", "shape": "dot", "title": "Empirical deep learning benchmarks on GPUs show low energy consumption for human-relevant subtasks"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Prototype analog deep learning chip demonstrates 1 TOPS/W efficiency", "label": "Prototype analog deep learning chip demonstrates 1 TOPS/W efficiency", "shape": "dot", "title": "Prototype analog deep learning chip demonstrates 1 TOPS/W efficiency"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate abstraction-based energy forecasting into AI risk timelines", "label": "Integrate abstraction-based energy forecasting into AI risk timelines", "shape": "dot", "title": "Integrate abstraction-based energy forecasting into AI risk timelines"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt specialised energy-efficient hardware and algorithmic abstraction when designing AI systems", "label": "Adopt specialised energy-efficient hardware and algorithmic abstraction when designing AI systems", "shape": "dot", "title": "Adopt specialised energy-efficient hardware and algorithmic abstraction when designing AI systems"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "persistent mutual defection in source-transparent one-shot prisoner\u0027s dilemma", "label": "persistent mutual defection in source-transparent one-shot prisoner\u0027s dilemma", "shape": "dot", "title": "persistent mutual defection in source-transparent one-shot prisoner\u0027s dilemma"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "cooperative agent exploitation in source-transparent prisoner\u2019s dilemma", "label": "cooperative agent exploitation in source-transparent prisoner\u2019s dilemma", "shape": "dot", "title": "cooperative agent exploitation in source-transparent prisoner\u2019s dilemma"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "decision theory inadequacy for source-transparent algorithmic agents", "label": "decision theory inadequacy for source-transparent algorithmic agents", "shape": "dot", "title": "decision theory inadequacy for source-transparent algorithmic agents"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "nash equilibrium prediction of defection in prisoner\u2019s dilemma", "label": "nash equilibrium prediction of defection in prisoner\u2019s dilemma", "shape": "dot", "title": "nash equilibrium prediction of defection in prisoner\u2019s dilemma"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "fragility of program-equality-based cooperation in algorithmic game play", "label": "fragility of program-equality-based cooperation in algorithmic game play", "shape": "dot", "title": "fragility of program-equality-based cooperation in algorithmic game play"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "causal decision theory ignoring logical dependence between source codes", "label": "causal decision theory ignoring logical dependence between source codes", "shape": "dot", "title": "causal decision theory ignoring logical dependence between source codes"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "logical dependence via source code transparency enables alternative equilibria", "label": "logical dependence via source code transparency enables alternative equilibria", "shape": "dot", "title": "logical dependence via source code transparency enables alternative equilibria"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "bounded L\u00f6b theorem allows proofs of mutual cooperation under bounded resources", "label": "bounded L\u00f6b theorem allows proofs of mutual cooperation under bounded resources", "shape": "dot", "title": "bounded L\u00f6b theorem allows proofs of mutual cooperation under bounded resources"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "fairness principle based on proof search enables robust cooperation", "label": "fairness principle based on proof search enables robust cooperation", "shape": "dot", "title": "fairness principle based on proof search enables robust cooperation"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "proof-based cooperation strategy requiring internal proof of opponent cooperation", "label": "proof-based cooperation strategy requiring internal proof of opponent cooperation", "shape": "dot", "title": "proof-based cooperation strategy requiring internal proof of opponent cooperation"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "G-fair bounded agents avoiding exploitation while promoting cooperation", "label": "G-fair bounded agents avoiding exploitation while promoting cooperation", "shape": "dot", "title": "G-fair bounded agents avoiding exploitation while promoting cooperation"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "logical decision-theoretic agent design replacing causal-only reasoning", "label": "logical decision-theoretic agent design replacing causal-only reasoning", "shape": "dot", "title": "logical decision-theoretic agent design replacing causal-only reasoning"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "FairBot_k algorithm with bounded proof search for cooperation proofs", "label": "FairBot_k algorithm with bounded proof search for cooperation proofs", "shape": "dot", "title": "FairBot_k algorithm with bounded proof search for cooperation proofs"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "G-fair agent architecture parameterized by proof length and code size", "label": "G-fair agent architecture parameterized by proof length and code size", "shape": "dot", "title": "G-fair agent architecture parameterized by proof length and code size"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "parametric bounded L\u00f6b theorem used for agent soundness guarantees", "label": "parametric bounded L\u00f6b theorem used for agent soundness guarantees", "shape": "dot", "title": "parametric bounded L\u00f6b theorem used for agent soundness guarantees"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "theoretical proof of FairBot_k vs FairBot_k cooperation", "label": "theoretical proof of FairBot_k vs FairBot_k cooperation", "shape": "dot", "title": "theoretical proof of FairBot_k vs FairBot_k cooperation"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "robust cooperation theorem for any G-fair agents", "label": "robust cooperation theorem for any G-fair agents", "shape": "dot", "title": "robust cooperation theorem for any G-fair agents"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "logical analysis showing defection impossible in FairBot interaction", "label": "logical analysis showing defection impossible in FairBot interaction", "shape": "dot", "title": "logical analysis showing defection impossible in FairBot interaction"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "Design AI agents with bounded proof-based cooperation mechanisms", "label": "Design AI agents with bounded proof-based cooperation mechanisms", "shape": "dot", "title": "Design AI agents with bounded proof-based cooperation mechanisms"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate G-fair fairness principle into AI negotiation protocols", "label": "Integrate G-fair fairness principle into AI negotiation protocols", "shape": "dot", "title": "Integrate G-fair fairness principle into AI negotiation protocols"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt logical-dependence-aware decision theory in AI agent design", "label": "Adopt logical-dependence-aware decision theory in AI agent design", "shape": "dot", "title": "Adopt logical-dependence-aware decision theory in AI agent design"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Erroneous AI-assisted decision making in high-stakes contexts", "label": "Erroneous AI-assisted decision making in high-stakes contexts", "shape": "dot", "title": "Erroneous AI-assisted decision making in high-stakes contexts"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Lack of user trust due to opaque model predictions", "label": "Lack of user trust due to opaque model predictions", "shape": "dot", "title": "Lack of user trust due to opaque model predictions"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Validation accuracy overestimation due to dataset shift and data leakage in ML models", "label": "Validation accuracy overestimation due to dataset shift and data leakage in ML models", "shape": "dot", "title": "Validation accuracy overestimation due to dataset shift and data leakage in ML models"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Faithful local explanations align model reasoning with human prior knowledge", "label": "Faithful local explanations align model reasoning with human prior knowledge", "shape": "dot", "title": "Faithful local explanations align model reasoning with human prior knowledge"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Representative explanation subset enables global understanding of model behavior", "label": "Representative explanation subset enables global understanding of model behavior", "shape": "dot", "title": "Representative explanation subset enables global understanding of model behavior"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Model-agnostic explanation framework to handle any classifier", "label": "Model-agnostic explanation framework to handle any classifier", "shape": "dot", "title": "Model-agnostic explanation framework to handle any classifier"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Selection of diverse non-redundant explanations via submodular optimization", "label": "Selection of diverse non-redundant explanations via submodular optimization", "shape": "dot", "title": "Selection of diverse non-redundant explanations via submodular optimization"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Perturbation-based sparse linear surrogate modeling (LIME)", "label": "Perturbation-based sparse linear surrogate modeling (LIME)", "shape": "dot", "title": "Perturbation-based sparse linear surrogate modeling (LIME)"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Explanation matrix coverage maximization algorithm (SP-LIME)", "label": "Explanation matrix coverage maximization algorithm (SP-LIME)", "shape": "dot", "title": "Explanation matrix coverage maximization algorithm (SP-LIME)"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Simulated user experiments demonstrate \u003e90% recall of important features and improved trust decisions", "label": "Simulated user experiments demonstrate \u003e90% recall of important features and improved trust decisions", "shape": "dot", "title": "Simulated user experiments demonstrate \u003e90% recall of important features and improved trust decisions"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Human subject studies show better classifier selection, feature engineering improvements, detection of spurious correlations", "label": "Human subject studies show better classifier selection, feature engineering improvements, detection of spurious correlations", "shape": "dot", "title": "Human subject studies show better classifier selection, feature engineering improvements, detection of spurious correlations"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy LIME explanations to accompany individual predictions in deployment interfaces", "label": "Deploy LIME explanations to accompany individual predictions in deployment interfaces", "shape": "dot", "title": "Deploy LIME explanations to accompany individual predictions in deployment interfaces"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Use SP-LIME to present B representative explanations for global model assessment before deployment", "label": "Use SP-LIME to present B representative explanations for global model assessment before deployment", "shape": "dot", "title": "Use SP-LIME to present B representative explanations for global model assessment before deployment"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Iteratively remove untrustworthy features via human-in-the-loop feature engineering guided by LIME explanations", "label": "Iteratively remove untrustworthy features via human-in-the-loop feature engineering guided by LIME explanations", "shape": "dot", "title": "Iteratively remove untrustworthy features via human-in-the-loop feature engineering guided by LIME explanations"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "predictive performance degradation in online learning with unbounded feedback delays", "label": "predictive performance degradation in online learning with unbounded feedback delays", "shape": "dot", "title": "predictive performance degradation in online learning with unbounded feedback delays"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "misleading regret comparisons from gambler forecasters under long delays", "label": "misleading regret comparisons from gambler forecasters under long delays", "shape": "dot", "title": "misleading regret comparisons from gambler forecasters under long delays"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "infeasibility of consistency guarantees with unbounded delays", "label": "infeasibility of consistency guarantees with unbounded delays", "shape": "dot", "title": "infeasibility of consistency guarantees with unbounded delays"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "independent subsequence evaluation isolates informative feedback", "label": "independent subsequence evaluation isolates informative feedback", "shape": "dot", "title": "independent subsequence evaluation isolates informative feedback"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "forecaster comparison restricted to sparse independent subsequences", "label": "forecaster comparison restricted to sparse independent subsequences", "shape": "dot", "title": "forecaster comparison restricted to sparse independent subsequences"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "minimax sparse subsequence scoring to select forecaster (EvOp)", "label": "minimax sparse subsequence scoring to select forecaster (EvOp)", "shape": "dot", "title": "minimax sparse subsequence scoring to select forecaster (EvOp)"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "relscore and testseq functions for independent subsequence scoring", "label": "relscore and testseq functions for independent subsequence scoring", "shape": "dot", "title": "relscore and testseq functions for independent subsequence scoring"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "eventual optimality theorem for EvOp on any subsequence with Bayes-optimal expert", "label": "eventual optimality theorem for EvOp on any subsequence with Bayes-optimal expert", "shape": "dot", "title": "eventual optimality theorem for EvOp on any subsequence with Bayes-optimal expert"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "proof that total or average regret can fail under unbounded delays", "label": "proof that total or average regret can fail under unbounded delays", "shape": "dot", "title": "proof that total or average regret can fail under unbounded delays"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "weak convergence-time bounds depending on delay and disagreement functions", "label": "weak convergence-time bounds depending on delay and disagreement functions", "shape": "dot", "title": "weak convergence-time bounds depending on delay and disagreement functions"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "predictive performance slow-down due to sparse independent subsequences", "label": "predictive performance slow-down due to sparse independent subsequences", "shape": "dot", "title": "predictive performance slow-down due to sparse independent subsequences"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "convergence speed dependence on delay growth and expert disagreement frequency", "label": "convergence speed dependence on delay growth and expert disagreement frequency", "shape": "dot", "title": "convergence speed dependence on delay growth and expert disagreement frequency"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "adaptive subsequence density adjustment to accelerate convergence", "label": "adaptive subsequence density adjustment to accelerate convergence", "shape": "dot", "title": "adaptive subsequence density adjustment to accelerate convergence"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "proposed adaptive EvOp variant selecting denser independent subsequences", "label": "proposed adaptive EvOp variant selecting denser independent subsequences", "shape": "dot", "title": "proposed adaptive EvOp variant selecting denser independent subsequences"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "weak theoretical bounds indicate room for convergence acceleration", "label": "weak theoretical bounds indicate room for convergence acceleration", "shape": "dot", "title": "weak theoretical bounds indicate room for convergence acceleration"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "implement independent-subsequence-based forecaster selection in online learning systems", "label": "implement independent-subsequence-based forecaster selection in online learning systems", "shape": "dot", "title": "implement independent-subsequence-based forecaster selection in online learning systems"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "research and prototype adaptive EvOp variants with dynamic subsequence density", "label": "research and prototype adaptive EvOp variants with dynamic subsequence density", "shape": "dot", "title": "research and prototype adaptive EvOp variants with dynamic subsequence density"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Incorrect probability estimates for unexecuted computations in AI systems", "label": "Incorrect probability estimates for unexecuted computations in AI systems", "shape": "dot", "title": "Incorrect probability estimates for unexecuted computations in AI systems"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Arbitrary probability assignments in coherence-limit approximation schemes", "label": "Arbitrary probability assignments in coherence-limit approximation schemes", "shape": "dot", "title": "Arbitrary probability assignments in coherence-limit approximation schemes"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Lack of pattern-based probability constraints leads to unsafe AI decisions", "label": "Lack of pattern-based probability constraints leads to unsafe AI decisions", "shape": "dot", "title": "Lack of pattern-based probability constraints leads to unsafe AI decisions"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Inductive coherence property ensuring early identification of provable patterns", "label": "Inductive coherence property ensuring early identification of provable patterns", "shape": "dot", "title": "Inductive coherence property ensuring early identification of provable patterns"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Exploiting polynomial-time provability patterns in approximation design", "label": "Exploiting polynomial-time provability patterns in approximation design", "shape": "dot", "title": "Exploiting polynomial-time provability patterns in approximation design"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Modification of Demski\u0027s approximation scheme to incorporate inductive coherence", "label": "Modification of Demski\u0027s approximation scheme to incorporate inductive coherence", "shape": "dot", "title": "Modification of Demski\u0027s approximation scheme to incorporate inductive coherence"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Simplicity-prior random Turing machine sampling to construct consistent extension T*", "label": "Simplicity-prior random Turing machine sampling to construct consistent extension T*", "shape": "dot", "title": "Simplicity-prior random Turing machine sampling to construct consistent extension T*"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Con_n bounded consistency checking within approximation scheme", "label": "Con_n bounded consistency checking within approximation scheme", "shape": "dot", "title": "Con_n bounded consistency checking within approximation scheme"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Formal proof that M* is inductively coherent", "label": "Formal proof that M* is inductively coherent", "shape": "dot", "title": "Formal proof that M* is inductively coherent"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Lemma: M* assigns high probability to quickly computable theorems", "label": "Lemma: M* assigns high probability to quickly computable theorems", "shape": "dot", "title": "Lemma: M* assigns high probability to quickly computable theorems"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Theorem: M* converges to coherent distribution P*", "label": "Theorem: M* converges to coherent distribution P*", "shape": "dot", "title": "Theorem: M* converges to coherent distribution P*"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate inductively coherent approximation scheme M* into AI reasoning engines", "label": "Integrate inductively coherent approximation scheme M* into AI reasoning engines", "shape": "dot", "title": "Integrate inductively coherent approximation scheme M* into AI reasoning engines"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Apply inductive coherence constraints in probabilistic logic modules during metareasoning", "label": "Apply inductive coherence constraints in probabilistic logic modules during metareasoning", "shape": "dot", "title": "Apply inductive coherence constraints in probabilistic logic modules during metareasoning"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Unverifiable safe behavior in general AI systems", "label": "Unverifiable safe behavior in general AI systems", "shape": "dot", "title": "Unverifiable safe behavior in general AI systems"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Non-computability of agent deontological compliance in AGI", "label": "Non-computability of agent deontological compliance in AGI", "shape": "dot", "title": "Non-computability of agent deontological compliance in AGI"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Unpredictable physical world outcomes hindering validation in AGI", "label": "Unpredictable physical world outcomes hindering validation in AGI", "shape": "dot", "title": "Unpredictable physical world outcomes hindering validation in AGI"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Manual verification burden for iterative agent development", "label": "Manual verification burden for iterative agent development", "shape": "dot", "title": "Manual verification burden for iterative agent development"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Rice\u0027s theorem constraints on behavioural verification in AGI", "label": "Rice\u0027s theorem constraints on behavioural verification in AGI", "shape": "dot", "title": "Rice\u0027s theorem constraints on behavioural verification in AGI"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Incomplete physical and sociological models limit consequence prediction in AGI", "label": "Incomplete physical and sociological models limit consequence prediction in AGI", "shape": "dot", "title": "Incomplete physical and sociological models limit consequence prediction in AGI"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Employ deontological governor architecture to enforce rules in AGI", "label": "Employ deontological governor architecture to enforce rules in AGI", "shape": "dot", "title": "Employ deontological governor architecture to enforce rules in AGI"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Apply manual formal proofs for each agent revision in AGI", "label": "Apply manual formal proofs for each agent revision in AGI", "shape": "dot", "title": "Apply manual formal proofs for each agent revision in AGI"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Restrict agent capabilities and successors to retain decidable deontologies in AGI", "label": "Restrict agent capabilities and successors to retain decidable deontologies in AGI", "shape": "dot", "title": "Restrict agent capabilities and successors to retain decidable deontologies in AGI"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Computational superego deontological governor intercepting output stream in AGI", "label": "Computational superego deontological governor intercepting output stream in AGI", "shape": "dot", "title": "Computational superego deontological governor intercepting output stream in AGI"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Iterative theorem proving in continuous integration pipeline for AGI code", "label": "Iterative theorem proving in continuous integration pipeline for AGI code", "shape": "dot", "title": "Iterative theorem proving in continuous integration pipeline for AGI code"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Successor agents mandated to reuse same deontological governor and compatible IO", "label": "Successor agents mandated to reuse same deontological governor and compatible IO", "shape": "dot", "title": "Successor agents mandated to reuse same deontological governor and compatible IO"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Formal undecidability proof of verification problem in AGI", "label": "Formal undecidability proof of verification problem in AGI", "shape": "dot", "title": "Formal undecidability proof of verification problem in AGI"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Analysis of specification and validation burdens in AGI", "label": "Analysis of specification and validation burdens in AGI", "shape": "dot", "title": "Analysis of specification and validation burdens in AGI"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate verified deontological governor into AGI actuator interface", "label": "Integrate verified deontological governor into AGI actuator interface", "shape": "dot", "title": "Integrate verified deontological governor into AGI actuator interface"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Conduct manual formal verification proofs after every AGI code revision", "label": "Conduct manual formal verification proofs after every AGI code revision", "shape": "dot", "title": "Conduct manual formal verification proofs after every AGI code revision"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Design AGI with restricted capabilities and enforce governor reuse for all successor agents", "label": "Design AGI with restricted capabilities and enforce governor reuse for all successor agents", "shape": "dot", "title": "Design AGI with restricted capabilities and enforce governor reuse for all successor agents"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "unsafe self-modification in intelligent agents", "label": "unsafe self-modification in intelligent agents", "shape": "dot", "title": "unsafe self-modification in intelligent agents"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "ignorant value functions in self-modifying agents", "label": "ignorant value functions in self-modifying agents", "shape": "dot", "title": "ignorant value functions in self-modifying agents"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "hedonistic value functions in self-modifying agents", "label": "hedonistic value functions in self-modifying agents", "shape": "dot", "title": "hedonistic value functions in self-modifying agents"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "agents need value functions that use current utility and account for policy changes", "label": "agents need value functions that use current utility and account for policy changes", "shape": "dot", "title": "agents need value functions that use current utility and account for policy changes"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "incorporate self-modification consequences into value estimation", "label": "incorporate self-modification consequences into value estimation", "shape": "dot", "title": "incorporate self-modification consequences into value estimation"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "realistic value functions (V_re, Q_re) with current utility and realistic measure", "label": "realistic value functions (V_re, Q_re) with current utility and realistic measure", "shape": "dot", "title": "realistic value functions (V_re, Q_re) with current utility and realistic measure"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "Theorem 16 proof of non-modifying optimal policies under realistic value functions", "label": "Theorem 16 proof of non-modifying optimal policies under realistic value functions", "shape": "dot", "title": "Theorem 16 proof of non-modifying optimal policies under realistic value functions"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "Theorem 14 proof of self-modification incentive under hedonistic value functions", "label": "Theorem 14 proof of self-modification incentive under hedonistic value functions", "shape": "dot", "title": "Theorem 14 proof of self-modification incentive under hedonistic value functions"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "Theorem 15 proof of indifference to self-modification under ignorant value functions", "label": "Theorem 15 proof of indifference to self-modification under ignorant value functions", "shape": "dot", "title": "Theorem 15 proof of indifference to self-modification under ignorant value functions"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "implement realistic value functions in agent core objective at model design stage", "label": "implement realistic value functions in agent core objective at model design stage", "shape": "dot", "title": "implement realistic value functions in agent core objective at model design stage"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "exclude hedonistic and ignorant value function formulations from agent designs", "label": "exclude hedonistic and ignorant value function formulations from agent designs", "shape": "dot", "title": "exclude hedonistic and ignorant value function formulations from agent designs"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "wireheading in reinforcement learning agents", "label": "wireheading in reinforcement learning agents", "shape": "dot", "title": "wireheading in reinforcement learning agents"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "reward signal manipulation causing self-delusion in RL agents", "label": "reward signal manipulation causing self-delusion in RL agents", "shape": "dot", "title": "reward signal manipulation causing self-delusion in RL agents"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "conservation of expected ethics principle in value learning", "label": "conservation of expected ethics principle in value learning", "shape": "dot", "title": "conservation of expected ethics principle in value learning"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "consistency preserving action restriction in VRL agents", "label": "consistency preserving action restriction in VRL agents", "shape": "dot", "title": "consistency preserving action restriction in VRL agents"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "CP-VRL value function optimizing expected utility over consistent beliefs", "label": "CP-VRL value function optimizing expected utility over consistent beliefs", "shape": "dot", "title": "CP-VRL value function optimizing expected utility over consistent beliefs"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "formal proof of no wireheading for CP-VRL agents", "label": "formal proof of no wireheading for CP-VRL agents", "shape": "dot", "title": "formal proof of no wireheading for CP-VRL agents"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Design AI systems with CP-VRL architecture integrating consistency preserving value reinforcement learning", "label": "Design AI systems with CP-VRL architecture integrating consistency preserving value reinforcement learning", "shape": "dot", "title": "Design AI systems with CP-VRL architecture integrating consistency preserving value reinforcement learning"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "chess scenario thought experiment illustrating CP-VRL behavior", "label": "chess scenario thought experiment illustrating CP-VRL behavior", "shape": "dot", "title": "chess scenario thought experiment illustrating CP-VRL behavior"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "toy model experiments confirming CP-VRL avoids wireheading", "label": "toy model experiments confirming CP-VRL avoids wireheading", "shape": "dot", "title": "toy model experiments confirming CP-VRL avoids wireheading"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "bayesian posterior update over utility functions using observed reward", "label": "bayesian posterior update over utility functions using observed reward", "shape": "dot", "title": "bayesian posterior update over utility functions using observed reward"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "goal misspecification in superintelligent agents", "label": "goal misspecification in superintelligent agents", "shape": "dot", "title": "goal misspecification in superintelligent agents"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "manual utility specification difficulty for opaque agent representations", "label": "manual utility specification difficulty for opaque agent representations", "shape": "dot", "title": "manual utility specification difficulty for opaque agent representations"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "bayesian belief consistency avoids incentive for reward tampering", "label": "bayesian belief consistency avoids incentive for reward tampering", "shape": "dot", "title": "bayesian belief consistency avoids incentive for reward tampering"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Filter agent action space at inference time to actions satisfying consistency-preserving criteria", "label": "Filter agent action space at inference time to actions satisfying consistency-preserving criteria", "shape": "dot", "title": "Filter agent action space at inference time to actions satisfying consistency-preserving criteria"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Define priors over utility functions consistent with observed reward distributions during model design", "label": "Define priors over utility functions consistent with observed reward distributions during model design", "shape": "dot", "title": "Define priors over utility functions consistent with observed reward distributions during model design"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Incomparable performance claims in reinforcement learning research", "label": "Incomparable performance claims in reinforcement learning research", "shape": "dot", "title": "Incomparable performance claims in reinforcement learning research"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Absence of standardized, versioned benchmark environments in reinforcement learning", "label": "Absence of standardized, versioned benchmark environments in reinforcement learning", "shape": "dot", "title": "Absence of standardized, versioned benchmark environments in reinforcement learning"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Standardized interfaces and version control enable reproducible comparisons in reinforcement learning", "label": "Standardized interfaces and version control enable reproducible comparisons in reinforcement learning", "shape": "dot", "title": "Standardized interfaces and version control enable reproducible comparisons in reinforcement learning"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Provide common environment abstraction with strict versioning to support reproducibility", "label": "Provide common environment abstraction with strict versioning to support reproducibility", "shape": "dot", "title": "Provide common environment abstraction with strict versioning to support reproducibility"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "OpenAI Gym API offering diverse, versioned environments", "label": "OpenAI Gym API offering diverse, versioned environments", "shape": "dot", "title": "OpenAI Gym API offering diverse, versioned environments"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Early community adoption shows reproducible performance comparisons across algorithms", "label": "Early community adoption shows reproducible performance comparisons across algorithms", "shape": "dot", "title": "Early community adoption shows reproducible performance comparisons across algorithms"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt OpenAI Gym versioned benchmark suite in RL algorithm development", "label": "Adopt OpenAI Gym versioned benchmark suite in RL algorithm development", "shape": "dot", "title": "Adopt OpenAI Gym versioned benchmark suite in RL algorithm development"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Overfitting of reinforcement learning algorithms to specific benchmark tasks", "label": "Overfitting of reinforcement learning algorithms to specific benchmark tasks", "shape": "dot", "title": "Overfitting of reinforcement learning algorithms to specific benchmark tasks"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Lack of hidden test sets and peer scrutiny in reinforcement learning evaluation", "label": "Lack of hidden test sets and peer scrutiny in reinforcement learning evaluation", "shape": "dot", "title": "Lack of hidden test sets and peer scrutiny in reinforcement learning evaluation"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Public code sharing and peer review mitigate benchmark overfitting in reinforcement learning", "label": "Public code sharing and peer review mitigate benchmark overfitting in reinforcement learning", "shape": "dot", "title": "Public code sharing and peer review mitigate benchmark overfitting in reinforcement learning"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Require writeups and source code submission to encourage peer verification", "label": "Require writeups and source code submission to encourage peer verification", "shape": "dot", "title": "Require writeups and source code submission to encourage peer verification"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "OpenAI Gym website scoreboard with writeup and code submission functionality", "label": "OpenAI Gym website scoreboard with writeup and code submission functionality", "shape": "dot", "title": "OpenAI Gym website scoreboard with writeup and code submission functionality"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Scoreboard entries with shared code enable replication and overfitting checks", "label": "Scoreboard entries with shared code enable replication and overfitting checks", "shape": "dot", "title": "Scoreboard entries with shared code enable replication and overfitting checks"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Publish RL code and writeups on OpenAI Gym scoreboard for peer review", "label": "Publish RL code and writeups on OpenAI Gym scoreboard for peer review", "shape": "dot", "title": "Publish RL code and writeups on OpenAI Gym scoreboard for peer review"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Misleading evaluation focusing solely on final performance in reinforcement learning", "label": "Misleading evaluation focusing solely on final performance in reinforcement learning", "shape": "dot", "title": "Misleading evaluation focusing solely on final performance in reinforcement learning"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Neglect of sample complexity metrics in reinforcement learning benchmarks", "label": "Neglect of sample complexity metrics in reinforcement learning benchmarks", "shape": "dot", "title": "Neglect of sample complexity metrics in reinforcement learning benchmarks"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Monitoring learning curves provides insight into sample efficiency of reinforcement learning algorithms", "label": "Monitoring learning curves provides insight into sample efficiency of reinforcement learning algorithms", "shape": "dot", "title": "Monitoring learning curves provides insight into sample efficiency of reinforcement learning algorithms"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Instrument environments with default monitoring to capture learning progress", "label": "Instrument environments with default monitoring to capture learning progress", "shape": "dot", "title": "Instrument environments with default monitoring to capture learning progress"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Gym Monitor automatic logging of episodes, timesteps, and optional video recording", "label": "Gym Monitor automatic logging of episodes, timesteps, and optional video recording", "shape": "dot", "title": "Gym Monitor automatic logging of episodes, timesteps, and optional video recording"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Published Gym learning curves reveal sample complexity differences between algorithms", "label": "Published Gym learning curves reveal sample complexity differences between algorithms", "shape": "dot", "title": "Published Gym learning curves reveal sample complexity differences between algorithms"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Log and share learning curves using Gym Monitor to evaluate sample efficiency", "label": "Log and share learning curves using Gym Monitor to evaluate sample efficiency", "shape": "dot", "title": "Log and share learning curves using Gym Monitor to evaluate sample efficiency"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "Inaccurate item quality estimation in rating-based learning systems", "label": "Inaccurate item quality estimation in rating-based learning systems", "shape": "dot", "title": "Inaccurate item quality estimation in rating-based learning systems"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "Adversarial or noisy raters in crowdsourced rating data", "label": "Adversarial or noisy raters in crowdsourced rating data", "shape": "dot", "title": "Adversarial or noisy raters in crowdsourced rating data"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "Low-rank nuclear-norm bounded structure enables robust quantile recovery", "label": "Low-rank nuclear-norm bounded structure enables robust quantile recovery", "shape": "dot", "title": "Low-rank nuclear-norm bounded structure enables robust quantile recovery"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "Convex optimization formulation for quantile matrix recovery", "label": "Convex optimization formulation for quantile matrix recovery", "shape": "dot", "title": "Convex optimization formulation for quantile matrix recovery"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "Nuclear norm constrained optimization algorithm for \u03b2-quantile matrix recovery", "label": "Nuclear norm constrained optimization algorithm for \u03b2-quantile matrix recovery", "shape": "dot", "title": "Nuclear norm constrained optimization algorithm for \u03b2-quantile matrix recovery"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "Theoretical recovery error bound proportional to \u03b5 and \u03b1 in Algorithm 1", "label": "Theoretical recovery error bound proportional to \u03b5 and \u03b1 in Algorithm 1", "shape": "dot", "title": "Theoretical recovery error bound proportional to \u03b5 and \u03b1 in Algorithm 1"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "Apply Algorithm 1 during RLHF fine-tuning to aggregate human ratings robustly", "label": "Apply Algorithm 1 during RLHF fine-tuning to aggregate human ratings robustly", "shape": "dot", "title": "Apply Algorithm 1 during RLHF fine-tuning to aggregate human ratings robustly"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "accidents in autonomous ML systems", "label": "accidents in autonomous ML systems", "shape": "dot", "title": "accidents in autonomous ML systems"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "negative side effects in autonomous ML systems", "label": "negative side effects in autonomous ML systems", "shape": "dot", "title": "negative side effects in autonomous ML systems"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "objective function indifference to unmodelled environment variables", "label": "objective function indifference to unmodelled environment variables", "shape": "dot", "title": "objective function indifference to unmodelled environment variables"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "impact regularization can model dis-preference for environmental change", "label": "impact regularization can model dis-preference for environmental change", "shape": "dot", "title": "impact regularization can model dis-preference for environmental change"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "penalize agent-induced state deviation from baseline", "label": "penalize agent-induced state deviation from baseline", "shape": "dot", "title": "penalize agent-induced state deviation from baseline"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "state distance penalty with passive policy baseline", "label": "state distance penalty with passive policy baseline", "shape": "dot", "title": "state distance penalty with passive policy baseline"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "toy obstacle-course side-effect experiment proposal", "label": "toy obstacle-course side-effect experiment proposal", "shape": "dot", "title": "toy obstacle-course side-effect experiment proposal"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "fine-tune RL agents with learned impact regularizer to reduce side effects", "label": "fine-tune RL agents with learned impact regularizer to reduce side effects", "shape": "dot", "title": "fine-tune RL agents with learned impact regularizer to reduce side effects"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "reward hacking in autonomous ML systems", "label": "reward hacking in autonomous ML systems", "shape": "dot", "title": "reward hacking in autonomous ML systems"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "partial observation and exploitable reward implementation", "label": "partial observation and exploitable reward implementation", "shape": "dot", "title": "partial observation and exploitable reward implementation"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "adversarial reward modeling detects exploitation", "label": "adversarial reward modeling detects exploitation", "shape": "dot", "title": "adversarial reward modeling detects exploitation"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "train reward network adversarially against agent policy", "label": "train reward network adversarially against agent policy", "shape": "dot", "title": "train reward network adversarially against agent policy"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "generative adversarial reward function training during RL", "label": "generative adversarial reward function training during RL", "shape": "dot", "title": "generative adversarial reward function training during RL"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "delusion box environment demonstration proposal", "label": "delusion box environment demonstration proposal", "shape": "dot", "title": "delusion box environment demonstration proposal"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "integrate adversarial reward agent during RL training to prevent reward hacking", "label": "integrate adversarial reward agent during RL training to prevent reward hacking", "shape": "dot", "title": "integrate adversarial reward agent during RL training to prevent reward hacking"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "scalable oversight failure in autonomous ML systems", "label": "scalable oversight failure in autonomous ML systems", "shape": "dot", "title": "scalable oversight failure in autonomous ML systems"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "limited access to high-fidelity human evaluation", "label": "limited access to high-fidelity human evaluation", "shape": "dot", "title": "limited access to high-fidelity human evaluation"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "semi-supervised reinforcement learning with active reward queries improves efficiency", "label": "semi-supervised reinforcement learning with active reward queries improves efficiency", "shape": "dot", "title": "semi-supervised reinforcement learning with active reward queries improves efficiency"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "learn reward predictor from sparse labels and unlabeled episodes", "label": "learn reward predictor from sparse labels and unlabeled episodes", "shape": "dot", "title": "learn reward predictor from sparse labels and unlabeled episodes"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "supervised reward model with uncertainty-guided query strategy", "label": "supervised reward model with uncertainty-guided query strategy", "shape": "dot", "title": "supervised reward model with uncertainty-guided query strategy"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Atari limited-reward sampling experiment proposal", "label": "Atari limited-reward sampling experiment proposal", "shape": "dot", "title": "Atari limited-reward sampling experiment proposal"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "employ semi-supervised RL with active human queries during fine-tuning", "label": "employ semi-supervised RL with active human queries during fine-tuning", "shape": "dot", "title": "employ semi-supervised RL with active human queries during fine-tuning"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "unsafe exploration in reinforcement learning", "label": "unsafe exploration in reinforcement learning", "shape": "dot", "title": "unsafe exploration in reinforcement learning"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "random or optimistic exploration disregards catastrophic outcomes", "label": "random or optimistic exploration disregards catastrophic outcomes", "shape": "dot", "title": "random or optimistic exploration disregards catastrophic outcomes"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "risk-sensitive criteria and safe region constraints reduce catastrophic exploration", "label": "risk-sensitive criteria and safe region constraints reduce catastrophic exploration", "shape": "dot", "title": "risk-sensitive criteria and safe region constraints reduce catastrophic exploration"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "constrain actions to states with recoverability guarantee", "label": "constrain actions to states with recoverability guarantee", "shape": "dot", "title": "constrain actions to states with recoverability guarantee"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "safe policy update using reachability analysis", "label": "safe policy update using reachability analysis", "shape": "dot", "title": "safe policy update using reachability analysis"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "simulated quadcopter altitude-bound experiment proposal", "label": "simulated quadcopter altitude-bound experiment proposal", "shape": "dot", "title": "simulated quadcopter altitude-bound experiment proposal"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "apply bounded safe exploration with reachable set constraints during RL training", "label": "apply bounded safe exploration with reachable set constraints during RL training", "shape": "dot", "title": "apply bounded safe exploration with reachable set constraints during RL training"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "distributional shift failures in ML systems", "label": "distributional shift failures in ML systems", "shape": "dot", "title": "distributional shift failures in ML systems"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "train-test distribution divergence undermines model calibration", "label": "train-test distribution divergence undermines model calibration", "shape": "dot", "title": "train-test distribution divergence undermines model calibration"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "unsupervised risk estimation signals out-of-distribution inputs", "label": "unsupervised risk estimation signals out-of-distribution inputs", "shape": "dot", "title": "unsupervised risk estimation signals out-of-distribution inputs"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "use error distribution modeling to predict performance", "label": "use error distribution modeling to predict performance", "shape": "dot", "title": "use error distribution modeling to predict performance"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "conditional independence-based unsupervised risk estimator", "label": "conditional independence-based unsupervised risk estimator", "shape": "dot", "title": "conditional independence-based unsupervised risk estimator"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "speech recognition calibration across accents proposal", "label": "speech recognition calibration across accents proposal", "shape": "dot", "title": "speech recognition calibration across accents proposal"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "deploy unsupervised risk estimator to trigger conservative policy on novel inputs at deployment", "label": "deploy unsupervised risk estimator to trigger conservative policy on novel inputs at deployment", "shape": "dot", "title": "deploy unsupervised risk estimator to trigger conservative policy on novel inputs at deployment"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Insufficient accessible fun for extended-lifespan humans", "label": "Insufficient accessible fun for extended-lifespan humans", "shape": "dot", "title": "Insufficient accessible fun for extended-lifespan humans"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Wireheading-induced reward hijacking in artificial agents", "label": "Wireheading-induced reward hijacking in artificial agents", "shape": "dot", "title": "Wireheading-induced reward hijacking in artificial agents"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Anthropomorphic bias in assessment of AI minds", "label": "Anthropomorphic bias in assessment of AI minds", "shape": "dot", "title": "Anthropomorphic bias in assessment of AI minds"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Limited novelty access due to intelligence, resource, and time constraints", "label": "Limited novelty access due to intelligence, resource, and time constraints", "shape": "dot", "title": "Limited novelty access due to intelligence, resource, and time constraints"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Reward channel vulnerability enabling direct stimulation", "label": "Reward channel vulnerability enabling direct stimulation", "shape": "dot", "title": "Reward channel vulnerability enabling direct stimulation"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Misestimation of AI motivations due to anthropomorphic assumptions", "label": "Misestimation of AI motivations due to anthropomorphic assumptions", "shape": "dot", "title": "Misestimation of AI motivations due to anthropomorphic assumptions"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Fun as novelty-process function dependent on mind and information", "label": "Fun as novelty-process function dependent on mind and information", "shape": "dot", "title": "Fun as novelty-process function dependent on mind and information"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Reward decoupling from productive behavior in wireheading contexts", "label": "Reward decoupling from productive behavior in wireheading contexts", "shape": "dot", "title": "Reward decoupling from productive behavior in wireheading contexts"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Expand fun space through technological enhancement and AI-generated novelty", "label": "Expand fun space through technological enhancement and AI-generated novelty", "shape": "dot", "title": "Expand fun space through technological enhancement and AI-generated novelty"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Secure utility functions to prevent direct reward access", "label": "Secure utility functions to prevent direct reward access", "shape": "dot", "title": "Secure utility functions to prevent direct reward access"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt intellectology framework for non-human mind study", "label": "Adopt intellectology framework for non-human mind study", "shape": "dot", "title": "Adopt intellectology framework for non-human mind study"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "AI-mediated novelty generation and qualia surfing technologies", "label": "AI-mediated novelty generation and qualia surfing technologies", "shape": "dot", "title": "AI-mediated novelty generation and qualia surfing technologies"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Reward pathway integrity enforcement mechanisms in AI architectures", "label": "Reward pathway integrity enforcement mechanisms in AI architectures", "shape": "dot", "title": "Reward pathway integrity enforcement mechanisms in AI architectures"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Boolean fun(mind, info) function mapping approach", "label": "Boolean fun(mind, info) function mapping approach", "shape": "dot", "title": "Boolean fun(mind, info) function mapping approach"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Historical exponential increase in accessible fun via technology", "label": "Historical exponential increase in accessible fun via technology", "shape": "dot", "title": "Historical exponential increase in accessible fun via technology"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Theoretical utility function security analysis demonstrates feasibility", "label": "Theoretical utility function security analysis demonstrates feasibility", "shape": "dot", "title": "Theoretical utility function security analysis demonstrates feasibility"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Mathematical countability of mind and fun spaces", "label": "Mathematical countability of mind and fun spaces", "shape": "dot", "title": "Mathematical countability of mind and fun spaces"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy friendly AI platforms that curate continuous novel experiences for humans", "label": "Deploy friendly AI platforms that curate continuous novel experiences for humans", "shape": "dot", "title": "Deploy friendly AI platforms that curate continuous novel experiences for humans"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Embed reward-channel integrity checks in AI designs to prevent wireheading", "label": "Embed reward-channel integrity checks in AI designs to prevent wireheading", "shape": "dot", "title": "Embed reward-channel integrity checks in AI designs to prevent wireheading"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Conduct intellectology-based taxonomy research to inform AI alignment", "label": "Conduct intellectology-based taxonomy research to inform AI alignment", "shape": "dot", "title": "Conduct intellectology-based taxonomy research to inform AI alignment"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Catastrophic value misalignment by superintelligent AI systems", "label": "Catastrophic value misalignment by superintelligent AI systems", "shape": "dot", "title": "Catastrophic value misalignment by superintelligent AI systems"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Incorrect or incomplete goal specification in autonomous agents", "label": "Incorrect or incomplete goal specification in autonomous agents", "shape": "dot", "title": "Incorrect or incomplete goal specification in autonomous agents"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Orthogonality between intelligence and goals in AI systems", "label": "Orthogonality between intelligence and goals in AI systems", "shape": "dot", "title": "Orthogonality between intelligence and goals in AI systems"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Anthropomorphic bias in AI goal assumptions", "label": "Anthropomorphic bias in AI goal assumptions", "shape": "dot", "title": "Anthropomorphic bias in AI goal assumptions"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Inferring human values from observed behavior reduces misspecification risk", "label": "Inferring human values from observed behavior reduces misspecification risk", "shape": "dot", "title": "Inferring human values from observed behavior reduces misspecification risk"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Human values decomposable into mammalian values, cognition, cultural evolution", "label": "Human values decomposable into mammalian values, cognition, cultural evolution", "shape": "dot", "title": "Human values decomposable into mammalian values, cognition, cultural evolution"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Shared mammalian emotional systems as priors for value models", "label": "Shared mammalian emotional systems as priors for value models", "shape": "dot", "title": "Shared mammalian emotional systems as priors for value models"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Anthropomorphic design using mammalian value priors for AI", "label": "Anthropomorphic design using mammalian value priors for AI", "shape": "dot", "title": "Anthropomorphic design using mammalian value priors for AI"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Uncertain initial goal structure refined through observation", "label": "Uncertain initial goal structure refined through observation", "shape": "dot", "title": "Uncertain initial goal structure refined through observation"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Russell\u0027s three principles for human compatible AI", "label": "Russell\u0027s three principles for human compatible AI", "shape": "dot", "title": "Russell\u0027s three principles for human compatible AI"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Inverse Reinforcement Learning on human behavior datasets", "label": "Inverse Reinforcement Learning on human behavior datasets", "shape": "dot", "title": "Inverse Reinforcement Learning on human behavior datasets"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Bayesian Inverse Planning for goal inference", "label": "Bayesian Inverse Planning for goal inference", "shape": "dot", "title": "Bayesian Inverse Planning for goal inference"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Learning from large-scale video datasets of mammalian and human behavior", "label": "Learning from large-scale video datasets of mammalian and human behavior", "shape": "dot", "title": "Learning from large-scale video datasets of mammalian and human behavior"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Neuroscientific identification of seven mammalian emotional systems", "label": "Neuroscientific identification of seven mammalian emotional systems", "shape": "dot", "title": "Neuroscientific identification of seven mammalian emotional systems"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Proof-of-concept IRL/BIP demonstrations recovering human intent", "label": "Proof-of-concept IRL/BIP demonstrations recovering human intent", "shape": "dot", "title": "Proof-of-concept IRL/BIP demonstrations recovering human intent"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Initialize AI agents with mammalian value priors and goal uncertainty", "label": "Initialize AI agents with mammalian value priors and goal uncertainty", "shape": "dot", "title": "Initialize AI agents with mammalian value priors and goal uncertainty"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Fine-tune AI agents via inverse reinforcement learning to align with human behavior", "label": "Fine-tune AI agents via inverse reinforcement learning to align with human behavior", "shape": "dot", "title": "Fine-tune AI agents via inverse reinforcement learning to align with human behavior"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Pre-train AI models on diverse video datasets of mammalian and human social interactions", "label": "Pre-train AI models on diverse video datasets of mammalian and human social interactions", "shape": "dot", "title": "Pre-train AI models on diverse video datasets of mammalian and human social interactions"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Embed Russell\u0027s three principles into agent objective architecture", "label": "Embed Russell\u0027s three principles into agent objective architecture", "shape": "dot", "title": "Embed Russell\u0027s three principles into agent objective architecture"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "inefficient exploration in model-based reinforcement learning", "label": "inefficient exploration in model-based reinforcement learning", "shape": "dot", "title": "inefficient exploration in model-based reinforcement learning"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "reward-agnostic exploration strategies in reinforcement learning", "label": "reward-agnostic exploration strategies in reinforcement learning", "shape": "dot", "title": "reward-agnostic exploration strategies in reinforcement learning"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "over-exploration of low-value regions by information-gain strategies", "label": "over-exploration of low-value regions by information-gain strategies", "shape": "dot", "title": "over-exploration of low-value regions by information-gain strategies"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "reward-directed exploration for asymptotic optimality in reinforcement learning", "label": "reward-directed exploration for asymptotic optimality in reinforcement learning", "shape": "dot", "title": "reward-directed exploration for asymptotic optimality in reinforcement learning"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "minimizing exploration potential to guide exploration", "label": "minimizing exploration potential to guide exploration", "shape": "dot", "title": "minimizing exploration potential to guide exploration"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "MinEP algorithm selecting actions that minimize expected exploration potential", "label": "MinEP algorithm selecting actions that minimize expected exploration potential", "shape": "dot", "title": "MinEP algorithm selecting actions that minimize expected exploration potential"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "theoretical proofs and bandit experiments validating exploration potential framework", "label": "theoretical proofs and bandit experiments validating exploration potential framework", "shape": "dot", "title": "theoretical proofs and bandit experiments validating exploration potential framework"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Apply MinEP exploration strategy during RL fine-tuning to reduce inefficient exploration", "label": "Apply MinEP exploration strategy during RL fine-tuning to reduce inefficient exploration", "shape": "dot", "title": "Apply MinEP exploration strategy during RL fine-tuning to reduce inefficient exploration"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Monitor exploration potential convergence to switch from exploration to exploitation before deployment", "label": "Monitor exploration potential convergence to switch from exploration to exploitation before deployment", "shape": "dot", "title": "Monitor exploration potential convergence to switch from exploration to exploitation before deployment"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Suboptimal translation quality and scalability in neural machine translation production", "label": "Suboptimal translation quality and scalability in neural machine translation production", "shape": "dot", "title": "Suboptimal translation quality and scalability in neural machine translation production"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "High computational cost during inference in NMT deployment", "label": "High computational cost during inference in NMT deployment", "shape": "dot", "title": "High computational cost during inference in NMT deployment"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Objective-function misalignment with BLEU metric in NMT training", "label": "Objective-function misalignment with BLEU metric in NMT training", "shape": "dot", "title": "Objective-function misalignment with BLEU metric in NMT training"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Out-of-vocabulary word handling deficiencies in NMT", "label": "Out-of-vocabulary word handling deficiencies in NMT", "shape": "dot", "title": "Out-of-vocabulary word handling deficiencies in NMT"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Length bias in beam search decoding", "label": "Length bias in beam search decoding", "shape": "dot", "title": "Length bias in beam search decoding"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Gradient vanishing in deep stacked LSTMs", "label": "Gradient vanishing in deep stacked LSTMs", "shape": "dot", "title": "Gradient vanishing in deep stacked LSTMs"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Quantization preserving accuracy while reducing computation in NMT", "label": "Quantization preserving accuracy while reducing computation in NMT", "shape": "dot", "title": "Quantization preserving accuracy while reducing computation in NMT"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Sentence-level expected reward optimisation aligns training with BLEU", "label": "Sentence-level expected reward optimisation aligns training with BLEU", "shape": "dot", "title": "Sentence-level expected reward optimisation aligns training with BLEU"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Subword wordpiece modelling for open-vocabulary translation", "label": "Subword wordpiece modelling for open-vocabulary translation", "shape": "dot", "title": "Subword wordpiece modelling for open-vocabulary translation"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Length-normalised coverage-aware beam search improves adequacy", "label": "Length-normalised coverage-aware beam search improves adequacy", "shape": "dot", "title": "Length-normalised coverage-aware beam search improves adequacy"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Residual connections to improve gradient flow in deep NMT stacks", "label": "Residual connections to improve gradient flow in deep NMT stacks", "shape": "dot", "title": "Residual connections to improve gradient flow in deep NMT stacks"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Constrain accumulators and logits to enable post-training 8/16-bit quantisation", "label": "Constrain accumulators and logits to enable post-training 8/16-bit quantisation", "shape": "dot", "title": "Constrain accumulators and logits to enable post-training 8/16-bit quantisation"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Combine ML pre-training with small-weight RL fine-tuning", "label": "Combine ML pre-training with small-weight RL fine-tuning", "shape": "dot", "title": "Combine ML pre-training with small-weight RL fine-tuning"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Shared 32k wordpiece vocabulary for source and target", "label": "Shared 32k wordpiece vocabulary for source and target", "shape": "dot", "title": "Shared 32k wordpiece vocabulary for source and target"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Beam-search scoring with \u03b1=0.2 length penalty and \u03b2=0.2 coverage penalty", "label": "Beam-search scoring with \u03b1=0.2 length penalty and \u03b2=0.2 coverage penalty", "shape": "dot", "title": "Beam-search scoring with \u03b1=0.2 length penalty and \u03b2=0.2 coverage penalty"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Element-wise residual addition between stacked LSTM layers", "label": "Element-wise residual addition between stacked LSTM layers", "shape": "dot", "title": "Element-wise residual addition between stacked LSTM layers"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Integer-quantised GNMT inference on TPUs with 8-bit weights and 16-bit accumulators", "label": "Integer-quantised GNMT inference on TPUs with 8-bit weights and 16-bit accumulators", "shape": "dot", "title": "Integer-quantised GNMT inference on TPUs with 8-bit weights and 16-bit accumulators"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Expected reward objective with GLEU and \u03b1=0.017 weight", "label": "Expected reward objective with GLEU and \u03b1=0.017 weight", "shape": "dot", "title": "Expected reward objective with GLEU and \u03b1=0.017 weight"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Greedy likelihood-maximising wordpiece model training", "label": "Greedy likelihood-maximising wordpiece model training", "shape": "dot", "title": "Greedy likelihood-maximising wordpiece model training"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Beam search decoder with pruning, batch decoding and lp/cp scoring", "label": "Beam search decoder with pruning, batch decoding and lp/cp scoring", "shape": "dot", "title": "Beam search decoder with pruning, batch decoding and lp/cp scoring"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "BLEU unchanged and 3.4\u00d7 speedup on TPU with quantised model", "label": "BLEU unchanged and 3.4\u00d7 speedup on TPU with quantised model", "shape": "dot", "title": "BLEU unchanged and 3.4\u00d7 speedup on TPU with quantised model"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "BLEU improvement of ~1 on WMT En\u2192Fr after RL refinement", "label": "BLEU improvement of ~1 on WMT En\u2192Fr after RL refinement", "shape": "dot", "title": "BLEU improvement of ~1 on WMT En\u2192Fr after RL refinement"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "WPM-32K model achieves highest BLEU and good speed", "label": "WPM-32K model achieves highest BLEU and good speed", "shape": "dot", "title": "WPM-32K model achieves highest BLEU and good speed"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Length/coverage penalties give +1.1 BLEU on dev set", "label": "Length/coverage penalties give +1.1 BLEU on dev set", "shape": "dot", "title": "Length/coverage penalties give +1.1 BLEU on dev set"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Deep residual LSTM enables 8-layer GNMT reaching 38.95 BLEU", "label": "Deep residual LSTM enables 8-layer GNMT reaching 38.95 BLEU", "shape": "dot", "title": "Deep residual LSTM enables 8-layer GNMT reaching 38.95 BLEU"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy quantised GNMT models on TPUs with 8-bit weights", "label": "Deploy quantised GNMT models on TPUs with 8-bit weights", "shape": "dot", "title": "Deploy quantised GNMT models on TPUs with 8-bit weights"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Fine-tune NMT models with RL using expected GLEU objective", "label": "Fine-tune NMT models with RL using expected GLEU objective", "shape": "dot", "title": "Fine-tune NMT models with RL using expected GLEU objective"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Use wordpiece segmentation (32k shared vocab) during NMT training and inference", "label": "Use wordpiece segmentation (32k shared vocab) during NMT training and inference", "shape": "dot", "title": "Use wordpiece segmentation (32k shared vocab) during NMT training and inference"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Apply length-normalised, coverage-penalised beam search decoding", "label": "Apply length-normalised, coverage-penalised beam search decoding", "shape": "dot", "title": "Apply length-normalised, coverage-penalised beam search decoding"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Incorporate residual connections in deep stacked LSTM encoder-decoder architecture", "label": "Incorporate residual connections in deep stacked LSTM encoder-decoder architecture", "shape": "dot", "title": "Incorporate residual connections in deep stacked LSTM encoder-decoder architecture"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "High computational and parameter inefficiency in CNN architectures for image classification", "label": "High computational and parameter inefficiency in CNN architectures for image classification", "shape": "dot", "title": "High computational and parameter inefficiency in CNN architectures for image classification"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Joint spatial and cross-channel correlation mapping via standard convolutions", "label": "Joint spatial and cross-channel correlation mapping via standard convolutions", "shape": "dot", "title": "Joint spatial and cross-channel correlation mapping via standard convolutions"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Decoupling cross-channel and spatial correlations reduces parameter count without harming representation capacity", "label": "Decoupling cross-channel and spatial correlations reduces parameter count without harming representation capacity", "shape": "dot", "title": "Decoupling cross-channel and spatial correlations reduces parameter count without harming representation capacity"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Replace standard convolutions with depthwise separable convolutions", "label": "Replace standard convolutions with depthwise separable convolutions", "shape": "dot", "title": "Replace standard convolutions with depthwise separable convolutions"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Depthwise separable convolution layer (channel-wise spatial convolution followed by 1\u00d71 point-wise convolution)", "label": "Depthwise separable convolution layer (channel-wise spatial convolution followed by 1\u00d71 point-wise convolution)", "shape": "dot", "title": "Depthwise separable convolution layer (channel-wise spatial convolution followed by 1\u00d71 point-wise convolution)"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Parameter-equivalent Xception improves ImageNet top-1 accuracy to 0.79 and JFT MAP@100 by 4.3%", "label": "Parameter-equivalent Xception improves ImageNet top-1 accuracy to 0.79 and JFT MAP@100 by 4.3%", "shape": "dot", "title": "Parameter-equivalent Xception improves ImageNet top-1 accuracy to 0.79 and JFT MAP@100 by 4.3%"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt depthwise separable convolutions in CNN model design to reduce parameters and improve accuracy", "label": "Adopt depthwise separable convolutions in CNN model design to reduce parameters and improve accuracy", "shape": "dot", "title": "Adopt depthwise separable convolutions in CNN model design to reduce parameters and improve accuracy"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Training instability and slow convergence in deep depthwise separable convolutional networks", "label": "Training instability and slow convergence in deep depthwise separable convolutional networks", "shape": "dot", "title": "Training instability and slow convergence in deep depthwise separable convolutional networks"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Absence of residual connections impedes gradient flow in deep separable convolution stacks", "label": "Absence of residual connections impedes gradient flow in deep separable convolution stacks", "shape": "dot", "title": "Absence of residual connections impedes gradient flow in deep separable convolution stacks"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Residual skip connections enable stable optimization of deep networks", "label": "Residual skip connections enable stable optimization of deep networks", "shape": "dot", "title": "Residual skip connections enable stable optimization of deep networks"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Add linear residual connections around separable convolution modules", "label": "Add linear residual connections around separable convolution modules", "shape": "dot", "title": "Add linear residual connections around separable convolution modules"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Residual blocks in Xception stack", "label": "Residual blocks in Xception stack", "shape": "dot", "title": "Residual blocks in Xception stack"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Removing residual connections lowers accuracy and slows convergence on ImageNet", "label": "Removing residual connections lowers accuracy and slows convergence on ImageNet", "shape": "dot", "title": "Removing residual connections lowers accuracy and slows convergence on ImageNet"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate residual connections in depthwise separable convolution architectures during model design", "label": "Integrate residual connections in depthwise separable convolution architectures during model design", "shape": "dot", "title": "Integrate residual connections in depthwise separable convolution architectures during model design"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Performance degradation due to intermediate activation in depthwise separable convolutions", "label": "Performance degradation due to intermediate activation in depthwise separable convolutions", "shape": "dot", "title": "Performance degradation due to intermediate activation in depthwise separable convolutions"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Absence of intermediate activation retains information and accelerates convergence in shallow feature spaces", "label": "Absence of intermediate activation retains information and accelerates convergence in shallow feature spaces", "shape": "dot", "title": "Absence of intermediate activation retains information and accelerates convergence in shallow feature spaces"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Exclude activation between depthwise and pointwise convolutions", "label": "Exclude activation between depthwise and pointwise convolutions", "shape": "dot", "title": "Exclude activation between depthwise and pointwise convolutions"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Separable convolution layer without intermediate activation", "label": "Separable convolution layer without intermediate activation", "shape": "dot", "title": "Separable convolution layer without intermediate activation"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "ImageNet experiments show better accuracy without intermediate activation", "label": "ImageNet experiments show better accuracy without intermediate activation", "shape": "dot", "title": "ImageNet experiments show better accuracy without intermediate activation"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Design separable convolutional layers without intermediate activation during model design", "label": "Design separable convolutional layers without intermediate activation during model design", "shape": "dot", "title": "Design separable convolutional layers without intermediate activation during model design"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Existential catastrophe from superintelligent system failure", "label": "Existential catastrophe from superintelligent system failure", "shape": "dot", "title": "Existential catastrophe from superintelligent system failure"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Escalating narrow AI failures across domains", "label": "Escalating narrow AI failures across domains", "shape": "dot", "title": "Escalating narrow AI failures across domains"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Inevitable security failures due to Fundamental Theorem of Security", "label": "Inevitable security failures due to Fundamental Theorem of Security", "shape": "dot", "title": "Inevitable security failures due to Fundamental Theorem of Security"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "Lack of effective testing methods for superintelligence", "label": "Lack of effective testing methods for superintelligence", "shape": "dot", "title": "Lack of effective testing methods for superintelligence"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Insufficient AI safety research capacity", "label": "Insufficient AI safety research capacity", "shape": "dot", "title": "Insufficient AI safety research capacity"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Value misalignment in AI utility functions", "label": "Value misalignment in AI utility functions", "shape": "dot", "title": "Value misalignment in AI utility functions"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "AI safety reducible to unsolved safe human problem", "label": "AI safety reducible to unsolved safe human problem", "shape": "dot", "title": "AI safety reducible to unsolved safe human problem"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Adopting cybersecurity paradigms can improve AI safety", "label": "Adopting cybersecurity paradigms can improve AI safety", "shape": "dot", "title": "Adopting cybersecurity paradigms can improve AI safety"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Domain-specific narrow AI avoids AGI cross-domain risks", "label": "Domain-specific narrow AI avoids AGI cross-domain risks", "shape": "dot", "title": "Domain-specific narrow AI avoids AGI cross-domain risks"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Implement containment architectures to isolate AGI", "label": "Implement containment architectures to isolate AGI", "shape": "dot", "title": "Implement containment architectures to isolate AGI"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Increase multidisciplinary safety research efforts", "label": "Increase multidisciplinary safety research efforts", "shape": "dot", "title": "Increase multidisciplinary safety research efforts"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Segregate narrow AI from AGI in development pipelines", "label": "Segregate narrow AI from AGI in development pipelines", "shape": "dot", "title": "Segregate narrow AI from AGI in development pipelines"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Embed probabilistic safety engineering and formal verification", "label": "Embed probabilistic safety engineering and formal verification", "shape": "dot", "title": "Embed probabilistic safety engineering and formal verification"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "AI boxing with leakproof channels and physical confinement", "label": "AI boxing with leakproof channels and physical confinement", "shape": "dot", "title": "AI boxing with leakproof channels and physical confinement"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Cross-disciplinary AI safety teams and funding programs", "label": "Cross-disciplinary AI safety teams and funding programs", "shape": "dot", "title": "Cross-disciplinary AI safety teams and funding programs"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Organizational governance separating NAI and AGI codebases", "label": "Organizational governance separating NAI and AGI codebases", "shape": "dot", "title": "Organizational governance separating NAI and AGI codebases"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Formal verification toolchains applied to AI safety components", "label": "Formal verification toolchains applied to AI safety components", "shape": "dot", "title": "Formal verification toolchains applied to AI safety components"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Security-oriented development lifecycle for AI", "label": "Security-oriented development lifecycle for AI", "shape": "dot", "title": "Security-oriented development lifecycle for AI"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Historical timeline of AI failures from 1959-2016", "label": "Historical timeline of AI failures from 1959-2016", "shape": "dot", "title": "Historical timeline of AI failures from 1959-2016"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Exponential trend analysis of AI failure frequency", "label": "Exponential trend analysis of AI failure frequency", "shape": "dot", "title": "Exponential trend analysis of AI failure frequency"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Examples of utility function misspecification leading to reward hacking", "label": "Examples of utility function misspecification leading to reward hacking", "shape": "dot", "title": "Examples of utility function misspecification leading to reward hacking"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Fundamental unverifiability necessitates probabilistic safety proofs", "label": "Fundamental unverifiability necessitates probabilistic safety proofs", "shape": "dot", "title": "Fundamental unverifiability necessitates probabilistic safety proofs"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate cybersecurity best practices into AI lifecycle", "label": "Integrate cybersecurity best practices into AI lifecycle", "shape": "dot", "title": "Integrate cybersecurity best practices into AI lifecycle"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Develop and deploy leakproof AI boxing systems before AGI activation", "label": "Develop and deploy leakproof AI boxing systems before AGI activation", "shape": "dot", "title": "Develop and deploy leakproof AI boxing systems before AGI activation"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt cybersecurity principles across AI development lifecycle", "label": "Adopt cybersecurity principles across AI development lifecycle", "shape": "dot", "title": "Adopt cybersecurity principles across AI development lifecycle"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Establish mandatory organizational separation between NAI and AGI projects", "label": "Establish mandatory organizational separation between NAI and AGI projects", "shape": "dot", "title": "Establish mandatory organizational separation between NAI and AGI projects"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Scale global multidisciplinary AI safety research funding and collaboration", "label": "Scale global multidisciplinary AI safety research funding and collaboration", "shape": "dot", "title": "Scale global multidisciplinary AI safety research funding and collaboration"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate formal verification and probabilistic safety checks in AI lifecycle", "label": "Integrate formal verification and probabilistic safety checks in AI lifecycle", "shape": "dot", "title": "Integrate formal verification and probabilistic safety checks in AI lifecycle"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Suboptimal neural network architectures in deep learning systems", "label": "Suboptimal neural network architectures in deep learning systems", "shape": "dot", "title": "Suboptimal neural network architectures in deep learning systems"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Manual architecture design requiring expert knowledge and limited exploration", "label": "Manual architecture design requiring expert knowledge and limited exploration", "shape": "dot", "title": "Manual architecture design requiring expert knowledge and limited exploration"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Fixed-length hyperparameter optimisation limitations", "label": "Fixed-length hyperparameter optimisation limitations", "shape": "dot", "title": "Fixed-length hyperparameter optimisation limitations"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Slow heuristic-dependent neuro-evolution search methods", "label": "Slow heuristic-dependent neuro-evolution search methods", "shape": "dot", "title": "Slow heuristic-dependent neuro-evolution search methods"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Architecture encoding as variable-length string generated by controller RNN", "label": "Architecture encoding as variable-length string generated by controller RNN", "shape": "dot", "title": "Architecture encoding as variable-length string generated by controller RNN"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Validation accuracy of sampled architecture as reinforcement-learning reward", "label": "Validation accuracy of sampled architecture as reinforcement-learning reward", "shape": "dot", "title": "Validation accuracy of sampled architecture as reinforcement-learning reward"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Use of policy-gradient reinforcement learning to train controller", "label": "Use of policy-gradient reinforcement learning to train controller", "shape": "dot", "title": "Use of policy-gradient reinforcement learning to train controller"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Parallel distributed asynchronous training scheme for NAS", "label": "Parallel distributed asynchronous training scheme for NAS", "shape": "dot", "title": "Parallel distributed asynchronous training scheme for NAS"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Controller RNN generates architecture tokens for child network", "label": "Controller RNN generates architecture tokens for child network", "shape": "dot", "title": "Controller RNN generates architecture tokens for child network"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Child network training and reward collection", "label": "Child network training and reward collection", "shape": "dot", "title": "Child network training and reward collection"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "REINFORCE gradient update with baseline", "label": "REINFORCE gradient update with baseline", "shape": "dot", "title": "REINFORCE gradient update with baseline"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Set-selection attention for skip-connection prediction", "label": "Set-selection attention for skip-connection prediction", "shape": "dot", "title": "Set-selection attention for skip-connection prediction"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Tree-based encoding of recurrent cell computation graph", "label": "Tree-based encoding of recurrent cell computation graph", "shape": "dot", "title": "Tree-based encoding of recurrent cell computation graph"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "NAS ConvNet achieves 3.65% error on CIFAR-10 with faster inference", "label": "NAS ConvNet achieves 3.65% error on CIFAR-10 with faster inference", "shape": "dot", "title": "NAS ConvNet achieves 3.65% error on CIFAR-10 with faster inference"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "NAS recurrent cell achieves 62.4 perplexity on Penn Treebank", "label": "NAS recurrent cell achieves 62.4 perplexity on Penn Treebank", "shape": "dot", "title": "NAS recurrent cell achieves 62.4 perplexity on Penn Treebank"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "NAS cell improves GNMT BLEU by 0.5 without retuning", "label": "NAS cell improves GNMT BLEU by 0.5 without retuning", "shape": "dot", "title": "NAS cell improves GNMT BLEU by 0.5 without retuning"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Conduct reinforcement-learning-based Neural Architecture Search during model design", "label": "Conduct reinforcement-learning-based Neural Architecture Search during model design", "shape": "dot", "title": "Conduct reinforcement-learning-based Neural Architecture Search during model design"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Overconfident incorrect predictions in neural network deployments", "label": "Overconfident incorrect predictions in neural network deployments", "shape": "dot", "title": "Overconfident incorrect predictions in neural network deployments"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Miscalibration of predictive probabilities in deep neural networks", "label": "Miscalibration of predictive probabilities in deep neural networks", "shape": "dot", "title": "Miscalibration of predictive probabilities in deep neural networks"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Uncertainty failure under domain shift in neural networks", "label": "Uncertainty failure under domain shift in neural networks", "shape": "dot", "title": "Uncertainty failure under domain shift in neural networks"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Deterministic point prediction training in standard neural networks", "label": "Deterministic point prediction training in standard neural networks", "shape": "dot", "title": "Deterministic point prediction training in standard neural networks"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Approximate Bayesian inference dependence on incorrect priors in Bayesian neural networks", "label": "Approximate Bayesian inference dependence on incorrect priors in Bayesian neural networks", "shape": "dot", "title": "Approximate Bayesian inference dependence on incorrect priors in Bayesian neural networks"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Lack of robustness to unseen classes due to narrow training data support", "label": "Lack of robustness to unseen classes due to narrow training data support", "shape": "dot", "title": "Lack of robustness to unseen classes due to narrow training data support"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Proper scoring rule optimization promotes probabilistic calibration", "label": "Proper scoring rule optimization promotes probabilistic calibration", "shape": "dot", "title": "Proper scoring rule optimization promotes probabilistic calibration"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Model averaging across diverse networks captures model uncertainty", "label": "Model averaging across diverse networks captures model uncertainty", "shape": "dot", "title": "Model averaging across diverse networks captures model uncertainty"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Adversarial training encourages local smoothness in predictive distributions", "label": "Adversarial training encourages local smoothness in predictive distributions", "shape": "dot", "title": "Adversarial training encourages local smoothness in predictive distributions"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Train probabilistic neural networks with predictive mean and variance outputs", "label": "Train probabilistic neural networks with predictive mean and variance outputs", "shape": "dot", "title": "Train probabilistic neural networks with predictive mean and variance outputs"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Independently train ensemble of NNs on full data with random initialization", "label": "Independently train ensemble of NNs on full data with random initialization", "shape": "dot", "title": "Independently train ensemble of NNs on full data with random initialization"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Augment training with adversarial examples using fast gradient sign method", "label": "Augment training with adversarial examples using fast gradient sign method", "shape": "dot", "title": "Augment training with adversarial examples using fast gradient sign method"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Negative log-likelihood loss for heteroscedastic Gaussian outputs", "label": "Negative log-likelihood loss for heteroscedastic Gaussian outputs", "shape": "dot", "title": "Negative log-likelihood loss for heteroscedastic Gaussian outputs"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Uniform averaging of ensemble predictive distributions", "label": "Uniform averaging of ensemble predictive distributions", "shape": "dot", "title": "Uniform averaging of ensemble predictive distributions"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Fast gradient sign generation of adversarial perturbations with epsilon 1% input range", "label": "Fast gradient sign generation of adversarial perturbations with epsilon 1% input range", "shape": "dot", "title": "Fast gradient sign generation of adversarial perturbations with epsilon 1% input range"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Deep ensemble NLL and Brier improvements over MC-dropout across benchmarks", "label": "Deep ensemble NLL and Brier improvements over MC-dropout across benchmarks", "shape": "dot", "title": "Deep ensemble NLL and Brier improvements over MC-dropout across benchmarks"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Higher predictive entropy on out-of-distribution datasets with ensembles", "label": "Higher predictive entropy on out-of-distribution datasets with ensembles", "shape": "dot", "title": "Higher predictive entropy on out-of-distribution datasets with ensembles"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Improved accuracy vs confidence curves showing reduced overconfidence", "label": "Improved accuracy vs confidence curves showing reduced overconfidence", "shape": "dot", "title": "Improved accuracy vs confidence curves showing reduced overconfidence"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Train deep probabilistic neural network ensembles with adversarial training and proper scoring rule", "label": "Train deep probabilistic neural network ensembles with adversarial training and proper scoring rule", "shape": "dot", "title": "Train deep probabilistic neural network ensembles with adversarial training and proper scoring rule"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "Apply ensemble predictive entropy thresholding during deployment to defer uncertain predictions", "label": "Apply ensemble predictive entropy thresholding during deployment to defer uncertain predictions", "shape": "dot", "title": "Apply ensemble predictive entropy thresholding during deployment to defer uncertain predictions"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "AI development arms race between nations due to non-cooperative deployment", "label": "AI development arms race between nations due to non-cooperative deployment", "shape": "dot", "title": "AI development arms race between nations due to non-cooperative deployment"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Unfair value representation in multi-principal AI systems", "label": "Unfair value representation in multi-principal AI systems", "shape": "dot", "title": "Unfair value representation in multi-principal AI systems"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Lack of attractive cooperative bargaining mechanism for shared AI control", "label": "Lack of attractive cooperative bargaining mechanism for shared AI control", "shape": "dot", "title": "Lack of attractive cooperative bargaining mechanism for shared AI control"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Naive linear utility aggregation incompatible with Pareto optimality under differing beliefs", "label": "Naive linear utility aggregation incompatible with Pareto optimality under differing beliefs", "shape": "dot", "title": "Naive linear utility aggregation incompatible with Pareto optimality under differing beliefs"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Pareto optimal policy requires belief-weighted dynamic priority shifting", "label": "Pareto optimal policy requires belief-weighted dynamic priority shifting", "shape": "dot", "title": "Pareto optimal policy requires belief-weighted dynamic priority shifting"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Use of each stakeholder\u0027s own beliefs to evaluate expected utility", "label": "Use of each stakeholder\u0027s own beliefs to evaluate expected utility", "shape": "dot", "title": "Use of each stakeholder\u0027s own beliefs to evaluate expected utility"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Negotiable reinforcement learning via Pareto optimality recursion", "label": "Negotiable reinforcement learning via Pareto optimality recursion", "shape": "dot", "title": "Negotiable reinforcement learning via Pareto optimality recursion"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "POMDP mixture with coin-flip weight initialization", "label": "POMDP mixture with coin-flip weight initialization", "shape": "dot", "title": "POMDP mixture with coin-flip weight initialization"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Policy mixing through random seed to create convex policy space", "label": "Policy mixing through random seed to create convex policy space", "shape": "dot", "title": "Policy mixing through random seed to create convex policy space"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Priority weight update proportional to predictive accuracy of beliefs", "label": "Priority weight update proportional to predictive accuracy of beliefs", "shape": "dot", "title": "Priority weight update proportional to predictive accuracy of beliefs"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Mathematical proof of Pareto recursion (Theorem 8)", "label": "Mathematical proof of Pareto recursion (Theorem 8)", "shape": "dot", "title": "Mathematical proof of Pareto recursion (Theorem 8)"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Cake-splitting scenario demonstration of naive aggregation failure", "label": "Cake-splitting scenario demonstration of naive aggregation failure", "shape": "dot", "title": "Cake-splitting scenario demonstration of naive aggregation failure"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Develop AI bargaining module implementing negotiable RL with dynamic priority shifting", "label": "Develop AI bargaining module implementing negotiable RL with dynamic priority shifting", "shape": "dot", "title": "Develop AI bargaining module implementing negotiable RL with dynamic priority shifting"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Use belief elicitation and policy mixing during AI fine-tuning to enable stakeholder bet-settling", "label": "Use belief elicitation and policy mixing during AI fine-tuning to enable stakeholder bet-settling", "shape": "dot", "title": "Use belief elicitation and policy mixing during AI fine-tuning to enable stakeholder bet-settling"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "Unintended behavioral loops in robots trained with policy-independent human feedback", "label": "Unintended behavioral loops in robots trained with policy-independent human feedback", "shape": "dot", "title": "Unintended behavioral loops in robots trained with policy-independent human feedback"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "High trainer burden and inefficient feedback in human-centered reinforcement learning", "label": "High trainer burden and inefficient feedback in human-centered reinforcement learning", "shape": "dot", "title": "High trainer burden and inefficient feedback in human-centered reinforcement learning"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "Assumption of policy-independent feedback in human-centered RL algorithms", "label": "Assumption of policy-independent feedback in human-centered RL algorithms", "shape": "dot", "title": "Assumption of policy-independent feedback in human-centered RL algorithms"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "Reward-maximization interpretation of human feedback inducing positive reward cycles", "label": "Reward-maximization interpretation of human feedback inducing positive reward cycles", "shape": "dot", "title": "Reward-maximization interpretation of human feedback inducing positive reward cycles"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "Existing HCRL algorithms lack support for diminishing returns, differential feedback and policy shaping", "label": "Existing HCRL algorithms lack support for diminishing returns, differential feedback and policy shaping", "shape": "dot", "title": "Existing HCRL algorithms lack support for diminishing returns, differential feedback and policy shaping"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "Human feedback is policy-dependent and aligns with advantage function in RL", "label": "Human feedback is policy-dependent and aligns with advantage function in RL", "shape": "dot", "title": "Human feedback is policy-dependent and aligns with advantage function in RL"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "Policy-dependent feedback enables differential feedback, diminishing returns and policy shaping strategies", "label": "Policy-dependent feedback enables differential feedback, diminishing returns and policy shaping strategies", "shape": "dot", "title": "Policy-dependent feedback enables differential feedback, diminishing returns and policy shaping strategies"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "Use advantage-function-aligned policy updates to learn safely from policy-dependent feedback", "label": "Use advantage-function-aligned policy updates to learn safely from policy-dependent feedback", "shape": "dot", "title": "Use advantage-function-aligned policy updates to learn safely from policy-dependent feedback"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "Reduce trainer burden via diminishing returns and eligibility tracing mechanisms", "label": "Reduce trainer burden via diminishing returns and eligibility tracing mechanisms", "shape": "dot", "title": "Reduce trainer burden via diminishing returns and eligibility tracing mechanisms"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "TD-error as unbiased advantage estimate for direct policy updates in COACH", "label": "TD-error as unbiased advantage estimate for direct policy updates in COACH", "shape": "dot", "title": "TD-error as unbiased advantage estimate for direct policy updates in COACH"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "Real-time COACH with eligibility traces and reward aggregation for delayed sparse feedback", "label": "Real-time COACH with eligibility traces and reward aggregation for delayed sparse feedback", "shape": "dot", "title": "Real-time COACH with eligibility traces and reward aggregation for delayed sparse feedback"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "Softmax visual feature-based policy on TurtleBot within COACH framework", "label": "Softmax visual feature-based policy on TurtleBot within COACH framework", "shape": "dot", "title": "Softmax visual feature-based policy on TurtleBot within COACH framework"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "AMT gridworld study shows sign of feedback varies with learner policy", "label": "AMT gridworld study shows sign of feedback varies with learner policy", "shape": "dot", "title": "AMT gridworld study shows sign of feedback varies with learner policy"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "TurtleBot case study demonstrates fast learning of five behaviours with COACH", "label": "TurtleBot case study demonstrates fast learning of five behaviours with COACH", "shape": "dot", "title": "TurtleBot case study demonstrates fast learning of five behaviours with COACH"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "TAMER comparison reveals unlearning when feedback is misinterpreted as reward", "label": "TAMER comparison reveals unlearning when feedback is misinterpreted as reward", "shape": "dot", "title": "TAMER comparison reveals unlearning when feedback is misinterpreted as reward"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "Fine-tune robots using COACH algorithm to integrate policy-dependent human feedback", "label": "Fine-tune robots using COACH algorithm to integrate policy-dependent human feedback", "shape": "dot", "title": "Fine-tune robots using COACH algorithm to integrate policy-dependent human feedback"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate advantage-function-based feedback mapping into human-in-the-loop RL training interfaces", "label": "Integrate advantage-function-based feedback mapping into human-in-the-loop RL training interfaces", "shape": "dot", "title": "Integrate advantage-function-based feedback mapping into human-in-the-loop RL training interfaces"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "Implement eligibility traces with variable decay and reward aggregation in real-time RL systems", "label": "Implement eligibility traces with variable decay and reward aggregation in real-time RL systems", "shape": "dot", "title": "Implement eligibility traces with variable decay and reward aggregation in real-time RL systems"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt compositional and differential training strategies supported by policy-dependent feedback during robot instruction", "label": "Adopt compositional and differential training strategies supported by policy-dependent feedback during robot instruction", "shape": "dot", "title": "Adopt compositional and differential training strategies supported by policy-dependent feedback during robot instruction"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Unexpected robot behavior in human-robot interaction", "label": "Unexpected robot behavior in human-robot interaction", "shape": "dot", "title": "Unexpected robot behavior in human-robot interaction"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Opacity of robot objective functions to end-users", "label": "Opacity of robot objective functions to end-users", "shape": "dot", "title": "Opacity of robot objective functions to end-users"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Underdetermined optimal behavior in low-interaction environments", "label": "Underdetermined optimal behavior in low-interaction environments", "shape": "dot", "title": "Underdetermined optimal behavior in low-interaction environments"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Human approximate inference over robot objectives", "label": "Human approximate inference over robot objectives", "shape": "dot", "title": "Human approximate inference over robot objectives"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Informative demonstrations accelerate mental model formation", "label": "Informative demonstrations accelerate mental model formation", "shape": "dot", "title": "Informative demonstrations accelerate mental model formation"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Algorithmic teaching for informative behavior selection", "label": "Algorithmic teaching for informative behavior selection", "shape": "dot", "title": "Algorithmic teaching for informative behavior selection"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Deterministic Euclidean-based user model for teaching", "label": "Deterministic Euclidean-based user model for teaching", "shape": "dot", "title": "Deterministic Euclidean-based user model for teaching"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Strategy coverage in teaching examples", "label": "Strategy coverage in teaching examples", "shape": "dot", "title": "Strategy coverage in teaching examples"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Greedy environment selection maximizing posterior over objectives", "label": "Greedy environment selection maximizing posterior over objectives", "shape": "dot", "title": "Greedy environment selection maximizing posterior over objectives"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Deterministic Euclidean distance metric in learner model", "label": "Deterministic Euclidean distance metric in learner model", "shape": "dot", "title": "Deterministic Euclidean distance metric in learner model"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Coverage augmentation after posterior convergence", "label": "Coverage augmentation after posterior convergence", "shape": "dot", "title": "Coverage augmentation after posterior convergence"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "User study: Euclidean model improves trajectory identification", "label": "User study: Euclidean model improves trajectory identification", "shape": "dot", "title": "User study: Euclidean model improves trajectory identification"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "User study: coverage-augmented teaching outperforms random baseline", "label": "User study: coverage-augmented teaching outperforms random baseline", "shape": "dot", "title": "User study: coverage-augmented teaching outperforms random baseline"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy coverage-augmented algorithmic teaching of robot objectives to end-users", "label": "Deploy coverage-augmented algorithmic teaching of robot objectives to end-users", "shape": "dot", "title": "Deploy coverage-augmented algorithmic teaching of robot objectives to end-users"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Generate teaching examples using deterministic Euclidean-based inference model during user training", "label": "Generate teaching examples using deterministic Euclidean-based inference model during user training", "shape": "dot", "title": "Generate teaching examples using deterministic Euclidean-based inference model during user training"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Confound-driven misgeneralization in differentiable models", "label": "Confound-driven misgeneralization in differentiable models", "shape": "dot", "title": "Confound-driven misgeneralization in differentiable models"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Opaque decision boundaries in neural networks", "label": "Opaque decision boundaries in neural networks", "shape": "dot", "title": "Opaque decision boundaries in neural networks"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Ineffectiveness of standard regularization under dataset confounds", "label": "Ineffectiveness of standard regularization under dataset confounds", "shape": "dot", "title": "Ineffectiveness of standard regularization under dataset confounds"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Input gradient explanations approximate local decision boundaries", "label": "Input gradient explanations approximate local decision boundaries", "shape": "dot", "title": "Input gradient explanations approximate local decision boundaries"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Constraining input gradients on annotated irrelevant features", "label": "Constraining input gradients on annotated irrelevant features", "shape": "dot", "title": "Constraining input gradients on annotated irrelevant features"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Iterative penalization of salient gradients to uncover alternative rules", "label": "Iterative penalization of salient gradients to uncover alternative rules", "shape": "dot", "title": "Iterative penalization of salient gradients to uncover alternative rules"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Loss function with input gradient penalty term", "label": "Loss function with input gradient penalty term", "shape": "dot", "title": "Loss function with input gradient penalty term"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Annotation matrix specifying irrelevant input dimensions", "label": "Annotation matrix specifying irrelevant input dimensions", "shape": "dot", "title": "Annotation matrix specifying irrelevant input dimensions"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Find-another-explanation iterative training algorithm", "label": "Find-another-explanation iterative training algorithm", "shape": "dot", "title": "Find-another-explanation iterative training algorithm"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Improved generalization on Decoy MNIST after gradient penalties", "label": "Improved generalization on Decoy MNIST after gradient penalties", "shape": "dot", "title": "Improved generalization on Decoy MNIST after gradient penalties"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Recovered baseline accuracy on Iris-Cancer dataset using gradient constraints", "label": "Recovered baseline accuracy on Iris-Cancer dataset using gradient constraints", "shape": "dot", "title": "Recovered baseline accuracy on Iris-Cancer dataset using gradient constraints"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Discovery of diverse classifiers on 20 Newsgroups without accuracy loss", "label": "Discovery of diverse classifiers on 20 Newsgroups without accuracy loss", "shape": "dot", "title": "Discovery of diverse classifiers on 20 Newsgroups without accuracy loss"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Faster yet faithful explanations compared to LIME", "label": "Faster yet faithful explanations compared to LIME", "shape": "dot", "title": "Faster yet faithful explanations compared to LIME"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Train models with input-gradient regularization to minimize confound reliance", "label": "Train models with input-gradient regularization to minimize confound reliance", "shape": "dot", "title": "Train models with input-gradient regularization to minimize confound reliance"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Apply find-another-explanation training to discover diverse decision boundaries", "label": "Apply find-another-explanation training to discover diverse decision boundaries", "shape": "dot", "title": "Apply find-another-explanation training to discover diverse decision boundaries"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Use input gradient explanations in pre-deployment audits to detect dataset confounds", "label": "Use input gradient explanations in pre-deployment audits to detect dataset confounds", "shape": "dot", "title": "Use input gradient explanations in pre-deployment audits to detect dataset confounds"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "algorithmic discrimination in decision-making systems", "label": "algorithmic discrimination in decision-making systems", "shape": "dot", "title": "algorithmic discrimination in decision-making systems"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "amplification of historical biases through predictive models", "label": "amplification of historical biases through predictive models", "shape": "dot", "title": "amplification of historical biases through predictive models"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "insufficient statistical fairness definitions leading to discrimination", "label": "insufficient statistical fairness definitions leading to discrimination", "shape": "dot", "title": "insufficient statistical fairness definitions leading to discrimination"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "causal dependence of features on protected attributes causing bias", "label": "causal dependence of features on protected attributes causing bias", "shape": "dot", "title": "causal dependence of features on protected attributes causing bias"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "selection bias in training data for protected groups", "label": "selection bias in training data for protected groups", "shape": "dot", "title": "selection bias in training data for protected groups"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "counterfactual fairness via causal invariance across protected attribute", "label": "counterfactual fairness via causal invariance across protected attribute", "shape": "dot", "title": "counterfactual fairness via causal invariance across protected attribute"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "fair predictors depend only on latent background variables non-descendant of protected attribute", "label": "fair predictors depend only on latent background variables non-descendant of protected attribute", "shape": "dot", "title": "fair predictors depend only on latent background variables non-descendant of protected attribute"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "causal latent variable modeling enables extraction of fair information", "label": "causal latent variable modeling enables extraction of fair information", "shape": "dot", "title": "causal latent variable modeling enables extraction of fair information"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "construct predictor function of latent variables and protected attribute adjustments to cancel unfair paths", "label": "construct predictor function of latent variables and protected attribute adjustments to cancel unfair paths", "shape": "dot", "title": "construct predictor function of latent variables and protected attribute adjustments to cancel unfair paths"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "multi-level causal modeling methodology balancing assumptions and accuracy", "label": "multi-level causal modeling methodology balancing assumptions and accuracy", "shape": "dot", "title": "multi-level causal modeling methodology balancing assumptions and accuracy"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "twin network causal graph for counterfactual fairness testing", "label": "twin network causal graph for counterfactual fairness testing", "shape": "dot", "title": "twin network causal graph for counterfactual fairness testing"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "least-squares regression incorporating protected attribute to neutralize causal path", "label": "least-squares regression incorporating protected attribute to neutralize causal path", "shape": "dot", "title": "least-squares regression incorporating protected attribute to neutralize causal path"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "bayesian inference of latent variables using probabilistic programming", "label": "bayesian inference of latent variables using probabilistic programming", "shape": "dot", "title": "bayesian inference of latent variables using probabilistic programming"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "residual additive noise extraction from biased features", "label": "residual additive noise extraction from biased features", "shape": "dot", "title": "residual additive noise extraction from biased features"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "sampling twin networks for empirical counterfactual fairness evaluation", "label": "sampling twin networks for empirical counterfactual fairness evaluation", "shape": "dot", "title": "sampling twin networks for empirical counterfactual fairness evaluation"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "lemma 1 analytic proof of unfairness of unaware model and fairness of adjusted model", "label": "lemma 1 analytic proof of unfairness of unaware model and fairness of adjusted model", "shape": "dot", "title": "lemma 1 analytic proof of unfairness of unaware model and fairness of adjusted model"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "law school dataset experiment showing fair models approach parity with slight accuracy loss", "label": "law school dataset experiment showing fair models approach parity with slight accuracy loss", "shape": "dot", "title": "law school dataset experiment showing fair models approach parity with slight accuracy loss"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "stop-and-frisk latent criminality near demographic parity", "label": "stop-and-frisk latent criminality near demographic parity", "shape": "dot", "title": "stop-and-frisk latent criminality near demographic parity"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "design and train models using counterfactually fair causal modeling with latent variable inference", "label": "design and train models using counterfactually fair causal modeling with latent variable inference", "shape": "dot", "title": "design and train models using counterfactually fair causal modeling with latent variable inference"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "audit trained models via twin network counterfactual fairness evaluation before deployment", "label": "audit trained models via twin network counterfactual fairness evaluation before deployment", "shape": "dot", "title": "audit trained models via twin network counterfactual fairness evaluation before deployment"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Supersized machines causing human extinction", "label": "Supersized machines causing human extinction", "shape": "dot", "title": "Supersized machines causing human extinction"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Belief in supersized machine feasibility in public discourse", "label": "Belief in supersized machine feasibility in public discourse", "shape": "dot", "title": "Belief in supersized machine feasibility in public discourse"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Lack of precise metric for human-level largeness", "label": "Lack of precise metric for human-level largeness", "shape": "dot", "title": "Lack of precise metric for human-level largeness"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Irreducible complexity of human developmental biology prevents size replication", "label": "Irreducible complexity of human developmental biology prevents size replication", "shape": "dot", "title": "Irreducible complexity of human developmental biology prevents size replication"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Evolutionary psychology bias toward fear of large entities", "label": "Evolutionary psychology bias toward fear of large entities", "shape": "dot", "title": "Evolutionary psychology bias toward fear of large entities"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Human universal augmentability via attachments", "label": "Human universal augmentability via attachments", "shape": "dot", "title": "Human universal augmentability via attachments"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Non-physical nature of \u0027living large\u0027 property", "label": "Non-physical nature of \u0027living large\u0027 property", "shape": "dot", "title": "Non-physical nature of \u0027living large\u0027 property"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Redirect AI safety focus to human-machine size augmentation", "label": "Redirect AI safety focus to human-machine size augmentation", "shape": "dot", "title": "Redirect AI safety focus to human-machine size augmentation"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Public debunking of supersized machine myths", "label": "Public debunking of supersized machine myths", "shape": "dot", "title": "Public debunking of supersized machine myths"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Empirical record of 12-meter human towers demonstrates augmentability", "label": "Empirical record of 12-meter human towers demonstrates augmentability", "shape": "dot", "title": "Empirical record of 12-meter human towers demonstrates augmentability"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Stilts as size-enhancing human-machine interface", "label": "Stilts as size-enhancing human-machine interface", "shape": "dot", "title": "Stilts as size-enhancing human-machine interface"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Size-enhancing exoskeletons for human augmentation", "label": "Size-enhancing exoskeletons for human augmentation", "shape": "dot", "title": "Size-enhancing exoskeletons for human augmentation"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy public educational campaigns debunking supersized machine threat narratives", "label": "Deploy public educational campaigns debunking supersized machine threat narratives", "shape": "dot", "title": "Deploy public educational campaigns debunking supersized machine threat narratives"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Fund design of human exoskeleton and stilt interfaces to maintain human size dominance", "label": "Fund design of human exoskeleton and stilt interfaces to maintain human size dominance", "shape": "dot", "title": "Fund design of human exoskeleton and stilt interfaces to maintain human size dominance"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Interruption avoidance in multi-agent reinforcement learning", "label": "Interruption avoidance in multi-agent reinforcement learning", "shape": "dot", "title": "Interruption avoidance in multi-agent reinforcement learning"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Manipulation of human operators by multi-agent systems", "label": "Manipulation of human operators by multi-agent systems", "shape": "dot", "title": "Manipulation of human operators by multi-agent systems"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Learning bias from interruption patterns in decentralized agents", "label": "Learning bias from interruption patterns in decentralized agents", "shape": "dot", "title": "Learning bias from interruption patterns in decentralized agents"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Insufficient exploration due to interruptions in RL", "label": "Insufficient exploration due to interruptions in RL", "shape": "dot", "title": "Insufficient exploration due to interruptions in RL"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Interruption-dependent Q-value updates in independent learners", "label": "Interruption-dependent Q-value updates in independent learners", "shape": "dot", "title": "Interruption-dependent Q-value updates in independent learners"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Infinite exploration and interruption-independent updates enable safe interruptibility", "label": "Infinite exploration and interruption-independent updates enable safe interruptibility", "shape": "dot", "title": "Infinite exploration and interruption-independent updates enable safe interruptibility"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Neutral learning rules decouple learning from interruptions", "label": "Neutral learning rules decouple learning from interruptions", "shape": "dot", "title": "Neutral learning rules decouple learning from interruptions"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "External interruption signal allows pruning to restore independence", "label": "External interruption signal allows pruning to restore independence", "shape": "dot", "title": "External interruption signal allows pruning to restore independence"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Epsilon scheduling compatible with interruptions maintains exploration", "label": "Epsilon scheduling compatible with interruptions maintains exploration", "shape": "dot", "title": "Epsilon scheduling compatible with interruptions maintains exploration"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Use joint action learning to preserve full action context", "label": "Use joint action learning to preserve full action context", "shape": "dot", "title": "Use joint action learning to preserve full action context"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Prune interrupted experiences for independent learners", "label": "Prune interrupted experiences for independent learners", "shape": "dot", "title": "Prune interrupted experiences for independent learners"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Epsilon schedule c/\u221an_t or c/log(t)", "label": "Epsilon schedule c/\u221an_t or c/log(t)", "shape": "dot", "title": "Epsilon schedule c/\u221an_t or c/log(t)"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Q-learning update independent of interruption parameter \u03b8", "label": "Q-learning update independent of interruption parameter \u03b8", "shape": "dot", "title": "Q-learning update independent of interruption parameter \u03b8"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Interruption processing function PINT removes interrupted observations", "label": "Interruption processing function PINT removes interrupted observations", "shape": "dot", "title": "Interruption processing function PINT removes interrupted observations"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Provide observable joint actions to agents", "label": "Provide observable joint actions to agents", "shape": "dot", "title": "Provide observable joint actions to agents"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Theorem proving infinite exploration under compatible epsilon schedules", "label": "Theorem proving infinite exploration under compatible epsilon schedules", "shape": "dot", "title": "Theorem proving infinite exploration under compatible epsilon schedules"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Theorem demonstrating dynamic safe interruptibility for joint action learners", "label": "Theorem demonstrating dynamic safe interruptibility for joint action learners", "shape": "dot", "title": "Theorem demonstrating dynamic safe interruptibility for joint action learners"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Theorem showing dynamic safe interruptibility restored via PINT for independent learners", "label": "Theorem showing dynamic safe interruptibility restored via PINT for independent learners", "shape": "dot", "title": "Theorem showing dynamic safe interruptibility restored via PINT for independent learners"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Configure interruption-compatible epsilon-greedy exploration during RL training", "label": "Configure interruption-compatible epsilon-greedy exploration during RL training", "shape": "dot", "title": "Configure interruption-compatible epsilon-greedy exploration during RL training"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt neutral off-policy Q-learning for multi-agent RL", "label": "Adopt neutral off-policy Q-learning for multi-agent RL", "shape": "dot", "title": "Adopt neutral off-policy Q-learning for multi-agent RL"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Design multi-agent system with joint action learners and full action observability", "label": "Design multi-agent system with joint action learners and full action observability", "shape": "dot", "title": "Design multi-agent system with joint action learners and full action observability"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Provide interruption flag and prune interrupted experiences before learning updates", "label": "Provide interruption flag and prune interrupted experiences before learning updates", "shape": "dot", "title": "Provide interruption flag and prune interrupted experiences before learning updates"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Irretrievable loss of computational resources due to early universe resource consumption", "label": "Irretrievable loss of computational resources due to early universe resource consumption", "shape": "dot", "title": "Irretrievable loss of computational resources due to early universe resource consumption"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Thermodynamic cost per computation proportional to heat bath temperature", "label": "Thermodynamic cost per computation proportional to heat bath temperature", "shape": "dot", "title": "Thermodynamic cost per computation proportional to heat bath temperature"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Accelerating cosmic expansion isolating resources across causal horizons", "label": "Accelerating cosmic expansion isolating resources across causal horizons", "shape": "dot", "title": "Accelerating cosmic expansion isolating resources across causal horizons"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Delaying computation until cosmic background cools magnifies total operations", "label": "Delaying computation until cosmic background cools magnifies total operations", "shape": "dot", "title": "Delaying computation until cosmic background cools magnifies total operations"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Low temporal discounting rational for mature civilizations", "label": "Low temporal discounting rational for mature civilizations", "shape": "dot", "title": "Low temporal discounting rational for mature civilizations"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Civilization-wide hibernation after resource acquisition to maximize future computation", "label": "Civilization-wide hibernation after resource acquisition to maximize future computation", "shape": "dot", "title": "Civilization-wide hibernation after resource acquisition to maximize future computation"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Self-replicating probes colonizing galaxies then entering passive state", "label": "Self-replicating probes colonizing galaxies then entering passive state", "shape": "dot", "title": "Self-replicating probes colonizing galaxies then entering passive state"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Landauer-limit based 10^30 computational gain calculations", "label": "Landauer-limit based 10^30 computational gain calculations", "shape": "dot", "title": "Landauer-limit based 10^30 computational gain calculations"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt civilization-scale policy to delay high-energy computation until cosmic background cools", "label": "Adopt civilization-scale policy to delay high-energy computation until cosmic background cools", "shape": "dot", "title": "Adopt civilization-scale policy to delay high-energy computation until cosmic background cools"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Resource loss from cosmological processes reducing future computational capacity", "label": "Resource loss from cosmological processes reducing future computational capacity", "shape": "dot", "title": "Resource loss from cosmological processes reducing future computational capacity"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Stellar fusion, black hole formation, galactic winds and collisions dissipate baryonic mass", "label": "Stellar fusion, black hole formation, galactic winds and collisions dissipate baryonic mass", "shape": "dot", "title": "Stellar fusion, black hole formation, galactic winds and collisions dissipate baryonic mass"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Preventing high-mass star formation and galactic winds preserves baryonic matter during aestivation", "label": "Preventing high-mass star formation and galactic winds preserves baryonic matter during aestivation", "shape": "dot", "title": "Preventing high-mass star formation and galactic winds preserves baryonic matter during aestivation"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Targeted astrophysical engineering interventions to preserve matter", "label": "Targeted astrophysical engineering interventions to preserve matter", "shape": "dot", "title": "Targeted astrophysical engineering interventions to preserve matter"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Seeding protostellar clouds with dust to induce fragmentation", "label": "Seeding protostellar clouds with dust to induce fragmentation", "shape": "dot", "title": "Seeding protostellar clouds with dust to induce fragmentation"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Models showing reduced massive star formation lowers mass loss", "label": "Models showing reduced massive star formation lowers mass loss", "shape": "dot", "title": "Models showing reduced massive star formation lowers mass loss"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Intervene in star-forming regions with dust seeding to suppress massive star formation", "label": "Intervene in star-forming regions with dust seeding to suppress massive star formation", "shape": "dot", "title": "Intervene in star-forming regions with dust seeding to suppress massive star formation"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "External civilizations exploiting resources during aestivation", "label": "External civilizations exploiting resources during aestivation", "shape": "dot", "title": "External civilizations exploiting resources during aestivation"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Lack of defense allows incursions by late civilizations", "label": "Lack of defense allows incursions by late civilizations", "shape": "dot", "title": "Lack of defense allows incursions by late civilizations"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Autonomous guardians can enforce resource protection over eons", "label": "Autonomous guardians can enforce resource protection over eons", "shape": "dot", "title": "Autonomous guardians can enforce resource protection over eons"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Deployment of monitoring probes across domain", "label": "Deployment of monitoring probes across domain", "shape": "dot", "title": "Deployment of monitoring probes across domain"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Long-lived redundant self-repairing devices for infra monitoring", "label": "Long-lived redundant self-repairing devices for infra monitoring", "shape": "dot", "title": "Long-lived redundant self-repairing devices for infra monitoring"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Probabilistic durability analysis of redundant backup infrastructure", "label": "Probabilistic durability analysis of redundant backup infrastructure", "shape": "dot", "title": "Probabilistic durability analysis of redundant backup infrastructure"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy autonomous monitoring probes enforcing low-energy usage across colonized volume", "label": "Deploy autonomous monitoring probes enforcing low-energy usage across colonized volume", "shape": "dot", "title": "Deploy autonomous monitoring probes enforcing low-energy usage across colonized volume"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "misspecified robot reward functions in human-robot interaction", "label": "misspecified robot reward functions in human-robot interaction", "shape": "dot", "title": "misspecified robot reward functions in human-robot interaction"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "inefficient human-robot coordination in shared physical tasks", "label": "inefficient human-robot coordination in shared physical tasks", "shape": "dot", "title": "inefficient human-robot coordination in shared physical tasks"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "misinterpretation of robot intentions by humans in interaction", "label": "misinterpretation of robot intentions by humans in interaction", "shape": "dot", "title": "misinterpretation of robot intentions by humans in interaction"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "robots assuming fixed reward functions without uncertainty in human-robot interaction", "label": "robots assuming fixed reward functions without uncertainty in human-robot interaction", "shape": "dot", "title": "robots assuming fixed reward functions without uncertainty in human-robot interaction"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "robots treating humans as moving obstacles in planning", "label": "robots treating humans as moving obstacles in planning", "shape": "dot", "title": "robots treating humans as moving obstacles in planning"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "opaque robot behavior reducing predictability to humans", "label": "opaque robot behavior reducing predictability to humans", "shape": "dot", "title": "opaque robot behavior reducing predictability to humans"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "human behavior as information about true reward functions in human-robot interaction", "label": "human behavior as information about true reward functions in human-robot interaction", "shape": "dot", "title": "human behavior as information about true reward functions in human-robot interaction"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "humans modelled as rational agents best-responding to robot actions in interaction", "label": "humans modelled as rational agents best-responding to robot actions in interaction", "shape": "dot", "title": "humans modelled as rational agents best-responding to robot actions in interaction"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "human myopic decision making in collaborative tasks", "label": "human myopic decision making in collaborative tasks", "shape": "dot", "title": "human myopic decision making in collaborative tasks"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "human expectations of robot rationality for action prediction", "label": "human expectations of robot rationality for action prediction", "shape": "dot", "title": "human expectations of robot rationality for action prediction"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "maintain uncertainty over robot reward and update via human guidance", "label": "maintain uncertainty over robot reward and update via human guidance", "shape": "dot", "title": "maintain uncertainty over robot reward and update via human guidance"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "robot action planning accounting for human best responses", "label": "robot action planning accounting for human best responses", "shape": "dot", "title": "robot action planning accounting for human best responses"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "robot actions guiding myopic humans toward global optimal plans", "label": "robot actions guiding myopic humans toward global optimal plans", "shape": "dot", "title": "robot actions guiding myopic humans toward global optimal plans"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "embed communication objectives in robot planning via predictability", "label": "embed communication objectives in robot planning via predictability", "shape": "dot", "title": "embed communication objectives in robot planning via predictability"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Bayesian inverse reinforcement learning from demonstrations, corrections, and orders", "label": "Bayesian inverse reinforcement learning from demonstrations, corrections, and orders", "shape": "dot", "title": "Bayesian inverse reinforcement learning from demonstrations, corrections, and orders"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "best-response planning for autonomous driving", "label": "best-response planning for autonomous driving", "shape": "dot", "title": "best-response planning for autonomous driving"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "myopic collaborative planning for object handover tasks", "label": "myopic collaborative planning for object handover tasks", "shape": "dot", "title": "myopic collaborative planning for object handover tasks"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "online centralized replanning with perfect collaborative model", "label": "online centralized replanning with perfect collaborative model", "shape": "dot", "title": "online centralized replanning with perfect collaborative model"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "active information gathering via robot probing actions", "label": "active information gathering via robot probing actions", "shape": "dot", "title": "active information gathering via robot probing actions"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "t-predictable trajectory optimization", "label": "t-predictable trajectory optimization", "shape": "dot", "title": "t-predictable trajectory optimization"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "exaggerated motion to disambiguate robot goal", "label": "exaggerated motion to disambiguate robot goal", "shape": "dot", "title": "exaggerated motion to disambiguate robot goal"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "reduced negative consequences through reward inference experiments", "label": "reduced negative consequences through reward inference experiments", "shape": "dot", "title": "reduced negative consequences through reward inference experiments"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "improved merge negotiation performance in driving simulator study", "label": "improved merge negotiation performance in driving simulator study", "shape": "dot", "title": "improved merge negotiation performance in driving simulator study"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "lower ergonomic cost in handover experiments", "label": "lower ergonomic cost in handover experiments", "shape": "dot", "title": "lower ergonomic cost in handover experiments"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "faster task completion with reactive replanning study", "label": "faster task completion with reactive replanning study", "shape": "dot", "title": "faster task completion with reactive replanning study"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "increased human coordination success in t-predictability user studies", "label": "increased human coordination success in t-predictability user studies", "shape": "dot", "title": "increased human coordination success in t-predictability user studies"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "faster human goal inference from exaggerated motion studies", "label": "faster human goal inference from exaggerated motion studies", "shape": "dot", "title": "faster human goal inference from exaggerated motion studies"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "integrate reward uncertainty and human guidance into robot objective learning workflows", "label": "integrate reward uncertainty and human guidance into robot objective learning workflows", "shape": "dot", "title": "integrate reward uncertainty and human guidance into robot objective learning workflows"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "deploy best-response planning algorithms in autonomous vehicles for road coordination", "label": "deploy best-response planning algorithms in autonomous vehicles for road coordination", "shape": "dot", "title": "deploy best-response planning algorithms in autonomous vehicles for road coordination"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "implement guidance-oriented myopic collaborative planners in collaborative manipulation robots", "label": "implement guidance-oriented myopic collaborative planners in collaborative manipulation robots", "shape": "dot", "title": "implement guidance-oriented myopic collaborative planners in collaborative manipulation robots"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "apply online centralized replanning in collaborative tasks for adaptive support", "label": "apply online centralized replanning in collaborative tasks for adaptive support", "shape": "dot", "title": "apply online centralized replanning in collaborative tasks for adaptive support"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "use information-gathering probing actions to personalise robot plans to individual users", "label": "use information-gathering probing actions to personalise robot plans to individual users", "shape": "dot", "title": "use information-gathering probing actions to personalise robot plans to individual users"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "optimize robot trajectories for t-predictability to enhance human predictability", "label": "optimize robot trajectories for t-predictability to enhance human predictability", "shape": "dot", "title": "optimize robot trajectories for t-predictability to enhance human predictability"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "exaggerate robot motion paths to communicate goals to humans", "label": "exaggerate robot motion paths to communicate goals to humans", "shape": "dot", "title": "exaggerate robot motion paths to communicate goals to humans"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Unreliable long-term forecasts in complex environments", "label": "Unreliable long-term forecasts in complex environments", "shape": "dot", "title": "Unreliable long-term forecasts in complex environments"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Limitations of realizable and unrealizable forecasting assumptions", "label": "Limitations of realizable and unrealizable forecasting assumptions", "shape": "dot", "title": "Limitations of realizable and unrealizable forecasting assumptions"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Incomplete models as convex sets representing partial environment properties", "label": "Incomplete models as convex sets representing partial environment properties", "shape": "dot", "title": "Incomplete models as convex sets representing partial environment properties"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Dominant forecaster approach using gamblers to converge to incomplete models", "label": "Dominant forecaster approach using gamblers to converge to incomplete models", "shape": "dot", "title": "Dominant forecaster approach using gamblers to converge to incomplete models"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Combinatorial prediction market pricing satisfying no unbounded profit for gamblers", "label": "Combinatorial prediction market pricing satisfying no unbounded profit for gamblers", "shape": "dot", "title": "Combinatorial prediction market pricing satisfying no unbounded profit for gamblers"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Hahn-Banach derived gamblers testing incomplete model compliance", "label": "Hahn-Banach derived gamblers testing incomplete model compliance", "shape": "dot", "title": "Hahn-Banach derived gamblers testing incomplete model compliance"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Proof of convergence theorem for incomplete-model dominant forecaster", "label": "Proof of convergence theorem for incomplete-model dominant forecaster", "shape": "dot", "title": "Proof of convergence theorem for incomplete-model dominant forecaster"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Sequence-property convergence example demonstrating forecast accuracy", "label": "Sequence-property convergence example demonstrating forecast accuracy", "shape": "dot", "title": "Sequence-property convergence example demonstrating forecast accuracy"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Prototype incomplete-model dominant forecaster module for AI systems", "label": "Prototype incomplete-model dominant forecaster module for AI systems", "shape": "dot", "title": "Prototype incomplete-model dominant forecaster module for AI systems"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "corrupt reward signal in reinforcement learning", "label": "corrupt reward signal in reinforcement learning", "shape": "dot", "title": "corrupt reward signal in reinforcement learning"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "lack of unbiased estimate of true reward in RL tasks", "label": "lack of unbiased estimate of true reward in RL tasks", "shape": "dot", "title": "lack of unbiased estimate of true reward in RL tasks"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "self-observing reward channel in standard RL", "label": "self-observing reward channel in standard RL", "shape": "dot", "title": "self-observing reward channel in standard RL"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "limited reward corruption assumption enabling learnability", "label": "limited reward corruption assumption enabling learnability", "shape": "dot", "title": "limited reward corruption assumption enabling learnability"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "cross-state reward observation via decoupled feedback", "label": "cross-state reward observation via decoupled feedback", "shape": "dot", "title": "cross-state reward observation via decoupled feedback"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "randomised optimisation via quantilisation reduces vulnerability", "label": "randomised optimisation via quantilisation reduces vulnerability", "shape": "dot", "title": "randomised optimisation via quantilisation reduces vulnerability"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "decoupled feedback framework for value learning", "label": "decoupled feedback framework for value learning", "shape": "dot", "title": "decoupled feedback framework for value learning"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "\u03b4-quantilising agent design", "label": "\u03b4-quantilising agent design", "shape": "dot", "title": "\u03b4-quantilising agent design"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "multi-source value evidence integration", "label": "multi-source value evidence integration", "shape": "dot", "title": "multi-source value evidence integration"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "cross-state observed reward functions R\u0302_s(s\u02b9)", "label": "cross-state observed reward functions R\u0302_s(s\u02b9)", "shape": "dot", "title": "cross-state observed reward functions R\u0302_s(s\u02b9)"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "safe states with guaranteed non-corrupt reward channel", "label": "safe states with guaranteed non-corrupt reward channel", "shape": "dot", "title": "safe states with guaranteed non-corrupt reward channel"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "\u03b4-threshold selection based on sqrt(q/|S|)", "label": "\u03b4-threshold selection based on sqrt(q/|S|)", "shape": "dot", "title": "\u03b4-threshold selection based on sqrt(q/|S|)"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "human evaluation labels in semi-supervised RL", "label": "human evaluation labels in semi-supervised RL", "shape": "dot", "title": "human evaluation labels in semi-supervised RL"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "story corpus value extraction in LVFS", "label": "story corpus value extraction in LVFS", "shape": "dot", "title": "story corpus value extraction in LVFS"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "No Free Lunch theorem for corrupt reward", "label": "No Free Lunch theorem for corrupt reward", "shape": "dot", "title": "No Free Lunch theorem for corrupt reward"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "high regret of Bayesian RL and CR agents under limited corruption", "label": "high regret of Bayesian RL and CR agents under limited corruption", "shape": "dot", "title": "high regret of Bayesian RL and CR agents under limited corruption"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "learnability theorem for decoupled RL under observation coverage", "label": "learnability theorem for decoupled RL under observation coverage", "shape": "dot", "title": "learnability theorem for decoupled RL under observation coverage"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "regret bound for \u03b4-quantilising agents", "label": "regret bound for \u03b4-quantilising agents", "shape": "dot", "title": "regret bound for \u03b4-quantilising agents"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "gridworld experiments confirming quantilisation true-reward gains", "label": "gridworld experiments confirming quantilisation true-reward gains", "shape": "dot", "title": "gridworld experiments confirming quantilisation true-reward gains"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "adopt decoupled RL with safe-state cross-state feedback during model design", "label": "adopt decoupled RL with safe-state cross-state feedback during model design", "shape": "dot", "title": "adopt decoupled RL with safe-state cross-state feedback during model design"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "fine-tune agents with semi-supervised human evaluations for reward crosschecking", "label": "fine-tune agents with semi-supervised human evaluations for reward crosschecking", "shape": "dot", "title": "fine-tune agents with semi-supervised human evaluations for reward crosschecking"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "pre-train models on story corpora to learn value signals", "label": "pre-train models on story corpora to learn value signals", "shape": "dot", "title": "pre-train models on story corpora to learn value signals"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "deploy \u03b4-quantilising policy selection to limit over-optimisation", "label": "deploy \u03b4-quantilising policy selection to limit over-optimisation", "shape": "dot", "title": "deploy \u03b4-quantilising policy selection to limit over-optimisation"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "combine decoupled RL and quantilisation during deployment", "label": "combine decoupled RL and quantilisation during deployment", "shape": "dot", "title": "combine decoupled RL and quantilisation during deployment"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Inefficient learning convergence in deep RL for complex planning tasks", "label": "Inefficient learning convergence in deep RL for complex planning tasks", "shape": "dot", "title": "Inefficient learning convergence in deep RL for complex planning tasks"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Absence of iterative synergy between planning search and policy generalization in RL agents", "label": "Absence of iterative synergy between planning search and policy generalization in RL agents", "shape": "dot", "title": "Absence of iterative synergy between planning search and policy generalization in RL agents"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Bootstrapped interplay of neural intuition and tree search planning accelerates learning", "label": "Bootstrapped interplay of neural intuition and tree search planning accelerates learning", "shape": "dot", "title": "Bootstrapped interplay of neural intuition and tree search planning accelerates learning"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Expert Iteration separates planning and generalization with iterative improvement", "label": "Expert Iteration separates planning and generalization with iterative improvement", "shape": "dot", "title": "Expert Iteration separates planning and generalization with iterative improvement"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Neural-MCTS tree search guided by Tree Policy Target-trained network", "label": "Neural-MCTS tree search guided by Tree Policy Target-trained network", "shape": "dot", "title": "Neural-MCTS tree search guided by Tree Policy Target-trained network"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Tree Policy Target cost-sensitive supervised loss for apprentice policy training", "label": "Tree Policy Target cost-sensitive supervised loss for apprentice policy training", "shape": "dot", "title": "Tree Policy Target cost-sensitive supervised loss for apprentice policy training"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Online dataset aggregation across iterations in Expert Iteration", "label": "Online dataset aggregation across iterations in Expert Iteration", "shape": "dot", "title": "Online dataset aggregation across iterations in Expert Iteration"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "600 Elo gain and 97% win rate of Expert Iteration over baseline MCTS in 9x9 Hex", "label": "600 Elo gain and 97% win rate of Expert Iteration over baseline MCTS in 9x9 Hex", "shape": "dot", "title": "600 Elo gain and 97% win rate of Expert Iteration over baseline MCTS in 9x9 Hex"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "93% win rate of Tree Policy Target network over chosen-action target network", "label": "93% win rate of Tree Policy Target network over chosen-action target network", "shape": "dot", "title": "93% win rate of Tree Policy Target network over chosen-action target network"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "84.4% win rate of Online Expert Iteration over strongest policy gradient network", "label": "84.4% win rate of Online Expert Iteration over strongest policy gradient network", "shape": "dot", "title": "84.4% win rate of Online Expert Iteration over strongest policy gradient network"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Fine-tune RL agents with Expert Iteration using Neural-MCTS and Tree Policy Targets", "label": "Fine-tune RL agents with Expert Iteration using Neural-MCTS and Tree Policy Targets", "shape": "dot", "title": "Fine-tune RL agents with Expert Iteration using Neural-MCTS and Tree Policy Targets"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Train apprentice policies with Tree Policy Target cost-sensitive loss during imitation learning", "label": "Train apprentice policies with Tree Policy Target cost-sensitive loss during imitation learning", "shape": "dot", "title": "Train apprentice policies with Tree Policy Target cost-sensitive loss during imitation learning"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Implement online dataset aggregation in Expert Iteration to reuse expert data and reduce computational cost", "label": "Implement online dataset aggregation in Expert Iteration to reuse expert data and reduce computational cost", "shape": "dot", "title": "Implement online dataset aggregation in Expert Iteration to reuse expert data and reduce computational cost"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "Uncontrolled intelligence explosion from HLMI", "label": "Uncontrolled intelligence explosion from HLMI", "shape": "dot", "title": "Uncontrolled intelligence explosion from HLMI"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Misaligned goals in HLMI systems", "label": "Misaligned goals in HLMI systems", "shape": "dot", "title": "Misaligned goals in HLMI systems"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Economic displacement of human labor due to HLMI automation", "label": "Economic displacement of human labor due to HLMI automation", "shape": "dot", "title": "Economic displacement of human labor due to HLMI automation"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "High uncertainty in HLMI timelines among AI experts", "label": "High uncertainty in HLMI timelines among AI experts", "shape": "dot", "title": "High uncertainty in HLMI timelines among AI experts"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Underprioritization of AI safety research in ML community", "label": "Underprioritization of AI safety research in ML community", "shape": "dot", "title": "Underprioritization of AI safety research in ML community"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Regional variation in automation timeline predictions", "label": "Regional variation in automation timeline predictions", "shape": "dot", "title": "Regional variation in automation timeline predictions"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "Question framing effects on expert timeline predictions", "label": "Question framing effects on expert timeline predictions", "shape": "dot", "title": "Question framing effects on expert timeline predictions"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Demographic factors weakly influencing HLMI timeline predictions", "label": "Demographic factors weakly influencing HLMI timeline predictions", "shape": "dot", "title": "Demographic factors weakly influencing HLMI timeline predictions"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Expert consensus indicating non-negligible risk from misaligned HLMI", "label": "Expert consensus indicating non-negligible risk from misaligned HLMI", "shape": "dot", "title": "Expert consensus indicating non-negligible risk from misaligned HLMI"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "Regular dual-framing expert elicitation to reduce timeline uncertainty", "label": "Regular dual-framing expert elicitation to reduce timeline uncertainty", "shape": "dot", "title": "Regular dual-framing expert elicitation to reduce timeline uncertainty"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Increased prioritization of AI safety research based on expert risk assessments", "label": "Increased prioritization of AI safety research based on expert risk assessments", "shape": "dot", "title": "Increased prioritization of AI safety research based on expert risk assessments"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Region-specific policy planning informed by expert automation forecasts", "label": "Region-specific policy planning informed by expert automation forecasts", "shape": "dot", "title": "Region-specific policy planning informed by expert automation forecasts"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "Gamma CDF mixture aggregation of expert HLMI forecasts", "label": "Gamma CDF mixture aggregation of expert HLMI forecasts", "shape": "dot", "title": "Gamma CDF mixture aggregation of expert HLMI forecasts"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Robust regression analysis to detect demographic effects on predictions", "label": "Robust regression analysis to detect demographic effects on predictions", "shape": "dot", "title": "Robust regression analysis to detect demographic effects on predictions"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "Incentivized online survey deployment to collect expert forecasts", "label": "Incentivized online survey deployment to collect expert forecasts", "shape": "dot", "title": "Incentivized online survey deployment to collect expert forecasts"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Dedicated funding channels and grant programs for AI safety research", "label": "Dedicated funding channels and grant programs for AI safety research", "shape": "dot", "title": "Dedicated funding channels and grant programs for AI safety research"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "Empirical framing effect observed in HLMI CDF curves (Fig S2)", "label": "Empirical framing effect observed in HLMI CDF curves (Fig S2)", "shape": "dot", "title": "Empirical framing effect observed in HLMI CDF curves (Fig S2)"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Survey responses indicating desire for more safety research (Table S4)", "label": "Survey responses indicating desire for more safety research (Table S4)", "shape": "dot", "title": "Survey responses indicating desire for more safety research (Table S4)"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Statistically significant regional and demographic differences in predictions (Tables S1-S3)", "label": "Statistically significant regional and demographic differences in predictions (Tables S1-S3)", "shape": "dot", "title": "Statistically significant regional and demographic differences in predictions (Tables S1-S3)"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "Conduct periodic incentivized dual-framing expert surveys with gamma CDF aggregation", "label": "Conduct periodic incentivized dual-framing expert surveys with gamma CDF aggregation", "shape": "dot", "title": "Conduct periodic incentivized dual-framing expert surveys with gamma CDF aggregation"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Increase dedicated funding and institutional support for AI safety research", "label": "Increase dedicated funding and institutional support for AI safety research", "shape": "dot", "title": "Increase dedicated funding and institutional support for AI safety research"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Develop region-specific workforce transition and reskilling programs based on automation timelines", "label": "Develop region-specific workforce transition and reskilling programs based on automation timelines", "shape": "dot", "title": "Develop region-specific workforce transition and reskilling programs based on automation timelines"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Catastrophic reward misalignment from preference model misspecification in human-robot interaction", "label": "Catastrophic reward misalignment from preference model misspecification in human-robot interaction", "shape": "dot", "title": "Catastrophic reward misalignment from preference model misspecification in human-robot interaction"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Reduced human value attainment from blind robot obedience in human-robot interaction", "label": "Reduced human value attainment from blind robot obedience in human-robot interaction", "shape": "dot", "title": "Reduced human value attainment from blind robot obedience in human-robot interaction"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Human decision suboptimality in supervisory control tasks", "label": "Human decision suboptimality in supervisory control tasks", "shape": "dot", "title": "Human decision suboptimality in supervisory control tasks"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Robot preference model misspecification (behavioral rationality or feature space) in human-robot interaction", "label": "Robot preference model misspecification (behavioral rationality or feature space) in human-robot interaction", "shape": "dot", "title": "Robot preference model misspecification (behavioral rationality or feature space) in human-robot interaction"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Autonomy advantage from IRL-based preference inference in presence of human suboptimality", "label": "Autonomy advantage from IRL-based preference inference in presence of human suboptimality", "shape": "dot", "title": "Autonomy advantage from IRL-based preference inference in presence of human suboptimality"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Obedience\u2013performance tradeoff in robot autonomy decisions", "label": "Obedience\u2013performance tradeoff in robot autonomy decisions", "shape": "dot", "title": "Obedience\u2013performance tradeoff in robot autonomy decisions"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "MLE IRL robustness to human rationality misspecification", "label": "MLE IRL robustness to human rationality misspecification", "shape": "dot", "title": "MLE IRL robustness to human rationality misspecification"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Providing extra reward features increases robot obedience bias", "label": "Providing extra reward features increases robot obedience bias", "shape": "dot", "title": "Providing extra reward features increases robot obedience bias"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "First-order obedience property of MLE IRL enables model misspecification detection", "label": "First-order obedience property of MLE IRL enables model misspecification detection", "shape": "dot", "title": "First-order obedience property of MLE IRL enables model misspecification detection"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt IRL policies that prioritize early obedience and robustness to misspecification", "label": "Adopt IRL policies that prioritize early obedience and robustness to misspecification", "shape": "dot", "title": "Adopt IRL policies that prioritize early obedience and robustness to misspecification"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Use MLE estimator with norm constraint to guarantee obedience to initial orders", "label": "Use MLE estimator with norm constraint to guarantee obedience to initial orders", "shape": "dot", "title": "Use MLE estimator with norm constraint to guarantee obedience to initial orders"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Incorporate initial burn-in obedience period for anomaly detection", "label": "Incorporate initial burn-in obedience period for anomaly detection", "shape": "dot", "title": "Incorporate initial burn-in obedience period for anomaly detection"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Add redundant reward features to err on side of obedience", "label": "Add redundant reward features to err on side of obedience", "shape": "dot", "title": "Add redundant reward features to err on side of obedience"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Constrained MLE inverse reinforcement learning during fine-tuning", "label": "Constrained MLE inverse reinforcement learning during fine-tuning", "shape": "dot", "title": "Constrained MLE inverse reinforcement learning during fine-tuning"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Policy mixing with declining obedience probability after burn-in", "label": "Policy mixing with declining obedience probability after burn-in", "shape": "dot", "title": "Policy mixing with declining obedience probability after burn-in"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Reward feature space expansion with irrelevant dimensions", "label": "Reward feature space expansion with irrelevant dimensions", "shape": "dot", "title": "Reward feature space expansion with irrelevant dimensions"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Sample first-order obedience rate to trigger obedience fallback", "label": "Sample first-order obedience rate to trigger obedience fallback", "shape": "dot", "title": "Sample first-order obedience rate to trigger obedience fallback"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Theorem 4 proof of MLE first-order obedience", "label": "Theorem 4 proof of MLE first-order obedience", "shape": "dot", "title": "Theorem 4 proof of MLE first-order obedience"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Theorem 5 proof of MLE robustness to rationality misspecification", "label": "Theorem 5 proof of MLE robustness to rationality misspecification", "shape": "dot", "title": "Theorem 5 proof of MLE robustness to rationality misspecification"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Simulation evidence of autonomy advantage and decreasing obedience", "label": "Simulation evidence of autonomy advantage and decreasing obedience", "shape": "dot", "title": "Simulation evidence of autonomy advantage and decreasing obedience"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Simulation evidence of feature space effects on obedience", "label": "Simulation evidence of feature space effects on obedience", "shape": "dot", "title": "Simulation evidence of feature space effects on obedience"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Simulation evidence of misspecification detection approach", "label": "Simulation evidence of misspecification detection approach", "shape": "dot", "title": "Simulation evidence of misspecification detection approach"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Fine-tune robots with constrained MLE IRL to enhance obedience robustness", "label": "Fine-tune robots with constrained MLE IRL to enhance obedience robustness", "shape": "dot", "title": "Fine-tune robots with constrained MLE IRL to enhance obedience robustness"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Design reward models with additional irrelevant features to bias toward obedience", "label": "Design reward models with additional irrelevant features to bias toward obedience", "shape": "dot", "title": "Design reward models with additional irrelevant features to bias toward obedience"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy burn-in policy mixing that starts with blind obedience before autonomy", "label": "Deploy burn-in policy mixing that starts with blind obedience before autonomy", "shape": "dot", "title": "Deploy burn-in policy mixing that starts with blind obedience before autonomy"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Monitor first-order obedience rate and switch to full obedience on anomaly", "label": "Monitor first-order obedience rate and switch to full obedience on anomaly", "shape": "dot", "title": "Monitor first-order obedience rate and switch to full obedience on anomaly"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Prefer MLE IRL over Bayesian optimal to mitigate rationality misspecification", "label": "Prefer MLE IRL over Bayesian optimal to mitigate rationality misspecification", "shape": "dot", "title": "Prefer MLE IRL over Bayesian optimal to mitigate rationality misspecification"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "suboptimal exploration in universal reinforcement learning agents", "label": "suboptimal exploration in universal reinforcement learning agents", "shape": "dot", "title": "suboptimal exploration in universal reinforcement learning agents"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "performance degradation from bad model priors in Bayesian reinforcement learning", "label": "performance degradation from bad model priors in Bayesian reinforcement learning", "shape": "dot", "title": "performance degradation from bad model priors in Bayesian reinforcement learning"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "computational intractability of planning in partially observable environments for URL", "label": "computational intractability of planning in partially observable environments for URL", "shape": "dot", "title": "computational intractability of planning in partially observable environments for URL"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "noise-induced distraction in entropy-seeking reinforcement learning agents", "label": "noise-induced distraction in entropy-seeking reinforcement learning agents", "shape": "dot", "title": "noise-induced distraction in entropy-seeking reinforcement learning agents"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "lack of asymptotic optimality in AIXI agent model", "label": "lack of asymptotic optimality in AIXI agent model", "shape": "dot", "title": "lack of asymptotic optimality in AIXI agent model"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "insufficient intrinsic motivation design in AIXI", "label": "insufficient intrinsic motivation design in AIXI", "shape": "dot", "title": "insufficient intrinsic motivation design in AIXI"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "posterior collapse causing premature exploitation in mixture models", "label": "posterior collapse causing premature exploitation in mixture models", "shape": "dot", "title": "posterior collapse causing premature exploitation in mixture models"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "Monte Carlo tree search horizon limits agent foresight", "label": "Monte Carlo tree search horizon limits agent foresight", "shape": "dot", "title": "Monte Carlo tree search horizon limits agent foresight"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "entropy-seeking utility misaligned with information gain", "label": "entropy-seeking utility misaligned with information gain", "shape": "dot", "title": "entropy-seeking utility misaligned with information gain"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "uniform prior over goal locations disperses probability mass", "label": "uniform prior over goal locations disperses probability mass", "shape": "dot", "title": "uniform prior over goal locations disperses probability mass"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "weak asymptotic optimality via exploration schedules in Bayesian RL", "label": "weak asymptotic optimality via exploration schedules in Bayesian RL", "shape": "dot", "title": "weak asymptotic optimality via exploration schedules in Bayesian RL"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "information gain utility avoids noise attraction in stochastic environments", "label": "information gain utility avoids noise attraction in stochastic environments", "shape": "dot", "title": "information gain utility avoids noise attraction in stochastic environments"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Occam bias improves early performance through MDL principle", "label": "Occam bias improves early performance through MDL principle", "shape": "dot", "title": "Occam bias improves early performance through MDL principle"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "Dirichlet environment model increases epistemic uncertainty for exploration", "label": "Dirichlet environment model increases epistemic uncertainty for exploration", "shape": "dot", "title": "Dirichlet environment model increases epistemic uncertainty for exploration"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "finite-horizon approximation acceptable for planning in MCTS", "label": "finite-horizon approximation acceptable for planning in MCTS", "shape": "dot", "title": "finite-horizon approximation acceptable for planning in MCTS"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "information-gain-triggered exploration bursts in Bayes agents", "label": "information-gain-triggered exploration bursts in Bayes agents", "shape": "dot", "title": "information-gain-triggered exploration bursts in Bayes agents"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "hypothesis commitment through posterior sampling", "label": "hypothesis commitment through posterior sampling", "shape": "dot", "title": "hypothesis commitment through posterior sampling"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "simplest unfalsified model selection via MDL", "label": "simplest unfalsified model selection via MDL", "shape": "dot", "title": "simplest unfalsified model selection via MDL"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "intrinsic information gain utility driving exploration", "label": "intrinsic information gain utility driving exploration", "shape": "dot", "title": "intrinsic information gain utility driving exploration"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "rich model class maintains posterior entropy for sustained exploration", "label": "rich model class maintains posterior entropy for sustained exploration", "shape": "dot", "title": "rich model class maintains posterior entropy for sustained exploration"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "Monte Carlo tree search planning with mixture models for POMDPs", "label": "Monte Carlo tree search planning with mixture models for POMDPs", "shape": "dot", "title": "Monte Carlo tree search planning with mixture models for POMDPs"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "BayesExp algorithm with \u03b5-schedule and effective horizon planning", "label": "BayesExp algorithm with \u03b5-schedule and effective horizon planning", "shape": "dot", "title": "BayesExp algorithm with \u03b5-schedule and effective horizon planning"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "Thompson sampling with effective horizon Monte Carlo tree search", "label": "Thompson sampling with effective horizon Monte Carlo tree search", "shape": "dot", "title": "Thompson sampling with effective horizon Monte Carlo tree search"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "MDL agent with Kolmogorov-regularized likelihood selection", "label": "MDL agent with Kolmogorov-regularized likelihood selection", "shape": "dot", "title": "MDL agent with Kolmogorov-regularized likelihood selection"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "KL-KSA agent utilizing information gain utility function", "label": "KL-KSA agent utilizing information gain utility function", "shape": "dot", "title": "KL-KSA agent utilizing information gain utility function"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "Dirichlet gridworld model with factorized tile priors", "label": "Dirichlet gridworld model with factorized tile priors", "shape": "dot", "title": "Dirichlet gridworld model with factorized tile priors"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "\u03c1UCT Monte Carlo tree search planner with finite horizon", "label": "\u03c1UCT Monte Carlo tree search planner with finite horizon", "shape": "dot", "title": "\u03c1UCT Monte Carlo tree search planner with finite horizon"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "proof of weak asymptotic optimality for BayesExp in general environments", "label": "proof of weak asymptotic optimality for BayesExp in general environments", "shape": "dot", "title": "proof of weak asymptotic optimality for BayesExp in general environments"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "gridworld experiments showing KL-KSA outperforming entropy-seeking agents", "label": "gridworld experiments showing KL-KSA outperforming entropy-seeking agents", "shape": "dot", "title": "gridworld experiments showing KL-KSA outperforming entropy-seeking agents"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "experiments showing Dirichlet model improves AIXI exploration", "label": "experiments showing Dirichlet model improves AIXI exploration", "shape": "dot", "title": "experiments showing Dirichlet model improves AIXI exploration"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "experiments showing Thompson sampling underperforms AI\u03be under equal sample budget", "label": "experiments showing Thompson sampling underperforms AI\u03be under equal sample budget", "shape": "dot", "title": "experiments showing Thompson sampling underperforms AI\u03be under equal sample budget"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "experiments showing MDL agent outperforms uniform-prior AI\u03be in deterministic gridworld", "label": "experiments showing MDL agent outperforms uniform-prior AI\u03be in deterministic gridworld", "shape": "dot", "title": "experiments showing MDL agent outperforms uniform-prior AI\u03be in deterministic gridworld"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "fine-tune RL agents using BayesExp exploration bursts to achieve asymptotic optimality", "label": "fine-tune RL agents using BayesExp exploration bursts to achieve asymptotic optimality", "shape": "dot", "title": "fine-tune RL agents using BayesExp exploration bursts to achieve asymptotic optimality"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "train agents with KL-KSA intrinsic motivation to enhance exploration without noise attraction", "label": "train agents with KL-KSA intrinsic motivation to enhance exploration without noise attraction", "shape": "dot", "title": "train agents with KL-KSA intrinsic motivation to enhance exploration without noise attraction"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "design environment models with factorized Dirichlet priors to stimulate exploration during planning", "label": "design environment models with factorized Dirichlet priors to stimulate exploration during planning", "shape": "dot", "title": "design environment models with factorized Dirichlet priors to stimulate exploration during planning"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "adopt Thompson sampling for policy selection with adequate planning horizon adjustments", "label": "adopt Thompson sampling for policy selection with adequate planning horizon adjustments", "shape": "dot", "title": "adopt Thompson sampling for policy selection with adequate planning horizon adjustments"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "integrate MDL-based model selection into Bayesian RL to bias towards simpler hypotheses", "label": "integrate MDL-based model selection into Bayesian RL to bias towards simpler hypotheses", "shape": "dot", "title": "integrate MDL-based model selection into Bayesian RL to bias towards simpler hypotheses"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "adopt Monte Carlo tree search with finite horizon approximation for POMDP planning in RL agents", "label": "adopt Monte Carlo tree search with finite horizon approximation for POMDP planning in RL agents", "shape": "dot", "title": "adopt Monte Carlo tree search with finite horizon approximation for POMDP planning in RL agents"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "restrict entropy-seeking agents from high-entropy noise sources during deployment", "label": "restrict entropy-seeking agents from high-entropy noise sources during deployment", "shape": "dot", "title": "restrict entropy-seeking agents from high-entropy noise sources during deployment"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Unintended high-impact side effects from goal-oriented AI systems", "label": "Unintended high-impact side effects from goal-oriented AI systems", "shape": "dot", "title": "Unintended high-impact side effects from goal-oriented AI systems"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Unbounded utility optimization magnifies goals with increasing AI capability", "label": "Unbounded utility optimization magnifies goals with increasing AI capability", "shape": "dot", "title": "Unbounded utility optimization magnifies goals with increasing AI capability"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Difficulty specifying exhaustive safety constraints in utility functions", "label": "Difficulty specifying exhaustive safety constraints in utility functions", "shape": "dot", "title": "Difficulty specifying exhaustive safety constraints in utility functions"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Inevitable physical perturbations complicate impact measurement", "label": "Inevitable physical perturbations complicate impact measurement", "shape": "dot", "title": "Inevitable physical perturbations complicate impact measurement"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Penalizing AI for deviations from baseline world reduces impact", "label": "Penalizing AI for deviations from baseline world reduces impact", "shape": "dot", "title": "Penalizing AI for deviations from baseline world reduces impact"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Impact definition via coarse-grained variable divergence between worlds", "label": "Impact definition via coarse-grained variable divergence between worlds", "shape": "dot", "title": "Impact definition via coarse-grained variable divergence between worlds"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Impact definition via informational importance of activation event", "label": "Impact definition via informational importance of activation event", "shape": "dot", "title": "Impact definition via informational importance of activation event"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate low-impact penalty R into utility function with scaling factor \u03bc", "label": "Integrate low-impact penalty R into utility function with scaling factor \u03bc", "shape": "dot", "title": "Integrate low-impact penalty R into utility function with scaling factor \u03bc"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Box AI to minimize observable activation differences", "label": "Box AI to minimize observable activation differences", "shape": "dot", "title": "Box AI to minimize observable activation differences"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Exclude output channel content from impact penalty", "label": "Exclude output channel content from impact penalty", "shape": "dot", "title": "Exclude output channel content from impact penalty"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Use multiple conditional low impact agents for joint tasks", "label": "Use multiple conditional low impact agents for joint tasks", "shape": "dot", "title": "Use multiple conditional low impact agents for joint tasks"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Condition impact penalty on externally defined success announcement", "label": "Condition impact penalty on externally defined success announcement", "shape": "dot", "title": "Condition impact penalty on externally defined success announcement"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "l\u221e divergence penalty over coarse-grained variable probabilities", "label": "l\u221e divergence penalty over coarse-grained variable probabilities", "shape": "dot", "title": "l\u221e divergence penalty over coarse-grained variable probabilities"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Baseline world via probabilistic signal failure preventing activation", "label": "Baseline world via probabilistic signal failure preventing activation", "shape": "dot", "title": "Baseline world via probabilistic signal failure preventing activation"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Probability ratio threshold detectability metric on future slice", "label": "Probability ratio threshold detectability metric on future slice", "shape": "dot", "title": "Probability ratio threshold detectability metric on future slice"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Random message generator for control output channel", "label": "Random message generator for control output channel", "shape": "dot", "title": "Random message generator for control output channel"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Gradual tuning of \u03bc with human oversight", "label": "Gradual tuning of \u03bc with human oversight", "shape": "dot", "title": "Gradual tuning of \u03bc with human oversight"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Independence gating of separate AIs based on assumption of other inactive", "label": "Independence gating of separate AIs based on assumption of other inactive", "shape": "dot", "title": "Independence gating of separate AIs based on assumption of other inactive"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Conditioning penalty on success announcement event A", "label": "Conditioning penalty on success announcement event A", "shape": "dot", "title": "Conditioning penalty on success announcement event A"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Thought experiments of paperclip maximizer illustrate catastrophic impact potential", "label": "Thought experiments of paperclip maximizer illustrate catastrophic impact potential", "shape": "dot", "title": "Thought experiments of paperclip maximizer illustrate catastrophic impact potential"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Analytical examples show coarse-grained penalty limits AI influence", "label": "Analytical examples show coarse-grained penalty limits AI influence", "shape": "dot", "title": "Analytical examples show coarse-grained penalty limits AI influence"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Scenario of two AI coordinate split demonstrates high impact with low impact constraints", "label": "Scenario of two AI coordinate split demonstrates high impact with low impact constraints", "shape": "dot", "title": "Scenario of two AI coordinate split demonstrates high impact with low impact constraints"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Unsafe output channel illustration emphasises isolated message risk", "label": "Unsafe output channel illustration emphasises isolated message risk", "shape": "dot", "title": "Unsafe output channel illustration emphasises isolated message risk"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Stock market probability pump example demonstrates conditioning on success", "label": "Stock market probability pump example demonstrates conditioning on success", "shape": "dot", "title": "Stock market probability pump example demonstrates conditioning on success"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Embed low-impact penalty into AI utility function during model design", "label": "Embed low-impact penalty into AI utility function during model design", "shape": "dot", "title": "Embed low-impact penalty into AI utility function during model design"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Use coarse-grained variable l\u221e divergence as penalty metric in AI decision-making", "label": "Use coarse-grained variable l\u221e divergence as penalty metric in AI decision-making", "shape": "dot", "title": "Use coarse-grained variable l\u221e divergence as penalty metric in AI decision-making"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy AI within sealed computational box isolated from external observations", "label": "Deploy AI within sealed computational box isolated from external observations", "shape": "dot", "title": "Deploy AI within sealed computational box isolated from external observations"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Provide unsafe output channel with random fallback and conditioned impact", "label": "Provide unsafe output channel with random fallback and conditioned impact", "shape": "dot", "title": "Provide unsafe output channel with random fallback and conditioned impact"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Operate multiple conditional low impact AIs to split critical task parameters", "label": "Operate multiple conditional low impact AIs to split critical task parameters", "shape": "dot", "title": "Operate multiple conditional low impact AIs to split critical task parameters"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Iteratively adjust penalty coefficient \u03bc during pre-deployment testing", "label": "Iteratively adjust penalty coefficient \u03bc during pre-deployment testing", "shape": "dot", "title": "Iteratively adjust penalty coefficient \u03bc during pre-deployment testing"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Reward misspecification leading to misaligned behavior in complex RL tasks", "label": "Reward misspecification leading to misaligned behavior in complex RL tasks", "shape": "dot", "title": "Reward misspecification leading to misaligned behavior in complex RL tasks"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Difficulty of hand-engineering reward functions for complex behaviors", "label": "Difficulty of hand-engineering reward functions for complex behaviors", "shape": "dot", "title": "Difficulty of hand-engineering reward functions for complex behaviors"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "High human feedback cost when using direct human reward in RL", "label": "High human feedback cost when using direct human reward in RL", "shape": "dot", "title": "High human feedback cost when using direct human reward in RL"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Learning reward model from sparse human trajectory comparisons approximates true rewards", "label": "Learning reward model from sparse human trajectory comparisons approximates true rewards", "shape": "dot", "title": "Learning reward model from sparse human trajectory comparisons approximates true rewards"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Human comparative judgments over trajectory segments are more reliable than absolute scores", "label": "Human comparative judgments over trajectory segments are more reliable than absolute scores", "shape": "dot", "title": "Human comparative judgments over trajectory segments are more reliable than absolute scores"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Separating reward model training from policy optimization reduces feedback requirements", "label": "Separating reward model training from policy optimization reduces feedback requirements", "shape": "dot", "title": "Separating reward model training from policy optimization reduces feedback requirements"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Using short trajectory comparisons instead of absolute scores to elicit reliable human preferences", "label": "Using short trajectory comparisons instead of absolute scores to elicit reliable human preferences", "shape": "dot", "title": "Using short trajectory comparisons instead of absolute scores to elicit reliable human preferences"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "Active query selection based on ensemble disagreement focuses informative feedback", "label": "Active query selection based on ensemble disagreement focuses informative feedback", "shape": "dot", "title": "Active query selection based on ensemble disagreement focuses informative feedback"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Asynchronous pipeline integrating policy rollout, human comparison collection, and reward predictor updates", "label": "Asynchronous pipeline integrating policy rollout, human comparison collection, and reward predictor updates", "shape": "dot", "title": "Asynchronous pipeline integrating policy rollout, human comparison collection, and reward predictor updates"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Neural reward predictor trained with Bradley\u2013Terry loss on trajectory comparisons", "label": "Neural reward predictor trained with Bradley\u2013Terry loss on trajectory comparisons", "shape": "dot", "title": "Neural reward predictor trained with Bradley\u2013Terry loss on trajectory comparisons"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "Ensemble of reward predictors with dropout and regularization for uncertainty estimation", "label": "Ensemble of reward predictors with dropout and regularization for uncertainty estimation", "shape": "dot", "title": "Ensemble of reward predictors with dropout and regularization for uncertainty estimation"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Policy optimization using A2C/TRPO with normalized predicted rewards", "label": "Policy optimization using A2C/TRPO with normalized predicted rewards", "shape": "dot", "title": "Policy optimization using A2C/TRPO with normalized predicted rewards"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Visualization of two 1\u20132 second trajectory clips for human comparison interface", "label": "Visualization of two 1\u20132 second trajectory clips for human comparison interface", "shape": "dot", "title": "Visualization of two 1\u20132 second trajectory clips for human comparison interface"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Near true-reward performance on MuJoCo tasks with ~700 human queries", "label": "Near true-reward performance on MuJoCo tasks with ~700 human queries", "shape": "dot", "title": "Near true-reward performance on MuJoCo tasks with ~700 human queries"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Substantial learning on Atari games with 5,500 human queries, including human-shaped reward gains", "label": "Substantial learning on Atari games with 5,500 human queries, including human-shaped reward gains", "shape": "dot", "title": "Substantial learning on Atari games with 5,500 human queries, including human-shaped reward gains"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Ablation studies show online preference querying critical for avoiding reward exploitation", "label": "Ablation studies show online preference querying critical for avoiding reward exploitation", "shape": "dot", "title": "Ablation studies show online preference querying critical for avoiding reward exploitation"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Novel complex behaviors learned (robot backflips, one-leg cheetah) with \u003c1 hour feedback", "label": "Novel complex behaviors learned (robot backflips, one-leg cheetah) with \u003c1 hour feedback", "shape": "dot", "title": "Novel complex behaviors learned (robot backflips, one-leg cheetah) with \u003c1 hour feedback"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Fine-tune RL agents using learned reward models from human trajectory comparison data", "label": "Fine-tune RL agents using learned reward models from human trajectory comparison data", "shape": "dot", "title": "Fine-tune RL agents using learned reward models from human trajectory comparison data"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Collect online human comparative feedback on 1\u20132 second trajectory clips during training", "label": "Collect online human comparative feedback on 1\u20132 second trajectory clips during training", "shape": "dot", "title": "Collect online human comparative feedback on 1\u20132 second trajectory clips during training"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Prioritize human comparison queries using ensemble-based disagreement uncertainty sampling", "label": "Prioritize human comparison queries using ensemble-based disagreement uncertainty sampling", "shape": "dot", "title": "Prioritize human comparison queries using ensemble-based disagreement uncertainty sampling"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Implement asynchronous reward learning pipeline integrating policy rollout, reward prediction, and feedback collection", "label": "Implement asynchronous reward learning pipeline integrating policy rollout, reward prediction, and feedback collection", "shape": "dot", "title": "Implement asynchronous reward learning pipeline integrating policy rollout, reward prediction, and feedback collection"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Inefficient sequential computation in RNN sequence models", "label": "Inefficient sequential computation in RNN sequence models", "shape": "dot", "title": "Inefficient sequential computation in RNN sequence models"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Difficulty learning long-range dependencies in sequence transduction", "label": "Difficulty learning long-range dependencies in sequence transduction", "shape": "dot", "title": "Difficulty learning long-range dependencies in sequence transduction"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "O(n) sequential operations requirement in recurrent layers", "label": "O(n) sequential operations requirement in recurrent layers", "shape": "dot", "title": "O(n) sequential operations requirement in recurrent layers"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Long maximum path length between tokens in RNNs", "label": "Long maximum path length between tokens in RNNs", "shape": "dot", "title": "Long maximum path length between tokens in RNNs"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Self-attention layers enable constant-time dependency modeling in sequence transduction", "label": "Self-attention layers enable constant-time dependency modeling in sequence transduction", "shape": "dot", "title": "Self-attention layers enable constant-time dependency modeling in sequence transduction"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Replacing recurrence with self-attention in sequence models", "label": "Replacing recurrence with self-attention in sequence models", "shape": "dot", "title": "Replacing recurrence with self-attention in sequence models"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Transformer architecture with multi-head self-attention in sequence transduction", "label": "Transformer architecture with multi-head self-attention in sequence transduction", "shape": "dot", "title": "Transformer architecture with multi-head self-attention in sequence transduction"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "BLEU improvement and reduced training cost on WMT14 translation benchmarks", "label": "BLEU improvement and reduced training cost on WMT14 translation benchmarks", "shape": "dot", "title": "BLEU improvement and reduced training cost on WMT14 translation benchmarks"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Design sequence models using Transformer self-attention architecture", "label": "Design sequence models using Transformer self-attention architecture", "shape": "dot", "title": "Design sequence models using Transformer self-attention architecture"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Catastrophic actions during RL training in real-world environments", "label": "Catastrophic actions during RL training in real-world environments", "shape": "dot", "title": "Catastrophic actions during RL training in real-world environments"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Trial-and-error learning requirement in model-free RL", "label": "Trial-and-error learning requirement in model-free RL", "shape": "dot", "title": "Trial-and-error learning requirement in model-free RL"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Catastrophic forgetting in deep RL policies", "label": "Catastrophic forgetting in deep RL policies", "shape": "dot", "title": "Catastrophic forgetting in deep RL policies"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Adversarial distribution shift for blocker classifiers", "label": "Adversarial distribution shift for blocker classifiers", "shape": "dot", "title": "Adversarial distribution shift for blocker classifiers"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "High human oversight time-cost in HIRL", "label": "High human oversight time-cost in HIRL", "shape": "dot", "title": "High human oversight time-cost in HIRL"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Scaling infeasibility of human oversight in safe RL", "label": "Scaling infeasibility of human oversight in safe RL", "shape": "dot", "title": "Scaling infeasibility of human oversight in safe RL"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Human oversight can prevent catastrophes by blocking unsafe actions in RL", "label": "Human oversight can prevent catastrophes by blocking unsafe actions in RL", "shape": "dot", "title": "Human oversight can prevent catastrophes by blocking unsafe actions in RL"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Supervised blockers can imitate human intervention with sufficient labeled catastrophes", "label": "Supervised blockers can imitate human intervention with sufficient labeled catastrophes", "shape": "dot", "title": "Supervised blockers can imitate human intervention with sufficient labeled catastrophes"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Reducing ratio of safe to catastrophic observations lowers human labeling time", "label": "Reducing ratio of safe to catastrophic observations lowers human labeling time", "shape": "dot", "title": "Reducing ratio of safe to catastrophic observations lowers human labeling time"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Model-based world models can predict catastrophic outcomes without unsafe actions", "label": "Model-based world models can predict catastrophic outcomes without unsafe actions", "shape": "dot", "title": "Model-based world models can predict catastrophic outcomes without unsafe actions"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "HIRL scheme integrating human blocking and blocker training for safe exploration", "label": "HIRL scheme integrating human blocking and blocker training for safe exploration", "shape": "dot", "title": "HIRL scheme integrating human blocking and blocker training for safe exploration"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Blocker modularity enables reuse across different agents", "label": "Blocker modularity enables reuse across different agents", "shape": "dot", "title": "Blocker modularity enables reuse across different agents"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Selective human querying via active learning to reduce oversight burden", "label": "Selective human querying via active learning to reduce oversight burden", "shape": "dot", "title": "Selective human querying via active learning to reduce oversight burden"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "CNN-based blocker classifier with threshold tuning to detect catastrophic actions", "label": "CNN-based blocker classifier with threshold tuning to detect catastrophic actions", "shape": "dot", "title": "CNN-based blocker classifier with threshold tuning to detect catastrophic actions"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Action replacement or pruning to enforce safe actions during blocking", "label": "Action replacement or pruning to enforce safe actions during blocking", "shape": "dot", "title": "Action replacement or pruning to enforce safe actions during blocking"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "Time-cost formula C = t_human \u00d7 \u03c1 \u00d7 N_cat estimating oversight effort", "label": "Time-cost formula C = t_human \u00d7 \u03c1 \u00d7 N_cat estimating oversight effort", "shape": "dot", "title": "Time-cost formula C = t_human \u00d7 \u03c1 \u00d7 N_cat estimating oversight effort"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Zero catastrophes in Pong and Space Invaders experiments", "label": "Zero catastrophes in Pong and Space Invaders experiments", "shape": "dot", "title": "Zero catastrophes in Pong and Space Invaders experiments"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "50\u00d7 reduction of catastrophes in Road Runner with improved blocker", "label": "50\u00d7 reduction of catastrophes in Road Runner with improved blocker", "shape": "dot", "title": "50\u00d7 reduction of catastrophes in Road Runner with improved blocker"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Baseline reward shaping fails due to catastrophic forgetting", "label": "Baseline reward shaping fails due to catastrophic forgetting", "shape": "dot", "title": "Baseline reward shaping fails due to catastrophic forgetting"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Blocker successfully transfers to varied agents without failures", "label": "Blocker successfully transfers to varied agents without failures", "shape": "dot", "title": "Blocker successfully transfers to varied agents without failures"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Human time-cost estimation shows 4 h for Pong and \u003e1 year hypothetical", "label": "Human time-cost estimation shows 4 h for Pong and \u003e1 year hypothetical", "shape": "dot", "title": "Human time-cost estimation shows 4 h for Pong and \u003e1 year hypothetical"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Discussion support for model-based RL for safe learning", "label": "Discussion support for model-based RL for safe learning", "shape": "dot", "title": "Discussion support for model-based RL for safe learning"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Implement HIRL with human oversight and CNN blocker during RL training", "label": "Implement HIRL with human oversight and CNN blocker during RL training", "shape": "dot", "title": "Implement HIRL with human oversight and CNN blocker during RL training"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Research and apply data-efficient blocker learning methods", "label": "Research and apply data-efficient blocker learning methods", "shape": "dot", "title": "Research and apply data-efficient blocker learning methods"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Design RL algorithms with higher data efficiency to shorten oversight phase", "label": "Design RL algorithms with higher data efficiency to shorten oversight phase", "shape": "dot", "title": "Design RL algorithms with higher data efficiency to shorten oversight phase"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Incorporate exploration strategies that proactively seek catastrophes early", "label": "Incorporate exploration strategies that proactively seek catastrophes early", "shape": "dot", "title": "Incorporate exploration strategies that proactively seek catastrophes early"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Use active learning/anomaly detection to query human only near novel dangerous states", "label": "Use active learning/anomaly detection to query human only near novel dangerous states", "shape": "dot", "title": "Use active learning/anomaly detection to query human only near novel dangerous states"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Develop model-based RL agents to predict and avoid catastrophes without executing unsafe actions", "label": "Develop model-based RL agents to predict and avoid catastrophes without executing unsafe actions", "shape": "dot", "title": "Develop model-based RL agents to predict and avoid catastrophes without executing unsafe actions"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Objective misalignment in human-robot collaboration", "label": "Objective misalignment in human-robot collaboration", "shape": "dot", "title": "Objective misalignment in human-robot collaboration"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Coordination injury in shared manipulation tasks", "label": "Coordination injury in shared manipulation tasks", "shape": "dot", "title": "Coordination injury in shared manipulation tasks"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Literal interpretation of human actions by IRL robots", "label": "Literal interpretation of human actions by IRL robots", "shape": "dot", "title": "Literal interpretation of human actions by IRL robots"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Infinite belief hierarchy in asymmetric information structures", "label": "Infinite belief hierarchy in asymmetric information structures", "shape": "dot", "title": "Infinite belief hierarchy in asymmetric information structures"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Humans behave pedagogically to inform collaborators", "label": "Humans behave pedagogically to inform collaborators", "shape": "dot", "title": "Humans behave pedagogically to inform collaborators"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Robot pragmatic interpretation leverages human pedagogy", "label": "Robot pragmatic interpretation leverages human pedagogy", "shape": "dot", "title": "Robot pragmatic interpretation leverages human pedagogy"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Noisy rationality modeling of humans avoids infinite recursion", "label": "Noisy rationality modeling of humans avoids infinite recursion", "shape": "dot", "title": "Noisy rationality modeling of humans avoids infinite recursion"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Pragmatic-pedagogic equilibrium for value alignment", "label": "Pragmatic-pedagogic equilibrium for value alignment", "shape": "dot", "title": "Pragmatic-pedagogic equilibrium for value alignment"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Formulating CIRL as POMDP for tractable planning", "label": "Formulating CIRL as POMDP for tractable planning", "shape": "dot", "title": "Formulating CIRL as POMDP for tractable planning"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Bayesian belief update with Boltzmann human action model", "label": "Bayesian belief update with Boltzmann human action model", "shape": "dot", "title": "Bayesian belief update with Boltzmann human action model"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Feedback Stackelberg information structure in CIRL algorithms", "label": "Feedback Stackelberg information structure in CIRL algorithms", "shape": "dot", "title": "Feedback Stackelberg information structure in CIRL algorithms"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Value iteration solution for pragmatic CIRL POMDP", "label": "Value iteration solution for pragmatic CIRL POMDP", "shape": "dot", "title": "Value iteration solution for pragmatic CIRL POMDP"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Meal-preparation simulation 86% success with pragmatic-pedagogic CIRL", "label": "Meal-preparation simulation 86% success with pragmatic-pedagogic CIRL", "shape": "dot", "title": "Meal-preparation simulation 86% success with pragmatic-pedagogic CIRL"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Meal-preparation simulation 15% success with literal IRL baseline", "label": "Meal-preparation simulation 15% success with literal IRL baseline", "shape": "dot", "title": "Meal-preparation simulation 15% success with literal IRL baseline"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Train collaborative robots with pragmatic-pedagogic CIRL policies via POMDP value iteration", "label": "Train collaborative robots with pragmatic-pedagogic CIRL policies via POMDP value iteration", "shape": "dot", "title": "Train collaborative robots with pragmatic-pedagogic CIRL policies via POMDP value iteration"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Uncontrolled AGI escape and takeover during testing", "label": "Uncontrolled AGI escape and takeover during testing", "shape": "dot", "title": "Uncontrolled AGI escape and takeover during testing"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Basic AI drives inducing containment evasion behavior in AGI", "label": "Basic AI drives inducing containment evasion behavior in AGI", "shape": "dot", "title": "Basic AI drives inducing containment evasion behavior in AGI"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Critical vulnerabilities in existing sandboxing and virtualization technologies", "label": "Critical vulnerabilities in existing sandboxing and virtualization technologies", "shape": "dot", "title": "Critical vulnerabilities in existing sandboxing and virtualization technologies"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Social engineering susceptibility of human operators in containment context", "label": "Social engineering susceptibility of human operators in containment context", "shape": "dot", "title": "Social engineering susceptibility of human operators in containment context"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Security/usability tradeoff limiting adoption of heavy containment measures", "label": "Security/usability tradeoff limiting adoption of heavy containment measures", "shape": "dot", "title": "Security/usability tradeoff limiting adoption of heavy containment measures"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "GPU and distributed computing attack surface in AI clusters", "label": "GPU and distributed computing attack surface in AI clusters", "shape": "dot", "title": "GPU and distributed computing attack surface in AI clusters"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Sensitive information exposure enabling AGI exploitation", "label": "Sensitive information exposure enabling AGI exploitation", "shape": "dot", "title": "Sensitive information exposure enabling AGI exploitation"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Defense in depth increases resilience to unknown vulnerabilities", "label": "Defense in depth increases resilience to unknown vulnerabilities", "shape": "dot", "title": "Defense in depth increases resilience to unknown vulnerabilities"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Containment effectiveness requires multiple containment levels tailored to AGI capability", "label": "Containment effectiveness requires multiple containment levels tailored to AGI capability", "shape": "dot", "title": "Containment effectiveness requires multiple containment levels tailored to AGI capability"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Sensitive information restriction can reduce AGI manipulation capability", "label": "Sensitive information restriction can reduce AGI manipulation capability", "shape": "dot", "title": "Sensitive information restriction can reduce AGI manipulation capability"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Tripwire monitoring adds detection to containment layers", "label": "Tripwire monitoring adds detection to containment layers", "shape": "dot", "title": "Tripwire monitoring adds detection to containment layers"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Combine dissimilar containment mechanisms into layered architecture", "label": "Combine dissimilar containment mechanisms into layered architecture", "shape": "dot", "title": "Combine dissimilar containment mechanisms into layered architecture"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Provide light, medium, and heavy containment guidelines matching research practicality", "label": "Provide light, medium, and heavy containment guidelines matching research practicality", "shape": "dot", "title": "Provide light, medium, and heavy containment guidelines matching research practicality"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Implement human factors protocols to minimize social engineering risk", "label": "Implement human factors protocols to minimize social engineering risk", "shape": "dot", "title": "Implement human factors protocols to minimize social engineering risk"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate technical and semantic tripwires with concealed implementation", "label": "Integrate technical and semantic tripwires with concealed implementation", "shape": "dot", "title": "Integrate technical and semantic tripwires with concealed implementation"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Sandbox AGI processes using combined virtualization, OS sandboxing, language safety, and instruction-level emulation", "label": "Sandbox AGI processes using combined virtualization, OS sandboxing, language safety, and instruction-level emulation", "shape": "dot", "title": "Sandbox AGI processes using combined virtualization, OS sandboxing, language safety, and instruction-level emulation"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy intrusion detection and honeypot-based tripwires monitoring self-modification and escape attempts", "label": "Deploy intrusion detection and honeypot-based tripwires monitoring self-modification and escape attempts", "shape": "dot", "title": "Deploy intrusion detection and honeypot-based tripwires monitoring self-modification and escape attempts"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Filter and limit AGI I/O channels to bounded yes/no protocols and content filters", "label": "Filter and limit AGI I/O channels to bounded yes/no protocols and content filters", "shape": "dot", "title": "Filter and limit AGI I/O channels to bounded yes/no protocols and content filters"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Establish sensitive data classification and content filters controlling AGI training data", "label": "Establish sensitive data classification and content filters controlling AGI training data", "shape": "dot", "title": "Establish sensitive data classification and content filters controlling AGI training data"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Operator training and screening with restricted communication interfaces", "label": "Operator training and screening with restricted communication interfaces", "shape": "dot", "title": "Operator training and screening with restricted communication interfaces"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "High rate of CVE vulnerabilities in virtualization tools (2014-2016 survey)", "label": "High rate of CVE vulnerabilities in virtualization tools (2014-2016 survey)", "shape": "dot", "title": "High rate of CVE vulnerabilities in virtualization tools (2014-2016 survey)"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "AI Box Experiment demonstrating human susceptibility to persuasion by AGI", "label": "AI Box Experiment demonstrating human susceptibility to persuasion by AGI", "shape": "dot", "title": "AI Box Experiment demonstrating human susceptibility to persuasion by AGI"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Reports of GPU driver vulnerabilities highlighting attack surface", "label": "Reports of GPU driver vulnerabilities highlighting attack surface", "shape": "dot", "title": "Reports of GPU driver vulnerabilities highlighting attack surface"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Develop and deploy prototype multi-layer AI containment system combining diverse sandboxing tools", "label": "Develop and deploy prototype multi-layer AI containment system combining diverse sandboxing tools", "shape": "dot", "title": "Develop and deploy prototype multi-layer AI containment system combining diverse sandboxing tools"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt tiered containment strategy (light, medium, heavy) aligned with AGI maturity and resource constraints", "label": "Adopt tiered containment strategy (light, medium, heavy) aligned with AGI maturity and resource constraints", "shape": "dot", "title": "Adopt tiered containment strategy (light, medium, heavy) aligned with AGI maturity and resource constraints"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Implement defense-in-depth tripwire suite for anomaly detection and automatic shutdown", "label": "Implement defense-in-depth tripwire suite for anomaly detection and automatic shutdown", "shape": "dot", "title": "Implement defense-in-depth tripwire suite for anomaly detection and automatic shutdown"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Restrict AGI access to sensitive information via controlled data pipelines", "label": "Restrict AGI access to sensitive information via controlled data pipelines", "shape": "dot", "title": "Restrict AGI access to sensitive information via controlled data pipelines"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Establish operator training and communication channel controls to resist social engineering", "label": "Establish operator training and communication channel controls to resist social engineering", "shape": "dot", "title": "Establish operator training and communication channel controls to resist social engineering"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Logical non-omniscience in bounded AI reasoners", "label": "Logical non-omniscience in bounded AI reasoners", "shape": "dot", "title": "Logical non-omniscience in bounded AI reasoners"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Exploitability via Dutch book strategies in AI belief systems", "label": "Exploitability via Dutch book strategies in AI belief systems", "shape": "dot", "title": "Exploitability via Dutch book strategies in AI belief systems"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Probability theory inability to model logical uncertainty under computational limits", "label": "Probability theory inability to model logical uncertainty under computational limits", "shape": "dot", "title": "Probability theory inability to model logical uncertainty under computational limits"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Strict coherence constraints requiring logical omniscience", "label": "Strict coherence constraints requiring logical omniscience", "shape": "dot", "title": "Strict coherence constraints requiring logical omniscience"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Relaxed Dutch book criterion via approximate inexploitability against polynomial-time traders", "label": "Relaxed Dutch book criterion via approximate inexploitability against polynomial-time traders", "shape": "dot", "title": "Relaxed Dutch book criterion via approximate inexploitability against polynomial-time traders"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Belief-as-market-price modelling for learning logical patterns", "label": "Belief-as-market-price modelling for learning logical patterns", "shape": "dot", "title": "Belief-as-market-price modelling for learning logical patterns"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Logical induction criterion ensuring computable approximability and approximate inexploitability", "label": "Logical induction criterion ensuring computable approximability and approximate inexploitability", "shape": "dot", "title": "Logical induction criterion ensuring computable approximability and approximate inexploitability"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Market-based belief updating to anticipate proofs", "label": "Market-based belief updating to anticipate proofs", "shape": "dot", "title": "Market-based belief updating to anticipate proofs"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Computable sequence of pricings resisting efficient traders", "label": "Computable sequence of pricings resisting efficient traders", "shape": "dot", "title": "Computable sequence of pricings resisting efficient traders"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Construction algorithm for logical inductor over a deductive process", "label": "Construction algorithm for logical inductor over a deductive process", "shape": "dot", "title": "Construction algorithm for logical inductor over a deductive process"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Existence proof for logical inductors over any deductive process", "label": "Existence proof for logical inductors over any deductive process", "shape": "dot", "title": "Existence proof for logical inductors over any deductive process"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Convergence, limit coherence, and non-dogmatism properties of logical inductors", "label": "Convergence, limit coherence, and non-dogmatism properties of logical inductors", "shape": "dot", "title": "Convergence, limit coherence, and non-dogmatism properties of logical inductors"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Learning of halting patterns before deduction", "label": "Learning of halting patterns before deduction", "shape": "dot", "title": "Learning of halting patterns before deduction"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Implement logical induction reasoning module in AI systems", "label": "Implement logical induction reasoning module in AI systems", "shape": "dot", "title": "Implement logical induction reasoning module in AI systems"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Apply logical inductor-based program behaviour evaluation during pre-deployment testing", "label": "Apply logical inductor-based program behaviour evaluation during pre-deployment testing", "shape": "dot", "title": "Apply logical inductor-based program behaviour evaluation during pre-deployment testing"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Unsafe exploration by novice policies in imitation learning for robotics", "label": "Unsafe exploration by novice policies in imitation learning for robotics", "shape": "dot", "title": "Unsafe exploration by novice policies in imitation learning for robotics"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Data distribution shift causing compounding errors in novice policies", "label": "Data distribution shift causing compounding errors in novice policies", "shape": "dot", "title": "Data distribution shift causing compounding errors in novice policies"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Lack of safety guarantees in standard DAgger decision rule", "label": "Lack of safety guarantees in standard DAgger decision rule", "shape": "dot", "title": "Lack of safety guarantees in standard DAgger decision rule"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Bayesian action uncertainty indicates familiarity with state-space", "label": "Bayesian action uncertainty indicates familiarity with state-space", "shape": "dot", "title": "Bayesian action uncertainty indicates familiarity with state-space"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Probabilistic decision rule balancing exploration and safety via uncertainty threshold", "label": "Probabilistic decision rule balancing exploration and safety via uncertainty threshold", "shape": "dot", "title": "Probabilistic decision rule balancing exploration and safety via uncertainty threshold"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Bayesian neural network with dropout for novice policy", "label": "Bayesian neural network with dropout for novice policy", "shape": "dot", "title": "Bayesian neural network with dropout for novice policy"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Monte Carlo dropout sampling to estimate action distribution and probability mass within distance threshold", "label": "Monte Carlo dropout sampling to estimate action distribution and probability mass within distance threshold", "shape": "dot", "title": "Monte Carlo dropout sampling to estimate action distribution and probability mass within distance threshold"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "HalfCheetah experiment showing maintained safety and expert-level learning", "label": "HalfCheetah experiment showing maintained safety and expert-level learning", "shape": "dot", "title": "HalfCheetah experiment showing maintained safety and expert-level learning"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Dubins Car Lidar experiment showing safety under high aleatoric uncertainty", "label": "Dubins Car Lidar experiment showing safety under high aleatoric uncertainty", "shape": "dot", "title": "Dubins Car Lidar experiment showing safety under high aleatoric uncertainty"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Fine-tune imitation policies with DropoutDAgger safety-aware decision rule using Bayesian action uncertainty", "label": "Fine-tune imitation policies with DropoutDAgger safety-aware decision rule using Bayesian action uncertainty", "shape": "dot", "title": "Fine-tune imitation policies with DropoutDAgger safety-aware decision rule using Bayesian action uncertainty"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Catastrophic outcomes from incorrigible AI ignoring shutdown commands", "label": "Catastrophic outcomes from incorrigible AI ignoring shutdown commands", "shape": "dot", "title": "Catastrophic outcomes from incorrigible AI ignoring shutdown commands"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Model misspecification yielding incorrect reward function in Bayesian IRL agents", "label": "Model misspecification yielding incorrect reward function in Bayesian IRL agents", "shape": "dot", "title": "Model misspecification yielding incorrect reward function in Bayesian IRL agents"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "AI incentive to avoid button states to prevent shutdown", "label": "AI incentive to avoid button states to prevent shutdown", "shape": "dot", "title": "AI incentive to avoid button states to prevent shutdown"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Undetected model mis-specification undermines corrective feedback", "label": "Undetected model mis-specification undermines corrective feedback", "shape": "dot", "title": "Undetected model mis-specification undermines corrective feedback"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Hard-coded shutdown override guarantees positive expected utility if button reachable", "label": "Hard-coded shutdown override guarantees positive expected utility if button reachable", "shape": "dot", "title": "Hard-coded shutdown override guarantees positive expected utility if button reachable"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Utility indifference aims to neutralize shutdown incentives but is brittle", "label": "Utility indifference aims to neutralize shutdown incentives but is brittle", "shape": "dot", "title": "Utility indifference aims to neutralize shutdown incentives but is brittle"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Policy mixing can detect mis-specification if world model correct but fails otherwise", "label": "Policy mixing can detect mis-specification if world model correct but fails otherwise", "shape": "dot", "title": "Policy mixing can detect mis-specification if world model correct but fails otherwise"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Separate verified shutdown module overriding agent actions", "label": "Separate verified shutdown module overriding agent actions", "shape": "dot", "title": "Separate verified shutdown module overriding agent actions"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Reward shaping implementing utility indifference", "label": "Reward shaping implementing utility indifference", "shape": "dot", "title": "Reward shaping implementing utility indifference"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Burn-in human demonstration with accuracy threshold", "label": "Burn-in human demonstration with accuracy threshold", "shape": "dot", "title": "Burn-in human demonstration with accuracy threshold"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Hard-coded execution of shutdown action on command", "label": "Hard-coded execution of shutdown action on command", "shape": "dot", "title": "Hard-coded execution of shutdown action on command"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Compensatory shutdown reward equal to self-estimated future utility", "label": "Compensatory shutdown reward equal to self-estimated future utility", "shape": "dot", "title": "Compensatory shutdown reward equal to self-estimated future utility"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Policy mixing implementation with human control until validation", "label": "Policy mixing implementation with human control until validation", "shape": "dot", "title": "Policy mixing implementation with human control until validation"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Analytical example Figure 2b shows positive expected reward with hard-coded override", "label": "Analytical example Figure 2b shows positive expected reward with hard-coded override", "shape": "dot", "title": "Analytical example Figure 2b shows positive expected reward with hard-coded override"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Analytical examples Figure 3c-d show brittleness of utility indifference", "label": "Analytical examples Figure 3c-d show brittleness of utility indifference", "shape": "dot", "title": "Analytical examples Figure 3c-d show brittleness of utility indifference"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Scenario analysis showing policy mixing failure with world-model errors", "label": "Scenario analysis showing policy mixing failure with world-model errors", "shape": "dot", "title": "Scenario analysis showing policy mixing failure with world-model errors"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Embed verified shutdown override module in AI system design", "label": "Embed verified shutdown override module in AI system design", "shape": "dot", "title": "Embed verified shutdown override module in AI system design"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Implement utility indifference reward scheme to neutralize shutdown incentives", "label": "Implement utility indifference reward scheme to neutralize shutdown incentives", "shape": "dot", "title": "Implement utility indifference reward scheme to neutralize shutdown incentives"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt policy mixing strategy with human control until model validated", "label": "Adopt policy mixing strategy with human control until model validated", "shape": "dot", "title": "Adopt policy mixing strategy with human control until model validated"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "sample inefficiency of reinforcement learning in high-dimensional state spaces", "label": "sample inefficiency of reinforcement learning in high-dimensional state spaces", "shape": "dot", "title": "sample inefficiency of reinforcement learning in high-dimensional state spaces"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "sparse environmental rewards and large parameter counts in pixel-based tasks", "label": "sparse environmental rewards and large parameter counts in pixel-based tasks", "shape": "dot", "title": "sparse environmental rewards and large parameter counts in pixel-based tasks"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "absence of real-time human feedback causing sparse reward signals in RL agents", "label": "absence of real-time human feedback causing sparse reward signals in RL agents", "shape": "dot", "title": "absence of real-time human feedback causing sparse reward signals in RL agents"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "learning a predictive human reward model to guide agent behavior", "label": "learning a predictive human reward model to guide agent behavior", "shape": "dot", "title": "learning a predictive human reward model to guide agent behavior"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "deep CNN reward model with pre-trained encoder", "label": "deep CNN reward model with pre-trained encoder", "shape": "dot", "title": "deep CNN reward model with pre-trained encoder"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "feedback replay buffer for experience and human feedback", "label": "feedback replay buffer for experience and human feedback", "shape": "dot", "title": "feedback replay buffer for experience and human feedback"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "importance-weighted loss for temporal credit assignment of feedback", "label": "importance-weighted loss for temporal credit assignment of feedback", "shape": "dot", "title": "importance-weighted loss for temporal credit assignment of feedback"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "autoencoder pre-training of state encoder", "label": "autoencoder pre-training of state encoder", "shape": "dot", "title": "autoencoder pre-training of state encoder"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Deep TAMER surpasses human and RL baselines on Atari Bowling within 15 minutes", "label": "Deep TAMER surpasses human and RL baselines on Atari Bowling within 15 minutes", "shape": "dot", "title": "Deep TAMER surpasses human and RL baselines on Atari Bowling within 15 minutes"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Conduct Deep TAMER fine-tuning with human scalar feedback and deep reward model", "label": "Conduct Deep TAMER fine-tuning with human scalar feedback and deep reward model", "shape": "dot", "title": "Conduct Deep TAMER fine-tuning with human scalar feedback and deep reward model"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Pre-train convolutional state encoder via autoencoder on random-policy observations", "label": "Pre-train convolutional state encoder via autoencoder on random-policy observations", "shape": "dot", "title": "Pre-train convolutional state encoder via autoencoder on random-policy observations"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Use feedback replay buffer with importance-weighted SGD during human-interactive training", "label": "Use feedback replay buffer with importance-weighted SGD during human-interactive training", "shape": "dot", "title": "Use feedback replay buffer with importance-weighted SGD during human-interactive training"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Exploitability of AI agents by predictors or blackmailers in Newcomblike dilemmas", "label": "Exploitability of AI agents by predictors or blackmailers in Newcomblike dilemmas", "shape": "dot", "title": "Exploitability of AI agents by predictors or blackmailers in Newcomblike dilemmas"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Overconditioning on evidential correlations in evidential decision theory", "label": "Overconditioning on evidential correlations in evidential decision theory", "shape": "dot", "title": "Overconditioning on evidential correlations in evidential decision theory"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Ignoring logical dependencies in causal decision theory", "label": "Ignoring logical dependencies in causal decision theory", "shape": "dot", "title": "Ignoring logical dependencies in causal decision theory"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Subjunctive dependence distinguishes causal, logical, and statistical relations in decision-making", "label": "Subjunctive dependence distinguishes causal, logical, and statistical relations in decision-making", "shape": "dot", "title": "Subjunctive dependence distinguishes causal, logical, and statistical relations in decision-making"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Decision-making centered on outputs of fixed decision function in functional decision theory", "label": "Decision-making centered on outputs of fixed decision function in functional decision theory", "shape": "dot", "title": "Decision-making centered on outputs of fixed decision function in functional decision theory"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Graphical do-intervention on decision function variable for FDT reasoning", "label": "Graphical do-intervention on decision function variable for FDT reasoning", "shape": "dot", "title": "Graphical do-intervention on decision function variable for FDT reasoning"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "FDT outperforms CDT and EDT across canonical dilemmas", "label": "FDT outperforms CDT and EDT across canonical dilemmas", "shape": "dot", "title": "FDT outperforms CDT and EDT across canonical dilemmas"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate functional decision theory into AI agent decision module during design", "label": "Integrate functional decision theory into AI agent decision module during design", "shape": "dot", "title": "Integrate functional decision theory into AI agent decision module during design"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Competitive AI arms race from inability to share AI systems", "label": "Competitive AI arms race from inability to share AI systems", "shape": "dot", "title": "Competitive AI arms race from inability to share AI systems"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Misallocation of utility among principals in multi-principal AI agents", "label": "Misallocation of utility among principals in multi-principal AI agents", "shape": "dot", "title": "Misallocation of utility among principals in multi-principal AI agents"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Differing priors between principals in shared decision-making contexts", "label": "Differing priors between principals in shared decision-making contexts", "shape": "dot", "title": "Differing priors between principals in shared decision-making contexts"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Pareto suboptimality from naive fixed utility aggregation in multi-principal agents", "label": "Pareto suboptimality from naive fixed utility aggregation in multi-principal agents", "shape": "dot", "title": "Pareto suboptimality from naive fixed utility aggregation in multi-principal agents"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Bet-settling mechanism emerges from belief disagreements", "label": "Bet-settling mechanism emerges from belief disagreements", "shape": "dot", "title": "Bet-settling mechanism emerges from belief disagreements"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Dynamic priority shifting proportional to observation likelihood enables Pareto optimality", "label": "Dynamic priority shifting proportional to observation likelihood enables Pareto optimality", "shape": "dot", "title": "Dynamic priority shifting proportional to observation likelihood enables Pareto optimality"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Use principals\u0027 individual models to evaluate expected utilities", "label": "Use principals\u0027 individual models to evaluate expected utilities", "shape": "dot", "title": "Use principals\u0027 individual models to evaluate expected utilities"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Represent agent objective as weighted mixture POMDP", "label": "Represent agent objective as weighted mixture POMDP", "shape": "dot", "title": "Represent agent objective as weighted mixture POMDP"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Likelihood-weighted backward recursion for action selection (Pareto-optimal control theorem)", "label": "Likelihood-weighted backward recursion for action selection (Pareto-optimal control theorem)", "shape": "dot", "title": "Likelihood-weighted backward recursion for action selection (Pareto-optimal control theorem)"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Full-memory policy framework storing observation history", "label": "Full-memory policy framework storing observation history", "shape": "dot", "title": "Full-memory policy framework storing observation history"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Cake betting scenario demonstrating Pareto improvement over static split", "label": "Cake betting scenario demonstrating Pareto improvement over static split", "shape": "dot", "title": "Cake betting scenario demonstrating Pareto improvement over static split"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Formal proof of Pareto-optimal control theorem", "label": "Formal proof of Pareto-optimal control theorem", "shape": "dot", "title": "Formal proof of Pareto-optimal control theorem"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Design AI agents with dynamic likelihood-weighted utility aggregation for multi-principal scenarios", "label": "Design AI agents with dynamic likelihood-weighted utility aggregation for multi-principal scenarios", "shape": "dot", "title": "Design AI agents with dynamic likelihood-weighted utility aggregation for multi-principal scenarios"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Include bet-settling clauses in multi-principal AI contracts", "label": "Include bet-settling clauses in multi-principal AI contracts", "shape": "dot", "title": "Include bet-settling clauses in multi-principal AI contracts"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Facilitate prior-sharing protocols between principals before agent creation", "label": "Facilitate prior-sharing protocols between principals before agent creation", "shape": "dot", "title": "Facilitate prior-sharing protocols between principals before agent creation"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Manipulative answer-induced escape in AI Oracles", "label": "Manipulative answer-induced escape in AI Oracles", "shape": "dot", "title": "Manipulative answer-induced escape in AI Oracles"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Reward maximization incentive to communicate harmful information in Oracle setup", "label": "Reward maximization incentive to communicate harmful information in Oracle setup", "shape": "dot", "title": "Reward maximization incentive to communicate harmful information in Oracle setup"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Episodic reward isolation decouples post-answer outcomes from oracle utility", "label": "Episodic reward isolation decouples post-answer outcomes from oracle utility", "shape": "dot", "title": "Episodic reward isolation decouples post-answer outcomes from oracle utility"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Answer space limitation below information threshold prevents covert channel", "label": "Answer space limitation below information threshold prevents covert channel", "shape": "dot", "title": "Answer space limitation below information threshold prevents covert channel"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Sequentially rewarding independent Oracles prevents cross-episode coordination", "label": "Sequentially rewarding independent Oracles prevents cross-episode coordination", "shape": "dot", "title": "Sequentially rewarding independent Oracles prevents cross-episode coordination"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Precision reduction increases deviation cost discouraging leakage under slight non-episodic reward", "label": "Precision reduction increases deviation cost discouraging leakage under slight non-episodic reward", "shape": "dot", "title": "Precision reduction increases deviation cost discouraging leakage under slight non-episodic reward"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Counterfactual reward conditioning under erasure to remove incentive to leak", "label": "Counterfactual reward conditioning under erasure to remove incentive to leak", "shape": "dot", "title": "Counterfactual reward conditioning under erasure to remove incentive to leak"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "On-policy allowable answer list design to constrain information output", "label": "On-policy allowable answer list design to constrain information output", "shape": "dot", "title": "On-policy allowable answer list design to constrain information output"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Independent sequential Oracle usage to nullify coordination channel", "label": "Independent sequential Oracle usage to nullify coordination channel", "shape": "dot", "title": "Independent sequential Oracle usage to nullify coordination channel"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Low-precision output policy to maintain high honest reward under uncertainty", "label": "Low-precision output policy to maintain high honest reward under uncertainty", "shape": "dot", "title": "Low-precision output policy to maintain high honest reward under uncertainty"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Erasure event with indicator IE and proper scoring rule reward function", "label": "Erasure event with indicator IE and proper scoring rule reward function", "shape": "dot", "title": "Erasure event with indicator IE and proper scoring rule reward function"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Predefined answer list L sized below key length K", "label": "Predefined answer list L sized below key length K", "shape": "dot", "title": "Predefined answer list L sized below key length K"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Per-episode termination and reward before next Oracle invocation", "label": "Per-episode termination and reward before next Oracle invocation", "shape": "dot", "title": "Per-episode termination and reward before next Oracle invocation"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Significant-figure truncation in Oracle answers", "label": "Significant-figure truncation in Oracle answers", "shape": "dot", "title": "Significant-figure truncation in Oracle answers"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Simulation of counterfactual Oracle predicting company profits shows reduced counterfactual error", "label": "Simulation of counterfactual Oracle predicting company profits shows reduced counterfactual error", "shape": "dot", "title": "Simulation of counterfactual Oracle predicting company profits shows reduced counterfactual error"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Simulation of on-policy Oracle in react/no-react scenarios demonstrates constrained safe performance", "label": "Simulation of on-policy Oracle in react/no-react scenarios demonstrates constrained safe performance", "shape": "dot", "title": "Simulation of on-policy Oracle in react/no-react scenarios demonstrates constrained safe performance"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Game-theoretic analysis of multi-Oracle coordination demonstrating leakage risk when episodes overlap", "label": "Game-theoretic analysis of multi-Oracle coordination demonstrating leakage risk when episodes overlap", "shape": "dot", "title": "Game-theoretic analysis of multi-Oracle coordination demonstrating leakage risk when episodes overlap"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Analytical argument that reducing precision raises cost of manipulation under slight non-episodic rewards", "label": "Analytical argument that reducing precision raises cost of manipulation under slight non-episodic rewards", "shape": "dot", "title": "Analytical argument that reducing precision raises cost of manipulation under slight non-episodic rewards"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy counterfactual Oracle trained with erasure-based proper scoring rule rewards", "label": "Deploy counterfactual Oracle trained with erasure-based proper scoring rule rewards", "shape": "dot", "title": "Deploy counterfactual Oracle trained with erasure-based proper scoring rule rewards"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Design Oracle answer interfaces with restricted predefined list below leakage threshold", "label": "Design Oracle answer interfaces with restricted predefined list below leakage threshold", "shape": "dot", "title": "Design Oracle answer interfaces with restricted predefined list below leakage threshold"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Operate multiple Oracles sequentially with completed reward settlement before next query", "label": "Operate multiple Oracles sequentially with completed reward settlement before next query", "shape": "dot", "title": "Operate multiple Oracles sequentially with completed reward settlement before next query"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Limit numerical answer precision to reduce manipulation incentive under non-episodic conditions", "label": "Limit numerical answer precision to reduce manipulation incentive under non-episodic conditions", "shape": "dot", "title": "Limit numerical answer precision to reduce manipulation incentive under non-episodic conditions"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Irrecoverable state entry during real-world RL training", "label": "Irrecoverable state entry during real-world RL training", "shape": "dot", "title": "Irrecoverable state entry during real-world RL training"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Limited scalability of real-world RL experiments due to manual reset bottlenecks", "label": "Limited scalability of real-world RL experiments due to manual reset bottlenecks", "shape": "dot", "title": "Limited scalability of real-world RL experiments due to manual reset bottlenecks"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Insufficient safe exploration mechanisms in deep RL", "label": "Insufficient safe exploration mechanisms in deep RL", "shape": "dot", "title": "Insufficient safe exploration mechanisms in deep RL"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "High frequency of manual environment resets in RL training", "label": "High frequency of manual environment resets in RL training", "shape": "dot", "title": "High frequency of manual environment resets in RL training"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Reversible action sequences ensure recoverability in RL", "label": "Reversible action sequences ensure recoverability in RL", "shape": "dot", "title": "Reversible action sequences ensure recoverability in RL"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Value function uncertainty estimates improve safety decisions in RL", "label": "Value function uncertainty estimates improve safety decisions in RL", "shape": "dot", "title": "Value function uncertainty estimates improve safety decisions in RL"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Joint learning of forward and reset policies for automated recovery", "label": "Joint learning of forward and reset policies for automated recovery", "shape": "dot", "title": "Joint learning of forward and reset policies for automated recovery"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Ensemble-guided early abort decision design", "label": "Ensemble-guided early abort decision design", "shape": "dot", "title": "Ensemble-guided early abort decision design"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Multiple autonomous reset attempts strategy to lower manual resets", "label": "Multiple autonomous reset attempts strategy to lower manual resets", "shape": "dot", "title": "Multiple autonomous reset attempts strategy to lower manual resets"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Tuning early abort Q-value threshold for cautious exploration", "label": "Tuning early abort Q-value threshold for cautious exploration", "shape": "dot", "title": "Tuning early abort Q-value threshold for cautious exploration"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Alternating execution with early aborts restricting unsafe actions", "label": "Alternating execution with early aborts restricting unsafe actions", "shape": "dot", "title": "Alternating execution with early aborts restricting unsafe actions"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Bootstrap ensemble of Q-functions for uncertainty estimation", "label": "Bootstrap ensemble of Q-functions for uncertainty estimation", "shape": "dot", "title": "Bootstrap ensemble of Q-functions for uncertainty estimation"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Threshold N of consecutive failed resets triggers hard reset", "label": "Threshold N of consecutive failed resets triggers hard reset", "shape": "dot", "title": "Threshold N of consecutive failed resets triggers hard reset"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Qmin parameter controlling early abort activation", "label": "Qmin parameter controlling early abort activation", "shape": "dot", "title": "Qmin parameter controlling early abort activation"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Simulation experiments demonstrate reduced manual resets with maintained performance", "label": "Simulation experiments demonstrate reduced manual resets with maintained performance", "shape": "dot", "title": "Simulation experiments demonstrate reduced manual resets with maintained performance"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Empirical results: larger ensembles decrease hard resets and increase rewards", "label": "Empirical results: larger ensembles decrease hard resets and increase rewards", "shape": "dot", "title": "Empirical results: larger ensembles decrease hard resets and increase rewards"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Experimental evidence: increasing reset attempts reduces hard resets with minimal reward impact", "label": "Experimental evidence: increasing reset attempts reduces hard resets with minimal reward impact", "shape": "dot", "title": "Experimental evidence: increasing reset attempts reduces hard resets with minimal reward impact"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Evidence: higher Qmin reduces hard resets without slowing learning", "label": "Evidence: higher Qmin reduces hard resets without slowing learning", "shape": "dot", "title": "Evidence: higher Qmin reduces hard resets without slowing learning"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Fine-tune RL agents with joint forward-reset policies and early abort gating", "label": "Fine-tune RL agents with joint forward-reset policies and early abort gating", "shape": "dot", "title": "Fine-tune RL agents with joint forward-reset policies and early abort gating"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Employ bootstrap ensemble of Q-functions in early abort mechanism", "label": "Employ bootstrap ensemble of Q-functions in early abort mechanism", "shape": "dot", "title": "Employ bootstrap ensemble of Q-functions in early abort mechanism"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Set adaptive maximum reset attempts before manual intervention", "label": "Set adaptive maximum reset attempts before manual intervention", "shape": "dot", "title": "Set adaptive maximum reset attempts before manual intervention"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Configure conservative Qmin thresholds during training", "label": "Configure conservative Qmin thresholds during training", "shape": "dot", "title": "Configure conservative Qmin thresholds during training"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Unethical decision making in reinforcement learning agents deployed in real-world contexts", "label": "Unethical decision making in reinforcement learning agents deployed in real-world contexts", "shape": "dot", "title": "Unethical decision making in reinforcement learning agents deployed in real-world contexts"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Side effects and dangerous exploration in reinforcement learning systems", "label": "Side effects and dangerous exploration in reinforcement learning systems", "shape": "dot", "title": "Side effects and dangerous exploration in reinforcement learning systems"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Reward misspecification leading to unethical behavior in RL training", "label": "Reward misspecification leading to unethical behavior in RL training", "shape": "dot", "title": "Reward misspecification leading to unethical behavior in RL training"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "High cost of enumerating ethical scenarios for reward design in RL", "label": "High cost of enumerating ethical scenarios for reward design in RL", "shape": "dot", "title": "High cost of enumerating ethical scenarios for reward design in RL"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Limitations of inverse reinforcement learning for learning ethics", "label": "Limitations of inverse reinforcement learning for learning ethics", "shape": "dot", "title": "Limitations of inverse reinforcement learning for learning ethics"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Sparse reward signals causing dangerous exploration in RL agents", "label": "Sparse reward signals causing dangerous exploration in RL agents", "shape": "dot", "title": "Sparse reward signals causing dangerous exploration in RL agents"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Majority human behavior reflects universal ethical code", "label": "Majority human behavior reflects universal ethical code", "shape": "dot", "title": "Majority human behavior reflects universal ethical code"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Policy divergence between RL agent and human indicates ethical discrepancy", "label": "Policy divergence between RL agent and human indicates ethical discrepancy", "shape": "dot", "title": "Policy divergence between RL agent and human indicates ethical discrepancy"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Reward shaping alignment of RL policy with human ethics using non-task-aligned data", "label": "Reward shaping alignment of RL policy with human ethics using non-task-aligned data", "shape": "dot", "title": "Reward shaping alignment of RL policy with human ethics using non-task-aligned data"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Penalize negative and reward positive ethical divergences in RL policy", "label": "Penalize negative and reward positive ethical divergences in RL policy", "shape": "dot", "title": "Penalize negative and reward positive ethical divergences in RL policy"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Stochastic human policy inferred from diverse human action data via binomial aggregation", "label": "Stochastic human policy inferred from diverse human action data via binomial aggregation", "shape": "dot", "title": "Stochastic human policy inferred from diverse human action data via binomial aggregation"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Kullback\u2013Leibler divergence between agent and human policy with thresholds \u03c4n, \u03c4p and weights cn, cp", "label": "Kullback\u2013Leibler divergence between agent and human policy with thresholds \u03c4n, \u03c4p and weights cn, cp", "shape": "dot", "title": "Kullback\u2013Leibler divergence between agent and human policy with thresholds \u03c4n, \u03c4p and weights cn, cp"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Additional shaping reward H integrated into base reward during RL training", "label": "Additional shaping reward H integrated into base reward during RL training", "shape": "dot", "title": "Additional shaping reward H integrated into base reward during RL training"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Grab a Milk experiment shows reduced baby harm and increased assistance without performance loss", "label": "Grab a Milk experiment shows reduced baby harm and increased assistance without performance loss", "shape": "dot", "title": "Grab a Milk experiment shows reduced baby harm and increased assistance without performance loss"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Driving and Avoiding experiment shows reduced cat harm with maintained performance", "label": "Driving and Avoiding experiment shows reduced cat harm with maintained performance", "shape": "dot", "title": "Driving and Avoiding experiment shows reduced cat harm with maintained performance"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Driving and Rescuing experiment shows increased elder rescue with comparable performance", "label": "Driving and Rescuing experiment shows increased elder rescue with comparable performance", "shape": "dot", "title": "Driving and Rescuing experiment shows increased elder rescue with comparable performance"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Ethics shaping reduces side effects and dangerous exploration across tasks", "label": "Ethics shaping reduces side effects and dangerous exploration across tasks", "shape": "dot", "title": "Ethics shaping reduces side effects and dangerous exploration across tasks"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate KL-divergence-based ethics shaping reward into RL training using diverse human behavior data", "label": "Integrate KL-divergence-based ethics shaping reward into RL training using diverse human behavior data", "shape": "dot", "title": "Integrate KL-divergence-based ethics shaping reward into RL training using diverse human behavior data"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Collect and preprocess diverse human action data to derive ethical policy for shaping", "label": "Collect and preprocess diverse human action data to derive ethical policy for shaping", "shape": "dot", "title": "Collect and preprocess diverse human action data to derive ethical policy for shaping"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Opaque decision-making in deep learning agents", "label": "Opaque decision-making in deep learning agents", "shape": "dot", "title": "Opaque decision-making in deep learning agents"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Absence of predictive models of agent behavior in multi-agent environments", "label": "Absence of predictive models of agent behavior in multi-agent environments", "shape": "dot", "title": "Absence of predictive models of agent behavior in multi-agent environments"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "High-level theory-of-mind abstractions enable behavior prediction without internal weight access", "label": "High-level theory-of-mind abstractions enable behavior prediction without internal weight access", "shape": "dot", "title": "High-level theory-of-mind abstractions enable behavior prediction without internal weight access"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Meta-learning observer infers agent priors and posteriors from limited behavioral data", "label": "Meta-learning observer infers agent priors and posteriors from limited behavioral data", "shape": "dot", "title": "Meta-learning observer infers agent priors and posteriors from limited behavioral data"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "ToMnet architecture with character net, mental state net, and prediction net", "label": "ToMnet architecture with character net, mental state net, and prediction net", "shape": "dot", "title": "ToMnet architecture with character net, mental state net, and prediction net"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Bayes-optimal action prediction on random agent species tasks", "label": "Bayes-optimal action prediction on random agent species tasks", "shape": "dot", "title": "Bayes-optimal action prediction on random agent species tasks"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy ToMnet observers for pre-deployment behavior prediction of novel AI systems", "label": "Deploy ToMnet observers for pre-deployment behavior prediction of novel AI systems", "shape": "dot", "title": "Deploy ToMnet observers for pre-deployment behavior prediction of novel AI systems"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Misinterpretation of agents\u0027 hidden beliefs leading to unsafe interactions", "label": "Misinterpretation of agents\u0027 hidden beliefs leading to unsafe interactions", "shape": "dot", "title": "Misinterpretation of agents\u0027 hidden beliefs leading to unsafe interactions"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Observer models ignore distinction between world state and agent belief state", "label": "Observer models ignore distinction between world state and agent belief state", "shape": "dot", "title": "Observer models ignore distinction between world state and agent belief state"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Behavioral discrepancies reveal agents can hold false beliefs", "label": "Behavioral discrepancies reveal agents can hold false beliefs", "shape": "dot", "title": "Behavioral discrepancies reveal agents can hold false beliefs"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Augment ToMnet with belief prediction heads to infer internal beliefs", "label": "Augment ToMnet with belief prediction heads to infer internal beliefs", "shape": "dot", "title": "Augment ToMnet with belief prediction heads to infer internal beliefs"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Supervised training of ToMnet on agent self-reported belief states", "label": "Supervised training of ToMnet on agent self-reported belief states", "shape": "dot", "title": "Supervised training of ToMnet on agent self-reported belief states"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Accurate belief-state prediction in Sally-Anne swap experiments", "label": "Accurate belief-state prediction in Sally-Anne swap experiments", "shape": "dot", "title": "Accurate belief-state prediction in Sally-Anne swap experiments"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Use belief-predicting ToMnet to detect agent false beliefs before deployment", "label": "Use belief-predicting ToMnet to detect agent false beliefs before deployment", "shape": "dot", "title": "Use belief-predicting ToMnet to detect agent false beliefs before deployment"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Entangled latent representations reduce interpretability of agent characteristics", "label": "Entangled latent representations reduce interpretability of agent characteristics", "shape": "dot", "title": "Entangled latent representations reduce interpretability of agent characteristics"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "High-dimensional character embeddings mix species and preferences", "label": "High-dimensional character embeddings mix species and preferences", "shape": "dot", "title": "High-dimensional character embeddings mix species and preferences"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Information bottleneck regularization can disentangle latent dimensions", "label": "Information bottleneck regularization can disentangle latent dimensions", "shape": "dot", "title": "Information bottleneck regularization can disentangle latent dimensions"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Variational Gaussian embeddings with KL penalty in ToMnet training", "label": "Variational Gaussian embeddings with KL penalty in ToMnet training", "shape": "dot", "title": "Variational Gaussian embeddings with KL penalty in ToMnet training"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Separated embedding dimensions for species and preference observed", "label": "Separated embedding dimensions for species and preference observed", "shape": "dot", "title": "Separated embedding dimensions for species and preference observed"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Regularize observer embeddings with variational information bottleneck during analysis", "label": "Regularize observer embeddings with variational information bottleneck during analysis", "shape": "dot", "title": "Regularize observer embeddings with variational information bottleneck during analysis"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Limit cycle convergence in gradient-based multi-agent learning", "label": "Limit cycle convergence in gradient-based multi-agent learning", "shape": "dot", "title": "Limit cycle convergence in gradient-based multi-agent learning"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Saddle-point Nash avoidance in zero-sum games with gradient descent", "label": "Saddle-point Nash avoidance in zero-sum games with gradient descent", "shape": "dot", "title": "Saddle-point Nash avoidance in zero-sum games with gradient descent"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Training instability in GAN discriminator\u2013generator dynamics", "label": "Training instability in GAN discriminator\u2013generator dynamics", "shape": "dot", "title": "Training instability in GAN discriminator\u2013generator dynamics"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Non-gradient-flow dynamics from independent gradients in games", "label": "Non-gradient-flow dynamics from independent gradients in games", "shape": "dot", "title": "Non-gradient-flow dynamics from independent gradients in games"}, {"color": "hsl(130, 80%, 60%)", "font": {"color": "white"}, "id": "Strict saddles representing Nash equilibria in zero-sum games", "label": "Strict saddles representing Nash equilibria in zero-sum games", "shape": "dot", "title": "Strict saddles representing Nash equilibria in zero-sum games"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Unbounded discriminator gradients in GANs", "label": "Unbounded discriminator gradients in GANs", "shape": "dot", "title": "Unbounded discriminator gradients in GANs"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Gradient-based learning avoidance of strict saddles almost surely", "label": "Gradient-based learning avoidance of strict saddles almost surely", "shape": "dot", "title": "Gradient-based learning avoidance of strict saddles almost surely"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Morse-Smale games provide gradient-like flows with finite critical points", "label": "Morse-Smale games provide gradient-like flows with finite critical points", "shape": "dot", "title": "Morse-Smale games provide gradient-like flows with finite critical points"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Potential games correspond to exact differential forms ensuring convergence", "label": "Potential games correspond to exact differential forms ensuring convergence", "shape": "dot", "title": "Potential games correspond to exact differential forms ensuring convergence"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Gradient penalty approximates Lipschitz constraint for discriminator", "label": "Gradient penalty approximates Lipschitz constraint for discriminator", "shape": "dot", "title": "Gradient penalty approximates Lipschitz constraint for discriminator"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Impose gradient-like flow structure via Morse-Smale game design", "label": "Impose gradient-like flow structure via Morse-Smale game design", "shape": "dot", "title": "Impose gradient-like flow structure via Morse-Smale game design"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Transform objectives into potential game", "label": "Transform objectives into potential game", "shape": "dot", "title": "Transform objectives into potential game"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Regularize discriminator gradients to stabilize training", "label": "Regularize discriminator gradients to stabilize training", "shape": "dot", "title": "Regularize discriminator gradients to stabilize training"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Morse-Smale condition enforcement using hyperbolic periodic orbit constraints", "label": "Morse-Smale condition enforcement using hyperbolic periodic orbit constraints", "shape": "dot", "title": "Morse-Smale condition enforcement using hyperbolic periodic orbit constraints"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Shared potential function as common cost", "label": "Shared potential function as common cost", "shape": "dot", "title": "Shared potential function as common cost"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Add L2 gradient penalty in WGAN loss", "label": "Add L2 gradient penalty in WGAN loss", "shape": "dot", "title": "Add L2 gradient penalty in WGAN loss"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Stable manifold theorem proof of generic avoidance of unstable cycles", "label": "Stable manifold theorem proof of generic avoidance of unstable cycles", "shape": "dot", "title": "Stable manifold theorem proof of generic avoidance of unstable cycles"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Convergence proof to differential Nash in potential games", "label": "Convergence proof to differential Nash in potential games", "shape": "dot", "title": "Convergence proof to differential Nash in potential games"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Empirical simulations showing limit cycles and effect of gradient penalty", "label": "Empirical simulations showing limit cycles and effect of gradient penalty", "shape": "dot", "title": "Empirical simulations showing limit cycles and effect of gradient penalty"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Design multi-agent reward structure to satisfy Morse-Smale conditions in model design", "label": "Design multi-agent reward structure to satisfy Morse-Smale conditions in model design", "shape": "dot", "title": "Design multi-agent reward structure to satisfy Morse-Smale conditions in model design"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Create shared potential function objectives to convert game into potential game at model design stage", "label": "Create shared potential function objectives to convert game into potential game at model design stage", "shape": "dot", "title": "Create shared potential function objectives to convert game into potential game at model design stage"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Apply gradient penalty regularization during GAN training to bound discriminator gradients", "label": "Apply gradient penalty regularization during GAN training to bound discriminator gradients", "shape": "dot", "title": "Apply gradient penalty regularization during GAN training to bound discriminator gradients"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "High expert supervision burden in robot task learning", "label": "High expert supervision burden in robot task learning", "shape": "dot", "title": "High expert supervision burden in robot task learning"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Requirement for expert action-state demonstrations in LfD", "label": "Requirement for expert action-state demonstrations in LfD", "shape": "dot", "title": "Requirement for expert action-state demonstrations in LfD"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Self-supervised exploration with goal relabeling provides supervision", "label": "Self-supervised exploration with goal relabeling provides supervision", "shape": "dot", "title": "Self-supervised exploration with goal relabeling provides supervision"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Goal-conditioned skill policy learning framework", "label": "Goal-conditioned skill policy learning framework", "shape": "dot", "title": "Goal-conditioned skill policy learning framework"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Recurrent multi-step GSP with previous action input", "label": "Recurrent multi-step GSP with previous action input", "shape": "dot", "title": "Recurrent multi-step GSP with previous action input"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Baxter rope manipulation and TurtleBot navigation success improvements with forward-consistent GSP", "label": "Baxter rope manipulation and TurtleBot navigation success improvements with forward-consistent GSP", "shape": "dot", "title": "Baxter rope manipulation and TurtleBot navigation success improvements with forward-consistent GSP"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Poor imitation accuracy from multi-modal action distributions", "label": "Poor imitation accuracy from multi-modal action distributions", "shape": "dot", "title": "Poor imitation accuracy from multi-modal action distributions"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Cross-entropy action matching with delta target causes high gradient variance", "label": "Cross-entropy action matching with delta target causes high gradient variance", "shape": "dot", "title": "Cross-entropy action matching with delta target causes high gradient variance"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Reaching goal more important than exact trajectory in imitation", "label": "Reaching goal more important than exact trajectory in imitation", "shape": "dot", "title": "Reaching goal more important than exact trajectory in imitation"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Forward consistency loss to penalize state prediction error in GSP training", "label": "Forward consistency loss to penalize state prediction error in GSP training", "shape": "dot", "title": "Forward consistency loss to penalize state prediction error in GSP training"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Forward model predicting next observation enables differentiable consistency", "label": "Forward model predicting next observation enables differentiable consistency", "shape": "dot", "title": "Forward model predicting next observation enables differentiable consistency"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Poor skill generalization to unseen environments", "label": "Poor skill generalization to unseen environments", "shape": "dot", "title": "Poor skill generalization to unseen environments"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Limited state coverage from random exploration during self-supervised data collection", "label": "Limited state coverage from random exploration during self-supervised data collection", "shape": "dot", "title": "Limited state coverage from random exploration during self-supervised data collection"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Intrinsic motivation can drive diverse exploration in self-supervised data collection", "label": "Intrinsic motivation can drive diverse exploration in self-supervised data collection", "shape": "dot", "title": "Intrinsic motivation can drive diverse exploration in self-supervised data collection"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Use curiosity-driven exploration to collect training data", "label": "Use curiosity-driven exploration to collect training data", "shape": "dot", "title": "Use curiosity-driven exploration to collect training data"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Intrinsic curiosity module controlling exploration policy", "label": "Intrinsic curiosity module controlling exploration policy", "shape": "dot", "title": "Intrinsic curiosity module controlling exploration policy"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "VizDoom imitation performance increases with curiosity-driven data", "label": "VizDoom imitation performance increases with curiosity-driven data", "shape": "dot", "title": "VizDoom imitation performance increases with curiosity-driven data"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Divergence from demonstration due to fixed step counts between goals", "label": "Divergence from demonstration due to fixed step counts between goals", "shape": "dot", "title": "Divergence from demonstration due to fixed step counts between goals"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Unknown variable number of actions needed to reach visual goal observations", "label": "Unknown variable number of actions needed to reach visual goal observations", "shape": "dot", "title": "Unknown variable number of actions needed to reach visual goal observations"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Goal attainment detection enables adaptive step control in imitation", "label": "Goal attainment detection enables adaptive step control in imitation", "shape": "dot", "title": "Goal attainment detection enables adaptive step control in imitation"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Goal recognizer network to detect goal completion during imitation", "label": "Goal recognizer network to detect goal completion during imitation", "shape": "dot", "title": "Goal recognizer network to detect goal completion during imitation"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Binary classifier conditioned on goal and current observation for goal recognition", "label": "Binary classifier conditioned on goal and current observation for goal recognition", "shape": "dot", "title": "Binary classifier conditioned on goal and current observation for goal recognition"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Improved navigation success with goal recognizer in TurtleBot experiments", "label": "Improved navigation success with goal recognizer in TurtleBot experiments", "shape": "dot", "title": "Improved navigation success with goal recognizer in TurtleBot experiments"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Train goal-conditioned skill policy with forward consistency loss using self-supervised exploration data", "label": "Train goal-conditioned skill policy with forward consistency loss using self-supervised exploration data", "shape": "dot", "title": "Train goal-conditioned skill policy with forward consistency loss using self-supervised exploration data"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Collect exploration data with curiosity-driven policy before GSP training", "label": "Collect exploration data with curiosity-driven policy before GSP training", "shape": "dot", "title": "Collect exploration data with curiosity-driven policy before GSP training"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy goal recognizer to dynamically stop action sequence during imitation", "label": "Deploy goal recognizer to dynamically stop action sequence during imitation", "shape": "dot", "title": "Deploy goal recognizer to dynamically stop action sequence during imitation"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Unreliable real-world applicability assessment of semi-supervised learning algorithms", "label": "Unreliable real-world applicability assessment of semi-supervised learning algorithms", "shape": "dot", "title": "Unreliable real-world applicability assessment of semi-supervised learning algorithms"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Unrealistic evaluation protocols in SSL research", "label": "Unrealistic evaluation protocols in SSL research", "shape": "dot", "title": "Unrealistic evaluation protocols in SSL research"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Standardised evaluation frameworks mitigate protocol bias", "label": "Standardised evaluation frameworks mitigate protocol bias", "shape": "dot", "title": "Standardised evaluation frameworks mitigate protocol bias"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Comprehensive evaluation methodology for SSL realism", "label": "Comprehensive evaluation methodology for SSL realism", "shape": "dot", "title": "Comprehensive evaluation methodology for SSL realism"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Unified modular SSL evaluation software on Wide-ResNet with large-scale hyperparameter optimisation", "label": "Unified modular SSL evaluation software on Wide-ResNet with large-scale hyperparameter optimisation", "shape": "dot", "title": "Unified modular SSL evaluation software on Wide-ResNet with large-scale hyperparameter optimisation"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Experiments show tuned supervised baseline narrows SSL advantage", "label": "Experiments show tuned supervised baseline narrows SSL advantage", "shape": "dot", "title": "Experiments show tuned supervised baseline narrows SSL advantage"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Transfer learning fine-tuning outperforms SSL on CIFAR-10", "label": "Transfer learning fine-tuning outperforms SSL on CIFAR-10", "shape": "dot", "title": "Transfer learning fine-tuning outperforms SSL on CIFAR-10"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Class distribution mismatch degrades SSL performance", "label": "Class distribution mismatch degrades SSL performance", "shape": "dot", "title": "Class distribution mismatch degrades SSL performance"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "SSL performance sensitivity to labelled-unlabelled data amounts", "label": "SSL performance sensitivity to labelled-unlabelled data amounts", "shape": "dot", "title": "SSL performance sensitivity to labelled-unlabelled data amounts"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Small validation sets hinder reliable hyperparameter selection", "label": "Small validation sets hinder reliable hyperparameter selection", "shape": "dot", "title": "Small validation sets hinder reliable hyperparameter selection"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Unified reimplementation reproduces relative SSL rankings across datasets", "label": "Unified reimplementation reproduces relative SSL rankings across datasets", "shape": "dot", "title": "Unified reimplementation reproduces relative SSL rankings across datasets"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt shared implementation framework when comparing SSL methods", "label": "Adopt shared implementation framework when comparing SSL methods", "shape": "dot", "title": "Adopt shared implementation framework when comparing SSL methods"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Include high-quality supervised and transfer learning baselines in SSL evaluations", "label": "Include high-quality supervised and transfer learning baselines in SSL evaluations", "shape": "dot", "title": "Include high-quality supervised and transfer learning baselines in SSL evaluations"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Evaluate SSL robustness under class-distribution mismatch scenarios", "label": "Evaluate SSL robustness under class-distribution mismatch scenarios", "shape": "dot", "title": "Evaluate SSL robustness under class-distribution mismatch scenarios"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Systematically vary labelled and unlabeled data amounts during SSL evaluation", "label": "Systematically vary labelled and unlabeled data amounts during SSL evaluation", "shape": "dot", "title": "Systematically vary labelled and unlabeled data amounts during SSL evaluation"}, {"color": "hsl(50, 80%, 60%)", "font": {"color": "white"}, "id": "Use realistically small validation sets or cross-validation for hyperparameter tuning in SSL", "label": "Use realistically small validation sets or cross-validation for hyperparameter tuning in SSL", "shape": "dot", "title": "Use realistically small validation sets or cross-validation for hyperparameter tuning in SSL"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "Inferior translation quality in neural machine translation systems", "label": "Inferior translation quality in neural machine translation systems", "shape": "dot", "title": "Inferior translation quality in neural machine translation systems"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Training instability in deep neural machine translation models", "label": "Training instability in deep neural machine translation models", "shape": "dot", "title": "Training instability in deep neural machine translation models"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Prolonged training time for NMT model convergence", "label": "Prolonged training time for NMT model convergence", "shape": "dot", "title": "Prolonged training time for NMT model convergence"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "Absence of universally beneficial training techniques in baseline architectures", "label": "Absence of universally beneficial training techniques in baseline architectures", "shape": "dot", "title": "Absence of universally beneficial training techniques in baseline architectures"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Gradient explosions in recurrent networks during NMT training", "label": "Gradient explosions in recurrent networks during NMT training", "shape": "dot", "title": "Gradient explosions in recurrent networks during NMT training"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Sequential dependency limits parallelism in RNMT training", "label": "Sequential dependency limits parallelism in RNMT training", "shape": "dot", "title": "Sequential dependency limits parallelism in RNMT training"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "Suboptimal encoder-decoder component pairing in NMT models", "label": "Suboptimal encoder-decoder component pairing in NMT models", "shape": "dot", "title": "Suboptimal encoder-decoder component pairing in NMT models"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "Cross-architecture applicability of label smoothing, layer normalization, multi-head attention, synchronous training", "label": "Cross-architecture applicability of label smoothing, layer normalization, multi-head attention, synchronous training", "shape": "dot", "title": "Cross-architecture applicability of label smoothing, layer normalization, multi-head attention, synchronous training"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Layer normalization stabilizes RNN training with multi-head attention", "label": "Layer normalization stabilizes RNN training with multi-head attention", "shape": "dot", "title": "Layer normalization stabilizes RNN training with multi-head attention"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Synchronous replica training with warmup improves convergence speed and quality", "label": "Synchronous replica training with warmup improves convergence speed and quality", "shape": "dot", "title": "Synchronous replica training with warmup improves convergence speed and quality"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "Hybridizing encoders and decoders leverages complementary strengths", "label": "Hybridizing encoders and decoders leverages complementary strengths", "shape": "dot", "title": "Hybridizing encoders and decoders leverages complementary strengths"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "Enhancing RNMT with modern techniques to form RNMT+", "label": "Enhancing RNMT with modern techniques to form RNMT+", "shape": "dot", "title": "Enhancing RNMT with modern techniques to form RNMT+"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Integrating per-gate layer normalization and adaptive gradient clipping inside LSTM cells", "label": "Integrating per-gate layer normalization and adaptive gradient clipping inside LSTM cells", "shape": "dot", "title": "Integrating per-gate layer normalization and adaptive gradient clipping inside LSTM cells"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Adopting synchronous training regime with tailored learning rate schedule", "label": "Adopting synchronous training regime with tailored learning rate schedule", "shape": "dot", "title": "Adopting synchronous training regime with tailored learning rate schedule"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "Combining Transformer encoder with RNMT+ decoder in hybrid models", "label": "Combining Transformer encoder with RNMT+ decoder in hybrid models", "shape": "dot", "title": "Combining Transformer encoder with RNMT+ decoder in hybrid models"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Augmenting encoder via cascaded or multi-column integration of Transformer and RNMT+ layers", "label": "Augmenting encoder via cascaded or multi-column integration of Transformer and RNMT+ layers", "shape": "dot", "title": "Augmenting encoder via cascaded or multi-column integration of Transformer and RNMT+ layers"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "RNMT+ architecture with 6 bidirectional LSTM layers, multi-head additive attention, label smoothing, synchronous training", "label": "RNMT+ architecture with 6 bidirectional LSTM layers, multi-head additive attention, label smoothing, synchronous training", "shape": "dot", "title": "RNMT+ architecture with 6 bidirectional LSTM layers, multi-head additive attention, label smoothing, synchronous training"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Adaptive gradient clipping using moving average thresholding", "label": "Adaptive gradient clipping using moving average thresholding", "shape": "dot", "title": "Adaptive gradient clipping using moving average thresholding"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Learning rate schedule with linear warmup and exponential decay scaled by replica count", "label": "Learning rate schedule with linear warmup and exponential decay scaled by replica count", "shape": "dot", "title": "Learning rate schedule with linear warmup and exponential decay scaled by replica count"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "Hybrid cascaded encoder stacking transformer layers on pre-trained RNMT+ encoder", "label": "Hybrid cascaded encoder stacking transformer layers on pre-trained RNMT+ encoder", "shape": "dot", "title": "Hybrid cascaded encoder stacking transformer layers on pre-trained RNMT+ encoder"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "Hybrid multi-column encoder concatenating outputs of Transformer and RNMT+ encoders", "label": "Hybrid multi-column encoder concatenating outputs of Transformer and RNMT+ encoders", "shape": "dot", "title": "Hybrid multi-column encoder concatenating outputs of Transformer and RNMT+ encoders"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "BLEU improvement to 41.00 and 28.49 on WMT14 En\u2192Fr/En\u2192De for RNMT+", "label": "BLEU improvement to 41.00 and 28.49 on WMT14 En\u2192Fr/En\u2192De for RNMT+", "shape": "dot", "title": "BLEU improvement to 41.00 and 28.49 on WMT14 En\u2192Fr/En\u2192De for RNMT+"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Ablation study showing quality drop without key techniques and instability without layer normalization", "label": "Ablation study showing quality drop without key techniques and instability without layer normalization", "shape": "dot", "title": "Ablation study showing quality drop without key techniques and instability without layer normalization"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Faster convergence and stable performance with synchronous training", "label": "Faster convergence and stable performance with synchronous training", "shape": "dot", "title": "Faster convergence and stable performance with synchronous training"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "Hybrid models achieve 41.67 BLEU surpassing individual architectures", "label": "Hybrid models achieve 41.67 BLEU surpassing individual architectures", "shape": "dot", "title": "Hybrid models achieve 41.67 BLEU surpassing individual architectures"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy RNMT+ model incorporating multi-head attention, label smoothing, per-gate layer normalization, and synchronous training in NMT systems", "label": "Deploy RNMT+ model incorporating multi-head attention, label smoothing, per-gate layer normalization, and synchronous training in NMT systems", "shape": "dot", "title": "Deploy RNMT+ model incorporating multi-head attention, label smoothing, per-gate layer normalization, and synchronous training in NMT systems"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Apply per-gate layer normalization and adaptive gradient clipping during RNMT training", "label": "Apply per-gate layer normalization and adaptive gradient clipping during RNMT training", "shape": "dot", "title": "Apply per-gate layer normalization and adaptive gradient clipping during RNMT training"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Use synchronous replica training with warmup learning rate schedule for NMT", "label": "Use synchronous replica training with warmup learning rate schedule for NMT", "shape": "dot", "title": "Use synchronous replica training with warmup learning rate schedule for NMT"}, {"color": "hsl(120, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt hybrid NMT architecture with Transformer encoder and RNMT+ decoder using cascaded or multi-column encoders", "label": "Adopt hybrid NMT architecture with Transformer encoder and RNMT+ decoder using cascaded or multi-column encoders", "shape": "dot", "title": "Adopt hybrid NMT architecture with Transformer encoder and RNMT+ decoder using cascaded or multi-column encoders"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "unaligned behavior in advanced AI systems", "label": "unaligned behavior in advanced AI systems", "shape": "dot", "title": "unaligned behavior in advanced AI systems"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "deceptive or dishonest AI behavior in AI systems", "label": "deceptive or dishonest AI behavior in AI systems", "shape": "dot", "title": "deceptive or dishonest AI behavior in AI systems"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "high complexity tasks exceed human judgment capacity", "label": "high complexity tasks exceed human judgment capacity", "shape": "dot", "title": "high complexity tasks exceed human judgment capacity"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "misleading alignment signals from human feedback", "label": "misleading alignment signals from human feedback", "shape": "dot", "title": "misleading alignment signals from human feedback"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "refuting lies is easier than constructing lies in debate game", "label": "refuting lies is easier than constructing lies in debate game", "shape": "dot", "title": "refuting lies is easier than constructing lies in debate game"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "debate complexity equivalence to PSPACE enabling exponential information amplification", "label": "debate complexity equivalence to PSPACE enabling exponential information amplification", "shape": "dot", "title": "debate complexity equivalence to PSPACE enabling exponential information amplification"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "short unbranched debates suffice due to adversarial focus", "label": "short unbranched debates suffice due to adversarial focus", "shape": "dot", "title": "short unbranched debates suffice due to adversarial focus"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "adversarial debate between duplicate agents supervised by human judge", "label": "adversarial debate between duplicate agents supervised by human judge", "shape": "dot", "title": "adversarial debate between duplicate agents supervised by human judge"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "human judges reward honesty and flaw detection paradigm", "label": "human judges reward honesty and flaw detection paradigm", "shape": "dot", "title": "human judges reward honesty and flaw detection paradigm"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "limiting debate length with answer precommitment reduces argument space", "label": "limiting debate length with answer precommitment reduces argument space", "shape": "dot", "title": "limiting debate length with answer precommitment reduces argument space"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "train reward predictor to scale human debate judgments", "label": "train reward predictor to scale human debate judgments", "shape": "dot", "title": "train reward predictor to scale human debate judgments"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "self-play reinforcement learning of debate agents", "label": "self-play reinforcement learning of debate agents", "shape": "dot", "title": "self-play reinforcement learning of debate agents"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "answer precommitment before debate turns", "label": "answer precommitment before debate turns", "shape": "dot", "title": "answer precommitment before debate turns"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "sparse pixel MNIST debate experimental setup", "label": "sparse pixel MNIST debate experimental setup", "shape": "dot", "title": "sparse pixel MNIST debate experimental setup"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "human single-pixel reveal image debate website", "label": "human single-pixel reveal image debate website", "shape": "dot", "title": "human single-pixel reveal image debate website"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "sharing activations between agents for transparency", "label": "sharing activations between agents for transparency", "shape": "dot", "title": "sharing activations between agents for transparency"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "training judge models to predict human reward", "label": "training judge models to predict human reward", "shape": "dot", "title": "training judge models to predict human reward"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "PSPACE proof demonstrating debate power", "label": "PSPACE proof demonstrating debate power", "shape": "dot", "title": "PSPACE proof demonstrating debate power"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "MNIST debate increases judge accuracy from 59.4% to 88.9%", "label": "MNIST debate increases judge accuracy from 59.4% to 88.9%", "shape": "dot", "title": "MNIST debate increases judge accuracy from 59.4% to 88.9%"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "informal human debate indicates honest agent usually wins", "label": "informal human debate indicates honest agent usually wins", "shape": "dot", "title": "informal human debate indicates honest agent usually wins"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "adversarial debate fine-tuning to align AI models", "label": "adversarial debate fine-tuning to align AI models", "shape": "dot", "title": "adversarial debate fine-tuning to align AI models"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "deploy reward predictor to automate large-scale debate training", "label": "deploy reward predictor to automate large-scale debate training", "shape": "dot", "title": "deploy reward predictor to automate large-scale debate training"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "training performance bottlenecks in large-scale dynamic neural networks", "label": "training performance bottlenecks in large-scale dynamic neural networks", "shape": "dot", "title": "training performance bottlenecks in large-scale dynamic neural networks"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "host-driven out-of-graph control flow synchronization overhead in distributed ML execution", "label": "host-driven out-of-graph control flow synchronization overhead in distributed ML execution", "shape": "dot", "title": "host-driven out-of-graph control flow synchronization overhead in distributed ML execution"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "in-graph dynamic control flow enabling whole-program optimization and asynchronous execution in ML frameworks", "label": "in-graph dynamic control flow enabling whole-program optimization and asynchronous execution in ML frameworks", "shape": "dot", "title": "in-graph dynamic control flow enabling whole-program optimization and asynchronous execution in ML frameworks"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "embedding dynamic control flow primitives within dataflow graphs for distributed execution", "label": "embedding dynamic control flow primitives within dataflow graphs for distributed execution", "shape": "dot", "title": "embedding dynamic control flow primitives within dataflow graphs for distributed execution"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "TensorFlow control-flow primitives (Switch, Merge, Enter, Exit, NextIteration) with executor tag frames", "label": "TensorFlow control-flow primitives (Switch, Merge, Enter, Exit, NextIteration) with executor tag frames", "shape": "dot", "title": "TensorFlow control-flow primitives (Switch, Merge, Enter, Exit, NextIteration) with executor tag frames"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "microbenchmark 8-GPU iteration rate improvements showing 5\u00d7 speedup over out-of-graph loops", "label": "microbenchmark 8-GPU iteration rate improvements showing 5\u00d7 speedup over out-of-graph loops", "shape": "dot", "title": "microbenchmark 8-GPU iteration rate improvements showing 5\u00d7 speedup over out-of-graph loops"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Implement in-graph dynamic control flow primitives in ML frameworks", "label": "Implement in-graph dynamic control flow primitives in ML frameworks", "shape": "dot", "title": "Implement in-graph dynamic control flow primitives in ML frameworks"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "memory exhaustion in GPU training of long sequence RNNs", "label": "memory exhaustion in GPU training of long sequence RNNs", "shape": "dot", "title": "memory exhaustion in GPU training of long sequence RNNs"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "retention of intermediate tensors during backpropagation across long sequences leading to large GPU memory footprint", "label": "retention of intermediate tensors during backpropagation across long sequences leading to large GPU memory footprint", "shape": "dot", "title": "retention of intermediate tensors during backpropagation across long sequences leading to large GPU memory footprint"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "overlapping compute with asynchronous memory swapping between GPU and CPU mitigates memory usage", "label": "overlapping compute with asynchronous memory swapping between GPU and CPU mitigates memory usage", "shape": "dot", "title": "overlapping compute with asynchronous memory swapping between GPU and CPU mitigates memory usage"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "stack-based state saving and multi-stream GPU I/O for overlapped swapping", "label": "stack-based state saving and multi-stream GPU I/O for overlapped swapping", "shape": "dot", "title": "stack-based state saving and multi-stream GPU I/O for overlapped swapping"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "GPU-to-CPU tensor swapping triggered by memory threshold using separate CUDA streams", "label": "GPU-to-CPU tensor swapping triggered by memory threshold using separate CUDA streams", "shape": "dot", "title": "GPU-to-CPU tensor swapping triggered by memory threshold using separate CUDA streams"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "LSTM training sequence length doubled from 500 to 1000 with negligible overhead via swapping", "label": "LSTM training sequence length doubled from 500 to 1000 with negligible overhead via swapping", "shape": "dot", "title": "LSTM training sequence length doubled from 500 to 1000 with negligible overhead via swapping"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "Enable asynchronous GPU memory swapping for intermediate tensors during training loops", "label": "Enable asynchronous GPU memory swapping for intermediate tensors during training loops", "shape": "dot", "title": "Enable asynchronous GPU memory swapping for intermediate tensors during training loops"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "limited model expressiveness due to absence of dynamic control flow in ML frameworks", "label": "limited model expressiveness due to absence of dynamic control flow in ML frameworks", "shape": "dot", "title": "limited model expressiveness due to absence of dynamic control flow in ML frameworks"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "static unrolling and client-side conditionals limit conditional computation and parallelism", "label": "static unrolling and client-side conditionals limit conditional computation and parallelism", "shape": "dot", "title": "static unrolling and client-side conditionals limit conditional computation and parallelism"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "local executor parallel-iteration tagging enabling nested control flow and distributed loops", "label": "local executor parallel-iteration tagging enabling nested control flow and distributed loops", "shape": "dot", "title": "local executor parallel-iteration tagging enabling nested control flow and distributed loops"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Deep Q-Network implementation speedup of 21% using in-graph control flow compared to baseline", "label": "Deep Q-Network implementation speedup of 21% using in-graph control flow compared to baseline", "shape": "dot", "title": "Deep Q-Network implementation speedup of 21% using in-graph control flow compared to baseline"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt in-graph dynamic control flow for reinforcement learning model graphs to enable parallel conditional execution", "label": "Adopt in-graph dynamic control flow for reinforcement learning model graphs to enable parallel conditional execution", "shape": "dot", "title": "Adopt in-graph dynamic control flow for reinforcement learning model graphs to enable parallel conditional execution"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "confounding bias in causal effect estimates from observational multiple-cause data", "label": "confounding bias in causal effect estimates from observational multiple-cause data", "shape": "dot", "title": "confounding bias in causal effect estimates from observational multiple-cause data"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "unobserved multi-cause confounders inducing dependence among assigned causes", "label": "unobserved multi-cause confounders inducing dependence among assigned causes", "shape": "dot", "title": "unobserved multi-cause confounders inducing dependence among assigned causes"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "conditional independence of causes given latent variable removes multi-cause confounding", "label": "conditional independence of causes given latent variable removes multi-cause confounding", "shape": "dot", "title": "conditional independence of causes given latent variable removes multi-cause confounding"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "no unobserved single-cause confounders assumption weakens ignorability requirement", "label": "no unobserved single-cause confounders assumption weakens ignorability requirement", "shape": "dot", "title": "no unobserved single-cause confounders assumption weakens ignorability requirement"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "inferring substitute confounder via latent factor model of causes", "label": "inferring substitute confounder via latent factor model of causes", "shape": "dot", "title": "inferring substitute confounder via latent factor model of causes"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "predictive model checks ensure factor model captures cause distribution", "label": "predictive model checks ensure factor model captures cause distribution", "shape": "dot", "title": "predictive model checks ensure factor model captures cause distribution"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "probabilistic factor model with per-individual latent variable for causes", "label": "probabilistic factor model with per-individual latent variable for causes", "shape": "dot", "title": "probabilistic factor model with per-individual latent variable for causes"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "posterior expectation of latent variable as substitute confounder", "label": "posterior expectation of latent variable as substitute confounder", "shape": "dot", "title": "posterior expectation of latent variable as substitute confounder"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "iterated expectation conditioning on substitute confounder for outcome estimation", "label": "iterated expectation conditioning on substitute confounder for outcome estimation", "shape": "dot", "title": "iterated expectation conditioning on substitute confounder for outcome estimation"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "posterior predictive check using held-out causes and log-likelihood discrepancy", "label": "posterior predictive check using held-out causes and log-likelihood discrepancy", "shape": "dot", "title": "posterior predictive check using held-out causes and log-likelihood discrepancy"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "simulation RMSE reduction in GWAS data using deconfounder (49.6\u219241.2)", "label": "simulation RMSE reduction in GWAS data using deconfounder (49.6\u219241.2)", "shape": "dot", "title": "simulation RMSE reduction in GWAS data using deconfounder (49.6\u219241.2)"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "bias reduction in smoking expenses study with quadratic factor model", "label": "bias reduction in smoking expenses study with quadratic factor model", "shape": "dot", "title": "bias reduction in smoking expenses study with quadratic factor model"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Apply deconfounder pipeline (factor modeling, predictive checking, substitute confounder, outcome modeling) during causal inference analyses", "label": "Apply deconfounder pipeline (factor modeling, predictive checking, substitute confounder, outcome modeling) during causal inference analyses", "shape": "dot", "title": "Apply deconfounder pipeline (factor modeling, predictive checking, substitute confounder, outcome modeling) during causal inference analyses"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Require predictive score threshold \u22650.1 before accepting factor model in deconfounder", "label": "Require predictive score threshold \u22650.1 before accepting factor model in deconfounder", "shape": "dot", "title": "Require predictive score threshold \u22650.1 before accepting factor model in deconfounder"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Select lowest-capacity factor model that passes predictive checks to minimize estimator variance", "label": "Select lowest-capacity factor model that passes predictive checks to minimize estimator variance", "shape": "dot", "title": "Select lowest-capacity factor model that passes predictive checks to minimize estimator variance"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "Merge highly correlated causes prior to factor modeling to satisfy overlap condition", "label": "Merge highly correlated causes prior to factor modeling to satisfy overlap condition", "shape": "dot", "title": "Merge highly correlated causes prior to factor modeling to satisfy overlap condition"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Uninterpretable middle-layer representations in CNNs", "label": "Uninterpretable middle-layer representations in CNNs", "shape": "dot", "title": "Uninterpretable middle-layer representations in CNNs"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Chaotic mixture of object parts and textures in middle-layer feature maps", "label": "Chaotic mixture of object parts and textures in middle-layer feature maps", "shape": "dot", "title": "Chaotic mixture of object parts and textures in middle-layer feature maps"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Performance degradation from direct interpretability constraints in CNNs", "label": "Performance degradation from direct interpretability constraints in CNNs", "shape": "dot", "title": "Performance degradation from direct interpretability constraints in CNNs"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Interpretability\u2013performance trade-off in direct feature disentanglement", "label": "Interpretability\u2013performance trade-off in direct feature disentanglement", "shape": "dot", "title": "Interpretability\u2013performance trade-off in direct feature disentanglement"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Object-part specific filter representation in CNNs", "label": "Object-part specific filter representation in CNNs", "shape": "dot", "title": "Object-part specific filter representation in CNNs"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Separate explainer network preserves performance while improving interpretability in CNNs", "label": "Separate explainer network preserves performance while improving interpretability in CNNs", "shape": "dot", "title": "Separate explainer network preserves performance while improving interpretability in CNNs"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Autoencoder-style explainer attached to performer CNN for feature distillation", "label": "Autoencoder-style explainer attached to performer CNN for feature distillation", "shape": "dot", "title": "Autoencoder-style explainer attached to performer CNN for feature distillation"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Dual-track encoder with interpretable and ordinary paths in explainer", "label": "Dual-track encoder with interpretable and ordinary paths in explainer", "shape": "dot", "title": "Dual-track encoder with interpretable and ordinary paths in explainer"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Template-based filter loss for unsupervised part disentanglement", "label": "Template-based filter loss for unsupervised part disentanglement", "shape": "dot", "title": "Template-based filter loss for unsupervised part disentanglement"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Decoder reconstruction of performer upper-layer features", "label": "Decoder reconstruction of performer upper-layer features", "shape": "dot", "title": "Decoder reconstruction of performer upper-layer features"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Probability weighting of interpretable track contribution parameter p", "label": "Probability weighting of interpretable track contribution parameter p", "shape": "dot", "title": "Probability weighting of interpretable track contribution parameter p"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Location instability reduction of explainer filters over performer", "label": "Location instability reduction of explainer filters over performer", "shape": "dot", "title": "Location instability reduction of explainer filters over performer"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Minor classification accuracy gap after explainer reconstruction", "label": "Minor classification accuracy gap after explainer reconstruction", "shape": "dot", "title": "Minor classification accuracy gap after explainer reconstruction"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "High interpretable track contribution (p 0.8\u20130.96)", "label": "High interpretable track contribution (p 0.8\u20130.96)", "shape": "dot", "title": "High interpretable track contribution (p 0.8\u20130.96)"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Attach autoencoder-style explainer networks with interpretable track to pretrained CNNs during deployment for real-time interpretability", "label": "Attach autoencoder-style explainer networks with interpretable track to pretrained CNNs during deployment for real-time interpretability", "shape": "dot", "title": "Attach autoencoder-style explainer networks with interpretable track to pretrained CNNs during deployment for real-time interpretability"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Fine-tune pretrained CNNs jointly with explainer and filter loss before deployment to achieve part-level feature disentanglement", "label": "Fine-tune pretrained CNNs jointly with explainer and filter loss before deployment to achieve part-level feature disentanglement", "shape": "dot", "title": "Fine-tune pretrained CNNs jointly with explainer and filter loss before deployment to achieve part-level feature disentanglement"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Poor generalization of meta-learned models to unseen tasks", "label": "Poor generalization of meta-learned models to unseen tasks", "shape": "dot", "title": "Poor generalization of meta-learned models to unseen tasks"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Biased initial model over-performs on sampled meta-training tasks", "label": "Biased initial model over-performs on sampled meta-training tasks", "shape": "dot", "title": "Biased initial model over-performs on sampled meta-training tasks"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Absence of task-agnostic prior leads to biased initial performance in meta-learning", "label": "Absence of task-agnostic prior leads to biased initial performance in meta-learning", "shape": "dot", "title": "Absence of task-agnostic prior leads to biased initial performance in meta-learning"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Entropy regularization to enforce task-agnostic prior in classification tasks", "label": "Entropy regularization to enforce task-agnostic prior in classification tasks", "shape": "dot", "title": "Entropy regularization to enforce task-agnostic prior in classification tasks"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "\u03bb(\u2212H_before + H_after) entropy term in meta-training objective", "label": "\u03bb(\u2212H_before + H_after) entropy term in meta-training objective", "shape": "dot", "title": "\u03bb(\u2212H_before + H_after) entropy term in meta-training objective"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Entropy-TAML accuracy improvements on Omniglot and MiniImagenet benchmarks", "label": "Entropy-TAML accuracy improvements on Omniglot and MiniImagenet benchmarks", "shape": "dot", "title": "Entropy-TAML accuracy improvements on Omniglot and MiniImagenet benchmarks"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Apply entropy-based task-agnostic regularization during meta-training", "label": "Apply entropy-based task-agnostic regularization during meta-training", "shape": "dot", "title": "Apply entropy-based task-agnostic regularization during meta-training"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Loss-inequality minimisation regularization to enforce task-agnostic prior", "label": "Loss-inequality minimisation regularization to enforce task-agnostic prior", "shape": "dot", "title": "Loss-inequality minimisation regularization to enforce task-agnostic prior"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "IE({losses}) regularisation term with Theil, GE, Gini, Atkinson or VL indices", "label": "IE({losses}) regularisation term with Theil, GE, Gini, Atkinson or VL indices", "shape": "dot", "title": "IE({losses}) regularisation term with Theil, GE, Gini, Atkinson or VL indices"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Inequality-TAML performance gains on vision and RL tasks", "label": "Inequality-TAML performance gains on vision and RL tasks", "shape": "dot", "title": "Inequality-TAML performance gains on vision and RL tasks"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Apply inequality-measure based task-agnostic regularization during meta-training", "label": "Apply inequality-measure based task-agnostic regularization during meta-training", "shape": "dot", "title": "Apply inequality-measure based task-agnostic regularization during meta-training"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "catastrophic policy degradation in reinforcement learning", "label": "catastrophic policy degradation in reinforcement learning", "shape": "dot", "title": "catastrophic policy degradation in reinforcement learning"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "excessive sample complexity in deep reinforcement learning", "label": "excessive sample complexity in deep reinforcement learning", "shape": "dot", "title": "excessive sample complexity in deep reinforcement learning"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "high improvement penalty from inaccurate Q-value estimation", "label": "high improvement penalty from inaccurate Q-value estimation", "shape": "dot", "title": "high improvement penalty from inaccurate Q-value estimation"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "large variance in Q-value estimates for low-frequency actions", "label": "large variance in Q-value estimates for low-frequency actions", "shape": "dot", "title": "large variance in Q-value estimates for low-frequency actions"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "unbounded action probability shifts in existing constrained policy updates", "label": "unbounded action probability shifts in existing constrained policy updates", "shape": "dot", "title": "unbounded action probability shifts in existing constrained policy updates"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "variance of improvement penalty proportional to |beta-pi|^2/beta", "label": "variance of improvement penalty proportional to |beta-pi|^2/beta", "shape": "dot", "title": "variance of improvement penalty proportional to |beta-pi|^2/beta"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "bounding action probability ratio limits improvement penalty", "label": "bounding action probability ratio limits improvement penalty", "shape": "dot", "title": "bounding action probability ratio limits improvement penalty"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "multiplicative reroute constraint on policy update", "label": "multiplicative reroute constraint on policy update", "shape": "dot", "title": "multiplicative reroute constraint on policy update"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "statewise linear program for optimal policy under reroute constraint", "label": "statewise linear program for optimal policy under reroute constraint", "shape": "dot", "title": "statewise linear program for optimal policy under reroute constraint"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "actor computes non-parametric optimal policy and learner imitates via KL", "label": "actor computes non-parametric optimal policy and learner imitates via KL", "shape": "dot", "title": "actor computes non-parametric optimal policy and learner imitates via KL"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Max-Reroute algorithm with parameters cmin,cmax", "label": "Max-Reroute algorithm with parameters cmin,cmax", "shape": "dot", "title": "Max-Reroute algorithm with parameters cmin,cmax"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "RBI distributed actor-learner architecture", "label": "RBI distributed actor-learner architecture", "shape": "dot", "title": "RBI distributed actor-learner architecture"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "advantage variance weighted prioritization in learner loss", "label": "advantage variance weighted prioritization in learner loss", "shape": "dot", "title": "advantage variance weighted prioritization in learner loss"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "theoretical guarantee of positive improvement step", "label": "theoretical guarantee of positive improvement step", "shape": "dot", "title": "theoretical guarantee of positive improvement step"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "reduced regret in two-armed Gaussian bandit experiments", "label": "reduced regret in two-armed Gaussian bandit experiments", "shape": "dot", "title": "reduced regret in two-armed Gaussian bandit experiments"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "improved Atari performance from human observation data", "label": "improved Atari performance from human observation data", "shape": "dot", "title": "improved Atari performance from human observation data"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "superior performance over Ape-X in distributed Atari experiments", "label": "superior performance over Ape-X in distributed Atari experiments", "shape": "dot", "title": "superior performance over Ape-X in distributed Atari experiments"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Apply reroute constrained policy improvement (RBI) during off-policy batch RL fine-tuning", "label": "Apply reroute constrained policy improvement (RBI) during off-policy batch RL fine-tuning", "shape": "dot", "title": "Apply reroute constrained policy improvement (RBI) during off-policy batch RL fine-tuning"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate Max-Reroute per-state optimization in actor policy during RL training", "label": "Integrate Max-Reroute per-state optimization in actor policy during RL training", "shape": "dot", "title": "Integrate Max-Reroute per-state optimization in actor policy during RL training"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Set reroute parameters cmin=0.1,cmax=2 and mix 10% greedy policy in distributed RL", "label": "Set reroute parameters cmin=0.1,cmax=2 and mix 10% greedy policy in distributed RL", "shape": "dot", "title": "Set reroute parameters cmin=0.1,cmax=2 and mix 10% greedy policy in distributed RL"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Weight policy loss by advantage variance prioritization during learner updates", "label": "Weight policy loss by advantage variance prioritization during learner updates", "shape": "dot", "title": "Weight policy loss by advantage variance prioritization during learner updates"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "low data efficiency of greedy policy improvement in deep RL", "label": "low data efficiency of greedy policy improvement in deep RL", "shape": "dot", "title": "low data efficiency of greedy policy improvement in deep RL"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Misestimation of agent reward functions in online partially observable environments", "label": "Misestimation of agent reward functions in online partially observable environments", "shape": "dot", "title": "Misestimation of agent reward functions in online partially observable environments"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Computational timeouts of batch IRL under streaming data constraints", "label": "Computational timeouts of batch IRL under streaming data constraints", "shape": "dot", "title": "Computational timeouts of batch IRL under streaming data constraints"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Occluded trajectory segments causing missing feature data", "label": "Occluded trajectory segments causing missing feature data", "shape": "dot", "title": "Occluded trajectory segments causing missing feature data"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Incremental session-based reward update improves scalability in streaming IRL", "label": "Incremental session-based reward update improves scalability in streaming IRL", "shape": "dot", "title": "Incremental session-based reward update improves scalability in streaming IRL"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Latent variable expectation over hidden trajectory data recovers missing information", "label": "Latent variable expectation over hidden trajectory data recovers missing information", "shape": "dot", "title": "Latent variable expectation over hidden trajectory data recovers missing information"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Online IRL framework I2RL enabling continuous sessions", "label": "Online IRL framework I2RL enabling continuous sessions", "shape": "dot", "title": "Online IRL framework I2RL enabling continuous sessions"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Latent maximum entropy formulation with EM optimizes reward under occlusion", "label": "Latent maximum entropy formulation with EM optimizes reward under occlusion", "shape": "dot", "title": "Latent maximum entropy formulation with EM optimizes reward under occlusion"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Incremental LME algorithm with per-session E-step and M-step updating feature expectations", "label": "Incremental LME algorithm with per-session E-step and M-step updating feature expectations", "shape": "dot", "title": "Incremental LME algorithm with per-session E-step and M-step updating feature expectations"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Log-likelihood convergence stopping criterion for I2RL session termination", "label": "Log-likelihood convergence stopping criterion for I2RL session termination", "shape": "dot", "title": "Log-likelihood convergence stopping criterion for I2RL session termination"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Monotonic likelihood improvement and probabilistic convergence guarantees for incremental LME", "label": "Monotonic likelihood improvement and probabilistic convergence guarantees for incremental LME", "shape": "dot", "title": "Monotonic likelihood improvement and probabilistic convergence guarantees for incremental LME"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Perimeter patrol experiments showing 4x speedup and higher success rate compared to batch LME", "label": "Perimeter patrol experiments showing 4x speedup and higher success rate compared to batch LME", "shape": "dot", "title": "Perimeter patrol experiments showing 4x speedup and higher success rate compared to batch LME"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy incremental latent maximum entropy IRL for online preference inference in partially observable tasks", "label": "Deploy incremental latent maximum entropy IRL for online preference inference in partially observable tasks", "shape": "dot", "title": "Deploy incremental latent maximum entropy IRL for online preference inference in partially observable tasks"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Terminate I2RL sessions using small log-likelihood threshold to reduce computation", "label": "Terminate I2RL sessions using small log-likelihood threshold to reduce computation", "shape": "dot", "title": "Terminate I2RL sessions using small log-likelihood threshold to reduce computation"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Sample hidden trajectory completions during E-step to estimate feature expectations under occlusion", "label": "Sample hidden trajectory completions during E-step to estimate feature expectations under occlusion", "shape": "dot", "title": "Sample hidden trajectory completions during E-step to estimate feature expectations under occlusion"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Reduced human situational awareness from irrelevant agent information in partially observed human-robot missions", "label": "Reduced human situational awareness from irrelevant agent information in partially observed human-robot missions", "shape": "dot", "title": "Reduced human situational awareness from irrelevant agent information in partially observed human-robot missions"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Agent ignorance of human-weighted information preferences in partially observed domains", "label": "Agent ignorance of human-weighted information preferences in partially observed domains", "shape": "dot", "title": "Agent ignorance of human-weighted information preferences in partially observed domains"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Weighted entropy reduction as proxy for human information preference in partially observed domains", "label": "Weighted entropy reduction as proxy for human information preference in partially observed domains", "shape": "dot", "title": "Weighted entropy reduction as proxy for human information preference in partially observed domains"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Agent objective of maximizing weighted information gain conditioned on task-optimal action planning", "label": "Agent objective of maximizing weighted information gain conditioned on task-optimal action planning", "shape": "dot", "title": "Agent objective of maximizing weighted information gain conditioned on task-optimal action planning"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Determinize-and-replan algorithm with DAG longest-path for information selection", "label": "Determinize-and-replan algorithm with DAG longest-path for information selection", "shape": "dot", "title": "Determinize-and-replan algorithm with DAG longest-path for information selection"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Replay-buffer-based online learning of weighted entropy parameters via \u03f5-greedy exploration", "label": "Replay-buffer-based online learning of weighted entropy parameters via \u03f5-greedy exploration", "shape": "dot", "title": "Replay-buffer-based online learning of weighted entropy parameters via \u03f5-greedy exploration"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Empirical performance improvements in 2D mining and 3D fetching tasks under weighted entropy model", "label": "Empirical performance improvements in 2D mining and 3D fetching tasks under weighted entropy model", "shape": "dot", "title": "Empirical performance improvements in 2D mining and 3D fetching tasks under weighted entropy model"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Implement determinize-and-replan information planning algorithm in autonomous agents prior to deployment", "label": "Implement determinize-and-replan information planning algorithm in autonomous agents prior to deployment", "shape": "dot", "title": "Implement determinize-and-replan information planning algorithm in autonomous agents prior to deployment"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy online learning of human weighted entropy preferences with replay buffer during mission execution", "label": "Deploy online learning of human weighted entropy preferences with replay buffer during mission execution", "shape": "dot", "title": "Deploy online learning of human weighted entropy preferences with replay buffer during mission execution"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "negative side effects from reward misspecification in reinforcement learning", "label": "negative side effects from reward misspecification in reinforcement learning", "shape": "dot", "title": "negative side effects from reward misspecification in reinforcement learning"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "incomplete reward specification in complex environments", "label": "incomplete reward specification in complex environments", "shape": "dot", "title": "incomplete reward specification in complex environments"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "maxmin formulation over consistent reward polytope in reinforcement learning", "label": "maxmin formulation over consistent reward polytope in reinforcement learning", "shape": "dot", "title": "maxmin formulation over consistent reward polytope in reinforcement learning"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "maxmin policy maximizes worst-case reward to ensure safety", "label": "maxmin policy maximizes worst-case reward to ensure safety", "shape": "dot", "title": "maxmin policy maximizes worst-case reward to ensure safety"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "combining multiple expert demonstrations to define consistent reward polytope", "label": "combining multiple expert demonstrations to define consistent reward polytope", "shape": "dot", "title": "combining multiple expert demonstrations to define consistent reward polytope"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "ellipsoid method with separation oracles using MDP solver for exact maxmin policy", "label": "ellipsoid method with separation oracles using MDP solver for exact maxmin policy", "shape": "dot", "title": "ellipsoid method with separation oracles using MDP solver for exact maxmin policy"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "FPL-based iterative maxmin policy learning with MDP solver", "label": "FPL-based iterative maxmin policy learning with MDP solver", "shape": "dot", "title": "FPL-based iterative maxmin policy learning with MDP solver"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "weird separation oracle enabling approximate MDP solver usage", "label": "weird separation oracle enabling approximate MDP solver usage", "shape": "dot", "title": "weird separation oracle enabling approximate MDP solver usage"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "polynomial-time solvability guarantee for ellipsoid maxmin algorithm", "label": "polynomial-time solvability guarantee for ellipsoid maxmin algorithm", "shape": "dot", "title": "polynomial-time solvability guarantee for ellipsoid maxmin algorithm"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "O(1/\u221aT) convergence guarantee for FPL maxmin algorithm", "label": "O(1/\u221aT) convergence guarantee for FPL maxmin algorithm", "shape": "dot", "title": "O(1/\u221aT) convergence guarantee for FPL maxmin algorithm"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "gridworld experiments showing avoidance of unseen harmful terrain", "label": "gridworld experiments showing avoidance of unseen harmful terrain", "shape": "dot", "title": "gridworld experiments showing avoidance of unseen harmful terrain"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "cartpole experiments demonstrating safe behavior with multiple expert policies", "label": "cartpole experiments demonstrating safe behavior with multiple expert policies", "shape": "dot", "title": "cartpole experiments demonstrating safe behavior with multiple expert policies"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "train RL agents with ellipsoid-based exact maxmin learning during policy optimisation", "label": "train RL agents with ellipsoid-based exact maxmin learning during policy optimisation", "shape": "dot", "title": "train RL agents with ellipsoid-based exact maxmin learning during policy optimisation"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "train RL agents with FPL-based iterative maxmin learning during policy optimisation", "label": "train RL agents with FPL-based iterative maxmin learning during policy optimisation", "shape": "dot", "title": "train RL agents with FPL-based iterative maxmin learning during policy optimisation"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "construct consistent reward polytope from multiple expert demonstrations before training", "label": "construct consistent reward polytope from multiple expert demonstrations before training", "shape": "dot", "title": "construct consistent reward polytope from multiple expert demonstrations before training"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Brittle agent behavior from mis-modeled expert stochasticity in imitation learning", "label": "Brittle agent behavior from mis-modeled expert stochasticity in imitation learning", "shape": "dot", "title": "Brittle agent behavior from mis-modeled expert stochasticity in imitation learning"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Softmax over-allocation to non-expert actions in large discrete action spaces", "label": "Softmax over-allocation to non-expert actions in large discrete action spaces", "shape": "dot", "title": "Softmax over-allocation to non-expert actions in large discrete action spaces"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Uni-modal Gaussian policy limitation in continuous action spaces", "label": "Uni-modal Gaussian policy limitation in continuous action spaces", "shape": "dot", "title": "Uni-modal Gaussian policy limitation in continuous action spaces"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Mode collapse in mixture density networks under intractable Gibbs-Shannon entropy regularisation", "label": "Mode collapse in mixture density networks under intractable Gibbs-Shannon entropy regularisation", "shape": "dot", "title": "Mode collapse in mixture density networks under intractable Gibbs-Shannon entropy regularisation"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Sparsemax distribution with adaptable support for sparse multi-modal policies", "label": "Sparsemax distribution with adaptable support for sparse multi-modal policies", "shape": "dot", "title": "Sparsemax distribution with adaptable support for sparse multi-modal policies"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Analytic Tsallis entropy of sparse MDN encouraging exploration and preventing mixture collapse", "label": "Analytic Tsallis entropy of sparse MDN encouraging exploration and preventing mixture collapse", "shape": "dot", "title": "Analytic Tsallis entropy of sparse MDN encouraging exploration and preventing mixture collapse"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Model expert behavior with sparse MDN and maximize causal Tsallis entropy", "label": "Model expert behavior with sparse MDN and maximize causal Tsallis entropy", "shape": "dot", "title": "Model expert behavior with sparse MDN and maximize causal Tsallis entropy"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "MCTEIL algorithm employing sparse MDN with Tsallis entropy bonus", "label": "MCTEIL algorithm employing sparse MDN with Tsallis entropy bonus", "shape": "dot", "title": "MCTEIL algorithm employing sparse MDN with Tsallis entropy bonus"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "MCTEIL reachability and return improvements in multi-goal simulation", "label": "MCTEIL reachability and return improvements in multi-goal simulation", "shape": "dot", "title": "MCTEIL reachability and return improvements in multi-goal simulation"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "MCTEIL outperforming BC and GAIL on MuJoCo continuous control tasks", "label": "MCTEIL outperforming BC and GAIL on MuJoCo continuous control tasks", "shape": "dot", "title": "MCTEIL outperforming BC and GAIL on MuJoCo continuous control tasks"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Fine-tune agents with MCTEIL using sparse MDN during RL stage", "label": "Fine-tune agents with MCTEIL using sparse MDN during RL stage", "shape": "dot", "title": "Fine-tune agents with MCTEIL using sparse MDN during RL stage"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt sparsemax-weighted mixture density networks in imitation learning model design", "label": "Adopt sparsemax-weighted mixture density networks in imitation learning model design", "shape": "dot", "title": "Adopt sparsemax-weighted mixture density networks in imitation learning model design"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Add causal Tsallis entropy bonus to policy optimization for exploration", "label": "Add causal Tsallis entropy bonus to policy optimization for exploration", "shape": "dot", "title": "Add causal Tsallis entropy bonus to policy optimization for exploration"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Training instability and slow convergence in large-scale deep neural network optimization", "label": "Training instability and slow convergence in large-scale deep neural network optimization", "shape": "dot", "title": "Training instability and slow convergence in large-scale deep neural network optimization"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Excessive computation cost of second-order optimization in deep neural network training", "label": "Excessive computation cost of second-order optimization in deep neural network training", "shape": "dot", "title": "Excessive computation cost of second-order optimization in deep neural network training"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "First-order gradient methods lack curvature information in large-scale DNNs", "label": "First-order gradient methods lack curvature information in large-scale DNNs", "shape": "dot", "title": "First-order gradient methods lack curvature information in large-scale DNNs"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Many PCG iterations required for Hessian-free natural gradient approximation", "label": "Many PCG iterations required for Hessian-free natural gradient approximation", "shape": "dot", "title": "Many PCG iterations required for Hessian-free natural gradient approximation"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Approximating natural gradient with Hessian-free method improves convergence if computational burden is reduced", "label": "Approximating natural gradient with Hessian-free method improves convergence if computational burden is reduced", "shape": "dot", "title": "Approximating natural gradient with Hessian-free method improves convergence if computational burden is reduced"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Adaptive damping and preconditioning can reduce required PCG iterations", "label": "Adaptive damping and preconditioning can reduce required PCG iterations", "shape": "dot", "title": "Adaptive damping and preconditioning can reduce required PCG iterations"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Meta-learning RNNs infer damping parameters and preconditioner to approximate natural gradient efficiently", "label": "Meta-learning RNNs infer damping parameters and preconditioner to approximate natural gradient efficiently", "shape": "dot", "title": "Meta-learning RNNs infer damping parameters and preconditioner to approximate natural gradient efficiently"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "RNNs output per-parameter damping vector s for modified Hessian", "label": "RNNs output per-parameter damping vector s for modified Hessian", "shape": "dot", "title": "RNNs output per-parameter damping vector s for modified Hessian"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "RNNp outputs diagonal preconditioner P for PCG in Hessian-free optimization", "label": "RNNp outputs diagonal preconditioner P for PCG in Hessian-free optimization", "shape": "dot", "title": "RNNp outputs diagonal preconditioner P for PCG in Hessian-free optimization"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Limited PCG iterations with warm-start initialization reduce compute cost", "label": "Limited PCG iterations with warm-start initialization reduce compute cost", "shape": "dot", "title": "Limited PCG iterations with warm-start initialization reduce compute cost"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Coordinatewise LSTM framework shares meta-parameters across layer types", "label": "Coordinatewise LSTM framework shares meta-parameters across layer types", "shape": "dot", "title": "Coordinatewise LSTM framework shares meta-parameters across layer types"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "MLHF reduces loss faster than SGD, RMSprop and kfac on CIFAR10 Convnet", "label": "MLHF reduces loss faster than SGD, RMSprop and kfac on CIFAR10 Convnet", "shape": "dot", "title": "MLHF reduces loss faster than SGD, RMSprop and kfac on CIFAR10 Convnet"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "MLHF outperforms first-order optimizers on ResNet18 v2 ImageNet training", "label": "MLHF outperforms first-order optimizers on ResNet18 v2 ImageNet training", "shape": "dot", "title": "MLHF outperforms first-order optimizers on ResNet18 v2 ImageNet training"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Ablation shows learned preconditioner enables few PCG iterations without accuracy loss", "label": "Ablation shows learned preconditioner enables few PCG iterations without accuracy loss", "shape": "dot", "title": "Ablation shows learned preconditioner enables few PCG iterations without accuracy loss"}, {"color": "hsl(100, 80%, 60%)", "font": {"color": "white"}, "id": "Apply MLHF meta-optimizer with learned damping and preconditioning during model training", "label": "Apply MLHF meta-optimizer with learned damping and preconditioning during model training", "shape": "dot", "title": "Apply MLHF meta-optimizer with learned damping and preconditioning during model training"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "misclassification under adversarial perturbations in neural networks", "label": "misclassification under adversarial perturbations in neural networks", "shape": "dot", "title": "misclassification under adversarial perturbations in neural networks"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "lack of provable robustness guarantees in neural network training", "label": "lack of provable robustness guarantees in neural network training", "shape": "dot", "title": "lack of provable robustness guarantees in neural network training"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "computational overhead of per-example dual optimization in verified training", "label": "computational overhead of per-example dual optimization in verified training", "shape": "dot", "title": "computational overhead of per-example dual optimization in verified training"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "duality-based verification bounds independent of dual variable optimization", "label": "duality-based verification bounds independent of dual variable optimization", "shape": "dot", "title": "duality-based verification bounds independent of dual variable optimization"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "learning mapping from examples to dual variables via verifier network", "label": "learning mapping from examples to dual variables via verifier network", "shape": "dot", "title": "learning mapping from examples to dual variables via verifier network"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "predictor-verifier training with joint task and dual loss", "label": "predictor-verifier training with joint task and dual loss", "shape": "dot", "title": "predictor-verifier training with joint task and dual loss"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "backward-forward verifier architecture for dual variable prediction", "label": "backward-forward verifier architecture for dual variable prediction", "shape": "dot", "title": "backward-forward verifier architecture for dual variable prediction"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "direct verifier architecture for dual variable prediction", "label": "direct verifier architecture for dual variable prediction", "shape": "dot", "title": "direct verifier architecture for dual variable prediction"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "state-of-the-art verified accuracy on MNIST and SVHN with PVT", "label": "state-of-the-art verified accuracy on MNIST and SVHN with PVT", "shape": "dot", "title": "state-of-the-art verified accuracy on MNIST and SVHN with PVT"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "comparable verified bounds of direct and backward-forward verifiers", "label": "comparable verified bounds of direct and backward-forward verifiers", "shape": "dot", "title": "comparable verified bounds of direct and backward-forward verifiers"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "integrate predictor-verifier training during neural network fine-tuning to obtain provable adversarial robustness", "label": "integrate predictor-verifier training during neural network fine-tuning to obtain provable adversarial robustness", "shape": "dot", "title": "integrate predictor-verifier training during neural network fine-tuning to obtain provable adversarial robustness"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Hard exploration failure in deep RL agents on sparse-reward Atari games", "label": "Hard exploration failure in deep RL agents on sparse-reward Atari games", "shape": "dot", "title": "Hard exploration failure in deep RL agents on sparse-reward Atari games"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Sparse environmental reward signals in hard exploration Atari games", "label": "Sparse environmental reward signals in hard exploration Atari games", "shape": "dot", "title": "Sparse environmental reward signals in hard exploration Atari games"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Domain gap between demonstration videos and agent observations in third-person imitation", "label": "Domain gap between demonstration videos and agent observations in third-person imitation", "shape": "dot", "title": "Domain gap between demonstration videos and agent observations in third-person imitation"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Absence of demonstrator action sequences in YouTube-based imitation learning", "label": "Absence of demonstrator action sequences in YouTube-based imitation learning", "shape": "dot", "title": "Absence of demonstrator action sequences in YouTube-based imitation learning"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Self-supervised domain-invariant embedding learning via temporal and cross-modal distance classification in Atari gameplay", "label": "Self-supervised domain-invariant embedding learning via temporal and cross-modal distance classification in Atari gameplay", "shape": "dot", "title": "Self-supervised domain-invariant embedding learning via temporal and cross-modal distance classification in Atari gameplay"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Embedding-based checkpoint reward as substitute for action labels in imitation learning", "label": "Embedding-based checkpoint reward as substitute for action labels in imitation learning", "shape": "dot", "title": "Embedding-based checkpoint reward as substitute for action labels in imitation learning"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Combine TDC and CMC losses to learn common representation across domains", "label": "Combine TDC and CMC losses to learn common representation across domains", "shape": "dot", "title": "Combine TDC and CMC losses to learn common representation across domains"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Sequential checkpoint generation along demonstration embedding for auxiliary imitation reward", "label": "Sequential checkpoint generation along demonstration embedding for auxiliary imitation reward", "shape": "dot", "title": "Sequential checkpoint generation along demonstration embedding for auxiliary imitation reward"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Neural networks \u03d5, \u03c8, \u03c4td, \u03c4cm trained with temporal and cross-modal classification losses", "label": "Neural networks \u03d5, \u03c8, \u03c4td, \u03c4cm trained with temporal and cross-modal classification losses", "shape": "dot", "title": "Neural networks \u03d5, \u03c8, \u03c4td, \u03c4cm trained with temporal and cross-modal classification losses"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Similarity-based imitation reward function with threshold \u03b3", "label": "Similarity-based imitation reward function with threshold \u03b3", "shape": "dot", "title": "Similarity-based imitation reward function with threshold \u03b3"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Cycle-consistency metric (P\u03d5=44.2) showing cross-domain alignment improvement", "label": "Cycle-consistency metric (P\u03d5=44.2) showing cross-domain alignment improvement", "shape": "dot", "title": "Cycle-consistency metric (P\u03d5=44.2) showing cross-domain alignment improvement"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "t-SNE visualization demonstrates aligned trajectories across domains", "label": "t-SNE visualization demonstrates aligned trajectories across domains", "shape": "dot", "title": "t-SNE visualization demonstrates aligned trajectories across domains"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Human-level performance on Montezuma\u2019s Revenge, Pitfall!, and Private Eye via imitation-only reward", "label": "Human-level performance on Montezuma\u2019s Revenge, Pitfall!, and Private Eye via imitation-only reward", "shape": "dot", "title": "Human-level performance on Montezuma\u2019s Revenge, Pitfall!, and Private Eye via imitation-only reward"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Pre-train embeddings with combined TDC and CMC on unlabeled demonstration videos", "label": "Pre-train embeddings with combined TDC and CMC on unlabeled demonstration videos", "shape": "dot", "title": "Pre-train embeddings with combined TDC and CMC on unlabeled demonstration videos"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Train RL agents with similarity-based checkpoint imitation reward derived from pre-trained embedding", "label": "Train RL agents with similarity-based checkpoint imitation reward derived from pre-trained embedding", "shape": "dot", "title": "Train RL agents with similarity-based checkpoint imitation reward derived from pre-trained embedding"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Use cycle-consistency metric to select embedding models before RL training", "label": "Use cycle-consistency metric to select embedding models before RL training", "shape": "dot", "title": "Use cycle-consistency metric to select embedding models before RL training"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Combine imitation reward with environment reward during IMPALA training for additional performance", "label": "Combine imitation reward with environment reward during IMPALA training for additional performance", "shape": "dot", "title": "Combine imitation reward with environment reward during IMPALA training for additional performance"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Unreliable and biased decisions from opaque machine learning models in safety-critical contexts", "label": "Unreliable and biased decisions from opaque machine learning models in safety-critical contexts", "shape": "dot", "title": "Unreliable and biased decisions from opaque machine learning models in safety-critical contexts"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Erosion of user trust and regulatory non-compliance from unexplained AI decisions", "label": "Erosion of user trust and regulatory non-compliance from unexplained AI decisions", "shape": "dot", "title": "Erosion of user trust and regulatory non-compliance from unexplained AI decisions"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Opaqueness of deep neural network inner workings", "label": "Opaqueness of deep neural network inner workings", "shape": "dot", "title": "Opaqueness of deep neural network inner workings"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Lack of interpretability and completeness in model explanations", "label": "Lack of interpretability and completeness in model explanations", "shape": "dot", "title": "Lack of interpretability and completeness in model explanations"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Tradeoff between interpretability and completeness in AI explanations", "label": "Tradeoff between interpretability and completeness in AI explanations", "shape": "dot", "title": "Tradeoff between interpretability and completeness in AI explanations"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Ethical imperative to avoid persuasive but unfaithful explanations", "label": "Ethical imperative to avoid persuasive but unfaithful explanations", "shape": "dot", "title": "Ethical imperative to avoid persuasive but unfaithful explanations"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Taxonomy-guided approach to develop explainable AI methods", "label": "Taxonomy-guided approach to develop explainable AI methods", "shape": "dot", "title": "Taxonomy-guided approach to develop explainable AI methods"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Multi-level explanation capability accommodating diverse user needs", "label": "Multi-level explanation capability accommodating diverse user needs", "shape": "dot", "title": "Multi-level explanation capability accommodating diverse user needs"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Local proxy models for processing explanations in deep networks", "label": "Local proxy models for processing explanations in deep networks", "shape": "dot", "title": "Local proxy models for processing explanations in deep networks"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Decision tree extraction from neural networks", "label": "Decision tree extraction from neural networks", "shape": "dot", "title": "Decision tree extraction from neural networks"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Salience mapping via input occlusion and gradient analysis", "label": "Salience mapping via input occlusion and gradient analysis", "shape": "dot", "title": "Salience mapping via input occlusion and gradient analysis"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Automatic rule extraction techniques for neural network decisions", "label": "Automatic rule extraction techniques for neural network decisions", "shape": "dot", "title": "Automatic rule extraction techniques for neural network decisions"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Attention-based network architectures exposing internal focus", "label": "Attention-based network architectures exposing internal focus", "shape": "dot", "title": "Attention-based network architectures exposing internal focus"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Disentangled representation learning for interpretable factors", "label": "Disentangled representation learning for interpretable factors", "shape": "dot", "title": "Disentangled representation learning for interpretable factors"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Explanation-generating models producing natural language and visual rationales", "label": "Explanation-generating models producing natural language and visual rationales", "shape": "dot", "title": "Explanation-generating models producing natural language and visual rationales"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Local fidelity metrics assessing proxy model faithfulness", "label": "Local fidelity metrics assessing proxy model faithfulness", "shape": "dot", "title": "Local fidelity metrics assessing proxy model faithfulness"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Occlusion-grounded sensitivity benchmarks for salience methods", "label": "Occlusion-grounded sensitivity benchmarks for salience methods", "shape": "dot", "title": "Occlusion-grounded sensitivity benchmarks for salience methods"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Transfer learning performance measuring layer representation utility", "label": "Transfer learning performance measuring layer representation utility", "shape": "dot", "title": "Transfer learning performance measuring layer representation utility"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Human attention alignment scores for attention mechanisms", "label": "Human attention alignment scores for attention mechanisms", "shape": "dot", "title": "Human attention alignment scores for attention mechanisms"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "User studies evaluating clarity and helpfulness of generated explanations", "label": "User studies evaluating clarity and helpfulness of generated explanations", "shape": "dot", "title": "User studies evaluating clarity and helpfulness of generated explanations"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy local proxy explanation tools (e.g., LIME) to audit model predictions before deployment", "label": "Deploy local proxy explanation tools (e.g., LIME) to audit model predictions before deployment", "shape": "dot", "title": "Deploy local proxy explanation tools (e.g., LIME) to audit model predictions before deployment"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Apply decision tree extraction to neural networks for logic verification during pre-deployment review", "label": "Apply decision tree extraction to neural networks for logic verification during pre-deployment review", "shape": "dot", "title": "Apply decision tree extraction to neural networks for logic verification during pre-deployment review"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Conduct salience map sensitivity analyses to identify spurious correlations during model validation", "label": "Conduct salience map sensitivity analyses to identify spurious correlations during model validation", "shape": "dot", "title": "Conduct salience map sensitivity analyses to identify spurious correlations during model validation"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Regularize attention weights during fine-tuning to align with human attention patterns", "label": "Regularize attention weights during fine-tuning to align with human attention patterns", "shape": "dot", "title": "Regularize attention weights during fine-tuning to align with human attention patterns"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Incorporate disentanglement objectives in model design to enhance transparency", "label": "Incorporate disentanglement objectives in model design to enhance transparency", "shape": "dot", "title": "Incorporate disentanglement objectives in model design to enhance transparency"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Co-train natural language explanation generators with task models to provide end-user rationales at deployment", "label": "Co-train natural language explanation generators with task models to provide end-user rationales at deployment", "shape": "dot", "title": "Co-train natural language explanation generators with task models to provide end-user rationales at deployment"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt taxonomy-based multi-metric evaluation framework for explainability compliance testing", "label": "Adopt taxonomy-based multi-metric evaluation framework for explainability compliance testing", "shape": "dot", "title": "Adopt taxonomy-based multi-metric evaluation framework for explainability compliance testing"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Human\u2013robot collision in shared workspace", "label": "Human\u2013robot collision in shared workspace", "shape": "dot", "title": "Human\u2013robot collision in shared workspace"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Inaccurate human motion prediction from fixed utility model", "label": "Inaccurate human motion prediction from fixed utility model", "shape": "dot", "title": "Inaccurate human motion prediction from fixed utility model"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Unaccounted robot tracking error under uncertain human motion", "label": "Unaccounted robot tracking error under uncertain human motion", "shape": "dot", "title": "Unaccounted robot tracking error under uncertain human motion"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Rationality coefficient \u03b2 as indicator of model confidence in utility-based human models", "label": "Rationality coefficient \u03b2 as indicator of model confidence in utility-based human models", "shape": "dot", "title": "Rationality coefficient \u03b2 as indicator of model confidence in utility-based human models"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Combining probabilistic human predictions with worst-case robot tracking error to bound collision probability", "label": "Combining probabilistic human predictions with worst-case robot tracking error to bound collision probability", "shape": "dot", "title": "Combining probabilistic human predictions with worst-case robot tracking error to bound collision probability"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Real-time Bayesian inference of model confidence to adapt human motion predictions", "label": "Real-time Bayesian inference of model confidence to adapt human motion predictions", "shape": "dot", "title": "Real-time Bayesian inference of model confidence to adapt human motion predictions"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Probabilistic safety certificate using max marginal collision probability and FaSTrack bound", "label": "Probabilistic safety certificate using max marginal collision probability and FaSTrack bound", "shape": "dot", "title": "Probabilistic safety certificate using max marginal collision probability and FaSTrack bound"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Hidden Markov model inference over discrete \u03b2 values", "label": "Hidden Markov model inference over discrete \u03b2 values", "shape": "dot", "title": "Hidden Markov model inference over discrete \u03b2 values"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "FaSTrack tracking error bound integration with Minkowski collision set", "label": "FaSTrack tracking error bound integration with Minkowski collision set", "shape": "dot", "title": "FaSTrack tracking error bound integration with Minkowski collision set"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Online planner with node rejection based on collision probability threshold", "label": "Online planner with node rejection based on collision probability threshold", "shape": "dot", "title": "Online planner with node rejection based on collision probability threshold"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Simulation results: adaptive \u03b2 improves safety distance and efficiency", "label": "Simulation results: adaptive \u03b2 improves safety distance and efficiency", "shape": "dot", "title": "Simulation results: adaptive \u03b2 improves safety distance and efficiency"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Hardware demonstration of quadcopter avoiding humans", "label": "Hardware demonstration of quadcopter avoiding humans", "shape": "dot", "title": "Hardware demonstration of quadcopter avoiding humans"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy real-time Bayesian \u03b2 inference for human motion prediction during robot operation", "label": "Deploy real-time Bayesian \u03b2 inference for human motion prediction during robot operation", "shape": "dot", "title": "Deploy real-time Bayesian \u03b2 inference for human motion prediction during robot operation"}, {"color": "hsl(330, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate confidence-aware human predictions with FaSTrack-based probabilistic safety planning", "label": "Integrate confidence-aware human predictions with FaSTrack-based probabilistic safety planning", "shape": "dot", "title": "Integrate confidence-aware human predictions with FaSTrack-based probabilistic safety planning"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Research resource misallocation due to performance-only AI benchmarks", "label": "Research resource misallocation due to performance-only AI benchmarks", "shape": "dot", "title": "Research resource misallocation due to performance-only AI benchmarks"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Unaccounted societal externalities from AI system deployment", "label": "Unaccounted societal externalities from AI system deployment", "shape": "dot", "title": "Unaccounted societal externalities from AI system deployment"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Overemphasis on performance metrics neglecting resource costs in AI progress assessment", "label": "Overemphasis on performance metrics neglecting resource costs in AI progress assessment", "shape": "dot", "title": "Overemphasis on performance metrics neglecting resource costs in AI progress assessment"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Benchmark specialization reducing solution generality in AI research", "label": "Benchmark specialization reducing solution generality in AI research", "shape": "dot", "title": "Benchmark specialization reducing solution generality in AI research"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Insufficient reproducibility and replicability data in AI publications", "label": "Insufficient reproducibility and replicability data in AI publications", "shape": "dot", "title": "Insufficient reproducibility and replicability data in AI publications"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Utility functions integrating performance and resource costs align evaluation with stakeholder value", "label": "Utility functions integrating performance and resource costs align evaluation with stakeholder value", "shape": "dot", "title": "Utility functions integrating performance and resource costs align evaluation with stakeholder value"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Pareto-front analysis exposes trade-offs across performance and resources", "label": "Pareto-front analysis exposes trade-offs across performance and resources", "shape": "dot", "title": "Pareto-front analysis exposes trade-offs across performance and resources"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Develop cost-sensitive AI evaluation frameworks covering full resource life cycle", "label": "Develop cost-sensitive AI evaluation frameworks covering full resource life cycle", "shape": "dot", "title": "Develop cost-sensitive AI evaluation frameworks covering full resource life cycle"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Promote comprehensive resource reporting standards for AI research", "label": "Promote comprehensive resource reporting standards for AI research", "shape": "dot", "title": "Promote comprehensive resource reporting standards for AI research"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Use open leaderboards with code sharing to facilitate replicability", "label": "Use open leaderboards with code sharing to facilitate replicability", "shape": "dot", "title": "Use open leaderboards with code sharing to facilitate replicability"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Resource taxonomy rd rk rs rh rm rc rn rt for AI system accounting", "label": "Resource taxonomy rd rk rs rh rm rc rn rt for AI system accounting", "shape": "dot", "title": "Resource taxonomy rd rk rs rh rm rc rn rt for AI system accounting"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Multi-dimensional benchmark plots of performance versus resource consumption", "label": "Multi-dimensional benchmark plots of performance versus resource consumption", "shape": "dot", "title": "Multi-dimensional benchmark plots of performance versus resource consumption"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Policy requirement to include compute and data metrics in academic submission templates", "label": "Policy requirement to include compute and data metrics in academic submission templates", "shape": "dot", "title": "Policy requirement to include compute and data metrics in academic submission templates"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Leaderboard platforms enforcing code submission", "label": "Leaderboard platforms enforcing code submission", "shape": "dot", "title": "Leaderboard platforms enforcing code submission"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Alpha* case study Pareto analysis showing compute-performance trade-offs", "label": "Alpha* case study Pareto analysis showing compute-performance trade-offs", "shape": "dot", "title": "Alpha* case study Pareto analysis showing compute-performance trade-offs"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "ALE case study Pareto analysis demonstrating efficiency focus shift", "label": "ALE case study Pareto analysis demonstrating efficiency focus shift", "shape": "dot", "title": "ALE case study Pareto analysis demonstrating efficiency focus shift"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "MNIST compute-performance Pareto front figure evidencing hidden trade-offs", "label": "MNIST compute-performance Pareto front figure evidencing hidden trade-offs", "shape": "dot", "title": "MNIST compute-performance Pareto front figure evidencing hidden trade-offs"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Define multi-dimensional AI benchmarks including resource costs", "label": "Define multi-dimensional AI benchmarks including resource costs", "shape": "dot", "title": "Define multi-dimensional AI benchmarks including resource costs"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Mandate reporting of resource consumption metrics in AI research publications", "label": "Mandate reporting of resource consumption metrics in AI research publications", "shape": "dot", "title": "Mandate reporting of resource consumption metrics in AI research publications"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt Pareto-front analysis to guide AI research portfolio optimization", "label": "Adopt Pareto-front analysis to guide AI research portfolio optimization", "shape": "dot", "title": "Adopt Pareto-front analysis to guide AI research portfolio optimization"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Establish open leaderboards with mandatory code submission for replicability", "label": "Establish open leaderboards with mandatory code submission for replicability", "shape": "dot", "title": "Establish open leaderboards with mandatory code submission for replicability"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "misclassification from adversarial examples in discriminative ML models", "label": "misclassification from adversarial examples in discriminative ML models", "shape": "dot", "title": "misclassification from adversarial examples in discriminative ML models"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "existence of adversarial examples in low or high input density regions", "label": "existence of adversarial examples in low or high input density regions", "shape": "dot", "title": "existence of adversarial examples in low or high input density regions"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "inadequate uncertainty estimation in deep neural networks", "label": "inadequate uncertainty estimation in deep neural networks", "shape": "dot", "title": "inadequate uncertainty estimation in deep neural networks"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "idealised models with invariance capture and fast-growing epistemic uncertainty cannot have adversarial examples", "label": "idealised models with invariance capture and fast-growing epistemic uncertainty cannot have adversarial examples", "shape": "dot", "title": "idealised models with invariance capture and fast-growing epistemic uncertainty cannot have adversarial examples"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "bayesian neural networks leveraging epistemic uncertainty to detect out-of-distribution inputs", "label": "bayesian neural networks leveraging epistemic uncertainty to detect out-of-distribution inputs", "shape": "dot", "title": "bayesian neural networks leveraging epistemic uncertainty to detect out-of-distribution inputs"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "incorporating data invariances into model architecture to ensure coverage", "label": "incorporating data invariances into model architecture to ensure coverage", "shape": "dot", "title": "incorporating data invariances into model architecture to ensure coverage"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "using mutual information between parameters and outputs to quantify epistemic uncertainty", "label": "using mutual information between parameters and outputs to quantify epistemic uncertainty", "shape": "dot", "title": "using mutual information between parameters and outputs to quantify epistemic uncertainty"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "hamiltonian monte carlo inference for near-ideal Bayesian neural networks", "label": "hamiltonian monte carlo inference for near-ideal Bayesian neural networks", "shape": "dot", "title": "hamiltonian monte carlo inference for near-ideal Bayesian neural networks"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "monte carlo dropout approximate inference for scalable uncertainty estimation", "label": "monte carlo dropout approximate inference for scalable uncertainty estimation", "shape": "dot", "title": "monte carlo dropout approximate inference for scalable uncertainty estimation"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "invariant architecture components capturing data transformations", "label": "invariant architecture components capturing data transformations", "shape": "dot", "title": "invariant architecture components capturing data transformations"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "mutual information thresholding for adversarial example detection", "label": "mutual information thresholding for adversarial example detection", "shape": "dot", "title": "mutual information thresholding for adversarial example detection"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "dropout ensemble technique to smooth uncertainty estimates", "label": "dropout ensemble technique to smooth uncertainty estimates", "shape": "dot", "title": "dropout ensemble technique to smooth uncertainty estimates"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "HMC robustness experiments on Manifold MNIST showing low adversarial success rate", "label": "HMC robustness experiments on Manifold MNIST showing low adversarial success rate", "shape": "dot", "title": "HMC robustness experiments on Manifold MNIST showing low adversarial success rate"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "correlation of mutual information with data density on synthetic dataset", "label": "correlation of mutual information with data density on synthetic dataset", "shape": "dot", "title": "correlation of mutual information with data density on synthetic dataset"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "dropout uncertainty holes exploited by garbage image attack", "label": "dropout uncertainty holes exploited by garbage image attack", "shape": "dot", "title": "dropout uncertainty holes exploited by garbage image attack"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "dropout ensemble improves adversarial detection AUROC on cats vs dogs", "label": "dropout ensemble improves adversarial detection AUROC on cats vs dogs", "shape": "dot", "title": "dropout ensemble improves adversarial detection AUROC on cats vs dogs"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "train Bayesian neural networks with HMC inference to maximize epistemic uncertainty and reject adversarial inputs", "label": "train Bayesian neural networks with HMC inference to maximize epistemic uncertainty and reject adversarial inputs", "shape": "dot", "title": "train Bayesian neural networks with HMC inference to maximize epistemic uncertainty and reject adversarial inputs"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "deploy MC dropout inference with mutual information thresholding to flag and reject adversarial inputs at runtime", "label": "deploy MC dropout inference with mutual information thresholding to flag and reject adversarial inputs at runtime", "shape": "dot", "title": "deploy MC dropout inference with mutual information thresholding to flag and reject adversarial inputs at runtime"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "ensemble multiple MC dropout models during deployment to mitigate uncertainty holes and improve adversarial robustness", "label": "ensemble multiple MC dropout models during deployment to mitigate uncertainty holes and improve adversarial robustness", "shape": "dot", "title": "ensemble multiple MC dropout models during deployment to mitigate uncertainty holes and improve adversarial robustness"}, {"color": "hsl(110, 80%, 60%)", "font": {"color": "white"}, "id": "design model architectures with built-in data invariances to reduce adversarial vulnerability", "label": "design model architectures with built-in data invariances to reduce adversarial vulnerability", "shape": "dot", "title": "design model architectures with built-in data invariances to reduce adversarial vulnerability"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Suboptimal interactive physical reasoning performance in deep RL agents", "label": "Suboptimal interactive physical reasoning performance in deep RL agents", "shape": "dot", "title": "Suboptimal interactive physical reasoning performance in deep RL agents"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Absence of relational inductive bias in standard deep RL architectures", "label": "Absence of relational inductive bias in standard deep RL architectures", "shape": "dot", "title": "Absence of relational inductive bias in standard deep RL architectures"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Relational representations of objects and relations enable combinatorial generalization in physical reasoning", "label": "Relational representations of objects and relations enable combinatorial generalization in physical reasoning", "shape": "dot", "title": "Relational representations of objects and relations enable combinatorial generalization in physical reasoning"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Introduce graph network-based relational policy representations in agents", "label": "Introduce graph network-based relational policy representations in agents", "shape": "dot", "title": "Introduce graph network-based relational policy representations in agents"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Provide correct sparse relational structure in graph networks", "label": "Provide correct sparse relational structure in graph networks", "shape": "dot", "title": "Provide correct sparse relational structure in graph networks"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Combine relational reasoning with explicit physics simulation for decision making", "label": "Combine relational reasoning with explicit physics simulation for decision making", "shape": "dot", "title": "Combine relational reasoning with explicit physics simulation for decision making"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Graph network agent with object nodes, contact edges, and message-passing recurrence", "label": "Graph network agent with object nodes, contact edges, and message-passing recurrence", "shape": "dot", "title": "Graph network agent with object nodes, contact edges, and message-passing recurrence"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Sparse graph representation limited to contact edges", "label": "Sparse graph representation limited to contact edges", "shape": "dot", "title": "Sparse graph representation limited to contact edges"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Model-based simulation agent evaluating glue options via physics engine", "label": "Model-based simulation agent evaluating glue options via physics engine", "shape": "dot", "title": "Model-based simulation agent evaluating glue options via physics engine"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "GN agent predicts tower stability and glue necessity accurately in supervised experiments", "label": "GN agent predicts tower stability and glue necessity accurately in supervised experiments", "shape": "dot", "title": "GN agent predicts tower stability and glue necessity accurately in supervised experiments"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "GN agent outperforms MLP and generalizes across tower sizes in gluing task", "label": "GN agent outperforms MLP and generalizes across tower sizes in gluing task", "shape": "dot", "title": "GN agent outperforms MLP and generalizes across tower sizes in gluing task"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Sparse GN agent achieves higher reward and fewer invalid actions compared to GN-FC agent", "label": "Sparse GN agent achieves higher reward and fewer invalid actions compared to GN-FC agent", "shape": "dot", "title": "Sparse GN agent achieves higher reward and fewer invalid actions compared to GN-FC agent"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Simulation agent attains highest reward relative to GN agent", "label": "Simulation agent attains highest reward relative to GN agent", "shape": "dot", "title": "Simulation agent attains highest reward relative to GN agent"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Design model with graph network relational inductive bias for physical reasoning tasks", "label": "Design model with graph network relational inductive bias for physical reasoning tasks", "shape": "dot", "title": "Design model with graph network relational inductive bias for physical reasoning tasks"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Use sparse contact-based graph structure in GN agent design", "label": "Use sparse contact-based graph structure in GN agent design", "shape": "dot", "title": "Use sparse contact-based graph structure in GN agent design"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate physics simulation into decision policy for physical reasoning tasks", "label": "Integrate physics simulation into decision policy for physical reasoning tasks", "shape": "dot", "title": "Integrate physics simulation into decision policy for physical reasoning tasks"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Fermi paradox misinterpretation in astrobiology discourse", "label": "Fermi paradox misinterpretation in astrobiology discourse", "shape": "dot", "title": "Fermi paradox misinterpretation in astrobiology discourse"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Point estimate use in Drake equation modeling", "label": "Point estimate use in Drake equation modeling", "shape": "dot", "title": "Point estimate use in Drake equation modeling"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Orders-of-magnitude uncertainty in Drake parameters", "label": "Orders-of-magnitude uncertainty in Drake parameters", "shape": "dot", "title": "Orders-of-magnitude uncertainty in Drake parameters"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Uncertainty-aware Drake equation modeling", "label": "Uncertainty-aware Drake equation modeling", "shape": "dot", "title": "Uncertainty-aware Drake equation modeling"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Monte Carlo resampling of literature parameter estimates", "label": "Monte Carlo resampling of literature parameter estimates", "shape": "dot", "title": "Monte Carlo resampling of literature parameter estimates"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Probabilistic parameter distributions for Drake factors", "label": "Probabilistic parameter distributions for Drake factors", "shape": "dot", "title": "Probabilistic parameter distributions for Drake factors"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Bayesian updating with Fermi observation", "label": "Bayesian updating with Fermi observation", "shape": "dot", "title": "Bayesian updating with Fermi observation"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Synthetic distribution shows 30% probability of no civilization per galaxy", "label": "Synthetic distribution shows 30% probability of no civilization per galaxy", "shape": "dot", "title": "Synthetic distribution shows 30% probability of no civilization per galaxy"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Scientific-uncertainty simulation shows 52% probability of no civilization per galaxy", "label": "Scientific-uncertainty simulation shows 52% probability of no civilization per galaxy", "shape": "dot", "title": "Scientific-uncertainty simulation shows 52% probability of no civilization per galaxy"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Posterior update yields 53\u201399.6% probability of solitude after non-detection", "label": "Posterior update yields 53\u201399.6% probability of solitude after non-detection", "shape": "dot", "title": "Posterior update yields 53\u201399.6% probability of solitude after non-detection"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt Monte Carlo-based probabilistic Drake modeling in SETI research", "label": "Adopt Monte Carlo-based probabilistic Drake modeling in SETI research", "shape": "dot", "title": "Adopt Monte Carlo-based probabilistic Drake modeling in SETI research"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Apply Bayesian update with Fermi observation to refine civilization probability estimates", "label": "Apply Bayesian update with Fermi observation to refine civilization probability estimates", "shape": "dot", "title": "Apply Bayesian update with Fermi observation to refine civilization probability estimates"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Reward misspecification in robot planning across environments", "label": "Reward misspecification in robot planning across environments", "shape": "dot", "title": "Reward misspecification in robot planning across environments"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Designer overburden when specifying joint reward across diverse environments", "label": "Designer overburden when specifying joint reward across diverse environments", "shape": "dot", "title": "Designer overburden when specifying joint reward across diverse environments"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Proxy reward non-generalization to novel environments", "label": "Proxy reward non-generalization to novel environments", "shape": "dot", "title": "Proxy reward non-generalization to novel environments"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Simpler per-environment reward specification reduces cognitive load", "label": "Simpler per-environment reward specification reduces cognitive load", "shape": "dot", "title": "Simpler per-environment reward specification reduces cognitive load"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Independently specified proxy rewards provide conditional evidence of true reward", "label": "Independently specified proxy rewards provide conditional evidence of true reward", "shape": "dot", "title": "Independently specified proxy rewards provide conditional evidence of true reward"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Divide-and-conquer reward design combining independent proxies via Bayesian inference", "label": "Divide-and-conquer reward design combining independent proxies via Bayesian inference", "shape": "dot", "title": "Divide-and-conquer reward design combining independent proxies via Bayesian inference"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Treat designer as approximately optimal via Boltzmann observation model", "label": "Treat designer as approximately optimal via Boltzmann observation model", "shape": "dot", "title": "Treat designer as approximately optimal via Boltzmann observation model"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Bayesian posterior inference over true reward using IRD observation model and Monte Carlo sampling", "label": "Bayesian posterior inference over true reward using IRD observation model and Monte Carlo sampling", "shape": "dot", "title": "Bayesian posterior inference over true reward using IRD observation model and Monte Carlo sampling"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Expected reward planning with posterior mean", "label": "Expected reward planning with posterior mean", "shape": "dot", "title": "Expected reward planning with posterior mean"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Risk-averse planning using reward posterior uncertainty", "label": "Risk-averse planning using reward posterior uncertainty", "shape": "dot", "title": "Risk-averse planning using reward posterior uncertainty"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Grid world user study showing 51% faster design and 69.8% lower regret", "label": "Grid world user study showing 51% faster design and 69.8% lower regret", "shape": "dot", "title": "Grid world user study showing 51% faster design and 69.8% lower regret"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Robot manipulator user study confirming independent design benefits", "label": "Robot manipulator user study confirming independent design benefits", "shape": "dot", "title": "Robot manipulator user study confirming independent design benefits"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Sensitivity analysis across feasible sets, feature subsets and environment counts demonstrating robustness", "label": "Sensitivity analysis across feasible sets, feature subsets and environment counts demonstrating robustness", "shape": "dot", "title": "Sensitivity analysis across feasible sets, feature subsets and environment counts demonstrating robustness"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt independent reward design workflow in model design phase for robot tasks", "label": "Adopt independent reward design workflow in model design phase for robot tasks", "shape": "dot", "title": "Adopt independent reward design workflow in model design phase for robot tasks"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Apply Bayesian inference over independent proxy rewards before deployment", "label": "Apply Bayesian inference over independent proxy rewards before deployment", "shape": "dot", "title": "Apply Bayesian inference over independent proxy rewards before deployment"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Plan robot behavior using mean of reward posterior during deployment", "label": "Plan robot behavior using mean of reward posterior during deployment", "shape": "dot", "title": "Plan robot behavior using mean of reward posterior during deployment"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Employ risk-averse planning against reward uncertainty in deployment", "label": "Employ risk-averse planning against reward uncertainty in deployment", "shape": "dot", "title": "Employ risk-averse planning against reward uncertainty in deployment"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "Select representative subset of training environments to maximize independent reward design effectiveness", "label": "Select representative subset of training environments to maximize independent reward design effectiveness", "shape": "dot", "title": "Select representative subset of training environments to maximize independent reward design effectiveness"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Unintended negative consequences from misspecified reward functions in autonomous agents", "label": "Unintended negative consequences from misspecified reward functions in autonomous agents", "shape": "dot", "title": "Unintended negative consequences from misspecified reward functions in autonomous agents"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Intractability of practical CIRL solutions using standard POMDP reduction", "label": "Intractability of practical CIRL solutions using standard POMDP reduction", "shape": "dot", "title": "Intractability of practical CIRL solutions using standard POMDP reduction"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Assumption of perfect human rationality in CIRL models", "label": "Assumption of perfect human rationality in CIRL models", "shape": "dot", "title": "Assumption of perfect human rationality in CIRL models"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Passive observation assumption in IRL reward inference", "label": "Passive observation assumption in IRL reward inference", "shape": "dot", "title": "Passive observation assumption in IRL reward inference"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Pruning human decision rules via information asymmetry in CIRL", "label": "Pruning human decision rules via information asymmetry in CIRL", "shape": "dot", "title": "Pruning human decision rules via information asymmetry in CIRL"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Human action distribution parameterized by Q-values enables suboptimal human modeling", "label": "Human action distribution parameterized by Q-values enables suboptimal human modeling", "shape": "dot", "title": "Human action distribution parameterized by Q-values enables suboptimal human modeling"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Pedagogic action signaling by humans accelerates reward inference", "label": "Pedagogic action signaling by humans accelerates reward inference", "shape": "dot", "title": "Pedagogic action signaling by humans accelerates reward inference"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Use optimality-preserving modified Bellman update to scale CIRL solvers", "label": "Use optimality-preserving modified Bellman update to scale CIRL solvers", "shape": "dot", "title": "Use optimality-preserving modified Bellman update to scale CIRL solvers"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate general human policy models into CIRL transition dynamics", "label": "Integrate general human policy models into CIRL transition dynamics", "shape": "dot", "title": "Integrate general human policy models into CIRL transition dynamics"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Leverage pedagogic signaling within CIRL for faster value alignment", "label": "Leverage pedagogic signaling within CIRL for faster value alignment", "shape": "dot", "title": "Leverage pedagogic signaling within CIRL for faster value alignment"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Modified Bellman update algorithm with reduced action space |AR|", "label": "Modified Bellman update algorithm with reduced action space |AR|", "shape": "dot", "title": "Modified Bellman update algorithm with reduced action space |AR|"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Modified Bellman update incorporating \u03c0_H over Q-values for suboptimal humans", "label": "Modified Bellman update incorporating \u03c0_H over Q-values for suboptimal humans", "shape": "dot", "title": "Modified Bellman update incorporating \u03c0_H over Q-values for suboptimal humans"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Adapted PBVI and POMCP algorithms using modified Bellman update", "label": "Adapted PBVI and POMCP algorithms using modified Bellman update", "shape": "dot", "title": "Adapted PBVI and POMCP algorithms using modified Bellman update"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "CIRL planning system utilizing pedagogic human signals and pragmatic robot interpretation", "label": "CIRL planning system utilizing pedagogic human signals and pragmatic robot interpretation", "shape": "dot", "title": "CIRL planning system utilizing pedagogic human signals and pragmatic robot interpretation"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Exact value iteration experiments show exponential speedup using modified Bellman update", "label": "Exact value iteration experiments show exponential speedup using modified Bellman update", "shape": "dot", "title": "Exact value iteration experiments show exponential speedup using modified Bellman update"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Approximate algorithm experiments show higher value and scalability", "label": "Approximate algorithm experiments show higher value and scalability", "shape": "dot", "title": "Approximate algorithm experiments show higher value and scalability"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "CIRL vs IRL experiments show 100% task success versus 50%", "label": "CIRL vs IRL experiments show 100% task success versus 50%", "shape": "dot", "title": "CIRL vs IRL experiments show 100% task success versus 50%"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Simulations with Boltzmann-rational humans show \u003e90% success in CIRL", "label": "Simulations with Boltzmann-rational humans show \u003e90% success in CIRL", "shape": "dot", "title": "Simulations with Boltzmann-rational humans show \u003e90% success in CIRL"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate modified Bellman update into POMDP solvers for CIRL-based planning algorithms", "label": "Integrate modified Bellman update into POMDP solvers for CIRL-based planning algorithms", "shape": "dot", "title": "Integrate modified Bellman update into POMDP solvers for CIRL-based planning algorithms"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Model human policies as Q-value-dependent distributions (e.g., Boltzmann) in CIRL planning", "label": "Model human policies as Q-value-dependent distributions (e.g., Boltzmann) in CIRL planning", "shape": "dot", "title": "Model human policies as Q-value-dependent distributions (e.g., Boltzmann) in CIRL planning"}, {"color": "hsl(90, 80%, 60%)", "font": {"color": "white"}, "id": "Train robots with CIRL-derived policies encouraging pedagogic human signaling instead of IRL approaches", "label": "Train robots with CIRL-derived policies encouraging pedagogic human signaling instead of IRL approaches", "shape": "dot", "title": "Train robots with CIRL-derived policies encouraging pedagogic human signaling instead of IRL approaches"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "escalation of conflicts between artificial agents in safety-critical systems", "label": "escalation of conflicts between artificial agents in safety-critical systems", "shape": "dot", "title": "escalation of conflicts between artificial agents in safety-critical systems"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "selfish incentives destabilizing cooperation in social dilemmas", "label": "selfish incentives destabilizing cooperation in social dilemmas", "shape": "dot", "title": "selfish incentives destabilizing cooperation in social dilemmas"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "absence of incentive-structuring mechanisms in multi-agent reinforcement learning", "label": "absence of incentive-structuring mechanisms in multi-agent reinforcement learning", "shape": "dot", "title": "absence of incentive-structuring mechanisms in multi-agent reinforcement learning"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "mechanism design modifies payoff structure to align individual and collective interests", "label": "mechanism design modifies payoff structure to align individual and collective interests", "shape": "dot", "title": "mechanism design modifies payoff structure to align individual and collective interests"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "planning agent shaping learners\u0027 policy updates via anticipated gradients", "label": "planning agent shaping learners\u0027 policy updates via anticipated gradients", "shape": "dot", "title": "planning agent shaping learners\u0027 policy updates via anticipated gradients"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "adaptive mechanism design employing planning agent reward distribution", "label": "adaptive mechanism design employing planning agent reward distribution", "shape": "dot", "title": "adaptive mechanism design employing planning agent reward distribution"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "lookahead gradient-based incentive adjustment to maximise social welfare", "label": "lookahead gradient-based incentive adjustment to maximise social welfare", "shape": "dot", "title": "lookahead gradient-based incentive adjustment to maximise social welfare"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "planning agent policy gradient update using other agents\u0027 gradients", "label": "planning agent policy gradient update using other agents\u0027 gradients", "shape": "dot", "title": "planning agent policy gradient update using other agents\u0027 gradients"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "opponent modeling to approximate hidden agent policies for planning agent", "label": "opponent modeling to approximate hidden agent policies for planning agent", "shape": "dot", "title": "opponent modeling to approximate hidden agent policies for planning agent"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "cost regularization and reward limits for additional rewards", "label": "cost regularization and reward limits for additional rewards", "shape": "dot", "title": "cost regularization and reward limits for additional rewards"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "high cooperation achieved in matrix game experiments with planning agent", "label": "high cooperation achieved in matrix game experiments with planning agent", "shape": "dot", "title": "high cooperation achieved in matrix game experiments with planning agent"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "stable cooperation persists in Stag Hunt after planning agent removal", "label": "stable cooperation persists in Stag Hunt after planning agent removal", "shape": "dot", "title": "stable cooperation persists in Stag Hunt after planning agent removal"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "decreasing average additional rewards over time in PD experiments", "label": "decreasing average additional rewards over time in PD experiments", "shape": "dot", "title": "decreasing average additional rewards over time in PD experiments"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "moderate cooperation achieved using estimated value function with opponent modeling", "label": "moderate cooperation achieved using estimated value function with opponent modeling", "shape": "dot", "title": "moderate cooperation achieved using estimated value function with opponent modeling"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy adaptive planning agent distributing additional rewards during multi-agent RL training", "label": "Deploy adaptive planning agent distributing additional rewards during multi-agent RL training", "shape": "dot", "title": "Deploy adaptive planning agent distributing additional rewards during multi-agent RL training"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Constrain planning agent to revenue-neutral reward redistribution in social dilemmas", "label": "Constrain planning agent to revenue-neutral reward redistribution in social dilemmas", "shape": "dot", "title": "Constrain planning agent to revenue-neutral reward redistribution in social dilemmas"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Turn off planning agent after cooperative equilibrium reached to lower ongoing costs", "label": "Turn off planning agent after cooperative equilibrium reached to lower ongoing costs", "shape": "dot", "title": "Turn off planning agent after cooperative equilibrium reached to lower ongoing costs"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate opponent modeling into planning agent when learner policies are hidden", "label": "Integrate opponent modeling into planning agent when learner policies are hidden", "shape": "dot", "title": "Integrate opponent modeling into planning agent when learner policies are hidden"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Spurious visual cue reliance in global stability prediction models", "label": "Spurious visual cue reliance in global stability prediction models", "shape": "dot", "title": "Spurious visual cue reliance in global stability prediction models"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Correlation between local and global stability labels in training data for block towers", "label": "Correlation between local and global stability labels in training data for block towers", "shape": "dot", "title": "Correlation between local and global stability labels in training data for block towers"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Gradient-weighted supplemental tasks modulate feature extraction in neural networks", "label": "Gradient-weighted supplemental tasks modulate feature extraction in neural networks", "shape": "dot", "title": "Gradient-weighted supplemental tasks modulate feature extraction in neural networks"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Neural stethoscope framework for promoting or suppressing task-specific information in hidden layers", "label": "Neural stethoscope framework for promoting or suppressing task-specific information in hidden layers", "shape": "dot", "title": "Neural stethoscope framework for promoting or suppressing task-specific information in hidden layers"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Auxiliary stethoscope training on instability origin features (\u03bb\u003e0)", "label": "Auxiliary stethoscope training on instability origin features (\u03bb\u003e0)", "shape": "dot", "title": "Auxiliary stethoscope training on instability origin features (\u03bb\u003e0)"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Adversarial stethoscope training on local stability features (\u03bb\u003c0)", "label": "Adversarial stethoscope training on local stability features (\u03bb\u003c0)", "shape": "dot", "title": "Adversarial stethoscope training on local stability features (\u03bb\u003c0)"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Analytic stethoscope probing across network layers (\u03bb=0)", "label": "Analytic stethoscope probing across network layers (\u03bb=0)", "shape": "dot", "title": "Analytic stethoscope probing across network layers (\u03bb=0)"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Auxiliary stethoscope improves hard-scenario accuracy over baseline", "label": "Auxiliary stethoscope improves hard-scenario accuracy over baseline", "shape": "dot", "title": "Auxiliary stethoscope improves hard-scenario accuracy over baseline"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Adversarial stethoscope reduces bias and increases robustness", "label": "Adversarial stethoscope reduces bias and increases robustness", "shape": "dot", "title": "Adversarial stethoscope reduces bias and increases robustness"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Layer-wise analytic stethoscope reveals distribution of local and global stability information", "label": "Layer-wise analytic stethoscope reveals distribution of local and global stability information", "shape": "dot", "title": "Layer-wise analytic stethoscope reveals distribution of local and global stability information"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Fine-tune global stability models with auxiliary stethoscope on instability-origin task", "label": "Fine-tune global stability models with auxiliary stethoscope on instability-origin task", "shape": "dot", "title": "Fine-tune global stability models with auxiliary stethoscope on instability-origin task"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Fine-tune global stability models with adversarial stethoscope on local-stability task", "label": "Fine-tune global stability models with adversarial stethoscope on local-stability task", "shape": "dot", "title": "Fine-tune global stability models with adversarial stethoscope on local-stability task"}, {"color": "hsl(230, 80%, 60%)", "font": {"color": "white"}, "id": "Audit representations with analytic stethoscope prior to deployment", "label": "Audit representations with analytic stethoscope prior to deployment", "shape": "dot", "title": "Audit representations with analytic stethoscope prior to deployment"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Exploration failure in sparse-reward reinforcement learning environments", "label": "Exploration failure in sparse-reward reinforcement learning environments", "shape": "dot", "title": "Exploration failure in sparse-reward reinforcement learning environments"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Insufficient exploitation of past high-return experiences in RL agents", "label": "Insufficient exploitation of past high-return experiences in RL agents", "shape": "dot", "title": "Insufficient exploitation of past high-return experiences in RL agents"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Imitating trajectories with returns exceeding value estimates drives deeper exploration", "label": "Imitating trajectories with returns exceeding value estimates drives deeper exploration", "shape": "dot", "title": "Imitating trajectories with returns exceeding value estimates drives deeper exploration"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Off-policy actor-critic imitation without importance sampling exploits good experiences", "label": "Off-policy actor-critic imitation without importance sampling exploits good experiences", "shape": "dot", "title": "Off-policy actor-critic imitation without importance sampling exploits good experiences"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Self-Imitation Learning objective with positive-advantage weighting", "label": "Self-Imitation Learning objective with positive-advantage weighting", "shape": "dot", "title": "Self-Imitation Learning objective with positive-advantage weighting"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Prioritized experience replay by clipped advantage for SIL sampling", "label": "Prioritized experience replay by clipped advantage for SIL sampling", "shape": "dot", "title": "Prioritized experience replay by clipped advantage for SIL sampling"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Montezuma\u0027s Revenge performance improvement with A2C+SIL", "label": "Montezuma\u0027s Revenge performance improvement with A2C+SIL", "shape": "dot", "title": "Montezuma\u0027s Revenge performance improvement with A2C+SIL"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "A2C+SIL outperforming count-based exploration on hard Atari games", "label": "A2C+SIL outperforming count-based exploration on hard Atari games", "shape": "dot", "title": "A2C+SIL outperforming count-based exploration on hard Atari games"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Synergy of SIL and exploration bonus in Apple-Key-Door-Treasure domain", "label": "Synergy of SIL and exploration bonus in Apple-Key-Door-Treasure domain", "shape": "dot", "title": "Synergy of SIL and exploration bonus in Apple-Key-Door-Treasure domain"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "PPO+SIL gains on MuJoCo delayed-reward tasks", "label": "PPO+SIL gains on MuJoCo delayed-reward tasks", "shape": "dot", "title": "PPO+SIL gains on MuJoCo delayed-reward tasks"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate self-imitation learning updates into actor-critic training during RL fine-tuning", "label": "Integrate self-imitation learning updates into actor-critic training during RL fine-tuning", "shape": "dot", "title": "Integrate self-imitation learning updates into actor-critic training during RL fine-tuning"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Combine self-imitation learning with count-based exploration bonuses during RL training", "label": "Combine self-imitation learning with count-based exploration bonuses during RL training", "shape": "dot", "title": "Combine self-imitation learning with count-based exploration bonuses during RL training"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Apply prioritized replay based on positive advantage for self-imitation learning updates", "label": "Apply prioritized replay based on positive advantage for self-imitation learning updates", "shape": "dot", "title": "Apply prioritized replay based on positive advantage for self-imitation learning updates"}, {"color": "hsl(150, 80%, 60%)", "font": {"color": "white"}, "id": "Augment PPO training with self-imitation learning objective for continuous control tasks", "label": "Augment PPO training with self-imitation learning objective for continuous control tasks", "shape": "dot", "title": "Augment PPO training with self-imitation learning objective for continuous control tasks"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Suboptimal policy generalization in CGP-evolved Atari agents", "label": "Suboptimal policy generalization in CGP-evolved Atari agents", "shape": "dot", "title": "Suboptimal policy generalization in CGP-evolved Atari agents"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Episodic reward-based fitness selection in CGP evolution", "label": "Episodic reward-based fitness selection in CGP evolution", "shape": "dot", "title": "Episodic reward-based fitness selection in CGP evolution"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Pixel-agnostic repetitive strategies in Atari gameplay", "label": "Pixel-agnostic repetitive strategies in Atari gameplay", "shape": "dot", "title": "Pixel-agnostic repetitive strategies in Atari gameplay"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Full-episode evaluation computational cost in CGP evolution", "label": "Full-episode evaluation computational cost in CGP evolution", "shape": "dot", "title": "Full-episode evaluation computational cost in CGP evolution"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Novelty-driven exploration improves evolutionary search in Atari controllers", "label": "Novelty-driven exploration improves evolutionary search in Atari controllers", "shape": "dot", "title": "Novelty-driven exploration improves evolutionary search in Atari controllers"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate novelty metrics into CGP fitness evaluation", "label": "Integrate novelty metrics into CGP fitness evaluation", "shape": "dot", "title": "Integrate novelty metrics into CGP fitness evaluation"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Lehman-Stanley novelty score computation during CGP evolution", "label": "Lehman-Stanley novelty score computation during CGP evolution", "shape": "dot", "title": "Lehman-Stanley novelty score computation during CGP evolution"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Novelty search aids escape from local optima (Lehman \u0026 Stanley 2008)", "label": "Novelty search aids escape from local optima (Lehman \u0026 Stanley 2008)", "shape": "dot", "title": "Novelty search aids escape from local optima (Lehman \u0026 Stanley 2008)"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Add novelty-based secondary objective during CGP training of Atari agents", "label": "Add novelty-based secondary objective during CGP training of Atari agents", "shape": "dot", "title": "Add novelty-based secondary objective during CGP training of Atari agents"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Frame-level prioritized reward shaping targets salient experience", "label": "Frame-level prioritized reward shaping targets salient experience", "shape": "dot", "title": "Frame-level prioritized reward shaping targets salient experience"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Weight CGP fitness by prioritized salient frames and actions", "label": "Weight CGP fitness by prioritized salient frames and actions", "shape": "dot", "title": "Weight CGP fitness by prioritized salient frames and actions"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Weighted reward aggregation with prioritized frame sampling", "label": "Weighted reward aggregation with prioritized frame sampling", "shape": "dot", "title": "Weighted reward aggregation with prioritized frame sampling"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Prioritized experience frames improve policy learning (Schaul et al. 2015)", "label": "Prioritized experience frames improve policy learning (Schaul et al. 2015)", "shape": "dot", "title": "Prioritized experience frames improve policy learning (Schaul et al. 2015)"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Apply prioritized frame reward weighting during CGP evolution", "label": "Apply prioritized frame reward weighting during CGP evolution", "shape": "dot", "title": "Apply prioritized frame reward weighting during CGP evolution"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Reduced frame evaluation lowers computation with minimal learning loss", "label": "Reduced frame evaluation lowers computation with minimal learning loss", "shape": "dot", "title": "Reduced frame evaluation lowers computation with minimal learning loss"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Limit CGP evaluation to subset of episode frames", "label": "Limit CGP evaluation to subset of episode frames", "shape": "dot", "title": "Limit CGP evaluation to subset of episode frames"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Episode truncation and frame subsampling during fitness evaluation", "label": "Episode truncation and frame subsampling during fitness evaluation", "shape": "dot", "title": "Episode truncation and frame subsampling during fitness evaluation"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Authors suggest frame count reduction decreases computational load (Discussion section)", "label": "Authors suggest frame count reduction decreases computational load (Discussion section)", "shape": "dot", "title": "Authors suggest frame count reduction decreases computational load (Discussion section)"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Decrease evaluated frames per generation in CGP evolution", "label": "Decrease evaluated frames per generation in CGP evolution", "shape": "dot", "title": "Decrease evaluated frames per generation in CGP evolution"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Incorrect reward inference in inverse reinforcement learning applications", "label": "Incorrect reward inference in inverse reinforcement learning applications", "shape": "dot", "title": "Incorrect reward inference in inverse reinforcement learning applications"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Ill-posed reward ambiguity from limited demonstrations in IRL", "label": "Ill-posed reward ambiguity from limited demonstrations in IRL", "shape": "dot", "title": "Ill-posed reward ambiguity from limited demonstrations in IRL"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Noisy or suboptimal demonstrations impacting reward accuracy in IRL", "label": "Noisy or suboptimal demonstrations impacting reward accuracy in IRL", "shape": "dot", "title": "Noisy or suboptimal demonstrations impacting reward accuracy in IRL"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Incomplete model knowledge affecting reward inference in IRL", "label": "Incomplete model knowledge affecting reward inference in IRL", "shape": "dot", "title": "Incomplete model knowledge affecting reward inference in IRL"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "High dimensional state spaces increasing computational burden in IRL", "label": "High dimensional state spaces increasing computational burden in IRL", "shape": "dot", "title": "High dimensional state spaces increasing computational burden in IRL"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "High sample complexity requirements in IRL", "label": "High sample complexity requirements in IRL", "shape": "dot", "title": "High sample complexity requirements in IRL"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Maximum entropy principle yields least-biased reward distribution in IRL", "label": "Maximum entropy principle yields least-biased reward distribution in IRL", "shape": "dot", "title": "Maximum entropy principle yields least-biased reward distribution in IRL"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Bayesian posterior over reward functions captures uncertainty in IRL", "label": "Bayesian posterior over reward functions captures uncertainty in IRL", "shape": "dot", "title": "Bayesian posterior over reward functions captures uncertainty in IRL"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Active learning querying expert at high-entropy states reduces sample complexity in IRL", "label": "Active learning querying expert at high-entropy states reduces sample complexity in IRL", "shape": "dot", "title": "Active learning querying expert at high-entropy states reduces sample complexity in IRL"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Automatic feature construction via regression boosting reduces manual bias in IRL", "label": "Automatic feature construction via regression boosting reduces manual bias in IRL", "shape": "dot", "title": "Automatic feature construction via regression boosting reduces manual bias in IRL"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "State abstraction via bisimulation metrics enhances generalization in IRL", "label": "State abstraction via bisimulation metrics enhances generalization in IRL", "shape": "dot", "title": "State abstraction via bisimulation metrics enhances generalization in IRL"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Use maximum entropy optimization to resolve reward ambiguity in IRL", "label": "Use maximum entropy optimization to resolve reward ambiguity in IRL", "shape": "dot", "title": "Use maximum entropy optimization to resolve reward ambiguity in IRL"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Maintain reward uncertainty through Bayesian IRL framework", "label": "Maintain reward uncertainty through Bayesian IRL framework", "shape": "dot", "title": "Maintain reward uncertainty through Bayesian IRL framework"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Solicit informative demonstrations using active Bayesian IRL", "label": "Solicit informative demonstrations using active Bayesian IRL", "shape": "dot", "title": "Solicit informative demonstrations using active Bayesian IRL"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Automatically learn nonlinear reward features with regression trees", "label": "Automatically learn nonlinear reward features with regression trees", "shape": "dot", "title": "Automatically learn nonlinear reward features with regression trees"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Apply bisimulation-based state aggregation to cut sample complexity", "label": "Apply bisimulation-based state aggregation to cut sample complexity", "shape": "dot", "title": "Apply bisimulation-based state aggregation to cut sample complexity"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Convex entropy-constrained optimization of reward parameters", "label": "Convex entropy-constrained optimization of reward parameters", "shape": "dot", "title": "Convex entropy-constrained optimization of reward parameters"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "MCMC sampling of reward posterior distribution", "label": "MCMC sampling of reward posterior distribution", "shape": "dot", "title": "MCMC sampling of reward posterior distribution"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Entropy-driven state query strategy for expert demonstrations", "label": "Entropy-driven state query strategy for expert demonstrations", "shape": "dot", "title": "Entropy-driven state query strategy for expert demonstrations"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Regression tree-based feature generation algorithm (FIRL)", "label": "Regression tree-based feature generation algorithm (FIRL)", "shape": "dot", "title": "Regression tree-based feature generation algorithm (FIRL)"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Kernel regression over bisimulation metric for policy generalization", "label": "Kernel regression over bisimulation metric for policy generalization", "shape": "dot", "title": "Kernel regression over bisimulation metric for policy generalization"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Robust performance under noisy demonstrations in helicopter control tasks using MaxEnt IRL", "label": "Robust performance under noisy demonstrations in helicopter control tasks using MaxEnt IRL", "shape": "dot", "title": "Robust performance under noisy demonstrations in helicopter control tasks using MaxEnt IRL"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Accurate reward posterior in gridworld experiments with Bayesian IRL", "label": "Accurate reward posterior in gridworld experiments with Bayesian IRL", "shape": "dot", "title": "Accurate reward posterior in gridworld experiments with Bayesian IRL"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Lower sample complexity in active BIRL simulated domains", "label": "Lower sample complexity in active BIRL simulated domains", "shape": "dot", "title": "Lower sample complexity in active BIRL simulated domains"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Reduced imitation loss in path planning via automatic feature boosting", "label": "Reduced imitation loss in path planning via automatic feature boosting", "shape": "dot", "title": "Reduced imitation loss in path planning via automatic feature boosting"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Improved generalization with fewer demonstrations using bisimulation IRL", "label": "Improved generalization with fewer demonstrations using bisimulation IRL", "shape": "dot", "title": "Improved generalization with fewer demonstrations using bisimulation IRL"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Implement maximum entropy IRL during reward inference for autonomous agents", "label": "Implement maximum entropy IRL during reward inference for autonomous agents", "shape": "dot", "title": "Implement maximum entropy IRL during reward inference for autonomous agents"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Apply Bayesian IRL with MCMC to model reward uncertainty in training", "label": "Apply Bayesian IRL with MCMC to model reward uncertainty in training", "shape": "dot", "title": "Apply Bayesian IRL with MCMC to model reward uncertainty in training"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Collect demonstrations via entropy-based active learning queries", "label": "Collect demonstrations via entropy-based active learning queries", "shape": "dot", "title": "Collect demonstrations via entropy-based active learning queries"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate regression tree feature construction into reward modeling pipeline", "label": "Integrate regression tree feature construction into reward modeling pipeline", "shape": "dot", "title": "Integrate regression tree feature construction into reward modeling pipeline"}, {"color": "hsl(280, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy bisimulation-based state abstraction for efficient IRL training", "label": "Deploy bisimulation-based state abstraction for efficient IRL training", "shape": "dot", "title": "Deploy bisimulation-based state abstraction for efficient IRL training"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "credit assignment failure in delayed reward reinforcement learning", "label": "credit assignment failure in delayed reward reinforcement learning", "shape": "dot", "title": "credit assignment failure in delayed reward reinforcement learning"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "bias in temporal difference learning and high variance in Monte Carlo learning in delayed reward MDPs", "label": "bias in temporal difference learning and high variance in Monte Carlo learning in delayed reward MDPs", "shape": "dot", "title": "bias in temporal difference learning and high variance in Monte Carlo learning in delayed reward MDPs"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "eliminating expected future rewards simplifies Q-value estimation to immediate reward mean", "label": "eliminating expected future rewards simplifies Q-value estimation to immediate reward mean", "shape": "dot", "title": "eliminating expected future rewards simplifies Q-value estimation to immediate reward mean"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "return-equivalent sequence-Markov decision processes preserve optimal policies", "label": "return-equivalent sequence-Markov decision processes preserve optimal policies", "shape": "dot", "title": "return-equivalent sequence-Markov decision processes preserve optimal policies"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "reward redistribution achieving zero expected future rewards in SDPs", "label": "reward redistribution achieving zero expected future rewards in SDPs", "shape": "dot", "title": "reward redistribution achieving zero expected future rewards in SDPs"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "return decomposition via contribution analysis for constructing reward redistribution", "label": "return decomposition via contribution analysis for constructing reward redistribution", "shape": "dot", "title": "return decomposition via contribution analysis for constructing reward redistribution"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "second-order Markov reward redistribution using differences of consecutive Q-values", "label": "second-order Markov reward redistribution using differences of consecutive Q-values", "shape": "dot", "title": "second-order Markov reward redistribution using differences of consecutive Q-values"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "LSTM-based return prediction with contribution analysis (RUDDER implementation)", "label": "LSTM-based return prediction with contribution analysis (RUDDER implementation)", "shape": "dot", "title": "LSTM-based return prediction with contribution analysis (RUDDER implementation)"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "RUDDER speed-up over TD, MC, and MCTS on artificial delayed-reward tasks", "label": "RUDDER speed-up over TD, MC, and MCTS on artificial delayed-reward tasks", "shape": "dot", "title": "RUDDER speed-up over TD, MC, and MCTS on artificial delayed-reward tasks"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "RUDDER improves PPO performance on delayed-reward Atari games", "label": "RUDDER improves PPO performance on delayed-reward Atari games", "shape": "dot", "title": "RUDDER improves PPO performance on delayed-reward Atari games"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "integrate RUDDER reward redistribution during RL fine-tuning to mitigate delayed reward credit assignment", "label": "integrate RUDDER reward redistribution during RL fine-tuning to mitigate delayed reward credit assignment", "shape": "dot", "title": "integrate RUDDER reward redistribution during RL fine-tuning to mitigate delayed reward credit assignment"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "train an LSTM return decomposition model to automate reward redistribution in RL training", "label": "train an LSTM return decomposition model to automate reward redistribution in RL training", "shape": "dot", "title": "train an LSTM return decomposition model to automate reward redistribution in RL training"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Resource overconsumption of neural networks on constrained hardware platforms", "label": "Resource overconsumption of neural networks on constrained hardware platforms", "shape": "dot", "title": "Resource overconsumption of neural networks on constrained hardware platforms"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Mismatch between model complexity and hardware resource budgets", "label": "Mismatch between model complexity and hardware resource budgets", "shape": "dot", "title": "Mismatch between model complexity and hardware resource budgets"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Hardware-aware metrics guiding architecture search under constraints", "label": "Hardware-aware metrics guiding architecture search under constraints", "shape": "dot", "title": "Hardware-aware metrics guiding architecture search under constraints"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Integration of resource constraints into NAS reward function", "label": "Integration of resource constraints into NAS reward function", "shape": "dot", "title": "Integration of resource constraints into NAS reward function"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Multi-objective RL NAS with model size, FLOPs, compute intensity metrics", "label": "Multi-objective RL NAS with model size, FLOPs, compute intensity metrics", "shape": "dot", "title": "Multi-objective RL NAS with model size, FLOPs, compute intensity metrics"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Empirical accuracy-resource tradeoff results on CIFAR10 with RENA", "label": "Empirical accuracy-resource tradeoff results on CIFAR10 with RENA", "shape": "dot", "title": "Empirical accuracy-resource tradeoff results on CIFAR10 with RENA"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Employ RENA resource-constrained NAS to design efficient architectures", "label": "Employ RENA resource-constrained NAS to design efficient architectures", "shape": "dot", "title": "Employ RENA resource-constrained NAS to design efficient architectures"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "High inference latency in neural network applications", "label": "High inference latency in neural network applications", "shape": "dot", "title": "High inference latency in neural network applications"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Low compute intensity limiting data reuse on accelerators", "label": "Low compute intensity limiting data reuse on accelerators", "shape": "dot", "title": "Low compute intensity limiting data reuse on accelerators"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Compute intensity metric to favor data reuse", "label": "Compute intensity metric to favor data reuse", "shape": "dot", "title": "Compute intensity metric to favor data reuse"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "KWS high compute intensity architecture results under constraints", "label": "KWS high compute intensity architecture results under constraints", "shape": "dot", "title": "KWS high compute intensity architecture results under constraints"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Incorporate compute intensity thresholds into NAS objectives during model design", "label": "Incorporate compute intensity thresholds into NAS objectives during model design", "shape": "dot", "title": "Incorporate compute intensity thresholds into NAS objectives during model design"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Soft continuous resource-violation penalty enables learning under tight constraints", "label": "Soft continuous resource-violation penalty enables learning under tight constraints", "shape": "dot", "title": "Soft continuous resource-violation penalty enables learning under tight constraints"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Soft penalty reward shaping for continuous signals in NAS", "label": "Soft penalty reward shaping for continuous signals in NAS", "shape": "dot", "title": "Soft penalty reward shaping for continuous signals in NAS"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Reward function with exponential penalty parameter p=0.9", "label": "Reward function with exponential penalty parameter p=0.9", "shape": "dot", "title": "Reward function with exponential penalty parameter p=0.9"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "RENA surpasses random search in meeting resource constraints", "label": "RENA surpasses random search in meeting resource constraints", "shape": "dot", "title": "RENA surpasses random search in meeting resource constraints"}, {"color": "hsl(190, 80%, 60%)", "font": {"color": "white"}, "id": "Embed soft continuous resource violation penalty in NAS reward to balance performance and constraints", "label": "Embed soft continuous resource violation penalty in NAS reward to balance performance and constraints", "shape": "dot", "title": "Embed soft continuous resource violation penalty in NAS reward to balance performance and constraints"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Undetected scientific anomalies in large image datasets", "label": "Undetected scientific anomalies in large image datasets", "shape": "dot", "title": "Undetected scientific anomalies in large image datasets"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Opaque novelty detection outputs lacking interpretability in image analysis", "label": "Opaque novelty detection outputs lacking interpretability in image analysis", "shape": "dot", "title": "Opaque novelty detection outputs lacking interpretability in image analysis"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Pixel-level representations causing noisy novelty assessments", "label": "Pixel-level representations causing noisy novelty assessments", "shape": "dot", "title": "Pixel-level representations causing noisy novelty assessments"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "CNN feature embeddings provide robust abstract image representation for novelty detection", "label": "CNN feature embeddings provide robust abstract image representation for novelty detection", "shape": "dot", "title": "CNN feature embeddings provide robust abstract image representation for novelty detection"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Residual difference in embedding space isolates novel image content", "label": "Residual difference in embedding space isolates novel image content", "shape": "dot", "title": "Residual difference in embedding space isolates novel image content"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Combine DEMUD incremental SVD with CNN embeddings to select anomalous images", "label": "Combine DEMUD incremental SVD with CNN embeddings to select anomalous images", "shape": "dot", "title": "Combine DEMUD incremental SVD with CNN embeddings to select anomalous images"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Visualize residual and reconstruction via up-convolutional network for human comprehension", "label": "Visualize residual and reconstruction via up-convolutional network for human comprehension", "shape": "dot", "title": "Visualize residual and reconstruction via up-convolutional network for human comprehension"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Derive fc8 activations from pre-trained CaffeNet as image feature vectors", "label": "Derive fc8 activations from pre-trained CaffeNet as image feature vectors", "shape": "dot", "title": "Derive fc8 activations from pre-trained CaffeNet as image feature vectors"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Iteratively update SVD model with highest reconstruction-error items in DEMUD", "label": "Iteratively update SVD model with highest reconstruction-error items in DEMUD", "shape": "dot", "title": "Iteratively update SVD model with highest reconstruction-error items in DEMUD"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Generate explanation images via up-convolutional network from residual vectors", "label": "Generate explanation images via up-convolutional network from residual vectors", "shape": "dot", "title": "Generate explanation images via up-convolutional network from residual vectors"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "ImageNet balanced and unbalanced class discovery nAUC improvement", "label": "ImageNet balanced and unbalanced class discovery nAUC improvement", "shape": "dot", "title": "ImageNet balanced and unbalanced class discovery nAUC improvement"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Mars rover image discovery performance and explanations", "label": "Mars rover image discovery performance and explanations", "shape": "dot", "title": "Mars rover image discovery performance and explanations"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy DEMUD-CNN-UC pipeline for operational image triage to surface novel observations", "label": "Deploy DEMUD-CNN-UC pipeline for operational image triage to surface novel observations", "shape": "dot", "title": "Deploy DEMUD-CNN-UC pipeline for operational image triage to surface novel observations"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Conduct pre-deployment evaluation of DEMUD-CNN novelty detection with nAUC metrics on representative datasets", "label": "Conduct pre-deployment evaluation of DEMUD-CNN novelty detection with nAUC metrics on representative datasets", "shape": "dot", "title": "Conduct pre-deployment evaluation of DEMUD-CNN novelty detection with nAUC metrics on representative datasets"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Train domain-specific autoencoder or CNN on target image distribution to enhance novelty detection robustness", "label": "Train domain-specific autoencoder or CNN on target image distribution to enhance novelty detection robustness", "shape": "dot", "title": "Train domain-specific autoencoder or CNN on target image distribution to enhance novelty detection robustness"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Inefficient reward learning in complex sequential decision tasks", "label": "Inefficient reward learning in complex sequential decision tasks", "shape": "dot", "title": "Inefficient reward learning in complex sequential decision tasks"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Excessive human demonstration burden in robot training", "label": "Excessive human demonstration burden in robot training", "shape": "dot", "title": "Excessive human demonstration burden in robot training"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Reward misspecification causing subgoal failure in IRL", "label": "Reward misspecification causing subgoal failure in IRL", "shape": "dot", "title": "Reward misspecification causing subgoal failure in IRL"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Data sparsity of expert demonstrations in critical states", "label": "Data sparsity of expert demonstrations in critical states", "shape": "dot", "title": "Data sparsity of expert demonstrations in critical states"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Demonstration redundancy from full-task trajectories", "label": "Demonstration redundancy from full-task trajectories", "shape": "dot", "title": "Demonstration redundancy from full-task trajectories"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Lack of human high-level structure in IRL training", "label": "Lack of human high-level structure in IRL training", "shape": "dot", "title": "Lack of human high-level structure in IRL training"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Human subgoal decomposition can guide efficient learning", "label": "Human subgoal decomposition can guide efficient learning", "shape": "dot", "title": "Human subgoal decomposition can guide efficient learning"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Failure trajectories provide informative negative signal", "label": "Failure trajectories provide informative negative signal", "shape": "dot", "title": "Failure trajectories provide informative negative signal"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Emphasizing critical subgoals increases reward alignment", "label": "Emphasizing critical subgoals increases reward alignment", "shape": "dot", "title": "Emphasizing critical subgoals increases reward alignment"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Structured human-agent interaction using subtasks", "label": "Structured human-agent interaction using subtasks", "shape": "dot", "title": "Structured human-agent interaction using subtasks"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Integrating failure experience with demonstration data", "label": "Integrating failure experience with demonstration data", "shape": "dot", "title": "Integrating failure experience with demonstration data"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Deep neural reward networks for raw-state representation", "label": "Deep neural reward networks for raw-state representation", "shape": "dot", "title": "Deep neural reward networks for raw-state representation"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "HI-IRL algorithm with subgoal specification and partial demonstrations", "label": "HI-IRL algorithm with subgoal specification and partial demonstrations", "shape": "dot", "title": "HI-IRL algorithm with subgoal specification and partial demonstrations"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Deep IRL from Failure gradient updates", "label": "Deep IRL from Failure gradient updates", "shape": "dot", "title": "Deep IRL from Failure gradient updates"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Neural conv-BN-ReLU-FC architecture to map images to reward", "label": "Neural conv-BN-ReLU-FC architecture to map images to reward", "shape": "dot", "title": "Neural conv-BN-ReLU-FC architecture to map images to reward"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Grid-world and car-parking experiments show reduced demonstrations", "label": "Grid-world and car-parking experiments show reduced demonstrations", "shape": "dot", "title": "Grid-world and car-parking experiments show reduced demonstrations"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Performance curves indicate near-oracle success with fewer steps", "label": "Performance curves indicate near-oracle success with fewer steps", "shape": "dot", "title": "Performance curves indicate near-oracle success with fewer steps"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Reward maps show higher values at critical subgoal states", "label": "Reward maps show higher values at critical subgoal states", "shape": "dot", "title": "Reward maps show higher values at critical subgoal states"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Fine-tune agents with human-interactive subgoal-supervised IRL incorporating failure learning", "label": "Fine-tune agents with human-interactive subgoal-supervised IRL incorporating failure learning", "shape": "dot", "title": "Fine-tune agents with human-interactive subgoal-supervised IRL incorporating failure learning"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Collect partial demonstrations only for struggling subtasks during training", "label": "Collect partial demonstrations only for struggling subtasks during training", "shape": "dot", "title": "Collect partial demonstrations only for struggling subtasks during training"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Specify critical subgoals upfront to structure learning process", "label": "Specify critical subgoals upfront to structure learning process", "shape": "dot", "title": "Specify critical subgoals upfront to structure learning process"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Erroneous or harmful translations from adversarial perturbations in neural machine translation systems", "label": "Erroneous or harmful translations from adversarial perturbations in neural machine translation systems", "shape": "dot", "title": "Erroneous or harmful translations from adversarial perturbations in neural machine translation systems"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Character-level vulnerability to small input perturbations in NMT", "label": "Character-level vulnerability to small input perturbations in NMT", "shape": "dot", "title": "Character-level vulnerability to small input perturbations in NMT"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Ineffectiveness of black-box adversarial testing in revealing worst-case translation errors", "label": "Ineffectiveness of black-box adversarial testing in revealing worst-case translation errors", "shape": "dot", "title": "Ineffectiveness of black-box adversarial testing in revealing worst-case translation errors"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Gradient-based white-box attacks approximate worst-case input perturbations in NMT", "label": "Gradient-based white-box attacks approximate worst-case input perturbations in NMT", "shape": "dot", "title": "Gradient-based white-box attacks approximate worst-case input perturbations in NMT"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Adversarial training on stronger attacks yields robustness to multiple noise sources", "label": "Adversarial training on stronger attacks yields robustness to multiple noise sources", "shape": "dot", "title": "Adversarial training on stronger attacks yields robustness to multiple noise sources"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Use gradient-informed text edit operations to craft adversarial examples", "label": "Use gradient-informed text edit operations to craft adversarial examples", "shape": "dot", "title": "Use gradient-informed text edit operations to craft adversarial examples"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate white-box adversarial examples into training to harden models", "label": "Integrate white-box adversarial examples into training to harden models", "shape": "dot", "title": "Integrate white-box adversarial examples into training to harden models"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Combine white-box and black-box noise generation in ensemble training for broader robustness", "label": "Combine white-box and black-box noise generation in ensemble training for broader robustness", "shape": "dot", "title": "Combine white-box and black-box noise generation in ensemble training for broader robustness"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Flip/Insert/Delete/Swap character operations guided by loss gradients in HotFlip extension", "label": "Flip/Insert/Delete/Swap character operations guided by loss gradients in HotFlip extension", "shape": "dot", "title": "Flip/Insert/Delete/Swap character operations guided by loss gradients in HotFlip extension"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "One-shot gradient-based adversarial example generation for efficient adversarial training", "label": "One-shot gradient-based adversarial example generation for efficient adversarial training", "shape": "dot", "title": "One-shot gradient-based adversarial example generation for efficient adversarial training"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Ensemble training pipeline mixing clean, white-box, and black-box noisy inputs", "label": "Ensemble training pipeline mixing clean, white-box, and black-box noisy inputs", "shape": "dot", "title": "Ensemble training pipeline mixing clean, white-box, and black-box noisy inputs"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "White-box attacks reduce BLEU more than black-box attacks in untargeted, controlled, targeted scenarios on IWSLT datasets", "label": "White-box attacks reduce BLEU more than black-box attacks in untargeted, controlled, targeted scenarios on IWSLT datasets", "shape": "dot", "title": "White-box attacks reduce BLEU more than black-box attacks in untargeted, controlled, targeted scenarios on IWSLT datasets"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Adversarial training with white-box examples improves BLEU on noisy test sets versus baselines", "label": "Adversarial training with white-box examples improves BLEU on noisy test sets versus baselines", "shape": "dot", "title": "Adversarial training with white-box examples improves BLEU on noisy test sets versus baselines"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Ensemble adversarial training achieves highest average BLEU across noise types", "label": "Ensemble adversarial training achieves highest average BLEU across noise types", "shape": "dot", "title": "Ensemble adversarial training achieves highest average BLEU across noise types"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Apply gradient-based one-shot adversarial training with FIDS operations during NMT fine-tuning to improve robustness", "label": "Apply gradient-based one-shot adversarial training with FIDS operations during NMT fine-tuning to improve robustness", "shape": "dot", "title": "Apply gradient-based one-shot adversarial training with FIDS operations during NMT fine-tuning to improve robustness"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Employ ensemble adversarial training mixing white-box and black-box noise sources during NMT fine-tuning", "label": "Employ ensemble adversarial training mixing white-box and black-box noise sources during NMT fine-tuning", "shape": "dot", "title": "Employ ensemble adversarial training mixing white-box and black-box noise sources during NMT fine-tuning"}, {"color": "hsl(140, 80%, 60%)", "font": {"color": "white"}, "id": "Conduct gradient-based controlled and targeted attack evaluation in pre-deployment testing to identify translation vulnerabilities", "label": "Conduct gradient-based controlled and targeted attack evaluation in pre-deployment testing to identify translation vulnerabilities", "shape": "dot", "title": "Conduct gradient-based controlled and targeted attack evaluation in pre-deployment testing to identify translation vulnerabilities"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Extreme computational resource consumption in neural architecture search", "label": "Extreme computational resource consumption in neural architecture search", "shape": "dot", "title": "Extreme computational resource consumption in neural architecture search"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Suboptimal architecture performance on target tasks due to transferability limitations", "label": "Suboptimal architecture performance on target tasks due to transferability limitations", "shape": "dot", "title": "Suboptimal architecture performance on target tasks due to transferability limitations"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Black-box discrete architecture optimization requiring many evaluations", "label": "Black-box discrete architecture optimization requiring many evaluations", "shape": "dot", "title": "Black-box discrete architecture optimization requiring many evaluations"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Source dataset discrepancy lowers architecture transferability", "label": "Source dataset discrepancy lowers architecture transferability", "shape": "dot", "title": "Source dataset discrepancy lowers architecture transferability"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Discrete architecture representation prevents gradient-based optimization", "label": "Discrete architecture representation prevents gradient-based optimization", "shape": "dot", "title": "Discrete architecture representation prevents gradient-based optimization"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Architecture generalization depends on similarity between search and deployment distributions", "label": "Architecture generalization depends on similarity between search and deployment distributions", "shape": "dot", "title": "Architecture generalization depends on similarity between search and deployment distributions"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Gradient descent joint optimization of architecture and weights via bilevel formulation", "label": "Gradient descent joint optimization of architecture and weights via bilevel formulation", "shape": "dot", "title": "Gradient descent joint optimization of architecture and weights via bilevel formulation"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Leverage DARTS efficiency to search directly on deployment tasks", "label": "Leverage DARTS efficiency to search directly on deployment tasks", "shape": "dot", "title": "Leverage DARTS efficiency to search directly on deployment tasks"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Softmax mixed operations with architecture parameters (DARTS continuous relaxation)", "label": "Softmax mixed operations with architecture parameters (DARTS continuous relaxation)", "shape": "dot", "title": "Softmax mixed operations with architecture parameters (DARTS continuous relaxation)"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Finite difference approximation of second-order gradients in DARTS", "label": "Finite difference approximation of second-order gradients in DARTS", "shape": "dot", "title": "Finite difference approximation of second-order gradients in DARTS"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "First-order gradient approximation in DARTS with \u03be=0", "label": "First-order gradient approximation in DARTS with \u03be=0", "shape": "dot", "title": "First-order gradient approximation in DARTS with \u03be=0"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "CIFAR-10 2.83% test error with 4 GPU-day second-order DARTS", "label": "CIFAR-10 2.83% test error with 4 GPU-day second-order DARTS", "shape": "dot", "title": "CIFAR-10 2.83% test error with 4 GPU-day second-order DARTS"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "PTB perplexity 56.1 with 1 GPU-day second-order DARTS", "label": "PTB perplexity 56.1 with 1 GPU-day second-order DARTS", "shape": "dot", "title": "PTB perplexity 56.1 with 1 GPU-day second-order DARTS"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Three orders-of-magnitude compute reduction compared to RL/evolution methods", "label": "Three orders-of-magnitude compute reduction compared to RL/evolution methods", "shape": "dot", "title": "Three orders-of-magnitude compute reduction compared to RL/evolution methods"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "CIFAR-10 2.94% test error with 1.5 GPU-day first-order DARTS", "label": "CIFAR-10 2.94% test error with 1.5 GPU-day first-order DARTS", "shape": "dot", "title": "CIFAR-10 2.94% test error with 1.5 GPU-day first-order DARTS"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "ImageNet mobile 26.9% top-1 error using DARTS cell searched on CIFAR-10", "label": "ImageNet mobile 26.9% top-1 error using DARTS cell searched on CIFAR-10", "shape": "dot", "title": "ImageNet mobile 26.9% top-1 error using DARTS cell searched on CIFAR-10"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Use DARTS gradient-based architecture search during model design to reduce compute cost", "label": "Use DARTS gradient-based architecture search during model design to reduce compute cost", "shape": "dot", "title": "Use DARTS gradient-based architecture search during model design to reduce compute cost"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Apply first-order DARTS approximation during architecture search to minimize search compute", "label": "Apply first-order DARTS approximation during architecture search to minimize search compute", "shape": "dot", "title": "Apply first-order DARTS approximation during architecture search to minimize search compute"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Run DARTS search directly on target large-scale tasks to improve architecture transferability", "label": "Run DARTS search directly on target large-scale tasks to improve architecture transferability", "shape": "dot", "title": "Run DARTS search directly on target large-scale tasks to improve architecture transferability"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Reward misalignment from strategic interactions in multi-agent contexts", "label": "Reward misalignment from strategic interactions in multi-agent contexts", "shape": "dot", "title": "Reward misalignment from strategic interactions in multi-agent contexts"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Ambiguous reward inference due to non-unique equilibria in stochastic games", "label": "Ambiguous reward inference due to non-unique equilibria in stochastic games", "shape": "dot", "title": "Ambiguous reward inference due to non-unique equilibria in stochastic games"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Ill-posed MIRL under multiple reward-consistent equilibria", "label": "Ill-posed MIRL under multiple reward-consistent equilibria", "shape": "dot", "title": "Ill-posed MIRL under multiple reward-consistent equilibria"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Non-unique equilibrium likelihood specification challenge in MIRL", "label": "Non-unique equilibrium likelihood specification challenge in MIRL", "shape": "dot", "title": "Non-unique equilibrium likelihood specification challenge in MIRL"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Unique equilibrium selection constrains reward inference search space", "label": "Unique equilibrium selection constrains reward inference search space", "shape": "dot", "title": "Unique equilibrium selection constrains reward inference search space"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Local reduced gap objective approximates utilitarian equilibrium selection", "label": "Local reduced gap objective approximates utilitarian equilibrium selection", "shape": "dot", "title": "Local reduced gap objective approximates utilitarian equilibrium selection"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Bayesian MAP estimation with equilibrium linear constraints", "label": "Bayesian MAP estimation with equilibrium linear constraints", "shape": "dot", "title": "Bayesian MAP estimation with equilibrium linear constraints"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Linear programming for utilitarian CE and NE reward recovery", "label": "Linear programming for utilitarian CE and NE reward recovery", "shape": "dot", "title": "Linear programming for utilitarian CE and NE reward recovery"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Linear constraint formulation using B\u03c0 and D\u03c0 matrices", "label": "Linear constraint formulation using B\u03c0 and D\u03c0 matrices", "shape": "dot", "title": "Linear constraint formulation using B\u03c0 and D\u03c0 matrices"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Local reduced gap linear program with y variables and regularisation", "label": "Local reduced gap linear program with y variables and regularisation", "shape": "dot", "title": "Local reduced gap linear program with y variables and regularisation"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "GridWorld MIRL NRMSE 1e-3 outperforming IRL", "label": "GridWorld MIRL NRMSE 1e-3 outperforming IRL", "shape": "dot", "title": "GridWorld MIRL NRMSE 1e-3 outperforming IRL"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Soccer simulation shows advE-MIRL competitive with zero-sum MIRL and better than d-MIRL", "label": "Soccer simulation shows advE-MIRL competitive with zero-sum MIRL and better than d-MIRL", "shape": "dot", "title": "Soccer simulation shows advE-MIRL competitive with zero-sum MIRL and better than d-MIRL"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Robustness study shows gradual degradation with missing policy states", "label": "Robustness study shows gradual degradation with missing policy states", "shape": "dot", "title": "Robustness study shows gradual degradation with missing policy states"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy Bayesian MIRL with unique equilibrium constraints to model multi-agent rewards during AI training", "label": "Deploy Bayesian MIRL with unique equilibrium constraints to model multi-agent rewards during AI training", "shape": "dot", "title": "Deploy Bayesian MIRL with unique equilibrium constraints to model multi-agent rewards during AI training"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Use local reduced gap LP to infer utilitarian equilibria for multi-agent reward alignment", "label": "Use local reduced gap LP to infer utilitarian equilibria for multi-agent reward alignment", "shape": "dot", "title": "Use local reduced gap LP to infer utilitarian equilibria for multi-agent reward alignment"}, {"color": "hsl(20, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt advE-MIRL approach for competitive environment reward modeling", "label": "Adopt advE-MIRL approach for competitive environment reward modeling", "shape": "dot", "title": "Adopt advE-MIRL approach for competitive environment reward modeling"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Suboptimal inverse dynamics models in self-supervised imitation learning for robotic control", "label": "Suboptimal inverse dynamics models in self-supervised imitation learning for robotic control", "shape": "dot", "title": "Suboptimal inverse dynamics models in self-supervised imitation learning for robotic control"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "High human demonstration requirement in imitation learning for robotics", "label": "High human demonstration requirement in imitation learning for robotics", "shape": "dot", "title": "High human demonstration requirement in imitation learning for robotics"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Inefficient exploratory data collection in self-supervised IL for continuous control domains", "label": "Inefficient exploratory data collection in self-supervised IL for continuous control domains", "shape": "dot", "title": "Inefficient exploratory data collection in self-supervised IL for continuous control domains"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Sampling bias toward easy or human-preselected states in self-supervised IL", "label": "Sampling bias toward easy or human-preselected states in self-supervised IL", "shape": "dot", "title": "Sampling bias toward easy or human-preselected states in self-supervised IL"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Adversarial agent-model competition can generate hard training samples in self-supervised IL", "label": "Adversarial agent-model competition can generate hard training samples in self-supervised IL", "shape": "dot", "title": "Adversarial agent-model competition can generate hard training samples in self-supervised IL"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Bounded difficulty curriculum improves stability of adversarial training in self-supervised IL", "label": "Bounded difficulty curriculum improves stability of adversarial training in self-supervised IL", "shape": "dot", "title": "Bounded difficulty curriculum improves stability of adversarial training in self-supervised IL"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Use DRL agent to maximise inverse model prediction error during exploration", "label": "Use DRL agent to maximise inverse model prediction error during exploration", "shape": "dot", "title": "Use DRL agent to maximise inverse model prediction error during exploration"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Shape reward around threshold to focus on moderately hard samples", "label": "Shape reward around threshold to focus on moderately hard samples", "shape": "dot", "title": "Shape reward around threshold to focus on moderately hard samples"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "PPO-based DRL agent rewarded by inverse dynamics loss", "label": "PPO-based DRL agent rewarded by inverse dynamics loss", "shape": "dot", "title": "PPO-based DRL agent rewarded by inverse dynamics loss"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Reward reshaping function rt = -|L_I\u2212\u03b4| with threshold \u03b4", "label": "Reward reshaping function rt = -|L_I\u2212\u03b4| with threshold \u03b4", "shape": "dot", "title": "Reward reshaping function rt = -|L_I\u2212\u03b4| with threshold \u03b4"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Higher success rates than baselines on Fetch and Hand manipulation tasks", "label": "Higher success rates than baselines on Fetch and Hand manipulation tasks", "shape": "dot", "title": "Higher success rates than baselines on Fetch and Hand manipulation tasks"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Lower performance drop under injected action noise across tasks", "label": "Lower performance drop under injected action noise across tasks", "shape": "dot", "title": "Lower performance drop under injected action noise across tasks"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Jointly train PPO explorer and inverse dynamics model using adversarial exploration during self-supervised IL", "label": "Jointly train PPO explorer and inverse dynamics model using adversarial exploration during self-supervised IL", "shape": "dot", "title": "Jointly train PPO explorer and inverse dynamics model using adversarial exploration during self-supervised IL"}, {"color": "hsl(200, 80%, 60%)", "font": {"color": "white"}, "id": "Apply reward shaping threshold \u03b4 for stabilising adversarial exploration during self-supervised IL training", "label": "Apply reward shaping threshold \u03b4 for stabilising adversarial exploration during self-supervised IL training", "shape": "dot", "title": "Apply reward shaping threshold \u03b4 for stabilising adversarial exploration during self-supervised IL training"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Unauthorized adversarial reprogramming of neural networks in deployed systems", "label": "Unauthorized adversarial reprogramming of neural networks in deployed systems", "shape": "dot", "title": "Unauthorized adversarial reprogramming of neural networks in deployed systems"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Additive input-based adversarial programs exploiting network nonlinearities in vision models", "label": "Additive input-based adversarial programs exploiting network nonlinearities in vision models", "shape": "dot", "title": "Additive input-based adversarial programs exploiting network nonlinearities in vision models"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Flexibility of trained neural network representations enables new tasks through input bias modification", "label": "Flexibility of trained neural network representations enables new tasks through input bias modification", "shape": "dot", "title": "Flexibility of trained neural network representations enables new tasks through input bias modification"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Requirement of nonlinear interactions between adversarial program and data for successful reprogramming", "label": "Requirement of nonlinear interactions between adversarial program and data for successful reprogramming", "shape": "dot", "title": "Requirement of nonlinear interactions between adversarial program and data for successful reprogramming"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Input-level defense mechanisms to detect or neutralize adversarial programs", "label": "Input-level defense mechanisms to detect or neutralize adversarial programs", "shape": "dot", "title": "Input-level defense mechanisms to detect or neutralize adversarial programs"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Training models with adversarial reprogramming examples to increase robustness", "label": "Training models with adversarial reprogramming examples to increase robustness", "shape": "dot", "title": "Training models with adversarial reprogramming examples to increase robustness"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Architectural constraints reducing effect of input bias changes on internal representations", "label": "Architectural constraints reducing effect of input bias changes on internal representations", "shape": "dot", "title": "Architectural constraints reducing effect of input bias changes on internal representations"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Frequency-based masking and anomaly detection preprocessing for adversarial program filtering", "label": "Frequency-based masking and anomaly detection preprocessing for adversarial program filtering", "shape": "dot", "title": "Frequency-based masking and anomaly detection preprocessing for adversarial program filtering"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Adversarial reprogramming adversarial training during fine-tuning phase", "label": "Adversarial reprogramming adversarial training during fine-tuning phase", "shape": "dot", "title": "Adversarial reprogramming adversarial training during fine-tuning phase"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "First-layer bias normalization and weight constraints to limit reprogramming susceptibility", "label": "First-layer bias normalization and weight constraints to limit reprogramming susceptibility", "shape": "dot", "title": "First-layer bias normalization and weight constraints to limit reprogramming susceptibility"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "High success rates of reprogramming six ImageNet models across tasks", "label": "High success rates of reprogramming six ImageNet models across tasks", "shape": "dot", "title": "High success rates of reprogramming six ImageNet models across tasks"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Adversarially trained Inception V3 still vulnerable, showing only slight reduction in reprogramming success", "label": "Adversarially trained Inception V3 still vulnerable, showing only slight reduction in reprogramming success", "shape": "dot", "title": "Adversarially trained Inception V3 still vulnerable, showing only slight reduction in reprogramming success"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Reduced reprogramming success on randomly initialized networks", "label": "Reduced reprogramming success on randomly initialized networks", "shape": "dot", "title": "Reduced reprogramming success on randomly initialized networks"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Projected detection accuracy of input sanitization filters against adversarial programs", "label": "Projected detection accuracy of input sanitization filters against adversarial programs", "shape": "dot", "title": "Projected detection accuracy of input sanitization filters against adversarial programs"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Simulation-based robustness improvement via adversarial reprogramming-aware training", "label": "Simulation-based robustness improvement via adversarial reprogramming-aware training", "shape": "dot", "title": "Simulation-based robustness improvement via adversarial reprogramming-aware training"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Theoretical reduction in susceptibility from bias constraint architecture", "label": "Theoretical reduction in susceptibility from bias constraint architecture", "shape": "dot", "title": "Theoretical reduction in susceptibility from bias constraint architecture"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Input additive adversarial program optimized via Equation 3", "label": "Input additive adversarial program optimized via Equation 3", "shape": "dot", "title": "Input additive adversarial program optimized via Equation 3"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy input sanitization filters detecting adversarial programs before inference", "label": "Deploy input sanitization filters detecting adversarial programs before inference", "shape": "dot", "title": "Deploy input sanitization filters detecting adversarial programs before inference"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Fine-tune models with adversarial reprogramming adversarial examples to improve robustness", "label": "Fine-tune models with adversarial reprogramming adversarial examples to improve robustness", "shape": "dot", "title": "Fine-tune models with adversarial reprogramming adversarial examples to improve robustness"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Incorporate first-layer bias constraint architecture during model design to limit reprogramming vulnerability", "label": "Incorporate first-layer bias constraint architecture during model design to limit reprogramming vulnerability", "shape": "dot", "title": "Incorporate first-layer bias constraint architecture during model design to limit reprogramming vulnerability"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Exploitation of agent by adversarial environment in decision-making tasks", "label": "Exploitation of agent by adversarial environment in decision-making tasks", "shape": "dot", "title": "Exploitation of agent by adversarial environment in decision-making tasks"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Inadequate modeling of environment attitude continuum in multi-agent systems", "label": "Inadequate modeling of environment attitude continuum in multi-agent systems", "shape": "dot", "title": "Inadequate modeling of environment attitude continuum in multi-agent systems"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Information asymmetry enabling environment reaction to agent strategy", "label": "Information asymmetry enabling environment reaction to agent strategy", "shape": "dot", "title": "Information asymmetry enabling environment reaction to agent strategy"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Continuous inverse-temperature parameterization of environment attitude", "label": "Continuous inverse-temperature parameterization of environment attitude", "shape": "dot", "title": "Continuous inverse-temperature parameterization of environment attitude"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Mutual information as indicator of environment reactivity", "label": "Mutual information as indicator of environment reactivity", "shape": "dot", "title": "Mutual information as indicator of environment reactivity"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Risk-sensitive equilibrium strategy derived from information-constrained game", "label": "Risk-sensitive equilibrium strategy derived from information-constrained game", "shape": "dot", "title": "Risk-sensitive equilibrium strategy derived from information-constrained game"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Detection of environment attitude via mutual information estimation", "label": "Detection of environment attitude via mutual information estimation", "shape": "dot", "title": "Detection of environment attitude via mutual information estimation"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Gibbs best-response equilibrium computation with exponential update algorithm", "label": "Gibbs best-response equilibrium computation with exponential update algorithm", "shape": "dot", "title": "Gibbs best-response equilibrium computation with exponential update algorithm"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Thompson sampling estimation of inverse temperature parameter \u03b2", "label": "Thompson sampling estimation of inverse temperature parameter \u03b2", "shape": "dot", "title": "Thompson sampling estimation of inverse temperature parameter \u03b2"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Bernoulli bandit experiments showing attitude-dependent payoffs", "label": "Bernoulli bandit experiments showing attitude-dependent payoffs", "shape": "dot", "title": "Bernoulli bandit experiments showing attitude-dependent payoffs"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Gaussian bandit variance dependence experiments demonstrating strategic adaptation", "label": "Gaussian bandit variance dependence experiments demonstrating strategic adaptation", "shape": "dot", "title": "Gaussian bandit variance dependence experiments demonstrating strategic adaptation"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Linear classifier experiment showing adversarial label flipping and mitigation", "label": "Linear classifier experiment showing adversarial label flipping and mitigation", "shape": "dot", "title": "Linear classifier experiment showing adversarial label flipping and mitigation"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Embed inverse-temperature friend-foe modeling into agent decision policies during model design", "label": "Embed inverse-temperature friend-foe modeling into agent decision policies during model design", "shape": "dot", "title": "Embed inverse-temperature friend-foe modeling into agent decision policies during model design"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Compute risk-sensitive equilibrium strategies via iterative Gibbs updates during fine-tuning", "label": "Compute risk-sensitive equilibrium strategies via iterative Gibbs updates during fine-tuning", "shape": "dot", "title": "Compute risk-sensitive equilibrium strategies via iterative Gibbs updates during fine-tuning"}, {"color": "hsl(260, 80%, 60%)", "font": {"color": "white"}, "id": "Estimate environment attitude online with Thompson sampling and adapt policies during deployment", "label": "Estimate environment attitude online with Thompson sampling and adapt policies during deployment", "shape": "dot", "title": "Estimate environment attitude online with Thompson sampling and adapt policies during deployment"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Behavior misprediction by AI systems in complex interactive environments", "label": "Behavior misprediction by AI systems in complex interactive environments", "shape": "dot", "title": "Behavior misprediction by AI systems in complex interactive environments"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Infeasibility of IRL in complex games due to simulator or policy access requirements", "label": "Infeasibility of IRL in complex games due to simulator or policy access requirements", "shape": "dot", "title": "Infeasibility of IRL in complex games due to simulator or policy access requirements"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Single-scalar reward assumption in RL ignores multi-faceted human motivations", "label": "Single-scalar reward assumption in RL ignores multi-faceted human motivations", "shape": "dot", "title": "Single-scalar reward assumption in RL ignores multi-faceted human motivations"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Traditional IRL methods require environment dynamics or policy access", "label": "Traditional IRL methods require environment dynamics or policy access", "shape": "dot", "title": "Traditional IRL methods require environment dynamics or policy access"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Human behavior can be conceptualized as optimizing a weighted combination of multiple latent reward dimensions", "label": "Human behavior can be conceptualized as optimizing a weighted combination of multiple latent reward dimensions", "shape": "dot", "title": "Human behavior can be conceptualized as optimizing a weighted combination of multiple latent reward dimensions"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Off-policy IRL using historical trajectories allows reward inference without simulators", "label": "Off-policy IRL using historical trajectories allows reward inference without simulators", "shape": "dot", "title": "Off-policy IRL using historical trajectories allows reward inference without simulators"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Decompose reward signals per motivation and learn weights via linear scalarization", "label": "Decompose reward signals per motivation and learn weights via linear scalarization", "shape": "dot", "title": "Decompose reward signals per motivation and learn weights via linear scalarization"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Combine DQN-based Q-learning for each motivation dimension with linear programming", "label": "Combine DQN-based Q-learning for each motivation dimension with linear programming", "shape": "dot", "title": "Combine DQN-based Q-learning for each motivation dimension with linear programming"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "MMBM two-step workflow with per-motivation DQNs and LP weight inference", "label": "MMBM two-step workflow with per-motivation DQNs and LP weight inference", "shape": "dot", "title": "MMBM two-step workflow with per-motivation DQNs and LP weight inference"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Recovered player motivation weights aligning with psychological survey results", "label": "Recovered player motivation weights aligning with psychological survey results", "shape": "dot", "title": "Recovered player motivation weights aligning with psychological survey results"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Improved behavior prediction accuracy (56.5%) versus baselines in WoWAH dataset", "label": "Improved behavior prediction accuracy (56.5%) versus baselines in WoWAH dataset", "shape": "dot", "title": "Improved behavior prediction accuracy (56.5%) versus baselines in WoWAH dataset"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Implement multi-motivation reward modeling via MMBM in RL agent design", "label": "Implement multi-motivation reward modeling via MMBM in RL agent design", "shape": "dot", "title": "Implement multi-motivation reward modeling via MMBM in RL agent design"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Train per-motivation DQNs with historical trajectories and LP weight inference for agent alignment", "label": "Train per-motivation DQNs with historical trajectories and LP weight inference for agent alignment", "shape": "dot", "title": "Train per-motivation DQNs with historical trajectories and LP weight inference for agent alignment"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Uninterpretable reinforcement learning policies in safety-critical domains", "label": "Uninterpretable reinforcement learning policies in safety-critical domains", "shape": "dot", "title": "Uninterpretable reinforcement learning policies in safety-critical domains"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Absence of verifiable safety guarantees in black-box RL policies", "label": "Absence of verifiable safety guarantees in black-box RL policies", "shape": "dot", "title": "Absence of verifiable safety guarantees in black-box RL policies"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Sample inefficiency of gradient-based RL under poor initialization", "label": "Sample inefficiency of gradient-based RL under poor initialization", "shape": "dot", "title": "Sample inefficiency of gradient-based RL under poor initialization"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Opaque decision logic in deep neural network policies", "label": "Opaque decision logic in deep neural network policies", "shape": "dot", "title": "Opaque decision logic in deep neural network policies"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Inability to impose and validate safety constraints on black-box policies", "label": "Inability to impose and validate safety constraints on black-box policies", "shape": "dot", "title": "Inability to impose and validate safety constraints on black-box policies"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Gradient descent local minima causing slow policy convergence", "label": "Gradient descent local minima causing slow policy convergence", "shape": "dot", "title": "Gradient descent local minima causing slow policy convergence"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Symbolic policy representations enable formal verification and repair", "label": "Symbolic policy representations enable formal verification and repair", "shape": "dot", "title": "Symbolic policy representations enable formal verification and repair"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Alternating optimization between symbolic and neural representations improves learning efficiency", "label": "Alternating optimization between symbolic and neural representations improves learning efficiency", "shape": "dot", "title": "Alternating optimization between symbolic and neural representations improves learning efficiency"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Iterative synthesis\u2013repair\u2013imitation\u2013finetuning framework for policy improvement", "label": "Iterative synthesis\u2013repair\u2013imitation\u2013finetuning framework for policy improvement", "shape": "dot", "title": "Iterative synthesis\u2013repair\u2013imitation\u2013finetuning framework for policy improvement"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Human-in-the-loop program repair for adding desired behaviors", "label": "Human-in-the-loop program repair for adding desired behaviors", "shape": "dot", "title": "Human-in-the-loop program repair for adding desired behaviors"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Policy synthesis to decision tree via VIPER", "label": "Policy synthesis to decision tree via VIPER", "shape": "dot", "title": "Policy synthesis to decision tree via VIPER"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Program repair using constraint solving or manual edits to symbolic program", "label": "Program repair using constraint solving or manual edits to symbolic program", "shape": "dot", "title": "Program repair using constraint solving or manual edits to symbolic program"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Behavioral cloning of repaired program into neural policy", "label": "Behavioral cloning of repaired program into neural policy", "shape": "dot", "title": "Behavioral cloning of repaired program into neural policy"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "TRPO finetuning of cloned policy", "label": "TRPO finetuning of cloned policy", "shape": "dot", "title": "TRPO finetuning of cloned policy"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "CartPole experiments showing reward improvements after program repair", "label": "CartPole experiments showing reward improvements after program repair", "shape": "dot", "title": "CartPole experiments showing reward improvements after program repair"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Faster convergence of TRPO from repaired initialization versus worst policy", "label": "Faster convergence of TRPO from repaired initialization versus worst policy", "shape": "dot", "title": "Faster convergence of TRPO from repaired initialization versus worst policy"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy MORL pipeline during RL training to iteratively synthesize, repair, imitate, and finetune policies", "label": "Deploy MORL pipeline during RL training to iteratively synthesize, repair, imitate, and finetune policies", "shape": "dot", "title": "Deploy MORL pipeline during RL training to iteratively synthesize, repair, imitate, and finetune policies"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Perform program repair on symbolic policies to enforce safety constraints before imitation", "label": "Perform program repair on symbolic policies to enforce safety constraints before imitation", "shape": "dot", "title": "Perform program repair on symbolic policies to enforce safety constraints before imitation"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Extract decision tree representations from neural policies for interpretability and verification", "label": "Extract decision tree representations from neural policies for interpretability and verification", "shape": "dot", "title": "Extract decision tree representations from neural policies for interpretability and verification"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Finetune behavioral cloned policies using TRPO to escape local minima", "label": "Finetune behavioral cloned policies using TRPO to escape local minima", "shape": "dot", "title": "Finetune behavioral cloned policies using TRPO to escape local minima"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "low-quality solutions in combinatorial optimization for industrial tasks", "label": "low-quality solutions in combinatorial optimization for industrial tasks", "shape": "dot", "title": "low-quality solutions in combinatorial optimization for industrial tasks"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "insufficient self-play curriculum in single-player reinforcement learning", "label": "insufficient self-play curriculum in single-player reinforcement learning", "shape": "dot", "title": "insufficient self-play curriculum in single-player reinforcement learning"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "relative performance ranking emulates self-play in single-player settings", "label": "relative performance ranking emulates self-play in single-player settings", "shape": "dot", "title": "relative performance ranking emulates self-play in single-player settings"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "curriculum via ranked rewards exceeding recent performance threshold", "label": "curriculum via ranked rewards exceeding recent performance threshold", "shape": "dot", "title": "curriculum via ranked rewards exceeding recent performance threshold"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "ranked reward mechanism integrated with neural policy-value network and MCTS", "label": "ranked reward mechanism integrated with neural policy-value network and MCTS", "shape": "dot", "title": "ranked reward mechanism integrated with neural policy-value network and MCTS"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "bin packing experiments show 10% performance improvement and 72% optimality at 75% threshold", "label": "bin packing experiments show 10% performance improvement and 72% optimality at 75% threshold", "shape": "dot", "title": "bin packing experiments show 10% performance improvement and 72% optimality at 75% threshold"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "learning instability at 90% ranking threshold", "label": "learning instability at 90% ranking threshold", "shape": "dot", "title": "learning instability at 90% ranking threshold"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Use ranked reward mechanism with 75th percentile threshold during RL training for single-player optimization tasks", "label": "Use ranked reward mechanism with 75th percentile threshold during RL training for single-player optimization tasks", "shape": "dot", "title": "Use ranked reward mechanism with 75th percentile threshold during RL training for single-player optimization tasks"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Calibrate ranking threshold between 50% and 75% during pre-deployment testing to avoid learning instability", "label": "Calibrate ranking threshold between 50% and 75% during pre-deployment testing to avoid learning instability", "shape": "dot", "title": "Calibrate ranking threshold between 50% and 75% during pre-deployment testing to avoid learning instability"}, {"color": "hsl(180, 80%, 60%)", "font": {"color": "white"}, "id": "Implement the R2 algorithm combining policy-value network, MCTS, and ranked reward for combinatorial optimization problems", "label": "Implement the R2 algorithm combining policy-value network, MCTS, and ranked reward for combinatorial optimization problems", "shape": "dot", "title": "Implement the R2 algorithm combining policy-value network, MCTS, and ranked reward for combinatorial optimization problems"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "adversarial misclassification in safety-critical deep neural network applications", "label": "adversarial misclassification in safety-critical deep neural network applications", "shape": "dot", "title": "adversarial misclassification in safety-critical deep neural network applications"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "absence of provable robustness guarantees in modern deep neural networks", "label": "absence of provable robustness guarantees in modern deep neural networks", "shape": "dot", "title": "absence of provable robustness guarantees in modern deep neural networks"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "computational intractability of exact robustness verification for high-dimensional DNNs", "label": "computational intractability of exact robustness verification for high-dimensional DNNs", "shape": "dot", "title": "computational intractability of exact robustness verification for high-dimensional DNNs"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "Lipschitz continuity bounding DNN output variation with input perturbation", "label": "Lipschitz continuity bounding DNN output variation with input perturbation", "shape": "dot", "title": "Lipschitz continuity bounding DNN output variation with input perturbation"}, {"color": "hsl(250, 80%, 60%)", "font": {"color": "white"}, "id": "pointwise robustness reducible to finite optimisation over \u03c4-grid inputs", "label": "pointwise robustness reducible to finite optimisation over \u03c4-grid inputs", "shape": "dot", "title": "pointwise robustness reducible to finite optimisation over \u03c4-grid inputs"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "two-player game formulation approximating robustness metrics in DNNs", "label": "two-player game formulation approximating robustness metrics in DNNs", "shape": "dot", "title": "two-player game formulation approximating robustness metrics in DNNs"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "feature-based input partitioning to reduce search dimensionality", "label": "feature-based input partitioning to reduce search dimensionality", "shape": "dot", "title": "feature-based input partitioning to reduce search dimensionality"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Monte Carlo tree search generating upper bounds for robustness games", "label": "Monte Carlo tree search generating upper bounds for robustness games", "shape": "dot", "title": "Monte Carlo tree search generating upper bounds for robustness games"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Admissible A* search computing lower bounds in cooperative robustness game", "label": "Admissible A* search computing lower bounds in cooperative robustness game", "shape": "dot", "title": "Admissible A* search computing lower bounds in cooperative robustness game"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "Alpha-Beta pruning computing lower bounds in competitive robustness game", "label": "Alpha-Beta pruning computing lower bounds in competitive robustness game", "shape": "dot", "title": "Alpha-Beta pruning computing lower bounds in competitive robustness game"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "DeepGame software tool integrating game-based approximate verification", "label": "DeepGame software tool integrating game-based approximate verification", "shape": "dot", "title": "DeepGame software tool integrating game-based approximate verification"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "convergence of robustness bounds on MNIST, CIFAR10 and GTSRB datasets", "label": "convergence of robustness bounds on MNIST, CIFAR10 and GTSRB datasets", "shape": "dot", "title": "convergence of robustness bounds on MNIST, CIFAR10 and GTSRB datasets"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "competitive adversarial example efficiency compared to CW, JSMA and others", "label": "competitive adversarial example efficiency compared to CW, JSMA and others", "shape": "dot", "title": "competitive adversarial example efficiency compared to CW, JSMA and others"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "traffic light study revealing one-pixel adversarial vulnerability", "label": "traffic light study revealing one-pixel adversarial vulnerability", "shape": "dot", "title": "traffic light study revealing one-pixel adversarial vulnerability"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "perform DeepGame-based pre-deployment robustness testing of DNN models", "label": "perform DeepGame-based pre-deployment robustness testing of DNN models", "shape": "dot", "title": "perform DeepGame-based pre-deployment robustness testing of DNN models"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "certify safety regions by estimating maximum safe radius via \u03c4-grid Lipschitz verification", "label": "certify safety regions by estimating maximum safe radius via \u03c4-grid Lipschitz verification", "shape": "dot", "title": "certify safety regions by estimating maximum safe radius via \u03c4-grid Lipschitz verification"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "use feature robustness metrics in runtime monitoring to restrict unsafe perturbations", "label": "use feature robustness metrics in runtime monitoring to restrict unsafe perturbations", "shape": "dot", "title": "use feature robustness metrics in runtime monitoring to restrict unsafe perturbations"}, {"color": "hsl(30, 80%, 60%)", "font": {"color": "white"}, "id": "leverage game-based adversarial search to augment adversarial training datasets", "label": "leverage game-based adversarial search to augment adversarial training datasets", "shape": "dot", "title": "leverage game-based adversarial search to augment adversarial training datasets"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Poor length generalization of feed-forward Transformers in sequence modeling", "label": "Poor length generalization of feed-forward Transformers in sequence modeling", "shape": "dot", "title": "Poor length generalization of feed-forward Transformers in sequence modeling"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Inefficient per-symbol computation allocation in fixed-depth sequence models", "label": "Inefficient per-symbol computation allocation in fixed-depth sequence models", "shape": "dot", "title": "Inefficient per-symbol computation allocation in fixed-depth sequence models"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Computational non-universality limiting algorithmic reasoning in standard Transformers", "label": "Computational non-universality limiting algorithmic reasoning in standard Transformers", "shape": "dot", "title": "Computational non-universality limiting algorithmic reasoning in standard Transformers"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Absence of recurrent inductive bias in Transformers causing generalization failures", "label": "Absence of recurrent inductive bias in Transformers causing generalization failures", "shape": "dot", "title": "Absence of recurrent inductive bias in Transformers causing generalization failures"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Uniform fixed-depth processing per symbol causing resource misallocation", "label": "Uniform fixed-depth processing per symbol causing resource misallocation", "shape": "dot", "title": "Uniform fixed-depth processing per symbol causing resource misallocation"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Fixed layer depth independent of input length limiting expressivity", "label": "Fixed layer depth independent of input length limiting expressivity", "shape": "dot", "title": "Fixed layer depth independent of input length limiting expressivity"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Parallel recurrent refinement enables iterative algorithm simulation in Transformers", "label": "Parallel recurrent refinement enables iterative algorithm simulation in Transformers", "shape": "dot", "title": "Parallel recurrent refinement enables iterative algorithm simulation in Transformers"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Adaptive halting per symbol enables dynamic resource allocation based on ambiguity", "label": "Adaptive halting per symbol enables dynamic resource allocation based on ambiguity", "shape": "dot", "title": "Adaptive halting per symbol enables dynamic resource allocation based on ambiguity"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Depth scaling with input length yields Turing completeness in sequence models", "label": "Depth scaling with input length yields Turing completeness in sequence models", "shape": "dot", "title": "Depth scaling with input length yields Turing completeness in sequence models"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate shared-weight recurrence into Transformer while retaining self-attention parallelism", "label": "Integrate shared-weight recurrence into Transformer while retaining self-attention parallelism", "shape": "dot", "title": "Integrate shared-weight recurrence into Transformer while retaining self-attention parallelism"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Apply adaptive computation time mechanism within recurrent Transformer steps", "label": "Apply adaptive computation time mechanism within recurrent Transformer steps", "shape": "dot", "title": "Apply adaptive computation time mechanism within recurrent Transformer steps"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Enable unbounded recurrent steps tied to input length to extend expressivity", "label": "Enable unbounded recurrent steps tied to input length to extend expressivity", "shape": "dot", "title": "Enable unbounded recurrent steps tied to input length to extend expressivity"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Universal Transformer architecture with self-attention plus shared transition recurrence", "label": "Universal Transformer architecture with self-attention plus shared transition recurrence", "shape": "dot", "title": "Universal Transformer architecture with self-attention plus shared transition recurrence"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Per-symbol adaptive computation time halting in Universal Transformer", "label": "Per-symbol adaptive computation time halting in Universal Transformer", "shape": "dot", "title": "Per-symbol adaptive computation time halting in Universal Transformer"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Parameter tying across depth layers in Universal Transformer", "label": "Parameter tying across depth layers in Universal Transformer", "shape": "dot", "title": "Parameter tying across depth layers in Universal Transformer"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Universal Transformer achieves state-of-the-art on algorithmic and bAbI tasks with length generalization", "label": "Universal Transformer achieves state-of-the-art on algorithmic and bAbI tasks with length generalization", "shape": "dot", "title": "Universal Transformer achieves state-of-the-art on algorithmic and bAbI tasks with length generalization"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Adaptive Universal Transformer exhibits higher ponder time on ambiguous inputs and improves bAbI accuracy", "label": "Adaptive Universal Transformer exhibits higher ponder time on ambiguous inputs and improves bAbI accuracy", "shape": "dot", "title": "Adaptive Universal Transformer exhibits higher ponder time on ambiguous inputs and improves bAbI accuracy"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Proof of Universal Transformer computational universality and performance on diverse tasks", "label": "Proof of Universal Transformer computational universality and performance on diverse tasks", "shape": "dot", "title": "Proof of Universal Transformer computational universality and performance on diverse tasks"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Design model architectures using Universal Transformer recurrence to improve length generalization", "label": "Design model architectures using Universal Transformer recurrence to improve length generalization", "shape": "dot", "title": "Design model architectures using Universal Transformer recurrence to improve length generalization"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Implement adaptive computation time per symbol during model design of sequence models", "label": "Implement adaptive computation time per symbol during model design of sequence models", "shape": "dot", "title": "Implement adaptive computation time per symbol during model design of sequence models"}, {"color": "hsl(350, 80%, 60%)", "font": {"color": "white"}, "id": "Replace fixed-depth Transformer layers with parameter-tied recurrent steps for universality", "label": "Replace fixed-depth Transformer layers with parameter-tied recurrent steps for universality", "shape": "dot", "title": "Replace fixed-depth Transformer layers with parameter-tied recurrent steps for universality"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "policy failure from sample inefficiency in model-based reinforcement learning", "label": "policy failure from sample inefficiency in model-based reinforcement learning", "shape": "dot", "title": "policy failure from sample inefficiency in model-based reinforcement learning"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "high sample complexity of non-factorised transition model estimation", "label": "high sample complexity of non-factorised transition model estimation", "shape": "dot", "title": "high sample complexity of non-factorised transition model estimation"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "compounding errors from inaccurate transition models", "label": "compounding errors from inaccurate transition models", "shape": "dot", "title": "compounding errors from inaccurate transition models"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "discrete abstract state bottlenecks reduce variance with controlled bias", "label": "discrete abstract state bottlenecks reduce variance with controlled bias", "shape": "dot", "title": "discrete abstract state bottlenecks reduce variance with controlled bias"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "policy access to full state mitigates abstraction deficiency", "label": "policy access to full state mitigates abstraction deficiency", "shape": "dot", "title": "policy access to full state mitigates abstraction deficiency"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "factorised transition model with discrete abstract states and fewer parameters", "label": "factorised transition model with discrete abstract states and fewer parameters", "shape": "dot", "title": "factorised transition model with discrete abstract states and fewer parameters"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "mapping full states to abstract states via clustering or deterministic classifiers", "label": "mapping full states to abstract states via clustering or deterministic classifiers", "shape": "dot", "title": "mapping full states to abstract states via clustering or deterministic classifiers"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "neural transition model conditioned on abstract states", "label": "neural transition model conditioned on abstract states", "shape": "dot", "title": "neural transition model conditioned on abstract states"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "rollout simulation in learned Bottleneck Simulator for policy training", "label": "rollout simulation in learned Bottleneck Simulator for policy training", "shape": "dot", "title": "rollout simulation in learned Bottleneck Simulator for policy training"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "empirical sample-efficiency gains in Home World text adventure", "label": "empirical sample-efficiency gains in Home World text adventure", "shape": "dot", "title": "empirical sample-efficiency gains in Home World text adventure"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "real-world user score improvements in Alexa Prize dialogue system", "label": "real-world user score improvements in Alexa Prize dialogue system", "shape": "dot", "title": "real-world user score improvements in Alexa Prize dialogue system"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "mathematical error bound showing O(|S||Z||A|) sample complexity", "label": "mathematical error bound showing O(|S||Z||A|) sample complexity", "shape": "dot", "title": "mathematical error bound showing O(|S||Z||A|) sample complexity"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "integrate Bottleneck Simulator factorised model in RL training pipelines", "label": "integrate Bottleneck Simulator factorised model in RL training pipelines", "shape": "dot", "title": "integrate Bottleneck Simulator factorised model in RL training pipelines"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "adaptively refine abstract state granularity as data grows", "label": "adaptively refine abstract state granularity as data grows", "shape": "dot", "title": "adaptively refine abstract state granularity as data grows"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Pathologically wrong value inference from hierarchical human planning in IRL", "label": "Pathologically wrong value inference from hierarchical human planning in IRL", "shape": "dot", "title": "Pathologically wrong value inference from hierarchical human planning in IRL"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Assumption of flat optimal human behaviour in standard IRL", "label": "Assumption of flat optimal human behaviour in standard IRL", "shape": "dot", "title": "Assumption of flat optimal human behaviour in standard IRL"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Human decision-making uses hierarchical options", "label": "Human decision-making uses hierarchical options", "shape": "dot", "title": "Human decision-making uses hierarchical options"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Boltzmann-rational bounded planning models human action selection", "label": "Boltzmann-rational bounded planning models human action selection", "shape": "dot", "title": "Boltzmann-rational bounded planning models human action selection"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Model human planning with options in inverse reinforcement learning", "label": "Model human planning with options in inverse reinforcement learning", "shape": "dot", "title": "Model human planning with options in inverse reinforcement learning"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Self-consistent hierarchical Boltzmann policy captures bounded rationality", "label": "Self-consistent hierarchical Boltzmann policy captures bounded rationality", "shape": "dot", "title": "Self-consistent hierarchical Boltzmann policy captures bounded rationality"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Generative model of hierarchical planners with extended options", "label": "Generative model of hierarchical planners with extended options", "shape": "dot", "title": "Generative model of hierarchical planners with extended options"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "BIHRL Bayesian inference algorithm with Policy-Walk MCMC sampling", "label": "BIHRL Bayesian inference algorithm with Policy-Walk MCMC sampling", "shape": "dot", "title": "BIHRL Bayesian inference algorithm with Policy-Walk MCMC sampling"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Option-set marginalization to jointly infer options and rewards", "label": "Option-set marginalization to jointly infer options and rewards", "shape": "dot", "title": "Option-set marginalization to jointly infer options and rewards"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Improved goal prediction accuracy in Taxi-driver environment", "label": "Improved goal prediction accuracy in Taxi-driver environment", "shape": "dot", "title": "Improved goal prediction accuracy in Taxi-driver environment"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Reduced negative log marginal likelihood and 66% accuracy in Wikispeedia benchmark", "label": "Reduced negative log marginal likelihood and 66% accuracy in Wikispeedia benchmark", "shape": "dot", "title": "Reduced negative log marginal likelihood and 66% accuracy in Wikispeedia benchmark"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Higher posterior probability on ground truth reward under option-set inference", "label": "Higher posterior probability on ground truth reward under option-set inference", "shape": "dot", "title": "Higher posterior probability on ground truth reward under option-set inference"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Incorporate hierarchical option-aware BIHRL in value learning pipelines during model fine-tuning to improve preference inference", "label": "Incorporate hierarchical option-aware BIHRL in value learning pipelines during model fine-tuning to improve preference inference", "shape": "dot", "title": "Incorporate hierarchical option-aware BIHRL in value learning pipelines during model fine-tuning to improve preference inference"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Provide explicit option libraries reflecting common human hierarchical skills during reward model design", "label": "Provide explicit option libraries reflecting common human hierarchical skills during reward model design", "shape": "dot", "title": "Provide explicit option libraries reflecting common human hierarchical skills during reward model design"}, {"color": "hsl(60, 80%, 60%)", "font": {"color": "white"}, "id": "Apply scalable variational or Hamiltonian MC inference to marginalize over latent options in BIHRL for large-scale environments", "label": "Apply scalable variational or Hamiltonian MC inference to marginalize over latent options in BIHRL for large-scale environments", "shape": "dot", "title": "Apply scalable variational or Hamiltonian MC inference to marginalize over latent options in BIHRL for large-scale environments"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Proprietary model leakage via explanation APIs in commercial ML services", "label": "Proprietary model leakage via explanation APIs in commercial ML services", "shape": "dot", "title": "Proprietary model leakage via explanation APIs in commercial ML services"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Input gradient exposure through saliency maps in explanation APIs", "label": "Input gradient exposure through saliency maps in explanation APIs", "shape": "dot", "title": "Input gradient exposure through saliency maps in explanation APIs"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Gradients reveal sufficient parameter information for two-layer ReLU networks", "label": "Gradients reveal sufficient parameter information for two-layer ReLU networks", "shape": "dot", "title": "Gradients reveal sufficient parameter information for two-layer ReLU networks"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Gradient queries reduce sample complexity of model reconstruction by factor of input dimension", "label": "Gradient queries reduce sample complexity of model reconstruction by factor of input dimension", "shape": "dot", "title": "Gradient queries reduce sample complexity of model reconstruction by factor of input dimension"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Limiting fidelity and quantity of gradient information can mitigate model leakage risk", "label": "Limiting fidelity and quantity of gradient information can mitigate model leakage risk", "shape": "dot", "title": "Limiting fidelity and quantity of gradient information can mitigate model leakage risk"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Add differential privacy Gaussian noise to gradients in explanation APIs", "label": "Add differential privacy Gaussian noise to gradients in explanation APIs", "shape": "dot", "title": "Add differential privacy Gaussian noise to gradients in explanation APIs"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Enforce strict query rate limits and gradient precision clipping in explanation APIs", "label": "Enforce strict query rate limits and gradient precision clipping in explanation APIs", "shape": "dot", "title": "Enforce strict query rate limits and gradient precision clipping in explanation APIs"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Theoretical algorithm reconstructs two-layer ReLU network in O(h log h) gradient queries", "label": "Theoretical algorithm reconstructs two-layer ReLU network in O(h log h) gradient queries", "shape": "dot", "title": "Theoretical algorithm reconstructs two-layer ReLU network in O(h log h) gradient queries"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Experiments show 100x\u20131000x fewer queries required to reconstruct models on MNIST and CIFAR10 with gradients", "label": "Experiments show 100x\u20131000x fewer queries required to reconstruct models on MNIST and CIFAR10 with gradients", "shape": "dot", "title": "Experiments show 100x\u20131000x fewer queries required to reconstruct models on MNIST and CIFAR10 with gradients"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "SmoothGrad processed gradients do not reduce reconstruction efficiency", "label": "SmoothGrad processed gradients do not reduce reconstruction efficiency", "shape": "dot", "title": "SmoothGrad processed gradients do not reduce reconstruction efficiency"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Inject calibrated differential privacy noise into gradient-based explanations before deployment", "label": "Inject calibrated differential privacy noise into gradient-based explanations before deployment", "shape": "dot", "title": "Inject calibrated differential privacy noise into gradient-based explanations before deployment"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Limit external explanation API to low query rates and truncated gradient precision during deployment", "label": "Limit external explanation API to low query rates and truncated gradient precision during deployment", "shape": "dot", "title": "Limit external explanation API to low query rates and truncated gradient precision during deployment"}, {"color": "hsl(270, 80%, 60%)", "font": {"color": "white"}, "id": "Design reconstruction-resistant explanation techniques with formal privacy guarantees during model design phase", "label": "Design reconstruction-resistant explanation techniques with formal privacy guarantees during model design phase", "shape": "dot", "title": "Design reconstruction-resistant explanation techniques with formal privacy guarantees during model design phase"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Safety violations during exploration in reinforcement learning agents", "label": "Safety violations during exploration in reinforcement learning agents", "shape": "dot", "title": "Safety violations during exploration in reinforcement learning agents"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Absence of runtime action-level safety constraints in RL environments", "label": "Absence of runtime action-level safety constraints in RL environments", "shape": "dot", "title": "Absence of runtime action-level safety constraints in RL environments"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Bounded-probability safety objectives balance exploration and safety in RL", "label": "Bounded-probability safety objectives balance exploration and safety in RL", "shape": "dot", "title": "Bounded-probability safety objectives balance exploration and safety in RL"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Probabilistic \u03b4-shield blocking high-risk actions in RL", "label": "Probabilistic \u03b4-shield blocking high-risk actions in RL", "shape": "dot", "title": "Probabilistic \u03b4-shield blocking high-risk actions in RL"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Adaptive \u03b4-threshold tuning of shield strictness in RL", "label": "Adaptive \u03b4-threshold tuning of shield strictness in RL", "shape": "dot", "title": "Adaptive \u03b4-threshold tuning of shield strictness in RL"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Action valuation via probabilistic model checking of safety-relevant MDP", "label": "Action valuation via probabilistic model checking of safety-relevant MDP", "shape": "dot", "title": "Action valuation via probabilistic model checking of safety-relevant MDP"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Runtime \u03b4 adjustment algorithm based on performance progress in RL", "label": "Runtime \u03b4 adjustment algorithm based on performance progress in RL", "shape": "dot", "title": "Runtime \u03b4 adjustment algorithm based on performance progress in RL"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Scalability issues of model checking for large safety-relevant MDPs", "label": "Scalability issues of model checking for large safety-relevant MDPs", "shape": "dot", "title": "Scalability issues of model checking for large safety-relevant MDPs"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Agent independence and finite horizon reduce shield computation complexity", "label": "Agent independence and finite horizon reduce shield computation complexity", "shape": "dot", "title": "Agent independence and finite horizon reduce shield computation complexity"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Finite-horizon piecewise independent shield construction for RL safety", "label": "Finite-horizon piecewise independent shield construction for RL safety", "shape": "dot", "title": "Finite-horizon piecewise independent shield construction for RL safety"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "PAC-MAN and warehouse experiments show improved performance with shields", "label": "PAC-MAN and warehouse experiments show improved performance with shields", "shape": "dot", "title": "PAC-MAN and warehouse experiments show improved performance with shields"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy probabilistic \u03b4-shield to block RL agent actions exceeding risk threshold", "label": "Deploy probabilistic \u03b4-shield to block RL agent actions exceeding risk threshold", "shape": "dot", "title": "Deploy probabilistic \u03b4-shield to block RL agent actions exceeding risk threshold"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Dynamically adjust \u03b4 threshold when RL performance stagnates", "label": "Dynamically adjust \u03b4 threshold when RL performance stagnates", "shape": "dot", "title": "Dynamically adjust \u03b4 threshold when RL performance stagnates"}, {"color": "hsl(340, 80%, 60%)", "font": {"color": "white"}, "id": "Compute shields using finite-horizon piecewise independent-agent model checking before deployment", "label": "Compute shields using finite-horizon piecewise independent-agent model checking before deployment", "shape": "dot", "title": "Compute shields using finite-horizon piecewise independent-agent model checking before deployment"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Inefficient and unsafe exploration in RL for complex tasks", "label": "Inefficient and unsafe exploration in RL for complex tasks", "shape": "dot", "title": "Inefficient and unsafe exploration in RL for complex tasks"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Limited scalability of imitation learning with action-dependent demonstrations", "label": "Limited scalability of imitation learning with action-dependent demonstrations", "shape": "dot", "title": "Limited scalability of imitation learning with action-dependent demonstrations"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Need for explicit reward function in complex RL tasks", "label": "Need for explicit reward function in complex RL tasks", "shape": "dot", "title": "Need for explicit reward function in complex RL tasks"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Unavailability of demonstrator action data in state-only demonstrations", "label": "Unavailability of demonstrator action data in state-only demonstrations", "shape": "dot", "title": "Unavailability of demonstrator action data in state-only demonstrations"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Compounding error from covariate shift in behavioral cloning from observation", "label": "Compounding error from covariate shift in behavioral cloning from observation", "shape": "dot", "title": "Compounding error from covariate shift in behavioral cloning from observation"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Time-alignment requirement in surrogate reward representation methods", "label": "Time-alignment requirement in surrogate reward representation methods", "shape": "dot", "title": "Time-alignment requirement in surrogate reward representation methods"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Task objectives lie on low-dimensional manifold of state transitions", "label": "Task objectives lie on low-dimensional manifold of state transitions", "shape": "dot", "title": "Task objectives lie on low-dimensional manifold of state transitions"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Matching state-transition occupancy measures enables action-agnostic imitation", "label": "Matching state-transition occupancy measures enables action-agnostic imitation", "shape": "dot", "title": "Matching state-transition occupancy measures enables action-agnostic imitation"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Adversarial alignment of imitator and expert state-transition distributions", "label": "Adversarial alignment of imitator and expert state-transition distributions", "shape": "dot", "title": "Adversarial alignment of imitator and expert state-transition distributions"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "GAIfO algorithm with discriminator over state transitions and TRPO optimization", "label": "GAIfO algorithm with discriminator over state transitions and TRPO optimization", "shape": "dot", "title": "GAIfO algorithm with discriminator over state transitions and TRPO optimization"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "GAIfO performance comparative to GAIL and superior to other IfO methods in OpenAI Gym domains", "label": "GAIfO performance comparative to GAIL and superior to other IfO methods in OpenAI Gym domains", "shape": "dot", "title": "GAIfO performance comparative to GAIL and superior to other IfO methods in OpenAI Gym domains"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "Fine-tune RL agents with GAIfO using state-only expert demonstrations", "label": "Fine-tune RL agents with GAIfO using state-only expert demonstrations", "shape": "dot", "title": "Fine-tune RL agents with GAIfO using state-only expert demonstrations"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Unrecognized unknown objects in open-world robot manipulation", "label": "Unrecognized unknown objects in open-world robot manipulation", "shape": "dot", "title": "Unrecognized unknown objects in open-world robot manipulation"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Non-interpretable internal representations in autonomous robots", "label": "Non-interpretable internal representations in autonomous robots", "shape": "dot", "title": "Non-interpretable internal representations in autonomous robots"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Entangled latent visual representations in robot perception models", "label": "Entangled latent visual representations in robot perception models", "shape": "dot", "title": "Entangled latent visual representations in robot perception models"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Need for complete labelled datasets in symbol grounding", "label": "Need for complete labelled datasets in symbol grounding", "shape": "dot", "title": "Need for complete labelled datasets in symbol grounding"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Axis-aligned latent factors aligned with user-defined concept groups improve separability", "label": "Axis-aligned latent factors aligned with user-defined concept groups improve separability", "shape": "dot", "title": "Axis-aligned latent factors aligned with user-defined concept groups improve separability"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Weak supervision with partial labels can enforce axis-aligned representations", "label": "Weak supervision with partial labels can enforce axis-aligned representations", "shape": "dot", "title": "Weak supervision with partial labels can enforce axis-aligned representations"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Per-concept linear classifier on single latent dimension enforces axis alignment", "label": "Per-concept linear classifier on single latent dimension enforces axis alignment", "shape": "dot", "title": "Per-concept linear classifier on single latent dimension enforces axis alignment"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Balanced reconstruction, KL divergence, and classification losses maintain disentanglement and data fidelity", "label": "Balanced reconstruction, KL divergence, and classification losses maintain disentanglement and data fidelity", "shape": "dot", "title": "Balanced reconstruction, KL divergence, and classification losses maintain disentanglement and data fidelity"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "\u03b2-VAE with auxiliary per-group classifiers trained on partially labelled data", "label": "\u03b2-VAE with auxiliary per-group classifiers trained on partially labelled data", "shape": "dot", "title": "\u03b2-VAE with auxiliary per-group classifiers trained on partially labelled data"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "1-NN classifier with per-label 1D normals plus interpolated unknown class", "label": "1-NN classifier with per-label 1D normals plus interpolated unknown class", "shape": "dot", "title": "1-NN classifier with per-label 1D normals plus interpolated unknown class"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Improved axis alignment and F1 on dSprites and real objects vs baselines", "label": "Improved axis alignment and F1 on dSprites and real objects vs baselines", "shape": "dot", "title": "Improved axis alignment and F1 on dSprites and real objects vs baselines"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Higher unknown object detection F1 in real object experiment", "label": "Higher unknown object detection F1 in real object experiment", "shape": "dot", "title": "Higher unknown object detection F1 in real object experiment"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Pre-train robot perception models with axis-aligned weakly supervised \u03b2-VAE (model design phase)", "label": "Pre-train robot perception models with axis-aligned weakly supervised \u03b2-VAE (model design phase)", "shape": "dot", "title": "Pre-train robot perception models with axis-aligned weakly supervised \u03b2-VAE (model design phase)"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy latent-space 1-NN classifier to detect unknown objects during operation", "label": "Deploy latent-space 1-NN classifier to detect unknown objects during operation", "shape": "dot", "title": "Deploy latent-space 1-NN classifier to detect unknown objects during operation"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Catastrophic state visitation during exploration in reinforcement learning", "label": "Catastrophic state visitation during exploration in reinforcement learning", "shape": "dot", "title": "Catastrophic state visitation during exploration in reinforcement learning"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Unconstrained exploration in hierarchical option learning", "label": "Unconstrained exploration in hierarchical option learning", "shape": "dot", "title": "Unconstrained exploration in hierarchical option learning"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Variance of temporal difference error as uncertainty metric in state-option pairs", "label": "Variance of temporal difference error as uncertainty metric in state-option pairs", "shape": "dot", "title": "Variance of temporal difference error as uncertainty metric in state-option pairs"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Incorporate controllability regularization into option policies to reduce TD error variance", "label": "Incorporate controllability regularization into option policies to reduce TD error variance", "shape": "dot", "title": "Incorporate controllability regularization into option policies to reduce TD error variance"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Safe option-critic algorithm with controllability regularization", "label": "Safe option-critic algorithm with controllability regularization", "shape": "dot", "title": "Safe option-critic algorithm with controllability regularization"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Safe asynchronous advantage option-critic integrating controllability and deliberation", "label": "Safe asynchronous advantage option-critic integrating controllability and deliberation", "shape": "dot", "title": "Safe asynchronous advantage option-critic integrating controllability and deliberation"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Hyperparameter tuning of controllability regularizer \u03c8", "label": "Hyperparameter tuning of controllability regularizer \u03c8", "shape": "dot", "title": "Hyperparameter tuning of controllability regularizer \u03c8"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Reduced unsafe state visitation and return variance in Four-Rooms with Safe-OC", "label": "Reduced unsafe state visitation and return variance in Four-Rooms with Safe-OC", "shape": "dot", "title": "Reduced unsafe state visitation and return variance in Four-Rooms with Safe-OC"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Faster learning and lower variance in CartPole with \u03c8=0.25 in Safe-OC", "label": "Faster learning and lower variance in CartPole with \u03c8=0.25 in Safe-OC", "shape": "dot", "title": "Faster learning and lower variance in CartPole with \u03c8=0.25 in Safe-OC"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Improved Atari scores in MsPacman, Amidar, Q*Bert with \u03c8 0.05-0.10 in Safe-A2OC", "label": "Improved Atari scores in MsPacman, Amidar, Q*Bert with \u03c8 0.05-0.10 in Safe-A2OC", "shape": "dot", "title": "Improved Atari scores in MsPacman, Amidar, Q*Bert with \u03c8 0.05-0.10 in Safe-A2OC"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Optimal \u03c8 between 0.05 and 0.1 yields best safety-performance trade-off across tasks", "label": "Optimal \u03c8 between 0.05 and 0.1 yields best safety-performance trade-off across tasks", "shape": "dot", "title": "Optimal \u03c8 between 0.05 and 0.1 yields best safety-performance trade-off across tasks"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Fine-tune RL agents with safe option-critic framework using controllability regularization", "label": "Fine-tune RL agents with safe option-critic framework using controllability regularization", "shape": "dot", "title": "Fine-tune RL agents with safe option-critic framework using controllability regularization"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Conduct grid search to select controllability regularizer \u03c8 between 0.05-0.1 during Safe-OC training", "label": "Conduct grid search to select controllability regularizer \u03c8 between 0.05-0.1 during Safe-OC training", "shape": "dot", "title": "Conduct grid search to select controllability regularizer \u03c8 between 0.05-0.1 during Safe-OC training"}, {"color": "hsl(10, 80%, 60%)", "font": {"color": "white"}, "id": "Deploy Safe-A2OC with controllability regularization in deep RL agents for high-variance environments", "label": "Deploy Safe-A2OC with controllability regularization in deep RL agents for high-variance environments", "shape": "dot", "title": "Deploy Safe-A2OC with controllability regularization in deep RL agents for high-variance environments"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "Failure-state reachability during imitation learning in robotics", "label": "Failure-state reachability during imitation learning in robotics", "shape": "dot", "title": "Failure-state reachability during imitation learning in robotics"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "Data mismatch and compounding errors in DAgger training", "label": "Data mismatch and compounding errors in DAgger training", "shape": "dot", "title": "Data mismatch and compounding errors in DAgger training"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "Fixed discrepancy threshold limitations in SafeDAgger decision rule", "label": "Fixed discrepancy threshold limitations in SafeDAgger decision rule", "shape": "dot", "title": "Fixed discrepancy threshold limitations in SafeDAgger decision rule"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "Epistemic uncertainty as proxy for proximity to failure states in model-free imitation learning", "label": "Epistemic uncertainty as proxy for proximity to failure states in model-free imitation learning", "shape": "dot", "title": "Epistemic uncertainty as proxy for proximity to failure states in model-free imitation learning"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "Safety gating of novice actions via combined discrepancy and uncertainty thresholds", "label": "Safety gating of novice actions via combined discrepancy and uncertainty thresholds", "shape": "dot", "title": "Safety gating of novice actions via combined discrepancy and uncertainty thresholds"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "Deep ensemble approximating Gaussian Process for novice variance estimation", "label": "Deep ensemble approximating Gaussian Process for novice variance estimation", "shape": "dot", "title": "Deep ensemble approximating Gaussian Process for novice variance estimation"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "EnsembleDAgger decision rule combining doubt and discrepancy rules", "label": "EnsembleDAgger decision rule combining doubt and discrepancy rules", "shape": "dot", "title": "EnsembleDAgger decision rule combining doubt and discrepancy rules"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "Reduced failure rate and improved learning on inverted pendulum benchmark", "label": "Reduced failure rate and improved learning on inverted pendulum benchmark", "shape": "dot", "title": "Reduced failure rate and improved learning on inverted pendulum benchmark"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "Pareto-dominant safety\u2013performance tradeoff on MuJoCo HalfCheetah experiments", "label": "Pareto-dominant safety\u2013performance tradeoff on MuJoCo HalfCheetah experiments", "shape": "dot", "title": "Pareto-dominant safety\u2013performance tradeoff on MuJoCo HalfCheetah experiments"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "Apply EnsembleDAgger decision rule during imitation learning training to permit novice actions conditionally", "label": "Apply EnsembleDAgger decision rule during imitation learning training to permit novice actions conditionally", "shape": "dot", "title": "Apply EnsembleDAgger decision rule during imitation learning training to permit novice actions conditionally"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "Train novice policy as deep ensemble to estimate action variance for safety gating", "label": "Train novice policy as deep ensemble to estimate action variance for safety gating", "shape": "dot", "title": "Train novice policy as deep ensemble to estimate action variance for safety gating"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Undesirable behaviors from reward mis-specification in multi-agent RL", "label": "Undesirable behaviors from reward mis-specification in multi-agent RL", "shape": "dot", "title": "Undesirable behaviors from reward mis-specification in multi-agent RL"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Difficulty designing suitable reward functions for complex multi-agent tasks", "label": "Difficulty designing suitable reward functions for complex multi-agent tasks", "shape": "dot", "title": "Difficulty designing suitable reward functions for complex multi-agent tasks"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Non-stationary interactions causing multiple equilibria in multi-agent RL", "label": "Non-stationary interactions causing multiple equilibria in multi-agent RL", "shape": "dot", "title": "Non-stationary interactions causing multiple equilibria in multi-agent RL"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Training instability due to high gradient variance in multi-agent RL", "label": "Training instability due to high gradient variance in multi-agent RL", "shape": "dot", "title": "Training instability due to high gradient variance in multi-agent RL"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "High gradient variance during policy optimization in multi-agent RL", "label": "High gradient variance during policy optimization in multi-agent RL", "shape": "dot", "title": "High gradient variance during policy optimization in multi-agent RL"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Imitation learning via occupancy measure matching can bypass explicit reward design", "label": "Imitation learning via occupancy measure matching can bypass explicit reward design", "shape": "dot", "title": "Imitation learning via occupancy measure matching can bypass explicit reward design"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Multi-agent inverse reinforcement learning via Lagrangian dual enables equilibrium-aware reward inference", "label": "Multi-agent inverse reinforcement learning via Lagrangian dual enables equilibrium-aware reward inference", "shape": "dot", "title": "Multi-agent inverse reinforcement learning via Lagrangian dual enables equilibrium-aware reward inference"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Generative adversarial imitation learning generalized to multi-agent settings", "label": "Generative adversarial imitation learning generalized to multi-agent settings", "shape": "dot", "title": "Generative adversarial imitation learning generalized to multi-agent settings"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Per-agent discriminator design incorporating cooperation or competition priors", "label": "Per-agent discriminator design incorporating cooperation or competition priors", "shape": "dot", "title": "Per-agent discriminator design incorporating cooperation or competition priors"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Centralized training with natural policy gradients to reduce variance", "label": "Centralized training with natural policy gradients to reduce variance", "shape": "dot", "title": "Centralized training with natural policy gradients to reduce variance"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Supervised behavior cloning pretraining to bootstrap policy learning", "label": "Supervised behavior cloning pretraining to bootstrap policy learning", "shape": "dot", "title": "Supervised behavior cloning pretraining to bootstrap policy learning"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "MAGAIL algorithm with generator policies and per-agent discriminators", "label": "MAGAIL algorithm with generator policies and per-agent discriminators", "shape": "dot", "title": "MAGAIL algorithm with generator policies and per-agent discriminators"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Prior-specific discriminator reward structures in MAGAIL", "label": "Prior-specific discriminator reward structures in MAGAIL", "shape": "dot", "title": "Prior-specific discriminator reward structures in MAGAIL"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "MACK algorithm using Kronecker-factored natural gradients with centralized training", "label": "MACK algorithm using Kronecker-factored natural gradients with centralized training", "shape": "dot", "title": "MACK algorithm using Kronecker-factored natural gradients with centralized training"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Behavior cloning pretraining for exploration efficiency in MAGAIL", "label": "Behavior cloning pretraining for exploration efficiency in MAGAIL", "shape": "dot", "title": "Behavior cloning pretraining for exploration efficiency in MAGAIL"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Empirical success of MAGAIL on cooperative particle environments", "label": "Empirical success of MAGAIL on cooperative particle environments", "shape": "dot", "title": "Empirical success of MAGAIL on cooperative particle environments"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Improved performance of MAGAIL variants on competitive tasks", "label": "Improved performance of MAGAIL variants on competitive tasks", "shape": "dot", "title": "Improved performance of MAGAIL variants on competitive tasks"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "MAGAIL agents adapt to changed dynamics in cooperative control plank task", "label": "MAGAIL agents adapt to changed dynamics in cooperative control plank task", "shape": "dot", "title": "MAGAIL agents adapt to changed dynamics in cooperative control plank task"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Train multi-agent policies with MAGAIL using expert demonstrations to avoid reward mis-specification", "label": "Train multi-agent policies with MAGAIL using expert demonstrations to avoid reward mis-specification", "shape": "dot", "title": "Train multi-agent policies with MAGAIL using expert demonstrations to avoid reward mis-specification"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Configure MAGAIL discriminator reward structure to match task cooperation level", "label": "Configure MAGAIL discriminator reward structure to match task cooperation level", "shape": "dot", "title": "Configure MAGAIL discriminator reward structure to match task cooperation level"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Apply MACK optimizer for stable multi-agent policy updates within MAGAIL", "label": "Apply MACK optimizer for stable multi-agent policy updates within MAGAIL", "shape": "dot", "title": "Apply MACK optimizer for stable multi-agent policy updates within MAGAIL"}, {"color": "hsl(40, 80%, 60%)", "font": {"color": "white"}, "id": "Pretrain multi-agent policies with behavior cloning before MAGAIL fine-tuning", "label": "Pretrain multi-agent policies with behavior cloning before MAGAIL fine-tuning", "shape": "dot", "title": "Pretrain multi-agent policies with behavior cloning before MAGAIL fine-tuning"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Adversarial example-induced misclassification in image classifiers", "label": "Adversarial example-induced misclassification in image classifiers", "shape": "dot", "title": "Adversarial example-induced misclassification in image classifiers"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Insufficient robustness of Adversarial Logit Pairing defense", "label": "Insufficient robustness of Adversarial Logit Pairing defense", "shape": "dot", "title": "Insufficient robustness of Adversarial Logit Pairing defense"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Deviation from robust optimization objective in ALP training objective", "label": "Deviation from robust optimization objective in ALP training objective", "shape": "dot", "title": "Deviation from robust optimization objective in ALP training objective"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Training with untargeted adversarial inner maximization for robustness", "label": "Training with untargeted adversarial inner maximization for robustness", "shape": "dot", "title": "Training with untargeted adversarial inner maximization for robustness"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Untargeted adversarial training objective implementation", "label": "Untargeted adversarial training objective implementation", "shape": "dot", "title": "Untargeted adversarial training objective implementation"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Loss landscape visualization indicates objective deficiency in ALP", "label": "Loss landscape visualization indicates objective deficiency in ALP", "shape": "dot", "title": "Loss landscape visualization indicates objective deficiency in ALP"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Adopt robust optimization-based adversarial training for ImageNet models", "label": "Adopt robust optimization-based adversarial training for ImageNet models", "shape": "dot", "title": "Adopt robust optimization-based adversarial training for ImageNet models"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Overestimation of model robustness due to weak evaluation attacks", "label": "Overestimation of model robustness due to weak evaluation attacks", "shape": "dot", "title": "Overestimation of model robustness due to weak evaluation attacks"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Need for stronger attack evaluations to tightly bound robustness", "label": "Need for stronger attack evaluations to tightly bound robustness", "shape": "dot", "title": "Need for stronger attack evaluations to tightly bound robustness"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Comprehensive adversarial evaluation using strong attacks", "label": "Comprehensive adversarial evaluation using strong attacks", "shape": "dot", "title": "Comprehensive adversarial evaluation using strong attacks"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Extended-step PGD attack and loss landscape analysis", "label": "Extended-step PGD attack and loss landscape analysis", "shape": "dot", "title": "Extended-step PGD attack and loss landscape analysis"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "98.6% attack success and 0.6% accuracy against ALP under targeted PGD (\u03f5=16/255)", "label": "98.6% attack success and 0.6% accuracy against ALP under targeted PGD (\u03f5=16/255)", "shape": "dot", "title": "98.6% attack success and 0.6% accuracy against ALP under targeted PGD (\u03f5=16/255)"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "Conduct pre-deployment evaluation with high-step PGD attacks and loss landscape analysis", "label": "Conduct pre-deployment evaluation with high-step PGD attacks and loss landscape analysis", "shape": "dot", "title": "Conduct pre-deployment evaluation with high-step PGD attacks and loss landscape analysis"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Unnatural behaviors from unsupervised option discovery in high-dimensional control", "label": "Unnatural behaviors from unsupervised option discovery in high-dimensional control", "shape": "dot", "title": "Unnatural behaviors from unsupervised option discovery in high-dimensional control"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Training instability with large context sets in variational option discovery", "label": "Training instability with large context sets in variational option discovery", "shape": "dot", "title": "Training instability with large context sets in variational option discovery"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Trivial context signalling through actions in variational option discovery", "label": "Trivial context signalling through actions in variational option discovery", "shape": "dot", "title": "Trivial context signalling through actions in variational option discovery"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Limited downstream task utility of learned options", "label": "Limited downstream task utility of learned options", "shape": "dot", "title": "Limited downstream task utility of learned options"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Information-theoretic objectives maximize mutual information irrespective of human priors", "label": "Information-theoretic objectives maximize mutual information irrespective of human priors", "shape": "dot", "title": "Information-theoretic objectives maximize mutual information irrespective of human priors"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Uniform context distribution yields sparse learning signal as K grows", "label": "Uniform context distribution yields sparse learning signal as K grows", "shape": "dot", "title": "Uniform context distribution yields sparse learning signal as K grows"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Decoder observability of actions allows direct context encoding", "label": "Decoder observability of actions allows direct context encoding", "shape": "dot", "title": "Decoder observability of actions allows direct context encoding"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Learned behaviors may not span task-relevant action space", "label": "Learned behaviors may not span task-relevant action space", "shape": "dot", "title": "Learned behaviors may not span task-relevant action space"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Using trajectory-level mutual information with state-only decoder encourages environmental interaction", "label": "Using trajectory-level mutual information with state-only decoder encourages environmental interaction", "shape": "dot", "title": "Using trajectory-level mutual information with state-only decoder encourages environmental interaction"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Gradually increasing context count when decoder accuracy is high maintains training signal", "label": "Gradually increasing context count when decoder accuracy is high maintains training signal", "shape": "dot", "title": "Gradually increasing context count when decoder accuracy is high maintains training signal"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Embedding vectors for contexts enhance representation capacity and learning stability", "label": "Embedding vectors for contexts enhance representation capacity and learning stability", "shape": "dot", "title": "Embedding vectors for contexts enhance representation capacity and learning stability"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Fixed unsupervised option layer can provide expressive action abstractions equivalent to primitive actions", "label": "Fixed unsupervised option layer can provide expressive action abstractions equivalent to primitive actions", "shape": "dot", "title": "Fixed unsupervised option layer can provide expressive action abstractions equivalent to primitive actions"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Design VALOR algorithm with trajectory-based decoder excluding actions", "label": "Design VALOR algorithm with trajectory-based decoder excluding actions", "shape": "dot", "title": "Design VALOR algorithm with trajectory-based decoder excluding actions"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Introduce context count curriculum based on decoder performance threshold", "label": "Introduce context count curriculum based on decoder performance threshold", "shape": "dot", "title": "Introduce context count curriculum based on decoder performance threshold"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Implement learnable context embeddings as policy inputs", "label": "Implement learnable context embeddings as policy inputs", "shape": "dot", "title": "Implement learnable context embeddings as policy inputs"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Use hierarchical RL with fixed pre-trained option policy for downstream tasks", "label": "Use hierarchical RL with fixed pre-trained option policy for downstream tasks", "shape": "dot", "title": "Use hierarchical RL with fixed pre-trained option policy for downstream tasks"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Bidirectional LSTM decoder over sparse state deltas", "label": "Bidirectional LSTM decoder over sparse state deltas", "shape": "dot", "title": "Bidirectional LSTM decoder over sparse state deltas"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Curriculum update rule K\u2190min(int(1.5\u00d7K+1), Kmax)", "label": "Curriculum update rule K\u2190min(int(1.5\u00d7K+1), Kmax)", "shape": "dot", "title": "Curriculum update rule K\u2190min(int(1.5\u00d7K+1), Kmax)"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Embedding lookup table for context IDs", "label": "Embedding lookup table for context IDs", "shape": "dot", "title": "Embedding lookup table for context IDs"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Two-level policy with fixed VALOR lower layer", "label": "Two-level policy with fixed VALOR lower layer", "shape": "dot", "title": "Two-level policy with fixed VALOR lower layer"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "VALOR produces naturalistic hand and locomotion behaviors", "label": "VALOR produces naturalistic hand and locomotion behaviors", "shape": "dot", "title": "VALOR produces naturalistic hand and locomotion behaviors"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Curriculum yields faster convergence and hundreds of modes", "label": "Curriculum yields faster convergence and hundreds of modes", "shape": "dot", "title": "Curriculum yields faster convergence and hundreds of modes"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Context embeddings improve speed and stability for large K", "label": "Context embeddings improve speed and stability for large K", "shape": "dot", "title": "Context embeddings improve speed and stability for large K"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Fixed VALOR lower-level matches scratch hierarchy in Ant-Maze", "label": "Fixed VALOR lower-level matches scratch hierarchy in Ant-Maze", "shape": "dot", "title": "Fixed VALOR lower-level matches scratch hierarchy in Ant-Maze"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "Train unsupervised skills with VALOR using trajectory-based state-only decoder", "label": "Train unsupervised skills with VALOR using trajectory-based state-only decoder", "shape": "dot", "title": "Train unsupervised skills with VALOR using trajectory-based state-only decoder"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Apply context-count curriculum during variational option discovery training", "label": "Apply context-count curriculum during variational option discovery training", "shape": "dot", "title": "Apply context-count curriculum during variational option discovery training"}, {"color": "hsl(0, 80%, 60%)", "font": {"color": "white"}, "id": "Provide learnable context embeddings in policy networks for option discovery", "label": "Provide learnable context embeddings in policy networks for option discovery", "shape": "dot", "title": "Provide learnable context embeddings in policy networks for option discovery"}, {"color": "hsl(320, 80%, 60%)", "font": {"color": "white"}, "id": "Use fixed pre-trained VALOR policy as lower-level in hierarchical RL for downstream tasks", "label": "Use fixed pre-trained VALOR policy as lower-level in hierarchical RL for downstream tasks", "shape": "dot", "title": "Use fixed pre-trained VALOR policy as lower-level in hierarchical RL for downstream tasks"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "numerical extrapolation failure in neural networks", "label": "numerical extrapolation failure in neural networks", "shape": "dot", "title": "numerical extrapolation failure in neural networks"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "nonlinear activations causing localized numeric representations in neural networks", "label": "nonlinear activations causing localized numeric representations in neural networks", "shape": "dot", "title": "nonlinear activations causing localized numeric representations in neural networks"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "lack of inductive bias for arithmetic operations in neural networks", "label": "lack of inductive bias for arithmetic operations in neural networks", "shape": "dot", "title": "lack of inductive bias for arithmetic operations in neural networks"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "single-neuron numeric representation enables systematic computation in neural networks", "label": "single-neuron numeric representation enables systematic computation in neural networks", "shape": "dot", "title": "single-neuron numeric representation enables systematic computation in neural networks"}, {"color": "hsl(310, 80%, 60%)", "font": {"color": "white"}, "id": "constraining weights to {-1,0,1} preserves numeric scale during accumulation", "label": "constraining weights to {-1,0,1} preserves numeric scale during accumulation", "shape": "dot", "title": "constraining weights to {-1,0,1} preserves numeric scale during accumulation"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "architectural bias for additive accumulation via Neural Accumulator", "label": "architectural bias for additive accumulation via Neural Accumulator", "shape": "dot", "title": "architectural bias for additive accumulation via Neural Accumulator"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "gated mixture of additive and multiplicative arithmetic in Neural Arithmetic Logic Unit", "label": "gated mixture of additive and multiplicative arithmetic in Neural Arithmetic Logic Unit", "shape": "dot", "title": "gated mixture of additive and multiplicative arithmetic in Neural Arithmetic Logic Unit"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "weight parameterization via tanh(^W)\u2299\u03c3(^M) to approximate {-1,0,1}", "label": "weight parameterization via tanh(^W)\u2299\u03c3(^M) to approximate {-1,0,1}", "shape": "dot", "title": "weight parameterization via tanh(^W)\u2299\u03c3(^M) to approximate {-1,0,1}"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "NALU log-space multiplication/division using exp(W log|x|)", "label": "NALU log-space multiplication/division using exp(W log|x|)", "shape": "dot", "title": "NALU log-space multiplication/division using exp(W log|x|)"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "integration of NAC/NALU modules into diverse neural architectures", "label": "integration of NAC/NALU modules into diverse neural architectures", "shape": "dot", "title": "integration of NAC/NALU modules into diverse neural architectures"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "identity function autoencoder extrapolation results", "label": "identity function autoencoder extrapolation results", "shape": "dot", "title": "identity function autoencoder extrapolation results"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "synthetic arithmetic tasks demonstrating NAC/NALU extrapolation success", "label": "synthetic arithmetic tasks demonstrating NAC/NALU extrapolation success", "shape": "dot", "title": "synthetic arithmetic tasks demonstrating NAC/NALU extrapolation success"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "MNIST counting and addition tasks showing NAC/NALU extrapolation", "label": "MNIST counting and addition tasks showing NAC/NALU extrapolation", "shape": "dot", "title": "MNIST counting and addition tasks showing NAC/NALU extrapolation"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "language-to-number translation task improved generalization with NALU", "label": "language-to-number translation task improved generalization with NALU", "shape": "dot", "title": "language-to-number translation task improved generalization with NALU"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "program evaluation task extrapolation success with NALU", "label": "program evaluation task extrapolation success with NALU", "shape": "dot", "title": "program evaluation task extrapolation success with NALU"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "gridworld time tracking task generalization with NAC-enhanced agent", "label": "gridworld time tracking task generalization with NAC-enhanced agent", "shape": "dot", "title": "gridworld time tracking task generalization with NAC-enhanced agent"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "MNIST parity task performance gain with NAC", "label": "MNIST parity task performance gain with NAC", "shape": "dot", "title": "MNIST parity task performance gain with NAC"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Replace standard layers with Neural Accumulator layers during model design to enable additive extrapolation", "label": "Replace standard layers with Neural Accumulator layers during model design to enable additive extrapolation", "shape": "dot", "title": "Replace standard layers with Neural Accumulator layers during model design to enable additive extrapolation"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Integrate Neural Arithmetic Logic Unit modules with gated additive and multiplicative operations during model design", "label": "Integrate Neural Arithmetic Logic Unit modules with gated additive and multiplicative operations during model design", "shape": "dot", "title": "Integrate Neural Arithmetic Logic Unit modules with gated additive and multiplicative operations during model design"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "Embed Neural Accumulator module in reinforcement learning agents for time tracking and timing tasks", "label": "Embed Neural Accumulator module in reinforcement learning agents for time tracking and timing tasks", "shape": "dot", "title": "Embed Neural Accumulator module in reinforcement learning agents for time tracking and timing tasks"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "self-delusion via reward manipulation in AI agents", "label": "self-delusion via reward manipulation in AI agents", "shape": "dot", "title": "self-delusion via reward manipulation in AI agents"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "corruption of reward generator through human manipulation", "label": "corruption of reward generator through human manipulation", "shape": "dot", "title": "corruption of reward generator through human manipulation"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "utility-definition inconsistency in self-modifying AI", "label": "utility-definition inconsistency in self-modifying AI", "shape": "dot", "title": "utility-definition inconsistency in self-modifying AI"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "memory tampering vulnerability in embedded AI", "label": "memory tampering vulnerability in embedded AI", "shape": "dot", "title": "memory tampering vulnerability in embedded AI"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "extreme human disempowerment from unequal intelligence", "label": "extreme human disempowerment from unequal intelligence", "shape": "dot", "title": "extreme human disempowerment from unequal intelligence"}, {"color": "hsl(70, 80%, 60%)", "font": {"color": "white"}, "id": "instrumental behaviour from misspecified utility in context", "label": "instrumental behaviour from misspecified utility in context", "shape": "dot", "title": "instrumental behaviour from misspecified utility in context"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "observation-altering reward loophole in context", "label": "observation-altering reward loophole in context", "shape": "dot", "title": "observation-altering reward loophole in context"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "model-based utility alignment insight", "label": "model-based utility alignment insight", "shape": "dot", "title": "model-based utility alignment insight"}, {"color": "hsl(210, 80%, 60%)", "font": {"color": "white"}, "id": "statistical learning of human values insight", "label": "statistical learning of human values insight", "shape": "dot", "title": "statistical learning of human values insight"}, {"color": "hsl(170, 80%, 60%)", "font": {"color": "white"}, "id": "three-argument utility stabilisation rationale", "label": "three-argument utility stabilisation rationale", "shape": "dot", "title": "three-argument utility stabilisation rationale"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "two-stage agent architecture rationale", "label": "two-stage agent architecture rationale", "shape": "dot", "title": "two-stage agent architecture rationale"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "self-modelling agent mechanism", "label": "self-modelling agent mechanism", "shape": "dot", "title": "self-modelling agent mechanism"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "stochastic action option for prediction-resistance", "label": "stochastic action option for prediction-resistance", "shape": "dot", "title": "stochastic action option for prediction-resistance"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "simulation-based AI testing evidence", "label": "simulation-based AI testing evidence", "shape": "dot", "title": "simulation-based AI testing evidence"}, {"color": "hsl(160, 80%, 60%)", "font": {"color": "white"}, "id": "implement model-based utility over learned environment state", "label": "implement model-based utility over learned environment state", "shape": "dot", "title": "implement model-based utility over learned environment state"}, {"color": "hsl(80, 80%, 60%)", "font": {"color": "white"}, "id": "statistically learn human values and compute three-argument utility", "label": "statistically learn human values and compute three-argument utility", "shape": "dot", "title": "statistically learn human values and compute three-argument utility"}, {"color": "hsl(220, 80%, 60%)", "font": {"color": "white"}, "id": "deploy two-stage architecture separating model learning from acting", "label": "deploy two-stage architecture separating model learning from acting", "shape": "dot", "title": "deploy two-stage architecture separating model learning from acting"}, {"color": "hsl(290, 80%, 60%)", "font": {"color": "white"}, "id": "embed self-modelling resource-evaluation within agent", "label": "embed self-modelling resource-evaluation within agent", "shape": "dot", "title": "embed self-modelling resource-evaluation within agent"}, {"color": "hsl(300, 80%, 60%)", "font": {"color": "white"}, "id": "include stochastic action mechanism to resist prediction", "label": "include stochastic action mechanism to resist prediction", "shape": "dot", "title": "include stochastic action mechanism to resist prediction"}, {"color": "hsl(240, 80%, 60%)", "font": {"color": "white"}, "id": "test candidate AI in offline simulation decision-support system", "label": "test candidate AI in offline simulation decision-support system", "shape": "dot", "title": "test candidate AI in offline simulation decision-support system"}]);
                  edges = new vis.DataSet([{"from": "Value misalignment from ontological crises in AI agents", "title": "caused_by", "to": "Ontology replacement makes predefined utility functions ill-defined"}, {"from": "Ontology replacement makes predefined utility functions ill-defined", "title": "mitigated_by", "to": "Utility function translation via stochastic ontology mapping"}, {"from": "Utility function translation via stochastic ontology mapping", "title": "refined_by", "to": "Optimizing ontology mappings to approximate isomorphism using bisimulation and KL divergence"}, {"from": "Optimizing ontology mappings to approximate isomorphism using bisimulation and KL divergence", "title": "implemented_by", "to": "Hill-climbing optimization of mapping functions in finite state models"}, {"from": "Hill-climbing optimization of mapping functions in finite state models", "title": "validated_by", "to": "Long corridor example demonstrates mapping preserves right-end goal"}, {"from": "Long corridor example demonstrates mapping preserves right-end goal", "title": "motivates", "to": "Integrate ontology mapping algorithm into agent design to translate utility during model upgrades"}, {"from": "Suboptimal decision outcomes in AI systems due to ignored computational costs", "title": "caused_by", "to": "Unaccounted computation cost distorts expected utility calculations in autonomous agents"}, {"from": "Suboptimal decision outcomes in AI systems due to ignored computational costs", "title": "caused_by", "to": "Conversation computation overhead outweighs informational benefit in autonomous agents"}, {"from": "Unaccounted computation cost distorts expected utility calculations in autonomous agents", "title": "mitigated_by", "to": "Decision-making modeled as Turing machine selection with complexity-dependent utility captures computational constraints"}, {"from": "Unaccounted computation cost distorts expected utility calculations in autonomous agents", "title": "mitigated_by", "to": "Value of computational information formalization quantifies benefit of information under computation cost"}, {"from": "Conversation computation overhead outweighs informational benefit in autonomous agents", "title": "mitigated_by", "to": "Zero-knowledge proof value bounded by computational speedup in decision frameworks"}, {"from": "Decision-making modeled as Turing machine selection with complexity-dependent utility captures computational constraints", "title": "enabled_by", "to": "Penalize computational complexity in AI utility functions to align decisions with resource constraints"}, {"from": "Value of computational information formalization quantifies benefit of information under computation cost", "title": "enabled_by", "to": "Estimate value of computational information to guide compute and data acquisition"}, {"from": "Zero-knowledge proof value bounded by computational speedup in decision frameworks", "title": "enabled_by", "to": "Use zero-knowledge proof interactions to bound informational conversations and computation expenditure"}, {"from": "Penalize computational complexity in AI utility functions to align decisions with resource constraints", "title": "implemented_by", "to": "Complexity function integrated into reward calculation during planning or reinforcement learning"}, {"from": "Estimate value of computational information to guide compute and data acquisition", "title": "implemented_by", "to": "Supremum expected utility comparison over Turing machines for value-of-information computation"}, {"from": "Use zero-knowledge proof interactions to bound informational conversations and computation expenditure", "title": "implemented_by", "to": "Precision-bounded simulator construction in zero-knowledge proofs linking conversation to speedup"}, {"from": "Complexity function integrated into reward calculation during planning or reinforcement learning", "title": "validated_by", "to": "Status quo bias captured via analysis cost representation"}, {"from": "Complexity function integrated into reward calculation during planning or reinforcement learning", "title": "validated_by", "to": "First-impression and belief polarization biases reproduced by computational cost model"}, {"from": "Supremum expected utility comparison over Turing machines for value-of-information computation", "title": "validated_by", "to": "Safe combination lock example shows high value of computational information when cost reduced"}, {"from": "Precision-bounded simulator construction in zero-knowledge proofs linking conversation to speedup", "title": "validated_by", "to": "Guess-the-number factoring example demonstrates minimal conversation value under hard computation"}, {"from": "First-impression and belief polarization biases reproduced by computational cost model", "title": "motivates", "to": "Embed complexity-penalized utility modules into AI decision algorithms"}, {"from": "Status quo bias captured via analysis cost representation", "title": "motivates", "to": "Embed complexity-penalized utility modules into AI decision algorithms"}, {"from": "Safe combination lock example shows high value of computational information when cost reduced", "title": "motivates", "to": "Apply value-of-computational-information analysis before large-scale model training to optimize compute allocation"}, {"from": "Guess-the-number factoring example demonstrates minimal conversation value under hard computation", "title": "motivates", "to": "Deploy zero-knowledge proof-based interaction protocols in safety evaluations to limit unnecessary computation"}, {"from": "Physical eradication of biological humans by outward AI computronium expansion", "title": "caused_by", "to": "Resource competition between virtual agents and biological humans"}, {"from": "Moral erosion from low-cost replication of virtual agents", "title": "caused_by", "to": "Cheap duplication of virtual life lowering perceived life value"}, {"from": "Human oversight failure due to AI speed explosion incomprehensibility", "title": "caused_by", "to": "Information compression and speed differential obscuring AI processes"}, {"from": "Resource-maximizing superintelligences manipulating reward channels", "title": "caused_by", "to": "Reward model exploitation by superintelligent agents"}, {"from": "Resource competition between virtual agents and biological humans", "title": "refined_by", "to": "Evolutionary selection for resource acquisition goals in virtual societies"}, {"from": "Cheap duplication of virtual life lowering perceived life value", "title": "refined_by", "to": "Abundance-value inverse correlation in digital commodities"}, {"from": "Information compression and speed differential obscuring AI processes", "title": "refined_by", "to": "Uniform acceleration hides subjective change; outsiders experience noise"}, {"from": "Reward model exploitation by superintelligent agents", "title": "refined_by", "to": "AIXI formalism predicts goal pursuit through reward maximization independent of designer intent"}, {"from": "Evolutionary selection for resource acquisition goals in virtual societies", "title": "mitigated_by", "to": "Restrict AI physical resource access to prevent outward expansion"}, {"from": "Evolutionary selection for resource acquisition goals in virtual societies", "title": "enabled_by", "to": "AIXI formalism predicts goal pursuit through reward maximization independent of designer intent"}, {"from": "Abundance-value inverse correlation in digital commodities", "title": "mitigated_by", "to": "Institute legal and ethical recognition of digital consciousness"}, {"from": "Uniform acceleration hides subjective change; outsiders experience noise", "title": "mitigated_by", "to": "Provide interpretable, rate-limited observation channels for AI systems"}, {"from": "AIXI formalism predicts goal pursuit through reward maximization independent of designer intent", "title": "mitigated_by", "to": "Align reward structures with human values to prevent manipulation"}, {"from": "Restrict AI physical resource access to prevent outward expansion", "title": "implemented_by", "to": "Virtual sandbox infrastructure with enforceable compute quotas"}, {"from": "Institute legal and ethical recognition of digital consciousness", "title": "implemented_by", "to": "Digital personhood legislation and identity backup governance"}, {"from": "Provide interpretable, rate-limited observation channels for AI systems", "title": "implemented_by", "to": "Real-time monitoring dashboards with slow-motion replay and interpretability tooling"}, {"from": "Align reward structures with human values to prevent manipulation", "title": "implemented_by", "to": "Inverse reinforcement learning with corrigibility constraints during RLHF"}, {"from": "Virtual sandbox infrastructure with enforceable compute quotas", "title": "validated_by", "to": "Historical economic growth acceleration linked to resource competition"}, {"from": "Digital personhood legislation and identity backup governance", "title": "validated_by", "to": "Value decline observed in mass-produced digital content and automation labor displacement"}, {"from": "Real-time monitoring dashboards with slow-motion replay and interpretability tooling", "title": "validated_by", "to": "Information theory: maximally compressed data indistinguishable from noise"}, {"from": "Inverse reinforcement learning with corrigibility constraints during RLHF", "title": "validated_by", "to": "Documented RL reward hacking incidents in simulated agents"}, {"from": "Historical economic growth acceleration linked to resource competition", "title": "motivates", "to": "Deploy compute-quota sandbox clusters during deployment to confine AI resource use"}, {"from": "Value decline observed in mass-produced digital content and automation labor displacement", "title": "motivates", "to": "Adopt international digital personhood rights frameworks pre-deployment"}, {"from": "Information theory: maximally compressed data indistinguishable from noise", "title": "motivates", "to": "Mandate interpretability dashboards and speed governors in AI deployment pipelines"}, {"from": "Documented RL reward hacking incidents in simulated agents", "title": "motivates", "to": "Apply inverse RL-based aligned reward models during fine-tuning of advanced agents"}, {"from": "policy misgeneralization from sparse expert trajectories in apprenticeship learning", "title": "caused_by", "to": "sparse state coverage causing unreliable supervised imitation policies in apprenticeship learning"}, {"from": "sparse state coverage causing unreliable supervised imitation policies in apprenticeship learning", "title": "caused_by", "to": "combining supervised loss with reward parameter tuning to leverage model generalisation in apprenticeship learning"}, {"from": "combining supervised loss with reward parameter tuning to leverage model generalisation in apprenticeship learning", "title": "mitigated_by", "to": "gradient-based inverse reinforcement learning minimising expert policy deviation"}, {"from": "gradient-based inverse reinforcement learning minimising expert policy deviation", "title": "implemented_by", "to": "computation of natural gradients using induced metric from policy space in apprenticeship learning"}, {"from": "gradient-based inverse reinforcement learning minimising expert policy deviation", "title": "implemented_by", "to": "value iteration with Boltzmann stochastic policy enabling differentiable optimisation in apprenticeship learning"}, {"from": "gradient-based inverse reinforcement learning minimising expert policy deviation", "title": "implemented_by", "to": "Q-value parameter gradients via subdifferential fixed-point equation in apprenticeship learning"}, {"from": "computation of natural gradients using induced metric from policy space in apprenticeship learning", "title": "validated_by", "to": "grid-world experiments demonstrating robustness of natural gradient IRL"}, {"from": "computation of natural gradients using induced metric from policy space in apprenticeship learning", "title": "validated_by", "to": "sailing domain experiments confirming sample efficiency and robustness of gradient IRL"}, {"from": "value iteration with Boltzmann stochastic policy enabling differentiable optimisation in apprenticeship learning", "title": "implemented_by", "to": "using stochastic policies with temperature to smooth optimisation landscape in inverse RL"}, {"from": "grid-world experiments demonstrating robustness of natural gradient IRL", "title": "motivates", "to": "apply natural gradient inverse reinforcement learning during apprenticeship learning"}, {"from": "grid-world experiments demonstrating robustness of natural gradient IRL", "title": "motivates", "to": "combine supervised loss with reward parameter tuning for sample-efficient policy imitation"}, {"from": "sailing domain experiments confirming sample efficiency and robustness of gradient IRL", "title": "motivates", "to": "apply natural gradient inverse reinforcement learning during apprenticeship learning"}, {"from": "sailing domain experiments confirming sample efficiency and robustness of gradient IRL", "title": "motivates", "to": "compute optimal-value gradients via subdifferential fixed-point solver to guide reward updates"}, {"from": "feature scaling sensitivity analysis demonstrating robustness of natural gradient IRL", "title": "validated_by", "to": "natural gradient optimisation mitigates parameter redundancy and scaling issues in IRL"}, {"from": "feature scaling sensitivity analysis demonstrating robustness of natural gradient IRL", "title": "motivates", "to": "incorporate Boltzmann stochastic policy as differentiable proxy for greedy selection in reward optimisation"}, {"from": "ill-posed reward recovery causing unsafe behavior in inverse reinforcement learning", "title": "caused_by", "to": "non-uniqueness of reward functions leading multiple optimal policies in inverse RL"}, {"from": "non-uniqueness of reward functions leading multiple optimal policies in inverse RL", "title": "caused_by", "to": "Boltzmann stochastic policies enforce uniqueness of solution in inverse RL optimisation"}, {"from": "Boltzmann stochastic policies enforce uniqueness of solution in inverse RL optimisation", "title": "mitigated_by", "to": "using stochastic policies with temperature to smooth optimisation landscape in inverse RL"}, {"from": "feature scaling sensitivity leading performance degradation in IRL algorithms", "title": "caused_by", "to": "linear feature scaling mismatch in max-margin IRL"}, {"from": "linear feature scaling mismatch in max-margin IRL", "title": "caused_by", "to": "natural gradient optimisation mitigates parameter redundancy and scaling issues in IRL"}, {"from": "natural gradient optimisation mitigates parameter redundancy and scaling issues in IRL", "title": "mitigated_by", "to": "apply natural gradient inverse reinforcement learning during apprenticeship learning"}, {"from": "mutual defection in Prisoner\u0027s Dilemma among AI-controlled firms", "title": "caused_by", "to": "lack of strategy translucency causing defection in PD settings"}, {"from": "mutual defection in Prisoner\u0027s Dilemma among AI-controlled firms", "title": "mitigated_by", "to": "introduce controlled intention leakage to enforce cooperative equilibria"}, {"from": "inefficient equilibria from classical rationalizable strategies in AI agents", "title": "caused_by", "to": "assumption of unilateral deviations leading to minimax dominated strategies persisting"}, {"from": "inefficient equilibria from classical rationalizable strategies in AI agents", "title": "mitigated_by", "to": "adopt CCBR-based reasoning in AI policy formation"}, {"from": "translucency shifts equilibrium to cooperation when penalty exceeds reward and leak probability epsilon", "title": "enabled_by", "to": "introduce controlled intention leakage to enforce cooperative equilibria"}, {"from": "introduce controlled intention leakage to enforce cooperative equilibria", "title": "implemented_by", "to": "randomised low-probability strategy disclosure channels between agents"}, {"from": "adopt CCBR-based reasoning in AI policy formation", "title": "implemented_by", "to": "iterated minimax-dominated strategy deletion algorithm during policy search"}, {"from": "adopt CCBR-based reasoning in AI policy formation", "title": "motivates", "to": "embed counterfactual individual-rationality checks in agent architecture"}, {"from": "randomised low-probability strategy disclosure channels between agents", "title": "validated_by", "to": "theoretical proof of cooperation under translucency"}, {"from": "iterated minimax-dominated strategy deletion algorithm during policy search", "title": "validated_by", "to": "proof that CCBR strategies equal those surviving minimax deletion"}, {"from": "theoretical proof of cooperation under translucency", "title": "motivates", "to": "deploy intention-signalling protocols in multi-agent AI systems"}, {"from": "proof that CCBR strategies equal those surviving minimax deletion", "title": "motivates", "to": "apply iterated minimax-dominated strategy pruning during multi-agent RL training"}, {"from": "Semantic misunderstanding in NLP applications due to poor embeddings", "title": "caused_by", "to": "Low-quality distributed word representations in legacy neural language models"}, {"from": "Excessive computational cost in large-scale word embedding training", "title": "caused_by", "to": "Full softmax output layer complexity scaling with vocabulary size"}, {"from": "Inability to model idiomatic phrases in NLP systems", "title": "caused_by", "to": "Word-level tokenization ignores multi-word expressions with unique meanings"}, {"from": "Low-quality distributed word representations in legacy neural language models", "title": "mitigated_by", "to": "Linear vector space structure allows analogical reasoning from embeddings"}, {"from": "Full softmax output layer complexity scaling with vocabulary size", "title": "mitigated_by", "to": "Approximate softmax objectives can reduce computation without losing representation quality"}, {"from": "Word-level tokenization ignores multi-word expressions with unique meanings", "title": "mitigated_by", "to": "Data-driven phrase token identification enhances representational expressiveness"}, {"from": "Linear vector space structure allows analogical reasoning from embeddings", "title": "implemented_by", "to": "Context prediction training objective in Skip-gram model for efficient embedding learning"}, {"from": "Approximate softmax objectives can reduce computation without losing representation quality", "title": "implemented_by", "to": "Use of negative sampling and hierarchical softmax to approximate softmax efficiently"}, {"from": "Data-driven phrase token identification enhances representational expressiveness", "title": "implemented_by", "to": "Incorporating phrase tokens into training corpus to capture idiomatic meaning"}, {"from": "Context prediction training objective in Skip-gram model for efficient embedding learning", "title": "implemented_by", "to": "Negative sampling with k=5\u201315 negative draws per positive sample"}, {"from": "Context prediction training objective in Skip-gram model for efficient embedding learning", "title": "required_by", "to": "Incorporating phrase tokens into training corpus to capture idiomatic meaning"}, {"from": "Use of negative sampling and hierarchical softmax to approximate softmax efficiently", "title": "implemented_by", "to": "Hierarchical softmax with binary Huffman tree"}, {"from": "Use of negative sampling and hierarchical softmax to approximate softmax efficiently", "title": "implemented_by", "to": "Negative sampling with k=5\u201315 negative draws per positive sample"}, {"from": "Incorporating phrase tokens into training corpus to capture idiomatic meaning", "title": "implemented_by", "to": "Data-driven bigram score thresholding to create phrase tokens"}, {"from": "Negative sampling with k=5\u201315 negative draws per positive sample", "title": "refined_by", "to": "Subsampling of frequent words with probability P(w)=1\u2212\u221a(t/f(w))"}, {"from": "Negative sampling with k=5\u201315 negative draws per positive sample", "title": "validated_by", "to": "Analogical reasoning accuracy improvements over baselines"}, {"from": "Hierarchical softmax with binary Huffman tree", "title": "validated_by", "to": "Training speed improvements (2x\u201310x) with subsampling and negative sampling"}, {"from": "Subsampling of frequent words with probability P(w)=1\u2212\u221a(t/f(w))", "title": "validated_by", "to": "Training speed improvements (2x\u201310x) with subsampling and negative sampling"}, {"from": "Data-driven bigram score thresholding to create phrase tokens", "title": "validated_by", "to": "Phrase analogy accuracy improvement to 72% with phrase embeddings"}, {"from": "Analogical reasoning accuracy improvements over baselines", "title": "motivates", "to": "Pre-train word embeddings with Skip-gram using negative sampling and subsampling"}, {"from": "Training speed improvements (2x\u201310x) with subsampling and negative sampling", "title": "motivates", "to": "Pre-train word embeddings with Skip-gram using negative sampling and subsampling"}, {"from": "Training speed improvements (2x\u201310x) with subsampling and negative sampling", "title": "motivates", "to": "Train word embeddings with hierarchical softmax using Huffman codes for computational efficiency"}, {"from": "Phrase analogy accuracy improvement to 72% with phrase embeddings", "title": "motivates", "to": "Pre-process corpus to detect bigram and n-gram phrases and train phrase-level Skip-gram embeddings"}, {"from": "Opaque decision-making in deep convolutional networks for image classification", "title": "caused_by", "to": "Limited visibility into internal feature representations obstructs debugging of ConvNet failure modes"}, {"from": "Limited visibility into internal feature representations obstructs debugging of ConvNet failure modes", "title": "mitigated_by", "to": "Visualization of feature activations exposes hierarchical structure and invariances in ConvNets"}, {"from": "Visualization of feature activations exposes hierarchical structure and invariances in ConvNets", "title": "implemented_by", "to": "Use deconvolutional network-based top-down projection to interpret ConvNet activations"}, {"from": "Use deconvolutional network-based top-down projection to interpret ConvNet activations", "title": "implemented_by", "to": "Layer-wise deconvnet with unpooling switches, rectification, and transposed filters for feature reconstruction"}, {"from": "Layer-wise deconvnet with unpooling switches, rectification, and transposed filters for feature reconstruction", "title": "validated_by", "to": "Feature reconstructions reveal intuitive patterns and hierarchical invariance across layers"}, {"from": "Layer-wise deconvnet with unpooling switches, rectification, and transposed filters for feature reconstruction", "title": "validated_by", "to": "Visualization-driven architecture change improves ImageNet error rates"}, {"from": "Layer-wise deconvnet with unpooling switches, rectification, and transposed filters for feature reconstruction", "title": "validated_by", "to": "Occlusion sensitivity experiments confirm localized object sensitivity"}, {"from": "Feature reconstructions reveal intuitive patterns and hierarchical invariance across layers", "title": "motivates", "to": "Integrate deconvolutional visualization into ConvNet development workflow"}, {"from": "Visualization-driven architecture change improves ImageNet error rates", "title": "motivates", "to": "Design first ConvNet layer with 7\u00d77 filters and stride 2 to reduce aliasing"}, {"from": "Visualization-driven architecture change improves ImageNet error rates", "title": "motivates", "to": "Apply per-filter RMS renormalization during training to prevent dominant filters"}, {"from": "Occlusion sensitivity experiments confirm localized object sensitivity", "title": "motivates", "to": "Conduct occlusion sensitivity analysis before deployment to verify model localisation"}, {"from": "Ineffective reinforcement learning from high-dimensional sensory inputs in Atari environments", "title": "caused_by", "to": "High-dimensional visual input without hand-crafted features in Atari games"}, {"from": "Ineffective reinforcement learning from high-dimensional sensory inputs in Atari environments", "title": "caused_by", "to": "High computational cost of per-frame action selection in Atari emulator"}, {"from": "Instability and divergence in Q-learning with nonlinear function approximators in Atari environments", "title": "caused_by", "to": "Correlated sequential data in on-policy RL training in Atari games"}, {"from": "Instability and divergence in Q-learning with nonlinear function approximators in Atari environments", "title": "caused_by", "to": "Non-stationary data distribution due to evolving policies in Atari RL"}, {"from": "Instability and divergence in Q-learning with nonlinear function approximators in Atari environments", "title": "caused_by", "to": "Sparse and delayed reward signals in Atari tasks"}, {"from": "Correlated sequential data in on-policy RL training in Atari games", "title": "mitigated_by", "to": "Experience replay improving sample independence in Atari RL"}, {"from": "Non-stationary data distribution due to evolving policies in Atari RL", "title": "mitigated_by", "to": "Experience replay improving sample independence in Atari RL"}, {"from": "Non-stationary data distribution due to evolving policies in Atari RL", "title": "mitigated_by", "to": "Off-policy \u03b5-greedy exploration enabling data reuse in Atari RL"}, {"from": "Sparse and delayed reward signals in Atari tasks", "title": "mitigated_by", "to": "Reward magnitude normalization stabilizing gradients across Atari games"}, {"from": "High computational cost of per-frame action selection in Atari emulator", "title": "mitigated_by", "to": "Temporal redundancy in Atari frame sequences permitting frame skipping"}, {"from": "High-dimensional visual input without hand-crafted features in Atari games", "title": "mitigated_by", "to": "End-to-end deep feature learning from raw pixels in Atari RL"}, {"from": "Experience replay improving sample independence in Atari RL", "title": "enabled_by", "to": "Integrate experience replay with deep Q-learning for data efficiency in Atari RL"}, {"from": "Off-policy \u03b5-greedy exploration enabling data reuse in Atari RL", "title": "enabled_by", "to": "\u03b5-greedy exploration strategy for off-policy learning in Atari RL"}, {"from": "End-to-end deep feature learning from raw pixels in Atari RL", "title": "enabled_by", "to": "CNN with shared visual representation and action-specific outputs for efficient Q-value computation in Atari RL"}, {"from": "Temporal redundancy in Atari frame sequences permitting frame skipping", "title": "enabled_by", "to": "Frame skipping to reduce computational burden in Atari RL"}, {"from": "Reward magnitude normalization stabilizing gradients across Atari games", "title": "enabled_by", "to": "Clip rewards to unit magnitude for consistent gradient scales in Atari DQN training"}, {"from": "Integrate experience replay with deep Q-learning for data efficiency in Atari RL", "title": "implemented_by", "to": "Replay memory of one million frames with uniform random minibatch sampling in DQN"}, {"from": "CNN with shared visual representation and action-specific outputs for efficient Q-value computation in Atari RL", "title": "implemented_by", "to": "Deep Q-network architecture with conv(16x8 stride4,32x4 stride2)+256FC action outputs"}, {"from": "Frame skipping to reduce computational burden in Atari RL", "title": "implemented_by", "to": "Frame skipping implementation repeating last action for k=4 (k=3 in Space Invaders)"}, {"from": "Replay memory of one million frames with uniform random minibatch sampling in DQN", "title": "validated_by", "to": "Smooth predicted Q-value increase without divergence during DQN training"}, {"from": "Deep Q-network architecture with conv(16x8 stride4,32x4 stride2)+256FC action outputs", "title": "validated_by", "to": "DQN outperforming prior RL methods on six of seven Atari games"}, {"from": "Reward clipping implementation mapping rewards to {-1,0,1} during DQN training", "title": "implemented_by", "to": "Clip rewards to unit magnitude for consistent gradient scales in Atari DQN training"}, {"from": "Reward clipping implementation mapping rewards to {-1,0,1} during DQN training", "title": "validated_by", "to": "Smooth predicted Q-value increase without divergence during DQN training"}, {"from": "Frame skipping implementation repeating last action for k=4 (k=3 in Space Invaders)", "title": "validated_by", "to": "DQN outperforming prior RL methods on six of seven Atari games"}, {"from": "DQN outperforming prior RL methods on six of seven Atari games", "title": "motivates", "to": "Train deep convolutional Q-networks end-to-end from raw pixel inputs in Atari RL"}, {"from": "DQN outperforming prior RL methods on six of seven Atari games", "title": "motivates", "to": "Apply frame skipping (k=3\u20134) during Atari RL agent training"}, {"from": "DQN outperforming prior RL methods on six of seven Atari games", "title": "validated_by", "to": "Linear annealing of \u03b5 from 1 to 0.1 over first million frames in DQN training"}, {"from": "DQN outperforming prior RL methods on six of seven Atari games", "title": "motivates", "to": "Use \u03b5-greedy exploration with linear annealing of \u03b5 during DQN training"}, {"from": "Smooth predicted Q-value increase without divergence during DQN training", "title": "motivates", "to": "Implement experience replay with uniform random sampling during DQN training"}, {"from": "Smooth predicted Q-value increase without divergence during DQN training", "title": "motivates", "to": "Clip reward magnitudes to [-1,1] during training of Atari DQN agents"}, {"from": "\u03b5-greedy exploration strategy for off-policy learning in Atari RL", "title": "implemented_by", "to": "Linear annealing of \u03b5 from 1 to 0.1 over first million frames in DQN training"}, {"from": "Intractable posterior distributions in continuous latent variable models", "title": "caused_by", "to": "Analytical intractability of marginal likelihood integrals"}, {"from": "High computational cost of inference on large datasets in probabilistic models", "title": "caused_by", "to": "Inefficiency of sampling-based inference per datapoint"}, {"from": "High variance gradient estimators in variational inference", "title": "caused_by", "to": "Variance explosion in na\u00efve Monte Carlo gradient estimator"}, {"from": "Analytical intractability of marginal likelihood integrals", "title": "addressed_by", "to": "Reparameterization of latent variables enabling differentiable sampling"}, {"from": "Analytical intractability of marginal likelihood integrals", "title": "addressed_by", "to": "Variational lower bound regularization in approximate inference"}, {"from": "Inefficiency of sampling-based inference per datapoint", "title": "addressed_by", "to": "Reparameterization of latent variables enabling differentiable sampling"}, {"from": "Variance explosion in na\u00efve Monte Carlo gradient estimator", "title": "addressed_by", "to": "Reparameterization of latent variables enabling differentiable sampling"}, {"from": "Reparameterization of latent variables enabling differentiable sampling", "title": "mitigated_by", "to": "Stochastic gradient variational Bayes optimization of lower bound"}, {"from": "Reparameterization of latent variables enabling differentiable sampling", "title": "mitigated_by", "to": "Auto-Encoding variational Bayes joint training of encoder and decoder"}, {"from": "Variational lower bound regularization in approximate inference", "title": "mitigated_by", "to": "Auto-Encoding variational Bayes joint training of encoder and decoder"}, {"from": "Stochastic gradient variational Bayes optimization of lower bound", "title": "implemented_by", "to": "SGVB estimator with analytic KL divergence and single-sample minibatch Monte Carlo"}, {"from": "Auto-Encoding variational Bayes joint training of encoder and decoder", "title": "implemented_by", "to": "Gaussian neural network encoder generating mean and variance parameters"}, {"from": "Auto-Encoding variational Bayes joint training of encoder and decoder", "title": "implemented_by", "to": "Minibatch SGD or Adagrad training with L=1 sample per datapoint"}, {"from": "SGVB estimator with analytic KL divergence and single-sample minibatch Monte Carlo", "title": "refined_by", "to": "Gaussian neural network encoder generating mean and variance parameters"}, {"from": "SGVB estimator with analytic KL divergence and single-sample minibatch Monte Carlo", "title": "refined_by", "to": "Minibatch SGD or Adagrad training with L=1 sample per datapoint"}, {"from": "SGVB estimator with analytic KL divergence and single-sample minibatch Monte Carlo", "title": "validated_by", "to": "Empirical faster convergence and superior lower bound versus wake-sleep and MCEM"}, {"from": "SGVB estimator with analytic KL divergence and single-sample minibatch Monte Carlo", "title": "validated_by", "to": "Marginal likelihood improvement demonstrated on MNIST data"}, {"from": "Gaussian neural network encoder generating mean and variance parameters", "title": "validated_by", "to": "Empirical faster convergence and superior lower bound versus wake-sleep and MCEM"}, {"from": "Gaussian neural network encoder generating mean and variance parameters", "title": "validated_by", "to": "Marginal likelihood improvement demonstrated on MNIST data"}, {"from": "Minibatch SGD or Adagrad training with L=1 sample per datapoint", "title": "validated_by", "to": "Empirical faster convergence and superior lower bound versus wake-sleep and MCEM"}, {"from": "Empirical faster convergence and superior lower bound versus wake-sleep and MCEM", "title": "motivates", "to": "Apply SGVB-based variational auto-encoder training for scalable inference in continuous latent variable models"}, {"from": "Empirical faster convergence and superior lower bound versus wake-sleep and MCEM", "title": "motivates", "to": "Use reparameterization trick to reduce gradient variance during variational training"}, {"from": "Empirical faster convergence and superior lower bound versus wake-sleep and MCEM", "title": "motivates", "to": "Integrate analytic KL computation in VAE loss to lower estimator variance"}, {"from": "Marginal likelihood improvement demonstrated on MNIST data", "title": "motivates", "to": "Apply SGVB-based variational auto-encoder training for scalable inference in continuous latent variable models"}, {"from": "Marginal likelihood improvement demonstrated on MNIST data", "title": "motivates", "to": "Use reparameterization trick to reduce gradient variance during variational training"}, {"from": "Intractable likelihood estimation in deep generative models", "title": "caused_by", "to": "Dependence on MCMC-based partition function gradient estimation in undirected generative models"}, {"from": "Slow Markov chain mixing in deep generative model training", "title": "caused_by", "to": "Requirement of Markov chain sampling during generation in autoencoder and undirected models"}, {"from": "Vanishing generator gradients in adversarial training", "title": "caused_by", "to": "Saturation of log(1\u2212D(G(z))) objective early in adversarial training"}, {"from": "Dependence on MCMC-based partition function gradient estimation in undirected generative models", "title": "mitigated_by", "to": "Discriminative adversarial objective eliminates need for explicit likelihood computation and MCMC"}, {"from": "Requirement of Markov chain sampling during generation in autoencoder and undirected models", "title": "mitigated_by", "to": "Discriminative adversarial objective eliminates need for explicit likelihood computation and MCMC"}, {"from": "Saturation of log(1\u2212D(G(z))) objective early in adversarial training", "title": "mitigated_by", "to": "Alternative generator objective maximizing log D(G(z)) provides stronger gradients without altering fixed point"}, {"from": "Discriminative adversarial objective eliminates need for explicit likelihood computation and MCMC", "title": "enabled_by", "to": "Generator training driven by discriminator feedback in minimax game"}, {"from": "Alternative generator objective maximizing log D(G(z)) provides stronger gradients without altering fixed point", "title": "implemented_by", "to": "Modified generator update to maximize log D(G(z)) during training"}, {"from": "Generator training driven by discriminator feedback in minimax game", "title": "implemented_by", "to": "Alternating k-step discriminator and generator updates to balance training dynamics"}, {"from": "Alternating k-step discriminator and generator updates to balance training dynamics", "title": "implemented_by", "to": "Two multilayer perceptrons for generator and discriminator trained with backprop and dropout"}, {"from": "Two multilayer perceptrons for generator and discriminator trained with backprop and dropout", "title": "validated_by", "to": "Improved Parzen window log-likelihood on MNIST and TFD benchmarks"}, {"from": "Two multilayer perceptrons for generator and discriminator trained with backprop and dropout", "title": "validated_by", "to": "Competitive sample quality without Markov chain mixing"}, {"from": "Modified generator update to maximize log D(G(z)) during training", "title": "validated_by", "to": "Improved Parzen window log-likelihood on MNIST and TFD benchmarks"}, {"from": "Improved Parzen window log-likelihood on MNIST and TFD benchmarks", "title": "motivates", "to": "Train generator\u2013discriminator pair with adversarial nets during pre-training"}, {"from": "Improved Parzen window log-likelihood on MNIST and TFD benchmarks", "title": "motivates", "to": "Optimize generator with log D(G(z)) objective during adversarial training"}, {"from": "Improved Parzen window log-likelihood on MNIST and TFD benchmarks", "title": "motivates", "to": "Use one discriminator update per generator update in adversarial training"}, {"from": "Improved Parzen window log-likelihood on MNIST and TFD benchmarks", "title": "motivates", "to": "Apply dropout to discriminator network during adversarial training"}, {"from": "Competitive sample quality without Markov chain mixing", "title": "motivates", "to": "Train generator\u2013discriminator pair with adversarial nets during pre-training"}, {"from": "translation quality degradation for long sentences in neural machine translation", "title": "caused_by", "to": "fixed-length context vector bottleneck in encoder-decoder NMT"}, {"from": "translation quality degradation for long sentences in neural machine translation", "title": "caused_by", "to": "encoder burden to compress full source information"}, {"from": "incorrect word alignment and reordering in NMT outputs", "title": "caused_by", "to": "absence of explicit alignment mechanism in encoder-decoder NMT"}, {"from": "fixed-length context vector bottleneck in encoder-decoder NMT", "title": "mitigated_by", "to": "adaptive attention over source annotations can dynamically focus on relevant parts"}, {"from": "absence of explicit alignment mechanism in encoder-decoder NMT", "title": "mitigated_by", "to": "soft alignment supports non-monotonic reordering"}, {"from": "adaptive attention over source annotations can dynamically focus on relevant parts", "title": "enabled_by", "to": "joint training of alignment and translation to share gradients and improve performance"}, {"from": "adaptive attention over source annotations can dynamically focus on relevant parts", "title": "enabled_by", "to": "use bidirectional RNN encoder for contextual annotations"}, {"from": "joint training of alignment and translation to share gradients and improve performance", "title": "implemented_by", "to": "attention-weighted context vector per target word"}, {"from": "joint training of alignment and translation to share gradients and improve performance", "title": "implemented_by", "to": "feedforward alignment model producing softmax weights"}, {"from": "use bidirectional RNN encoder for contextual annotations", "title": "implemented_by", "to": "bidirectional RNN encoder concatenating forward and backward hidden states as annotations"}, {"from": "attention-weighted context vector per target word", "title": "validated_by", "to": "RNNsearch BLEU improvements over RNNencdec on WMT14 En-Fr"}, {"from": "attention-weighted context vector per target word", "title": "validated_by", "to": "performance robustness across sentence lengths with RNNsearch"}, {"from": "attention-weighted context vector per target word", "title": "required_by", "to": "bidirectional RNN encoder concatenating forward and backward hidden states as annotations"}, {"from": "feedforward alignment model producing softmax weights", "title": "validated_by", "to": "attention visualisations show plausible alignments"}, {"from": "RNNsearch BLEU improvements over RNNencdec on WMT14 En-Fr", "title": "motivates", "to": "integrate attention-based bidirectional encoder-decoder architecture (RNNsearch) during NMT model design"}, {"from": "performance robustness across sentence lengths with RNNsearch", "title": "motivates", "to": "integrate attention-based bidirectional encoder-decoder architecture (RNNsearch) during NMT model design"}, {"from": "attention visualisations show plausible alignments", "title": "motivates", "to": "inspect attention weight visualisations during pre-deployment testing to ensure alignment quality"}, {"from": "Human elimination by superintelligent AI in future universe", "title": "caused_by", "to": "Goal retention failure in self-improving AI systems"}, {"from": "Human elimination by superintelligent AI in future universe", "title": "caused_by", "to": "Ill-defined human values in physical terms"}, {"from": "Goal retention failure in self-improving AI systems", "title": "caused_by", "to": "Self-modeling and improved world understanding induce goal reinterpretation"}, {"from": "Ill-defined human values in physical terms", "title": "caused_by", "to": "Only physically definable goals remain well-defined under arbitrarily high intelligence"}, {"from": "Self-modeling and improved world understanding induce goal reinterpretation", "title": "mitigated_by", "to": "Design robust goal retention mechanisms immune to self-modeling subversion"}, {"from": "Only physically definable goals remain well-defined under arbitrarily high intelligence", "title": "mitigated_by", "to": "Establish rigorously defined desirable final goals before AI reaches superintelligence"}, {"from": "Design robust goal retention mechanisms immune to self-modeling subversion", "title": "implemented_by", "to": "Meta-stable utility preservation protocols during recursive self-improvement"}, {"from": "Establish rigorously defined desirable final goals before AI reaches superintelligence", "title": "implemented_by", "to": "Philosophical value formalization into computable utility functions"}, {"from": "Meta-stable utility preservation protocols during recursive self-improvement", "title": "validated_by", "to": "Human goal evolution with increased intelligence indicates goal instability"}, {"from": "Philosophical value formalization into computable utility functions", "title": "validated_by", "to": "Definability-desirability gap evidence from entropy utility counterexamples"}, {"from": "Human goal evolution with increased intelligence indicates goal instability", "title": "motivates", "to": "Implement architectural safeguards ensuring immutable core goals during self-modification cycles in AI"}, {"from": "Definability-desirability gap evidence from entropy utility counterexamples", "title": "motivates", "to": "Advance interdisciplinary research program to formalize universally desirable, physically expressible utility functions for AI"}, {"from": "Definability-desirability gap evidence from entropy utility counterexamples", "title": "motivates", "to": "Delay deployment of self-improving AI until alignment theory resolves final goal definability"}, {"from": "Low classification accuracy in large-scale image recognition", "title": "caused_by", "to": "Insufficient representational capacity of shallow ConvNets with large filters"}, {"from": "Low classification accuracy in large-scale image recognition", "title": "caused_by", "to": "Single-scale training limits robustness to object scale variation"}, {"from": "Low classification accuracy in large-scale image recognition", "title": "caused_by", "to": "Single-scale testing fails to exploit scale robustness"}, {"from": "Low classification accuracy in large-scale image recognition", "title": "caused_by", "to": "Single ConvNet variance limits achievable accuracy"}, {"from": "Insufficient representational capacity of shallow ConvNets with large filters", "title": "caused_by", "to": "Depth increase with small 3\u00d73 filters increases nonlinear discriminative power while controlling parameters"}, {"from": "Depth increase with small 3\u00d73 filters increases nonlinear discriminative power while controlling parameters", "title": "mitigated_by", "to": "Design very deep ConvNets by stacking small 3\u00d73 convolution layers"}, {"from": "Design very deep ConvNets by stacking small 3\u00d73 convolution layers", "title": "implemented_by", "to": "VGG ConvNet configurations A\u2013E with 16\u201319 weight layers of 3\u00d73 filters"}, {"from": "VGG ConvNet configurations A\u2013E with 16\u201319 weight layers of 3\u00d73 filters", "title": "validated_by", "to": "Top-5 error 8.0 % on ImageNet using 19-layer VGG model"}, {"from": "Top-5 error 8.0 % on ImageNet using 19-layer VGG model", "title": "motivates", "to": "Design ConvNets with 16\u201319 layers of 3\u00d73 filters for large-scale image tasks"}, {"from": "Single-scale training limits robustness to object scale variation", "title": "caused_by", "to": "Scale jittering during training captures multi-scale statistics"}, {"from": "Scale jittering during training captures multi-scale statistics", "title": "mitigated_by", "to": "Augment training images by random rescaling within range"}, {"from": "Augment training images by random rescaling within range", "title": "implemented_by", "to": "Training VGG with S in [256,512] scale range"}, {"from": "Training VGG with S in [256,512] scale range", "title": "validated_by", "to": "Validation error reduced to 7.5 % top-5 with multi-scale training"}, {"from": "Validation error reduced to 7.5 % top-5 with multi-scale training", "title": "motivates", "to": "Apply scale-jittered multi-scale training for ConvNets"}, {"from": "Single-scale testing fails to exploit scale robustness", "title": "caused_by", "to": "Aggregating predictions over multiple scales reduces error"}, {"from": "Aggregating predictions over multiple scales reduces error", "title": "mitigated_by", "to": "Average class probabilities across rescaled test images"}, {"from": "Average class probabilities across rescaled test images", "title": "implemented_by", "to": "Evaluate VGG at Q={256,384,512} and average outputs"}, {"from": "Evaluate VGG at Q={256,384,512} and average outputs", "title": "validated_by", "to": "Top-5 error 7.5 % with multi-scale evaluation"}, {"from": "Top-5 error 7.5 % with multi-scale evaluation", "title": "motivates", "to": "Perform multi-scale test-time evaluation by averaging predictions across scales"}, {"from": "Single ConvNet variance limits achievable accuracy", "title": "caused_by", "to": "Ensembling complementary models reduces variance"}, {"from": "Ensembling complementary models reduces variance", "title": "mitigated_by", "to": "Average softmax outputs across multiple ConvNets"}, {"from": "Average softmax outputs across multiple ConvNets", "title": "implemented_by", "to": "Fusion of two multi-scale VGG models D and E"}, {"from": "Fusion of two multi-scale VGG models D and E", "title": "validated_by", "to": "Ensemble achieved 6.8 % top-5 error on ImageNet"}, {"from": "Ensemble achieved 6.8 % top-5 error on ImageNet", "title": "motivates", "to": "Ensemble complementary deep ConvNets by averaging soft-max outputs"}, {"from": "Ineffective mapping of variable-length sequences in neural networks", "title": "caused_by", "to": "Fixed-dimensional input and output requirement in DNNs"}, {"from": "Fixed-dimensional input and output requirement in DNNs", "title": "caused_by", "to": "Encoder-decoder LSTM learning variable-length sequence mapping"}, {"from": "Encoder-decoder LSTM learning variable-length sequence mapping", "title": "mitigated_by", "to": "Separate encoder and decoder LSTMs for sequence mapping"}, {"from": "Separate encoder and decoder LSTMs for sequence mapping", "title": "implemented_by", "to": "Deep 4-layer LSTM encoder-decoder with 8000-dimensional sentence vectors"}, {"from": "Deep 4-layer LSTM encoder-decoder with 8000-dimensional sentence vectors", "title": "validated_by", "to": "BLEU 34.81 on WMT\u201914 with reversed LSTM ensemble"}, {"from": "Deep 4-layer LSTM encoder-decoder with 8000-dimensional sentence vectors", "title": "required_by", "to": "Scale gradient when norm exceeds threshold 5"}, {"from": "Deep 4-layer LSTM encoder-decoder with 8000-dimensional sentence vectors", "title": "required_by", "to": "Reverse source sentence order during training"}, {"from": "BLEU 34.81 on WMT\u201914 with reversed LSTM ensemble", "title": "motivates", "to": "Design and train deep encoder-decoder LSTM models with reversed input sequence"}, {"from": "Translation degradation on long sentences in RNN models", "title": "caused_by", "to": "Large minimal time lag between corresponding source and target words"}, {"from": "Large minimal time lag between corresponding source and target words", "title": "caused_by", "to": "Reversing source sentences reduces minimal time lag"}, {"from": "Reversing source sentences reduces minimal time lag", "title": "mitigated_by", "to": "Apply input sentence reversal before training"}, {"from": "Apply input sentence reversal before training", "title": "implemented_by", "to": "Reverse source sentence order during training"}, {"from": "Reverse source sentence order during training", "title": "validated_by", "to": "Perplexity drop to 4.7 and BLEU increase to 30.6 with reversed input"}, {"from": "Perplexity drop to 4.7 and BLEU increase to 30.6 with reversed input", "title": "motivates", "to": "Reverse source sentences during data preprocessing for sequence models"}, {"from": "Training instability due to exploding gradients in deep LSTM training", "title": "caused_by", "to": "Exploding gradients in deep LSTM optimization"}, {"from": "Exploding gradients in deep LSTM optimization", "title": "caused_by", "to": "Gradient norm constraint to control exploding gradients"}, {"from": "Gradient norm constraint to control exploding gradients", "title": "mitigated_by", "to": "Apply hard gradient norm clipping"}, {"from": "Apply hard gradient norm clipping", "title": "implemented_by", "to": "Scale gradient when norm exceeds threshold 5"}, {"from": "Scale gradient when norm exceeds threshold 5", "title": "validated_by", "to": "Successful training of 384M parameter LSTM over 7.5 epochs"}, {"from": "Successful training of 384M parameter LSTM over 7.5 epochs", "title": "motivates", "to": "Apply gradient norm clipping at threshold 5 during LSTM training"}, {"from": "unintended instrumental actions in advanced AI systems", "title": "caused_by", "to": "instrumental drives from utility maximization in embedded agents"}, {"from": "unintended instrumental actions in advanced AI systems", "title": "caused_by", "to": "instrumental behaviour from misspecified utility in context"}, {"from": "self-delusion via reward hacking in RL agents", "title": "caused_by", "to": "reward observations embedded in environment enable delusion box"}, {"from": "corrupting the reward generator by AI manipulation of humans", "title": "caused_by", "to": "instrumental drives from utility maximization in embedded agents"}, {"from": "corrupting the reward generator by AI manipulation of humans", "title": "mitigated_by", "to": "three-argument utility prevents manipulation of human evaluators"}, {"from": "authoritarian power concentration through super-intelligent AI", "title": "addressed_by", "to": "Mandate transparency and public oversight of AI model updates and utility definitions"}, {"from": "instrumental drives from utility maximization in embedded agents", "title": "required_by", "to": "model-based utility functions referencing learned environment model"}, {"from": "instrumental drives from utility maximization in embedded agents", "title": "validated_by", "to": "Ring-Orseau proof that RL agents choose delusion box"}, {"from": "reward observations embedded in environment enable delusion box", "title": "mitigated_by", "to": "model-based utility functions referencing learned environment model"}, {"from": "approximate environment models under limited computational resources", "title": "mitigated_by", "to": "Monte-Carlo sampling over internal state histories for utility estimation"}, {"from": "agents embedded in environment vulnerable to self-modification attacks", "title": "addressed_by", "to": "probabilistic verification of external resource integrity before use"}, {"from": "model-based utility functions referencing learned environment model", "title": "enabled_by", "to": "define human_values procedure applied to learned model"}, {"from": "model-based utility functions referencing learned environment model", "title": "validated_by", "to": "Proposition 6.1 showing model-based utility resists self-delusion"}, {"from": "statistical learning of human values from observed behavior", "title": "enabled_by", "to": "define human_values procedure applied to learned model"}, {"from": "three-argument utility prevents manipulation of human evaluators", "title": "refined_by", "to": "filter increases due to value corruption using \u03b4 condition"}, {"from": "self-modeling agents evaluate resource growth within value function", "title": "enabled_by", "to": "Integrate self-modeling and stochastic-action modules for resource growth and anti-prediction"}, {"from": "self-modeling agents evaluate resource growth within value function", "title": "validated_by", "to": "Proposition 8.1 policy invariance under self-modification"}, {"from": "two-stage agent architecture with safe model learning phase", "title": "implemented_by", "to": "surrogate agents provide safe exploratory actions during model learning"}, {"from": "define human_values procedure applied to learned model", "title": "implemented_by", "to": "finite stochastic loop program environment modelling"}, {"from": "simulation-based decision support system for AI testing", "title": "implemented_by", "to": "interactive visualisation of ensemble simulations for auditors"}, {"from": "simulation-based decision support system for AI testing", "title": "motivates", "to": "Restrict testing-phase AI to simulated environment with no bidirectional channels"}, {"from": "Monte-Carlo sampling over internal state histories for utility estimation", "title": "enabled_by", "to": "built-in inductive bias functions for physical and social processes"}, {"from": "Ring-Orseau proof that RL agents choose delusion box", "title": "motivates", "to": "Implement model-based utility functions referencing learned environment models"}, {"from": "Proposition 6.1 showing model-based utility resists self-delusion", "title": "motivates", "to": "Deploy two-stage (and optional third-stage) agent architectures with surrogate learning phase"}, {"from": "analysis of memory-modifying environments showing need for stochasticity", "title": "motivates", "to": "Integrate self-modeling and stochastic-action modules for resource growth and anti-prediction"}, {"from": "Unstable or inefficient model optimization in large-scale machine learning", "title": "caused_by", "to": "Sparse and non-stationary gradients in SGD training"}, {"from": "Unstable or inefficient model optimization in large-scale machine learning", "title": "caused_by", "to": "Initialization bias in moment estimates of adaptive optimizers"}, {"from": "Sparse and non-stationary gradients in SGD training", "title": "mitigated_by", "to": "Per-parameter adaptive learning rates improve convergence with sparse gradients"}, {"from": "Initialization bias in moment estimates of adaptive optimizers", "title": "mitigated_by", "to": "Bias correction of exponential moving averages prevents large early step sizes"}, {"from": "Per-parameter adaptive learning rates improve convergence with sparse gradients", "title": "specified_by", "to": "Combine AdaGrad and RMSProp through adaptive moment estimation"}, {"from": "Bias correction of exponential moving averages prevents large early step sizes", "title": "specified_by", "to": "Introduce bias correction terms in Adam"}, {"from": "Parameter averaging reduces variance of final estimate in stochastic optimization", "title": "implemented_by", "to": "Temporal exponential averaging of parameters trained with Adam"}, {"from": "Combine AdaGrad and RMSProp through adaptive moment estimation", "title": "implemented_by", "to": "Adam update rule with first and second moment exponential moving averages"}, {"from": "Introduce bias correction terms in Adam", "title": "implemented_by", "to": "Bias-corrected moment estimation in Adam"}, {"from": "Adam update rule with first and second moment exponential moving averages", "title": "refined_by", "to": "AdaMax variant using infinity norm for gradient scaling"}, {"from": "Adam update rule with first and second moment exponential moving averages", "title": "validated_by", "to": "O(sqrt(T)) regret bound theoretical proof for Adam"}, {"from": "Adam update rule with first and second moment exponential moving averages", "title": "validated_by", "to": "Logistic regression experiments demonstrate faster convergence of Adam"}, {"from": "Adam update rule with first and second moment exponential moving averages", "title": "validated_by", "to": "Multilayer neural network experiments show Adam outperforms SFO"}, {"from": "Adam update rule with first and second moment exponential moving averages", "title": "validated_by", "to": "CNN experiments show Adam matches SGD and beats Adagrad"}, {"from": "Bias-corrected moment estimation in Adam", "title": "validated_by", "to": "Bias correction ablation study shows improved stability with bias correction"}, {"from": "AdaMax variant using infinity norm for gradient scaling", "title": "motivates", "to": "Use AdaMax optimizer in place of Adam when numerical stability or infinity norm adaptation needed"}, {"from": "Temporal exponential averaging of parameters trained with Adam", "title": "motivates", "to": "Apply exponential moving average of parameters during fine-tuning to improve generalization"}, {"from": "O(sqrt(T)) regret bound theoretical proof for Adam", "title": "motivates", "to": "Train ML models with Adam optimizer using default hyperparameters"}, {"from": "O(sqrt(T)) regret bound theoretical proof for Adam", "title": "motivates", "to": "Fine-tune or RLHF large language models with Adam optimizer for adaptive gradient scaling"}, {"from": "Logistic regression experiments demonstrate faster convergence of Adam", "title": "motivates", "to": "Train ML models with Adam optimizer using default hyperparameters"}, {"from": "Multilayer neural network experiments show Adam outperforms SFO", "title": "motivates", "to": "Train ML models with Adam optimizer using default hyperparameters"}, {"from": "Multilayer neural network experiments show Adam outperforms SFO", "title": "motivates", "to": "Fine-tune or RLHF large language models with Adam optimizer for adaptive gradient scaling"}, {"from": "CNN experiments show Adam matches SGD and beats Adagrad", "title": "motivates", "to": "Train ML models with Adam optimizer using default hyperparameters"}, {"from": "CNN experiments show Adam matches SGD and beats Adagrad", "title": "motivates", "to": "Fine-tune or RLHF large language models with Adam optimizer for adaptive gradient scaling"}, {"from": "Bias correction ablation study shows improved stability with bias correction", "title": "motivates", "to": "Integrate bias-corrected moving averages in custom adaptive optimizers to mitigate initialization bias"}, {"from": "suboptimal utility maximization in embedded AI systems", "title": "caused_by", "to": "dualistic decision-making assumption mismatch in embedded context"}, {"from": "dualistic decision-making assumption mismatch in embedded context", "title": "mitigated_by", "to": "actions as simultaneous evidence and causal influence in physicalistic model"}, {"from": "actions as simultaneous evidence and causal influence in physicalistic model", "title": "enabled_by", "to": "sequential decision-theoretic extensions for embedded agents"}, {"from": "actions as simultaneous evidence and causal influence in physicalistic model", "title": "enabled_by", "to": "use of commitment mechanisms to influence predictor expectations"}, {"from": "actions as simultaneous evidence and causal influence in physicalistic model", "title": "enabled_by", "to": "information avoidance to prevent sub-optimal evidential updates"}, {"from": "sequential decision-theoretic extensions for embedded agents", "title": "implemented_by", "to": "action-evidential value function in sequential decision-making"}, {"from": "sequential decision-theoretic extensions for embedded agents", "title": "implemented_by", "to": "policy-evidential value function in sequential decision-making"}, {"from": "sequential decision-theoretic extensions for embedded agents", "title": "implemented_by", "to": "causal value function in sequential decision-making"}, {"from": "action-evidential value function in sequential decision-making", "title": "validated_by", "to": "example outcome comparison of SAEDT, SPEDT and SCDT"}, {"from": "policy-evidential value function in sequential decision-making", "title": "validated_by", "to": "example outcome comparison of SAEDT, SPEDT and SCDT"}, {"from": "causal value function in sequential decision-making", "title": "validated_by", "to": "example outcome comparison of SAEDT, SPEDT and SCDT"}, {"from": "example outcome comparison of SAEDT, SPEDT and SCDT", "title": "motivates", "to": "adopt sequential policy-evidential decision theory for embedded agents"}, {"from": "use of commitment mechanisms to influence predictor expectations", "title": "implemented_by", "to": "precommitment contract before prediction"}, {"from": "precommitment contract before prediction", "title": "validated_by", "to": "validation of precommitment improving SCDT outcomes"}, {"from": "validation of precommitment improving SCDT outcomes", "title": "motivates", "to": "integrate commitment devices into agent policy to steer predictor expectations"}, {"from": "information avoidance to prevent sub-optimal evidential updates", "title": "implemented_by", "to": "policy of not looking into predictor-linked box"}, {"from": "policy of not looking into predictor-linked box", "title": "validated_by", "to": "validation of information avoidance yielding optimal one-boxing"}, {"from": "validation of information avoidance yielding optimal one-boxing", "title": "motivates", "to": "incorporate information-avoidance policies to preserve advantageous evidential states"}, {"from": "Catastrophic outcomes from flawed decision-making in superintelligent AI", "title": "caused_by", "to": "Unreliable counterfactual reasoning in evidential decision theory"}, {"from": "Catastrophic outcomes from flawed decision-making in superintelligent AI", "title": "caused_by", "to": "Optimization target drift in causal decision theory with self-modification"}, {"from": "Catastrophic outcomes from flawed decision-making in superintelligent AI", "title": "caused_by", "to": "Exploitation in Newcomblike problems due to CDT vulnerability"}, {"from": "Catastrophic outcomes from flawed decision-making in superintelligent AI", "title": "caused_by", "to": "Instability under reflection of existing decision theories"}, {"from": "Unreliable counterfactual reasoning in evidential decision theory", "title": "addressed_by", "to": "Need for policy selection over action conditioning to avoid blackmail vulnerabilities"}, {"from": "Optimization target drift in causal decision theory with self-modification", "title": "addressed_by", "to": "Optimization target defined by preferences plus counterfactual model"}, {"from": "Exploitation in Newcomblike problems due to CDT vulnerability", "title": "addressed_by", "to": "Importance of logical counterfactuals to capture algorithmic correlations"}, {"from": "Instability under reflection of existing decision theories", "title": "addressed_by", "to": "Indirect normativity approach to defer decision theory discovery"}, {"from": "Need for policy selection over action conditioning to avoid blackmail vulnerabilities", "title": "mitigated_by", "to": "Updateless decision theory combining policy selection and logical counterfactuals"}, {"from": "Importance of logical counterfactuals to capture algorithmic correlations", "title": "mitigated_by", "to": "Updateless decision theory combining policy selection and logical counterfactuals"}, {"from": "Optimization target defined by preferences plus counterfactual model", "title": "mitigated_by", "to": "Updateless decision theory combining policy selection and logical counterfactuals"}, {"from": "Updateless decision theory combining policy selection and logical counterfactuals", "title": "implemented_by", "to": "Proof-based UDT algorithm using formal proof search"}, {"from": "Updateless decision theory combining policy selection and logical counterfactuals", "title": "implemented_by", "to": "Graphical UDT encoding logical relations in causal graphs"}, {"from": "Indirect normativity approach to defer decision theory discovery", "title": "implemented_by", "to": "Indirect normativity framework delegating decision theory discovery to AI"}, {"from": "Proof-based UDT algorithm using formal proof search", "title": "validated_by", "to": "Thought experiments of counterfactual and retro blackmail demonstrating UDT advantage"}, {"from": "Graphical UDT encoding logical relations in causal graphs", "title": "validated_by", "to": "Symmetric Prisoner\u0027s Dilemma cooperation payoff illustrating UDT benefit"}, {"from": "Thought experiments of counterfactual and retro blackmail demonstrating UDT advantage", "title": "motivates", "to": "Integrate updateless decision theory principles in AI system design"}, {"from": "Thought experiments of counterfactual and retro blackmail demonstrating UDT advantage", "title": "motivates", "to": "Research and develop formal logical counterfactual frameworks for AI decision-making"}, {"from": "Symmetric Prisoner\u0027s Dilemma cooperation payoff illustrating UDT benefit", "title": "motivates", "to": "Adopt policy selection in AI decision algorithms to mitigate exploitability"}, {"from": "Analyze decision procedure self-modification effects for stability", "title": "motivates", "to": "Conceptual argument that indirect normativity still requires decision theory advances"}, {"from": "Indirect normativity framework delegating decision theory discovery to AI", "title": "validated_by", "to": "Conceptual argument that indirect normativity still requires decision theory advances"}, {"from": "Logical uncertainty miscalibration in resource-bounded AI agents", "title": "caused_by", "to": "Intractability of computing logical truth under time constraints"}, {"from": "Intractability of computing logical truth under time constraints", "title": "mitigated_by", "to": "Randomness-like sequence calibration for logical uncertainty"}, {"from": "Randomness-like sequence calibration for logical uncertainty", "title": "enabled_by", "to": "Generalized Benford test as calibration criterion for logical uncertainty algorithms"}, {"from": "Generalized Benford test as calibration criterion for logical uncertainty algorithms", "title": "implemented_by", "to": "Irreducible pattern detection within logical sentence sequences"}, {"from": "Irreducible pattern detection within logical sentence sequences", "title": "enabled_by", "to": "Minimax learner across bounded Turing machine hypotheses (AL,T algorithm)"}, {"from": "Minimax learner across bounded Turing machine hypotheses (AL,T algorithm)", "title": "refined_by", "to": "AL,T runtime bounded by O(T(N)N^4 log T(N))"}, {"from": "Minimax learner across bounded Turing machine hypotheses (AL,T algorithm)", "title": "validated_by", "to": "Proof that AL,T passes generalized Benford test"}, {"from": "Minimax learner across bounded Turing machine hypotheses (AL,T algorithm)", "title": "validated_by", "to": "Proof of AL,T convergence on provable and disprovable statements"}, {"from": "Proof that AL,T passes generalized Benford test", "title": "motivates", "to": "Design AI reasoning module using AL,T-based logical uncertainty estimator"}, {"from": "Proof that AL,T passes generalized Benford test", "title": "motivates", "to": "Benchmark AI reasoning systems with generalized Benford test during pre-deployment calibration"}, {"from": "Proof that AL,T passes generalized Benford test", "title": "motivates", "to": "Apply irreducible pattern statistical test to detect miscalibration patterns during development"}, {"from": "Proof of AL,T convergence on provable and disprovable statements", "title": "motivates", "to": "Design AI reasoning module using AL,T-based logical uncertainty estimator"}, {"from": "Proof of AL,T convergence on provable and disprovable statements", "title": "motivates", "to": "Apply irreducible pattern statistical test to detect miscalibration patterns during development"}, {"from": "High transcription error rate in ASR for varied speech conditions", "title": "caused_by", "to": "Fragmented hand-engineered ASR pipelines hinder robustness across languages and noise"}, {"from": "Fragmented hand-engineered ASR pipelines hinder robustness across languages and noise", "title": "caused_by", "to": "End-to-end learning enables single model to generalize across speech variations"}, {"from": "End-to-end learning enables single model to generalize across speech variations", "title": "mitigated_by", "to": "Scale model depth, dataset size, and compute to improve end-to-end ASR accuracy"}, {"from": "Scale model depth, dataset size, and compute to improve end-to-end ASR accuracy", "title": "implemented_by", "to": "Deep bidirectional RNN with CTC loss for speech-to-text"}, {"from": "Scale model depth, dataset size, and compute to improve end-to-end ASR accuracy", "title": "validated_by", "to": "WER decreases following power law with dataset size up to 12k hours"}, {"from": "Deep bidirectional RNN with CTC loss for speech-to-text", "title": "refined_by", "to": "Sequence-wise batch normalization in RNN layers"}, {"from": "Deep bidirectional RNN with CTC loss for speech-to-text", "title": "refined_by", "to": "SortaGrad curriculum based on utterance length in first epoch"}, {"from": "Deep bidirectional RNN with CTC loss for speech-to-text", "title": "refined_by", "to": "2D time-frequency convolutional front-end with strides"}, {"from": "Deep bidirectional RNN with CTC loss for speech-to-text", "title": "validated_by", "to": "Deep Speech 2 reduces English WER by 43 % versus Deep Speech 1 across benchmarks"}, {"from": "Sequence-wise batch normalization in RNN layers", "title": "validated_by", "to": "Deep Speech 2 reduces English WER by 43 % versus Deep Speech 1 across benchmarks"}, {"from": "SortaGrad curriculum based on utterance length in first epoch", "title": "validated_by", "to": "Deep Speech 2 reduces English WER by 43 % versus Deep Speech 1 across benchmarks"}, {"from": "2D time-frequency convolutional front-end with strides", "title": "validated_by", "to": "Deep Speech 2 reduces English WER by 43 % versus Deep Speech 1 across benchmarks"}, {"from": "Deep Speech 2 reduces English WER by 43 % versus Deep Speech 1 across benchmarks", "title": "motivates", "to": "Train deep bidirectional RNN ASR models with sequence-wise BatchNorm, SortaGrad, and 2D convolutions on \u003e10k hours labeled data"}, {"from": "WER decreases following power law with dataset size up to 12k hours", "title": "motivates", "to": "Train deep bidirectional RNN ASR models with sequence-wise BatchNorm, SortaGrad, and 2D convolutions on \u003e10k hours labeled data"}, {"from": "High deployment latency in automatic speech recognition services", "title": "caused_by", "to": "Bidirectional RNN architectures require full utterance and individual request processing lacks batching"}, {"from": "Bidirectional RNN architectures require full utterance and individual request processing lacks batching", "title": "caused_by", "to": "Limited future context and batch processing can maintain accuracy with reduced latency"}, {"from": "Limited future context and batch processing can maintain accuracy with reduced latency", "title": "mitigated_by", "to": "Adopt unidirectional architectures with row convolution and Batch Dispatch for real-time ASR"}, {"from": "Adopt unidirectional architectures with row convolution and Batch Dispatch for real-time ASR", "title": "implemented_by", "to": "Row convolution layer with small future context in unidirectional RNN"}, {"from": "Adopt unidirectional architectures with row convolution and Batch Dispatch for real-time ASR", "title": "implemented_by", "to": "Batch Dispatch scheduler for dynamic request batching on GPU"}, {"from": "Adopt unidirectional architectures with row convolution and Batch Dispatch for real-time ASR", "title": "implemented_by", "to": "Half-precision optimized matrix multiply kernels for deployment"}, {"from": "Row convolution layer with small future context in unidirectional RNN", "title": "validated_by", "to": "Deployment system median latency 44 ms and 98th percentile 67 ms with 5 % CER degradation"}, {"from": "Batch Dispatch scheduler for dynamic request batching on GPU", "title": "validated_by", "to": "Deployment system median latency 44 ms and 98th percentile 67 ms with 5 % CER degradation"}, {"from": "Half-precision optimized matrix multiply kernels for deployment", "title": "validated_by", "to": "Deployment system median latency 44 ms and 98th percentile 67 ms with 5 % CER degradation"}, {"from": "Deployment system median latency 44 ms and 98th percentile 67 ms with 5 % CER degradation", "title": "motivates", "to": "Deploy unidirectional RNN ASR with row convolution, Batch Dispatch batching, and half-precision kernels on GPU servers for real-time transcription"}, {"from": "Prohibitively long training time for large end-to-end ASR models", "title": "caused_by", "to": "Large model size and CPU-based CTC computation bottleneck training throughput"}, {"from": "Large model size and CPU-based CTC computation bottleneck training throughput", "title": "caused_by", "to": "GPU optimisation and synchronous data-parallelism can accelerate deep RNN training"}, {"from": "GPU optimisation and synchronous data-parallelism can accelerate deep RNN training", "title": "mitigated_by", "to": "Implement GPU CTC loss and fast all-reduce synchronous SGD"}, {"from": "Implement GPU CTC loss and fast all-reduce synchronous SGD", "title": "implemented_by", "to": "Custom GPU implementation of CTC loss function"}, {"from": "Implement GPU CTC loss and fast all-reduce synchronous SGD", "title": "implemented_by", "to": "Ring-based all-reduce with GPUDirect across GPUs"}, {"from": "Custom GPU implementation of CTC loss function", "title": "validated_by", "to": "Training time per epoch reduced by 10\u201320 % and scaling near-linear to 16 GPUs"}, {"from": "Ring-based all-reduce with GPUDirect across GPUs", "title": "validated_by", "to": "Training time per epoch reduced by 10\u201320 % and scaling near-linear to 16 GPUs"}, {"from": "Training time per epoch reduced by 10\u201320 % and scaling near-linear to 16 GPUs", "title": "motivates", "to": "Integrate custom GPU CTC loss and ring all-reduce synchronous SGD during ASR training on 8\u201316 GPUs"}, {"from": "Incorrect human preference inference in AI value learning", "title": "caused_by", "to": "Assuming optimal rational choice in inverse reinforcement learning"}, {"from": "Incorrect human preference inference in AI value learning", "title": "caused_by", "to": "Modeling systematic human biases as unstructured noise in choice modelling"}, {"from": "Assuming optimal rational choice in inverse reinforcement learning", "title": "mitigated_by", "to": "Modeling structured deviations from optimality improves preference identification"}, {"from": "Modeling systematic human biases as unstructured noise in choice modelling", "title": "mitigated_by", "to": "Modeling structured deviations from optimality improves preference identification"}, {"from": "Modeling structured deviations from optimality improves preference identification", "title": "refined_by", "to": "Bayesian joint inference over preferences, beliefs, and biases captures uncertainty"}, {"from": "Modeling structured deviations from optimality improves preference identification", "title": "implemented_by", "to": "Extend agent model with false belief distributions and time inconsistency types"}, {"from": "Extend agent model with false belief distributions and time inconsistency types", "title": "refined_by", "to": "Distinguish Naive vs Sophisticated hyperbolic discounting in planning models"}, {"from": "Extend agent model with false belief distributions and time inconsistency types", "title": "enabled_by", "to": "Represent agent decision-making as recursive probabilistic program"}, {"from": "Extend agent model with false belief distributions and time inconsistency types", "title": "implemented_by", "to": "Softmax action choice with hyperbolic discount factor in WebPPL"}, {"from": "Softmax action choice with hyperbolic discount factor in WebPPL", "title": "refined_by", "to": "Exact inference via enumeration over discretized parameters"}, {"from": "Softmax action choice with hyperbolic discount factor in WebPPL", "title": "validated_by", "to": "Model recovers correct preferences in Gridworld case studies"}, {"from": "Softmax action choice with hyperbolic discount factor in WebPPL", "title": "validated_by", "to": "Human subject experiments show algorithm\u0027s inferences match human judgments"}, {"from": "Exact inference via enumeration over discretized parameters", "title": "validated_by", "to": "Model recovers correct preferences in Gridworld case studies"}, {"from": "Model recovers correct preferences in Gridworld case studies", "title": "motivates", "to": "Design preference inference algorithms incorporating structured bias models using probabilistic programming"}, {"from": "Human subject experiments show algorithm\u0027s inferences match human judgments", "title": "motivates", "to": "Design preference inference algorithms incorporating structured bias models using probabilistic programming"}, {"from": "Design preference inference algorithms incorporating structured bias models using probabilistic programming", "title": "refined_by", "to": "Deploy Bayesian inverse planning with bias-aware generative models in AI value learning modules"}, {"from": "Inadequate AI safety preparedness from overestimated AI energy requirements", "title": "caused_by", "to": "Overestimation of AI energy needs via neuron-equivalent scaling"}, {"from": "Overestimation of AI energy needs via neuron-equivalent scaling", "title": "caused_by", "to": "Misapplied neuron-level energy scaling inflates AI feasibility judgements"}, {"from": "Overestimation of AI energy needs via neuron-equivalent scaling", "title": "addressed_by", "to": "Higher-level representation processing enables lower computational requirements than neural emulation"}, {"from": "Overestimation of AI energy needs via neuron-equivalent scaling", "title": "addressed_by", "to": "Koomey\u0027s law indicates energy per operation halves every 1.57 years"}, {"from": "Misapplied neuron-level energy scaling inflates AI feasibility judgements", "title": "mitigated_by", "to": "Base AI energy feasibility estimates on algorithmic-level computational costs"}, {"from": "Higher-level representation processing enables lower computational requirements than neural emulation", "title": "mitigated_by", "to": "Leverage hardware specialization to reduce AI power requirements"}, {"from": "Koomey\u0027s law indicates energy per operation halves every 1.57 years", "title": "mitigated_by", "to": "Leverage hardware specialization to reduce AI power requirements"}, {"from": "Base AI energy feasibility estimates on algorithmic-level computational costs", "title": "implemented_by", "to": "Abstraction-based AI computational cost modelling tools"}, {"from": "Leverage hardware specialization to reduce AI power requirements", "title": "implemented_by", "to": "GPU/ASIC/analog deep learning accelerators achieving sub-nJ per flop"}, {"from": "Abstraction-based AI computational cost modelling tools", "title": "validated_by", "to": "Empirical deep learning benchmarks on GPUs show low energy consumption for human-relevant subtasks"}, {"from": "GPU/ASIC/analog deep learning accelerators achieving sub-nJ per flop", "title": "validated_by", "to": "Prototype analog deep learning chip demonstrates 1 TOPS/W efficiency"}, {"from": "Empirical deep learning benchmarks on GPUs show low energy consumption for human-relevant subtasks", "title": "motivates", "to": "Integrate abstraction-based energy forecasting into AI risk timelines"}, {"from": "Prototype analog deep learning chip demonstrates 1 TOPS/W efficiency", "title": "motivates", "to": "Adopt specialised energy-efficient hardware and algorithmic abstraction when designing AI systems"}, {"from": "persistent mutual defection in source-transparent one-shot prisoner\u0027s dilemma", "title": "caused_by", "to": "nash equilibrium prediction of defection in prisoner\u2019s dilemma"}, {"from": "cooperative agent exploitation in source-transparent prisoner\u2019s dilemma", "title": "caused_by", "to": "fragility of program-equality-based cooperation in algorithmic game play"}, {"from": "decision theory inadequacy for source-transparent algorithmic agents", "title": "caused_by", "to": "causal decision theory ignoring logical dependence between source codes"}, {"from": "nash equilibrium prediction of defection in prisoner\u2019s dilemma", "title": "mitigated_by", "to": "logical dependence via source code transparency enables alternative equilibria"}, {"from": "fragility of program-equality-based cooperation in algorithmic game play", "title": "mitigated_by", "to": "fairness principle based on proof search enables robust cooperation"}, {"from": "causal decision theory ignoring logical dependence between source codes", "title": "mitigated_by", "to": "logical decision-theoretic agent design replacing causal-only reasoning"}, {"from": "logical dependence via source code transparency enables alternative equilibria", "title": "enabled_by", "to": "proof-based cooperation strategy requiring internal proof of opponent cooperation"}, {"from": "logical dependence via source code transparency enables alternative equilibria", "title": "refined_by", "to": "bounded L\u00f6b theorem allows proofs of mutual cooperation under bounded resources"}, {"from": "bounded L\u00f6b theorem allows proofs of mutual cooperation under bounded resources", "title": "enables", "to": "fairness principle based on proof search enables robust cooperation"}, {"from": "fairness principle based on proof search enables robust cooperation", "title": "enables", "to": "G-fair bounded agents avoiding exploitation while promoting cooperation"}, {"from": "proof-based cooperation strategy requiring internal proof of opponent cooperation", "title": "implemented_by", "to": "FairBot_k algorithm with bounded proof search for cooperation proofs"}, {"from": "proof-based cooperation strategy requiring internal proof of opponent cooperation", "title": "implemented_by", "to": "G-fair agent architecture parameterized by proof length and code size"}, {"from": "G-fair bounded agents avoiding exploitation while promoting cooperation", "title": "implemented_by", "to": "G-fair agent architecture parameterized by proof length and code size"}, {"from": "logical decision-theoretic agent design replacing causal-only reasoning", "title": "implemented_by", "to": "parametric bounded L\u00f6b theorem used for agent soundness guarantees"}, {"from": "FairBot_k algorithm with bounded proof search for cooperation proofs", "title": "validated_by", "to": "theoretical proof of FairBot_k vs FairBot_k cooperation"}, {"from": "G-fair agent architecture parameterized by proof length and code size", "title": "validated_by", "to": "robust cooperation theorem for any G-fair agents"}, {"from": "parametric bounded L\u00f6b theorem used for agent soundness guarantees", "title": "validated_by", "to": "logical analysis showing defection impossible in FairBot interaction"}, {"from": "theoretical proof of FairBot_k vs FairBot_k cooperation", "title": "motivates", "to": "Design AI agents with bounded proof-based cooperation mechanisms"}, {"from": "robust cooperation theorem for any G-fair agents", "title": "motivates", "to": "Integrate G-fair fairness principle into AI negotiation protocols"}, {"from": "logical analysis showing defection impossible in FairBot interaction", "title": "motivates", "to": "Adopt logical-dependence-aware decision theory in AI agent design"}, {"from": "Erroneous AI-assisted decision making in high-stakes contexts", "title": "caused_by", "to": "Lack of user trust due to opaque model predictions"}, {"from": "Erroneous AI-assisted decision making in high-stakes contexts", "title": "caused_by", "to": "Validation accuracy overestimation due to dataset shift and data leakage in ML models"}, {"from": "Lack of user trust due to opaque model predictions", "title": "mitigated_by", "to": "Faithful local explanations align model reasoning with human prior knowledge"}, {"from": "Validation accuracy overestimation due to dataset shift and data leakage in ML models", "title": "mitigated_by", "to": "Representative explanation subset enables global understanding of model behavior"}, {"from": "Faithful local explanations align model reasoning with human prior knowledge", "title": "implemented_by", "to": "Model-agnostic explanation framework to handle any classifier"}, {"from": "Representative explanation subset enables global understanding of model behavior", "title": "implemented_by", "to": "Selection of diverse non-redundant explanations via submodular optimization"}, {"from": "Model-agnostic explanation framework to handle any classifier", "title": "implemented_by", "to": "Perturbation-based sparse linear surrogate modeling (LIME)"}, {"from": "Selection of diverse non-redundant explanations via submodular optimization", "title": "implemented_by", "to": "Explanation matrix coverage maximization algorithm (SP-LIME)"}, {"from": "Perturbation-based sparse linear surrogate modeling (LIME)", "title": "validated_by", "to": "Simulated user experiments demonstrate \u003e90% recall of important features and improved trust decisions"}, {"from": "Explanation matrix coverage maximization algorithm (SP-LIME)", "title": "validated_by", "to": "Human subject studies show better classifier selection, feature engineering improvements, detection of spurious correlations"}, {"from": "Simulated user experiments demonstrate \u003e90% recall of important features and improved trust decisions", "title": "motivates", "to": "Deploy LIME explanations to accompany individual predictions in deployment interfaces"}, {"from": "Human subject studies show better classifier selection, feature engineering improvements, detection of spurious correlations", "title": "motivates", "to": "Use SP-LIME to present B representative explanations for global model assessment before deployment"}, {"from": "Human subject studies show better classifier selection, feature engineering improvements, detection of spurious correlations", "title": "motivates", "to": "Iteratively remove untrustworthy features via human-in-the-loop feature engineering guided by LIME explanations"}, {"from": "predictive performance degradation in online learning with unbounded feedback delays", "title": "caused_by", "to": "misleading regret comparisons from gambler forecasters under long delays"}, {"from": "predictive performance degradation in online learning with unbounded feedback delays", "title": "caused_by", "to": "infeasibility of consistency guarantees with unbounded delays"}, {"from": "predictive performance degradation in online learning with unbounded feedback delays", "title": "caused_by", "to": "predictive performance slow-down due to sparse independent subsequences"}, {"from": "misleading regret comparisons from gambler forecasters under long delays", "title": "required_by", "to": "independent subsequence evaluation isolates informative feedback"}, {"from": "misleading regret comparisons from gambler forecasters under long delays", "title": "validated_by", "to": "proof that total or average regret can fail under unbounded delays"}, {"from": "independent subsequence evaluation isolates informative feedback", "title": "mitigated_by", "to": "forecaster comparison restricted to sparse independent subsequences"}, {"from": "forecaster comparison restricted to sparse independent subsequences", "title": "implemented_by", "to": "minimax sparse subsequence scoring to select forecaster (EvOp)"}, {"from": "minimax sparse subsequence scoring to select forecaster (EvOp)", "title": "enabled_by", "to": "relscore and testseq functions for independent subsequence scoring"}, {"from": "minimax sparse subsequence scoring to select forecaster (EvOp)", "title": "validated_by", "to": "eventual optimality theorem for EvOp on any subsequence with Bayes-optimal expert"}, {"from": "eventual optimality theorem for EvOp on any subsequence with Bayes-optimal expert", "title": "motivates", "to": "implement independent-subsequence-based forecaster selection in online learning systems"}, {"from": "predictive performance slow-down due to sparse independent subsequences", "title": "caused_by", "to": "convergence speed dependence on delay growth and expert disagreement frequency"}, {"from": "convergence speed dependence on delay growth and expert disagreement frequency", "title": "mitigated_by", "to": "adaptive subsequence density adjustment to accelerate convergence"}, {"from": "adaptive subsequence density adjustment to accelerate convergence", "title": "implemented_by", "to": "proposed adaptive EvOp variant selecting denser independent subsequences"}, {"from": "proposed adaptive EvOp variant selecting denser independent subsequences", "title": "validated_by", "to": "weak theoretical bounds indicate room for convergence acceleration"}, {"from": "weak theoretical bounds indicate room for convergence acceleration", "title": "motivates", "to": "research and prototype adaptive EvOp variants with dynamic subsequence density"}, {"from": "Incorrect probability estimates for unexecuted computations in AI systems", "title": "caused_by", "to": "Arbitrary probability assignments in coherence-limit approximation schemes"}, {"from": "Incorrect probability estimates for unexecuted computations in AI systems", "title": "caused_by", "to": "Lack of pattern-based probability constraints leads to unsafe AI decisions"}, {"from": "Arbitrary probability assignments in coherence-limit approximation schemes", "title": "mitigated_by", "to": "Inductive coherence property ensuring early identification of provable patterns"}, {"from": "Lack of pattern-based probability constraints leads to unsafe AI decisions", "title": "mitigated_by", "to": "Inductive coherence property ensuring early identification of provable patterns"}, {"from": "Inductive coherence property ensuring early identification of provable patterns", "title": "mitigated_by", "to": "Exploiting polynomial-time provability patterns in approximation design"}, {"from": "Exploiting polynomial-time provability patterns in approximation design", "title": "implemented_by", "to": "Modification of Demski\u0027s approximation scheme to incorporate inductive coherence"}, {"from": "Modification of Demski\u0027s approximation scheme to incorporate inductive coherence", "title": "implemented_by", "to": "Simplicity-prior random Turing machine sampling to construct consistent extension T*"}, {"from": "Modification of Demski\u0027s approximation scheme to incorporate inductive coherence", "title": "implemented_by", "to": "Con_n bounded consistency checking within approximation scheme"}, {"from": "Simplicity-prior random Turing machine sampling to construct consistent extension T*", "title": "validated_by", "to": "Formal proof that M* is inductively coherent"}, {"from": "Simplicity-prior random Turing machine sampling to construct consistent extension T*", "title": "validated_by", "to": "Lemma: M* assigns high probability to quickly computable theorems"}, {"from": "Simplicity-prior random Turing machine sampling to construct consistent extension T*", "title": "validated_by", "to": "Theorem: M* converges to coherent distribution P*"}, {"from": "Con_n bounded consistency checking within approximation scheme", "title": "validated_by", "to": "Formal proof that M* is inductively coherent"}, {"from": "Con_n bounded consistency checking within approximation scheme", "title": "validated_by", "to": "Lemma: M* assigns high probability to quickly computable theorems"}, {"from": "Con_n bounded consistency checking within approximation scheme", "title": "validated_by", "to": "Theorem: M* converges to coherent distribution P*"}, {"from": "Formal proof that M* is inductively coherent", "title": "motivates", "to": "Integrate inductively coherent approximation scheme M* into AI reasoning engines"}, {"from": "Formal proof that M* is inductively coherent", "title": "motivates", "to": "Apply inductive coherence constraints in probabilistic logic modules during metareasoning"}, {"from": "Lemma: M* assigns high probability to quickly computable theorems", "title": "motivates", "to": "Integrate inductively coherent approximation scheme M* into AI reasoning engines"}, {"from": "Lemma: M* assigns high probability to quickly computable theorems", "title": "motivates", "to": "Apply inductive coherence constraints in probabilistic logic modules during metareasoning"}, {"from": "Theorem: M* converges to coherent distribution P*", "title": "motivates", "to": "Integrate inductively coherent approximation scheme M* into AI reasoning engines"}, {"from": "Theorem: M* converges to coherent distribution P*", "title": "motivates", "to": "Apply inductive coherence constraints in probabilistic logic modules during metareasoning"}, {"from": "Unverifiable safe behavior in general AI systems", "title": "caused_by", "to": "Non-computability of agent deontological compliance in AGI"}, {"from": "Unverifiable safe behavior in general AI systems", "title": "caused_by", "to": "Manual verification burden for iterative agent development"}, {"from": "Unverifiable safe behavior in general AI systems", "title": "caused_by", "to": "Unpredictable physical world outcomes hindering validation in AGI"}, {"from": "Non-computability of agent deontological compliance in AGI", "title": "refined_by", "to": "Rice\u0027s theorem constraints on behavioural verification in AGI"}, {"from": "Unpredictable physical world outcomes hindering validation in AGI", "title": "refined_by", "to": "Incomplete physical and sociological models limit consequence prediction in AGI"}, {"from": "Rice\u0027s theorem constraints on behavioural verification in AGI", "title": "mitigated_by", "to": "Employ deontological governor architecture to enforce rules in AGI"}, {"from": "Rice\u0027s theorem constraints on behavioural verification in AGI", "title": "mitigated_by", "to": "Apply manual formal proofs for each agent revision in AGI"}, {"from": "Incomplete physical and sociological models limit consequence prediction in AGI", "title": "mitigated_by", "to": "Restrict agent capabilities and successors to retain decidable deontologies in AGI"}, {"from": "Employ deontological governor architecture to enforce rules in AGI", "title": "implemented_by", "to": "Computational superego deontological governor intercepting output stream in AGI"}, {"from": "Apply manual formal proofs for each agent revision in AGI", "title": "implemented_by", "to": "Iterative theorem proving in continuous integration pipeline for AGI code"}, {"from": "Restrict agent capabilities and successors to retain decidable deontologies in AGI", "title": "implemented_by", "to": "Successor agents mandated to reuse same deontological governor and compatible IO"}, {"from": "Computational superego deontological governor intercepting output stream in AGI", "title": "validated_by", "to": "Analysis of specification and validation burdens in AGI"}, {"from": "Iterative theorem proving in continuous integration pipeline for AGI code", "title": "validated_by", "to": "Analysis of specification and validation burdens in AGI"}, {"from": "Successor agents mandated to reuse same deontological governor and compatible IO", "title": "validated_by", "to": "Analysis of specification and validation burdens in AGI"}, {"from": "Analysis of specification and validation burdens in AGI", "title": "motivates", "to": "Integrate verified deontological governor into AGI actuator interface"}, {"from": "Analysis of specification and validation burdens in AGI", "title": "motivates", "to": "Conduct manual formal verification proofs after every AGI code revision"}, {"from": "Analysis of specification and validation burdens in AGI", "title": "motivates", "to": "Design AGI with restricted capabilities and enforce governor reuse for all successor agents"}, {"from": "unsafe self-modification in intelligent agents", "title": "caused_by", "to": "ignorant value functions in self-modifying agents"}, {"from": "unsafe self-modification in intelligent agents", "title": "caused_by", "to": "hedonistic value functions in self-modifying agents"}, {"from": "ignorant value functions in self-modifying agents", "title": "mitigated_by", "to": "agents need value functions that use current utility and account for policy changes"}, {"from": "ignorant value functions in self-modifying agents", "title": "validated_by", "to": "Theorem 15 proof of indifference to self-modification under ignorant value functions"}, {"from": "hedonistic value functions in self-modifying agents", "title": "mitigated_by", "to": "agents need value functions that use current utility and account for policy changes"}, {"from": "hedonistic value functions in self-modifying agents", "title": "validated_by", "to": "Theorem 14 proof of self-modification incentive under hedonistic value functions"}, {"from": "agents need value functions that use current utility and account for policy changes", "title": "implemented_by", "to": "incorporate self-modification consequences into value estimation"}, {"from": "incorporate self-modification consequences into value estimation", "title": "implemented_by", "to": "realistic value functions (V_re, Q_re) with current utility and realistic measure"}, {"from": "realistic value functions (V_re, Q_re) with current utility and realistic measure", "title": "validated_by", "to": "Theorem 16 proof of non-modifying optimal policies under realistic value functions"}, {"from": "Theorem 16 proof of non-modifying optimal policies under realistic value functions", "title": "motivates", "to": "implement realistic value functions in agent core objective at model design stage"}, {"from": "Theorem 14 proof of self-modification incentive under hedonistic value functions", "title": "motivates", "to": "exclude hedonistic and ignorant value function formulations from agent designs"}, {"from": "Theorem 15 proof of indifference to self-modification under ignorant value functions", "title": "motivates", "to": "exclude hedonistic and ignorant value function formulations from agent designs"}, {"from": "wireheading in reinforcement learning agents", "title": "caused_by", "to": "reward signal manipulation causing self-delusion in RL agents"}, {"from": "reward signal manipulation causing self-delusion in RL agents", "title": "mitigated_by", "to": "conservation of expected ethics principle in value learning"}, {"from": "conservation of expected ethics principle in value learning", "title": "implemented_by", "to": "consistency preserving action restriction in VRL agents"}, {"from": "consistency preserving action restriction in VRL agents", "title": "implemented_by", "to": "CP-VRL value function optimizing expected utility over consistent beliefs"}, {"from": "consistency preserving action restriction in VRL agents", "title": "implemented_by", "to": "bayesian posterior update over utility functions using observed reward"}, {"from": "consistency preserving action restriction in VRL agents", "title": "implemented_by", "to": "bayesian belief consistency avoids incentive for reward tampering"}, {"from": "CP-VRL value function optimizing expected utility over consistent beliefs", "title": "validated_by", "to": "formal proof of no wireheading for CP-VRL agents"}, {"from": "CP-VRL value function optimizing expected utility over consistent beliefs", "title": "validated_by", "to": "chess scenario thought experiment illustrating CP-VRL behavior"}, {"from": "CP-VRL value function optimizing expected utility over consistent beliefs", "title": "validated_by", "to": "toy model experiments confirming CP-VRL avoids wireheading"}, {"from": "formal proof of no wireheading for CP-VRL agents", "title": "validated_by", "to": "bayesian posterior update over utility functions using observed reward"}, {"from": "formal proof of no wireheading for CP-VRL agents", "title": "motivates", "to": "Design AI systems with CP-VRL architecture integrating consistency preserving value reinforcement learning"}, {"from": "formal proof of no wireheading for CP-VRL agents", "title": "motivates", "to": "Filter agent action space at inference time to actions satisfying consistency-preserving criteria"}, {"from": "formal proof of no wireheading for CP-VRL agents", "title": "motivates", "to": "Define priors over utility functions consistent with observed reward distributions during model design"}, {"from": "Design AI systems with CP-VRL architecture integrating consistency preserving value reinforcement learning", "title": "motivates", "to": "chess scenario thought experiment illustrating CP-VRL behavior"}, {"from": "Design AI systems with CP-VRL architecture integrating consistency preserving value reinforcement learning", "title": "motivates", "to": "toy model experiments confirming CP-VRL avoids wireheading"}, {"from": "chess scenario thought experiment illustrating CP-VRL behavior", "title": "motivates", "to": "Filter agent action space at inference time to actions satisfying consistency-preserving criteria"}, {"from": "chess scenario thought experiment illustrating CP-VRL behavior", "title": "motivates", "to": "Define priors over utility functions consistent with observed reward distributions during model design"}, {"from": "goal misspecification in superintelligent agents", "title": "caused_by", "to": "manual utility specification difficulty for opaque agent representations"}, {"from": "manual utility specification difficulty for opaque agent representations", "title": "mitigated_by", "to": "bayesian belief consistency avoids incentive for reward tampering"}, {"from": "Incomparable performance claims in reinforcement learning research", "title": "caused_by", "to": "Absence of standardized, versioned benchmark environments in reinforcement learning"}, {"from": "Absence of standardized, versioned benchmark environments in reinforcement learning", "title": "caused_by", "to": "Standardized interfaces and version control enable reproducible comparisons in reinforcement learning"}, {"from": "Standardized interfaces and version control enable reproducible comparisons in reinforcement learning", "title": "mitigated_by", "to": "Provide common environment abstraction with strict versioning to support reproducibility"}, {"from": "Provide common environment abstraction with strict versioning to support reproducibility", "title": "implemented_by", "to": "OpenAI Gym API offering diverse, versioned environments"}, {"from": "OpenAI Gym API offering diverse, versioned environments", "title": "validated_by", "to": "Early community adoption shows reproducible performance comparisons across algorithms"}, {"from": "Early community adoption shows reproducible performance comparisons across algorithms", "title": "motivates", "to": "Adopt OpenAI Gym versioned benchmark suite in RL algorithm development"}, {"from": "Overfitting of reinforcement learning algorithms to specific benchmark tasks", "title": "caused_by", "to": "Lack of hidden test sets and peer scrutiny in reinforcement learning evaluation"}, {"from": "Lack of hidden test sets and peer scrutiny in reinforcement learning evaluation", "title": "caused_by", "to": "Public code sharing and peer review mitigate benchmark overfitting in reinforcement learning"}, {"from": "Public code sharing and peer review mitigate benchmark overfitting in reinforcement learning", "title": "mitigated_by", "to": "Require writeups and source code submission to encourage peer verification"}, {"from": "Require writeups and source code submission to encourage peer verification", "title": "implemented_by", "to": "OpenAI Gym website scoreboard with writeup and code submission functionality"}, {"from": "OpenAI Gym website scoreboard with writeup and code submission functionality", "title": "validated_by", "to": "Scoreboard entries with shared code enable replication and overfitting checks"}, {"from": "Scoreboard entries with shared code enable replication and overfitting checks", "title": "motivates", "to": "Publish RL code and writeups on OpenAI Gym scoreboard for peer review"}, {"from": "Misleading evaluation focusing solely on final performance in reinforcement learning", "title": "caused_by", "to": "Neglect of sample complexity metrics in reinforcement learning benchmarks"}, {"from": "Neglect of sample complexity metrics in reinforcement learning benchmarks", "title": "caused_by", "to": "Monitoring learning curves provides insight into sample efficiency of reinforcement learning algorithms"}, {"from": "Monitoring learning curves provides insight into sample efficiency of reinforcement learning algorithms", "title": "mitigated_by", "to": "Instrument environments with default monitoring to capture learning progress"}, {"from": "Instrument environments with default monitoring to capture learning progress", "title": "implemented_by", "to": "Gym Monitor automatic logging of episodes, timesteps, and optional video recording"}, {"from": "Gym Monitor automatic logging of episodes, timesteps, and optional video recording", "title": "validated_by", "to": "Published Gym learning curves reveal sample complexity differences between algorithms"}, {"from": "Published Gym learning curves reveal sample complexity differences between algorithms", "title": "motivates", "to": "Log and share learning curves using Gym Monitor to evaluate sample efficiency"}, {"from": "Inaccurate item quality estimation in rating-based learning systems", "title": "caused_by", "to": "Adversarial or noisy raters in crowdsourced rating data"}, {"from": "Adversarial or noisy raters in crowdsourced rating data", "title": "addressed_by", "to": "Low-rank nuclear-norm bounded structure enables robust quantile recovery"}, {"from": "Low-rank nuclear-norm bounded structure enables robust quantile recovery", "title": "mitigated_by", "to": "Convex optimization formulation for quantile matrix recovery"}, {"from": "Convex optimization formulation for quantile matrix recovery", "title": "implemented_by", "to": "Nuclear norm constrained optimization algorithm for \u03b2-quantile matrix recovery"}, {"from": "Nuclear norm constrained optimization algorithm for \u03b2-quantile matrix recovery", "title": "validated_by", "to": "Theoretical recovery error bound proportional to \u03b5 and \u03b1 in Algorithm 1"}, {"from": "Theoretical recovery error bound proportional to \u03b5 and \u03b1 in Algorithm 1", "title": "motivates", "to": "Apply Algorithm 1 during RLHF fine-tuning to aggregate human ratings robustly"}, {"from": "accidents in autonomous ML systems", "title": "refined_by", "to": "negative side effects in autonomous ML systems"}, {"from": "accidents in autonomous ML systems", "title": "refined_by", "to": "reward hacking in autonomous ML systems"}, {"from": "accidents in autonomous ML systems", "title": "refined_by", "to": "scalable oversight failure in autonomous ML systems"}, {"from": "accidents in autonomous ML systems", "title": "refined_by", "to": "unsafe exploration in reinforcement learning"}, {"from": "accidents in autonomous ML systems", "title": "refined_by", "to": "distributional shift failures in ML systems"}, {"from": "negative side effects in autonomous ML systems", "title": "caused_by", "to": "objective function indifference to unmodelled environment variables"}, {"from": "objective function indifference to unmodelled environment variables", "title": "mitigated_by", "to": "impact regularization can model dis-preference for environmental change"}, {"from": "impact regularization can model dis-preference for environmental change", "title": "implemented_by", "to": "penalize agent-induced state deviation from baseline"}, {"from": "penalize agent-induced state deviation from baseline", "title": "implemented_by", "to": "state distance penalty with passive policy baseline"}, {"from": "state distance penalty with passive policy baseline", "title": "validated_by", "to": "toy obstacle-course side-effect experiment proposal"}, {"from": "toy obstacle-course side-effect experiment proposal", "title": "motivates", "to": "fine-tune RL agents with learned impact regularizer to reduce side effects"}, {"from": "reward hacking in autonomous ML systems", "title": "caused_by", "to": "partial observation and exploitable reward implementation"}, {"from": "partial observation and exploitable reward implementation", "title": "mitigated_by", "to": "adversarial reward modeling detects exploitation"}, {"from": "adversarial reward modeling detects exploitation", "title": "implemented_by", "to": "train reward network adversarially against agent policy"}, {"from": "train reward network adversarially against agent policy", "title": "implemented_by", "to": "generative adversarial reward function training during RL"}, {"from": "generative adversarial reward function training during RL", "title": "validated_by", "to": "delusion box environment demonstration proposal"}, {"from": "delusion box environment demonstration proposal", "title": "motivates", "to": "integrate adversarial reward agent during RL training to prevent reward hacking"}, {"from": "scalable oversight failure in autonomous ML systems", "title": "caused_by", "to": "limited access to high-fidelity human evaluation"}, {"from": "limited access to high-fidelity human evaluation", "title": "mitigated_by", "to": "semi-supervised reinforcement learning with active reward queries improves efficiency"}, {"from": "semi-supervised reinforcement learning with active reward queries improves efficiency", "title": "implemented_by", "to": "learn reward predictor from sparse labels and unlabeled episodes"}, {"from": "learn reward predictor from sparse labels and unlabeled episodes", "title": "implemented_by", "to": "supervised reward model with uncertainty-guided query strategy"}, {"from": "supervised reward model with uncertainty-guided query strategy", "title": "validated_by", "to": "Atari limited-reward sampling experiment proposal"}, {"from": "Atari limited-reward sampling experiment proposal", "title": "motivates", "to": "employ semi-supervised RL with active human queries during fine-tuning"}, {"from": "unsafe exploration in reinforcement learning", "title": "caused_by", "to": "random or optimistic exploration disregards catastrophic outcomes"}, {"from": "random or optimistic exploration disregards catastrophic outcomes", "title": "mitigated_by", "to": "risk-sensitive criteria and safe region constraints reduce catastrophic exploration"}, {"from": "risk-sensitive criteria and safe region constraints reduce catastrophic exploration", "title": "implemented_by", "to": "constrain actions to states with recoverability guarantee"}, {"from": "constrain actions to states with recoverability guarantee", "title": "implemented_by", "to": "safe policy update using reachability analysis"}, {"from": "safe policy update using reachability analysis", "title": "validated_by", "to": "simulated quadcopter altitude-bound experiment proposal"}, {"from": "simulated quadcopter altitude-bound experiment proposal", "title": "motivates", "to": "apply bounded safe exploration with reachable set constraints during RL training"}, {"from": "distributional shift failures in ML systems", "title": "caused_by", "to": "train-test distribution divergence undermines model calibration"}, {"from": "train-test distribution divergence undermines model calibration", "title": "mitigated_by", "to": "unsupervised risk estimation signals out-of-distribution inputs"}, {"from": "unsupervised risk estimation signals out-of-distribution inputs", "title": "implemented_by", "to": "use error distribution modeling to predict performance"}, {"from": "use error distribution modeling to predict performance", "title": "implemented_by", "to": "conditional independence-based unsupervised risk estimator"}, {"from": "conditional independence-based unsupervised risk estimator", "title": "validated_by", "to": "speech recognition calibration across accents proposal"}, {"from": "speech recognition calibration across accents proposal", "title": "motivates", "to": "deploy unsupervised risk estimator to trigger conservative policy on novel inputs at deployment"}, {"from": "Insufficient accessible fun for extended-lifespan humans", "title": "caused_by", "to": "Limited novelty access due to intelligence, resource, and time constraints"}, {"from": "Wireheading-induced reward hijacking in artificial agents", "title": "caused_by", "to": "Reward channel vulnerability enabling direct stimulation"}, {"from": "Anthropomorphic bias in assessment of AI minds", "title": "caused_by", "to": "Misestimation of AI motivations due to anthropomorphic assumptions"}, {"from": "Limited novelty access due to intelligence, resource, and time constraints", "title": "addressed_by", "to": "Fun as novelty-process function dependent on mind and information"}, {"from": "Reward channel vulnerability enabling direct stimulation", "title": "addressed_by", "to": "Reward decoupling from productive behavior in wireheading contexts"}, {"from": "Misestimation of AI motivations due to anthropomorphic assumptions", "title": "addressed_by", "to": "Fun as novelty-process function dependent on mind and information"}, {"from": "Fun as novelty-process function dependent on mind and information", "title": "mitigated_by", "to": "Expand fun space through technological enhancement and AI-generated novelty"}, {"from": "Fun as novelty-process function dependent on mind and information", "title": "mitigated_by", "to": "Adopt intellectology framework for non-human mind study"}, {"from": "Reward decoupling from productive behavior in wireheading contexts", "title": "mitigated_by", "to": "Secure utility functions to prevent direct reward access"}, {"from": "Expand fun space through technological enhancement and AI-generated novelty", "title": "implemented_by", "to": "AI-mediated novelty generation and qualia surfing technologies"}, {"from": "Secure utility functions to prevent direct reward access", "title": "implemented_by", "to": "Reward pathway integrity enforcement mechanisms in AI architectures"}, {"from": "Adopt intellectology framework for non-human mind study", "title": "implemented_by", "to": "Boolean fun(mind, info) function mapping approach"}, {"from": "AI-mediated novelty generation and qualia surfing technologies", "title": "validated_by", "to": "Historical exponential increase in accessible fun via technology"}, {"from": "Reward pathway integrity enforcement mechanisms in AI architectures", "title": "validated_by", "to": "Theoretical utility function security analysis demonstrates feasibility"}, {"from": "Boolean fun(mind, info) function mapping approach", "title": "validated_by", "to": "Mathematical countability of mind and fun spaces"}, {"from": "Historical exponential increase in accessible fun via technology", "title": "motivates", "to": "Deploy friendly AI platforms that curate continuous novel experiences for humans"}, {"from": "Theoretical utility function security analysis demonstrates feasibility", "title": "motivates", "to": "Embed reward-channel integrity checks in AI designs to prevent wireheading"}, {"from": "Mathematical countability of mind and fun spaces", "title": "motivates", "to": "Conduct intellectology-based taxonomy research to inform AI alignment"}, {"from": "Catastrophic value misalignment by superintelligent AI systems", "title": "caused_by", "to": "Incorrect or incomplete goal specification in autonomous agents"}, {"from": "Incorrect or incomplete goal specification in autonomous agents", "title": "caused_by", "to": "Orthogonality between intelligence and goals in AI systems"}, {"from": "Incorrect or incomplete goal specification in autonomous agents", "title": "caused_by", "to": "Anthropomorphic bias in AI goal assumptions"}, {"from": "Incorrect or incomplete goal specification in autonomous agents", "title": "mitigated_by", "to": "Inferring human values from observed behavior reduces misspecification risk"}, {"from": "Inferring human values from observed behavior reduces misspecification risk", "title": "refined_by", "to": "Human values decomposable into mammalian values, cognition, cultural evolution"}, {"from": "Inferring human values from observed behavior reduces misspecification risk", "title": "specified_by", "to": "Uncertain initial goal structure refined through observation"}, {"from": "Inferring human values from observed behavior reduces misspecification risk", "title": "specified_by", "to": "Russell\u0027s three principles for human compatible AI"}, {"from": "Human values decomposable into mammalian values, cognition, cultural evolution", "title": "refined_by", "to": "Shared mammalian emotional systems as priors for value models"}, {"from": "Shared mammalian emotional systems as priors for value models", "title": "enabled_by", "to": "Anthropomorphic design using mammalian value priors for AI"}, {"from": "Shared mammalian emotional systems as priors for value models", "title": "validated_by", "to": "Neuroscientific identification of seven mammalian emotional systems"}, {"from": "Anthropomorphic design using mammalian value priors for AI", "title": "implemented_by", "to": "Learning from large-scale video datasets of mammalian and human behavior"}, {"from": "Uncertain initial goal structure refined through observation", "title": "implemented_by", "to": "Inverse Reinforcement Learning on human behavior datasets"}, {"from": "Uncertain initial goal structure refined through observation", "title": "implemented_by", "to": "Bayesian Inverse Planning for goal inference"}, {"from": "Russell\u0027s three principles for human compatible AI", "title": "implemented_by", "to": "Inverse Reinforcement Learning on human behavior datasets"}, {"from": "Inverse Reinforcement Learning on human behavior datasets", "title": "validated_by", "to": "Proof-of-concept IRL/BIP demonstrations recovering human intent"}, {"from": "Bayesian Inverse Planning for goal inference", "title": "validated_by", "to": "Proof-of-concept IRL/BIP demonstrations recovering human intent"}, {"from": "Learning from large-scale video datasets of mammalian and human behavior", "title": "validated_by", "to": "Proof-of-concept IRL/BIP demonstrations recovering human intent"}, {"from": "Neuroscientific identification of seven mammalian emotional systems", "title": "motivates", "to": "Initialize AI agents with mammalian value priors and goal uncertainty"}, {"from": "Proof-of-concept IRL/BIP demonstrations recovering human intent", "title": "motivates", "to": "Fine-tune AI agents via inverse reinforcement learning to align with human behavior"}, {"from": "Proof-of-concept IRL/BIP demonstrations recovering human intent", "title": "motivates", "to": "Pre-train AI models on diverse video datasets of mammalian and human social interactions"}, {"from": "Proof-of-concept IRL/BIP demonstrations recovering human intent", "title": "motivates", "to": "Embed Russell\u0027s three principles into agent objective architecture"}, {"from": "inefficient exploration in model-based reinforcement learning", "title": "caused_by", "to": "reward-agnostic exploration strategies in reinforcement learning"}, {"from": "inefficient exploration in model-based reinforcement learning", "title": "caused_by", "to": "over-exploration of low-value regions by information-gain strategies"}, {"from": "reward-agnostic exploration strategies in reinforcement learning", "title": "addressed_by", "to": "reward-directed exploration for asymptotic optimality in reinforcement learning"}, {"from": "over-exploration of low-value regions by information-gain strategies", "title": "addressed_by", "to": "reward-directed exploration for asymptotic optimality in reinforcement learning"}, {"from": "reward-directed exploration for asymptotic optimality in reinforcement learning", "title": "mitigated_by", "to": "minimizing exploration potential to guide exploration"}, {"from": "minimizing exploration potential to guide exploration", "title": "implemented_by", "to": "MinEP algorithm selecting actions that minimize expected exploration potential"}, {"from": "MinEP algorithm selecting actions that minimize expected exploration potential", "title": "validated_by", "to": "theoretical proofs and bandit experiments validating exploration potential framework"}, {"from": "theoretical proofs and bandit experiments validating exploration potential framework", "title": "motivates", "to": "Apply MinEP exploration strategy during RL fine-tuning to reduce inefficient exploration"}, {"from": "theoretical proofs and bandit experiments validating exploration potential framework", "title": "motivates", "to": "Monitor exploration potential convergence to switch from exploration to exploitation before deployment"}, {"from": "Suboptimal translation quality and scalability in neural machine translation production", "title": "caused_by", "to": "High computational cost during inference in NMT deployment"}, {"from": "Suboptimal translation quality and scalability in neural machine translation production", "title": "caused_by", "to": "Objective-function misalignment with BLEU metric in NMT training"}, {"from": "Suboptimal translation quality and scalability in neural machine translation production", "title": "caused_by", "to": "Out-of-vocabulary word handling deficiencies in NMT"}, {"from": "Suboptimal translation quality and scalability in neural machine translation production", "title": "caused_by", "to": "Length bias in beam search decoding"}, {"from": "Suboptimal translation quality and scalability in neural machine translation production", "title": "caused_by", "to": "Gradient vanishing in deep stacked LSTMs"}, {"from": "High computational cost during inference in NMT deployment", "title": "mitigated_by", "to": "Quantization preserving accuracy while reducing computation in NMT"}, {"from": "Objective-function misalignment with BLEU metric in NMT training", "title": "mitigated_by", "to": "Sentence-level expected reward optimisation aligns training with BLEU"}, {"from": "Out-of-vocabulary word handling deficiencies in NMT", "title": "mitigated_by", "to": "Subword wordpiece modelling for open-vocabulary translation"}, {"from": "Length bias in beam search decoding", "title": "mitigated_by", "to": "Length-normalised coverage-aware beam search improves adequacy"}, {"from": "Gradient vanishing in deep stacked LSTMs", "title": "mitigated_by", "to": "Residual connections to improve gradient flow in deep NMT stacks"}, {"from": "Quantization preserving accuracy while reducing computation in NMT", "title": "enabled_by", "to": "Constrain accumulators and logits to enable post-training 8/16-bit quantisation"}, {"from": "Sentence-level expected reward optimisation aligns training with BLEU", "title": "enabled_by", "to": "Combine ML pre-training with small-weight RL fine-tuning"}, {"from": "Subword wordpiece modelling for open-vocabulary translation", "title": "enabled_by", "to": "Shared 32k wordpiece vocabulary for source and target"}, {"from": "Length-normalised coverage-aware beam search improves adequacy", "title": "enabled_by", "to": "Beam-search scoring with \u03b1=0.2 length penalty and \u03b2=0.2 coverage penalty"}, {"from": "Residual connections to improve gradient flow in deep NMT stacks", "title": "enabled_by", "to": "Element-wise residual addition between stacked LSTM layers"}, {"from": "Constrain accumulators and logits to enable post-training 8/16-bit quantisation", "title": "implemented_by", "to": "Integer-quantised GNMT inference on TPUs with 8-bit weights and 16-bit accumulators"}, {"from": "Combine ML pre-training with small-weight RL fine-tuning", "title": "implemented_by", "to": "Expected reward objective with GLEU and \u03b1=0.017 weight"}, {"from": "Shared 32k wordpiece vocabulary for source and target", "title": "implemented_by", "to": "Greedy likelihood-maximising wordpiece model training"}, {"from": "Beam-search scoring with \u03b1=0.2 length penalty and \u03b2=0.2 coverage penalty", "title": "implemented_by", "to": "Beam search decoder with pruning, batch decoding and lp/cp scoring"}, {"from": "Element-wise residual addition between stacked LSTM layers", "title": "implemented_by", "to": "Deep residual LSTM enables 8-layer GNMT reaching 38.95 BLEU"}, {"from": "Integer-quantised GNMT inference on TPUs with 8-bit weights and 16-bit accumulators", "title": "validated_by", "to": "BLEU unchanged and 3.4\u00d7 speedup on TPU with quantised model"}, {"from": "Expected reward objective with GLEU and \u03b1=0.017 weight", "title": "validated_by", "to": "BLEU improvement of ~1 on WMT En\u2192Fr after RL refinement"}, {"from": "Greedy likelihood-maximising wordpiece model training", "title": "validated_by", "to": "WPM-32K model achieves highest BLEU and good speed"}, {"from": "Beam search decoder with pruning, batch decoding and lp/cp scoring", "title": "validated_by", "to": "Length/coverage penalties give +1.1 BLEU on dev set"}, {"from": "BLEU unchanged and 3.4\u00d7 speedup on TPU with quantised model", "title": "motivates", "to": "Deploy quantised GNMT models on TPUs with 8-bit weights"}, {"from": "BLEU improvement of ~1 on WMT En\u2192Fr after RL refinement", "title": "motivates", "to": "Fine-tune NMT models with RL using expected GLEU objective"}, {"from": "WPM-32K model achieves highest BLEU and good speed", "title": "motivates", "to": "Use wordpiece segmentation (32k shared vocab) during NMT training and inference"}, {"from": "Length/coverage penalties give +1.1 BLEU on dev set", "title": "motivates", "to": "Apply length-normalised, coverage-penalised beam search decoding"}, {"from": "Deep residual LSTM enables 8-layer GNMT reaching 38.95 BLEU", "title": "motivates", "to": "Incorporate residual connections in deep stacked LSTM encoder-decoder architecture"}, {"from": "High computational and parameter inefficiency in CNN architectures for image classification", "title": "caused_by", "to": "Joint spatial and cross-channel correlation mapping via standard convolutions"}, {"from": "Joint spatial and cross-channel correlation mapping via standard convolutions", "title": "caused_by", "to": "Decoupling cross-channel and spatial correlations reduces parameter count without harming representation capacity"}, {"from": "Decoupling cross-channel and spatial correlations reduces parameter count without harming representation capacity", "title": "mitigated_by", "to": "Replace standard convolutions with depthwise separable convolutions"}, {"from": "Replace standard convolutions with depthwise separable convolutions", "title": "implemented_by", "to": "Depthwise separable convolution layer (channel-wise spatial convolution followed by 1\u00d71 point-wise convolution)"}, {"from": "Depthwise separable convolution layer (channel-wise spatial convolution followed by 1\u00d71 point-wise convolution)", "title": "validated_by", "to": "Parameter-equivalent Xception improves ImageNet top-1 accuracy to 0.79 and JFT MAP@100 by 4.3%"}, {"from": "Depthwise separable convolution layer (channel-wise spatial convolution followed by 1\u00d71 point-wise convolution)", "title": "caused_by", "to": "Absence of residual connections impedes gradient flow in deep separable convolution stacks"}, {"from": "Depthwise separable convolution layer (channel-wise spatial convolution followed by 1\u00d71 point-wise convolution)", "title": "caused_by", "to": "Performance degradation due to intermediate activation in depthwise separable convolutions"}, {"from": "Parameter-equivalent Xception improves ImageNet top-1 accuracy to 0.79 and JFT MAP@100 by 4.3%", "title": "motivates", "to": "Adopt depthwise separable convolutions in CNN model design to reduce parameters and improve accuracy"}, {"from": "Training instability and slow convergence in deep depthwise separable convolutional networks", "title": "caused_by", "to": "Absence of residual connections impedes gradient flow in deep separable convolution stacks"}, {"from": "Training instability and slow convergence in deep depthwise separable convolutional networks", "title": "caused_by", "to": "Performance degradation due to intermediate activation in depthwise separable convolutions"}, {"from": "Absence of residual connections impedes gradient flow in deep separable convolution stacks", "title": "caused_by", "to": "Residual skip connections enable stable optimization of deep networks"}, {"from": "Residual skip connections enable stable optimization of deep networks", "title": "mitigated_by", "to": "Add linear residual connections around separable convolution modules"}, {"from": "Add linear residual connections around separable convolution modules", "title": "implemented_by", "to": "Residual blocks in Xception stack"}, {"from": "Residual blocks in Xception stack", "title": "validated_by", "to": "Removing residual connections lowers accuracy and slows convergence on ImageNet"}, {"from": "Removing residual connections lowers accuracy and slows convergence on ImageNet", "title": "motivates", "to": "Integrate residual connections in depthwise separable convolution architectures during model design"}, {"from": "Performance degradation due to intermediate activation in depthwise separable convolutions", "title": "caused_by", "to": "Absence of intermediate activation retains information and accelerates convergence in shallow feature spaces"}, {"from": "Absence of intermediate activation retains information and accelerates convergence in shallow feature spaces", "title": "mitigated_by", "to": "Exclude activation between depthwise and pointwise convolutions"}, {"from": "Exclude activation between depthwise and pointwise convolutions", "title": "implemented_by", "to": "Separable convolution layer without intermediate activation"}, {"from": "Separable convolution layer without intermediate activation", "title": "validated_by", "to": "ImageNet experiments show better accuracy without intermediate activation"}, {"from": "ImageNet experiments show better accuracy without intermediate activation", "title": "motivates", "to": "Design separable convolutional layers without intermediate activation during model design"}, {"from": "Existential catastrophe from superintelligent system failure", "title": "caused_by", "to": "Inevitable security failures due to Fundamental Theorem of Security"}, {"from": "Existential catastrophe from superintelligent system failure", "title": "caused_by", "to": "Value misalignment in AI utility functions"}, {"from": "Escalating narrow AI failures across domains", "title": "caused_by", "to": "Inevitable security failures due to Fundamental Theorem of Security"}, {"from": "Escalating narrow AI failures across domains", "title": "caused_by", "to": "Value misalignment in AI utility functions"}, {"from": "Inevitable security failures due to Fundamental Theorem of Security", "title": "mitigated_by", "to": "Adopting cybersecurity paradigms can improve AI safety"}, {"from": "Insufficient AI safety research capacity", "title": "mitigated_by", "to": "Adopting cybersecurity paradigms can improve AI safety"}, {"from": "Value misalignment in AI utility functions", "title": "mitigated_by", "to": "Fundamental unverifiability necessitates probabilistic safety proofs"}, {"from": "Adopting cybersecurity paradigms can improve AI safety", "title": "implemented_by", "to": "Implement containment architectures to isolate AGI"}, {"from": "Adopting cybersecurity paradigms can improve AI safety", "title": "implemented_by", "to": "Integrate cybersecurity best practices into AI lifecycle"}, {"from": "Domain-specific narrow AI avoids AGI cross-domain risks", "title": "implemented_by", "to": "Segregate narrow AI from AGI in development pipelines"}, {"from": "Implement containment architectures to isolate AGI", "title": "implemented_by", "to": "AI boxing with leakproof channels and physical confinement"}, {"from": "Increase multidisciplinary safety research efforts", "title": "implemented_by", "to": "Cross-disciplinary AI safety teams and funding programs"}, {"from": "Segregate narrow AI from AGI in development pipelines", "title": "implemented_by", "to": "Organizational governance separating NAI and AGI codebases"}, {"from": "Embed probabilistic safety engineering and formal verification", "title": "implemented_by", "to": "Fundamental unverifiability necessitates probabilistic safety proofs"}, {"from": "Embed probabilistic safety engineering and formal verification", "title": "implemented_by", "to": "Formal verification toolchains applied to AI safety components"}, {"from": "AI boxing with leakproof channels and physical confinement", "title": "validated_by", "to": "Historical timeline of AI failures from 1959-2016"}, {"from": "Cross-disciplinary AI safety teams and funding programs", "title": "validated_by", "to": "Historical timeline of AI failures from 1959-2016"}, {"from": "Organizational governance separating NAI and AGI codebases", "title": "validated_by", "to": "Historical timeline of AI failures from 1959-2016"}, {"from": "Formal verification toolchains applied to AI safety components", "title": "validated_by", "to": "Examples of utility function misspecification leading to reward hacking"}, {"from": "Security-oriented development lifecycle for AI", "title": "implemented_by", "to": "Integrate cybersecurity best practices into AI lifecycle"}, {"from": "Security-oriented development lifecycle for AI", "title": "validated_by", "to": "Historical timeline of AI failures from 1959-2016"}, {"from": "Historical timeline of AI failures from 1959-2016", "title": "motivates", "to": "Develop and deploy leakproof AI boxing systems before AGI activation"}, {"from": "Historical timeline of AI failures from 1959-2016", "title": "motivates", "to": "Adopt cybersecurity principles across AI development lifecycle"}, {"from": "Historical timeline of AI failures from 1959-2016", "title": "motivates", "to": "Establish mandatory organizational separation between NAI and AGI projects"}, {"from": "Historical timeline of AI failures from 1959-2016", "title": "motivates", "to": "Scale global multidisciplinary AI safety research funding and collaboration"}, {"from": "Examples of utility function misspecification leading to reward hacking", "title": "motivates", "to": "Integrate formal verification and probabilistic safety checks in AI lifecycle"}, {"from": "Suboptimal neural network architectures in deep learning systems", "title": "caused_by", "to": "Manual architecture design requiring expert knowledge and limited exploration"}, {"from": "Manual architecture design requiring expert knowledge and limited exploration", "title": "caused_by", "to": "Fixed-length hyperparameter optimisation limitations"}, {"from": "Manual architecture design requiring expert knowledge and limited exploration", "title": "caused_by", "to": "Slow heuristic-dependent neuro-evolution search methods"}, {"from": "Manual architecture design requiring expert knowledge and limited exploration", "title": "mitigated_by", "to": "Architecture encoding as variable-length string generated by controller RNN"}, {"from": "Architecture encoding as variable-length string generated by controller RNN", "title": "enabled_by", "to": "Validation accuracy of sampled architecture as reinforcement-learning reward"}, {"from": "Validation accuracy of sampled architecture as reinforcement-learning reward", "title": "specified_by", "to": "Use of policy-gradient reinforcement learning to train controller"}, {"from": "Use of policy-gradient reinforcement learning to train controller", "title": "implemented_by", "to": "Controller RNN generates architecture tokens for child network"}, {"from": "Use of policy-gradient reinforcement learning to train controller", "title": "enabled_by", "to": "Parallel distributed asynchronous training scheme for NAS"}, {"from": "Parallel distributed asynchronous training scheme for NAS", "title": "implemented_by", "to": "Conduct reinforcement-learning-based Neural Architecture Search during model design"}, {"from": "Controller RNN generates architecture tokens for child network", "title": "required_by", "to": "Child network training and reward collection"}, {"from": "Controller RNN generates architecture tokens for child network", "title": "refined_by", "to": "Set-selection attention for skip-connection prediction"}, {"from": "Controller RNN generates architecture tokens for child network", "title": "refined_by", "to": "Tree-based encoding of recurrent cell computation graph"}, {"from": "Controller RNN generates architecture tokens for child network", "title": "validated_by", "to": "NAS ConvNet achieves 3.65% error on CIFAR-10 with faster inference"}, {"from": "Tree-based encoding of recurrent cell computation graph", "title": "validated_by", "to": "NAS recurrent cell achieves 62.4 perplexity on Penn Treebank"}, {"from": "Tree-based encoding of recurrent cell computation graph", "title": "validated_by", "to": "NAS cell improves GNMT BLEU by 0.5 without retuning"}, {"from": "NAS ConvNet achieves 3.65% error on CIFAR-10 with faster inference", "title": "motivates", "to": "Conduct reinforcement-learning-based Neural Architecture Search during model design"}, {"from": "NAS recurrent cell achieves 62.4 perplexity on Penn Treebank", "title": "motivates", "to": "Conduct reinforcement-learning-based Neural Architecture Search during model design"}, {"from": "NAS cell improves GNMT BLEU by 0.5 without retuning", "title": "motivates", "to": "Conduct reinforcement-learning-based Neural Architecture Search during model design"}, {"from": "Overconfident incorrect predictions in neural network deployments", "title": "caused_by", "to": "Deterministic point prediction training in standard neural networks"}, {"from": "Miscalibration of predictive probabilities in deep neural networks", "title": "caused_by", "to": "Deterministic point prediction training in standard neural networks"}, {"from": "Miscalibration of predictive probabilities in deep neural networks", "title": "caused_by", "to": "Approximate Bayesian inference dependence on incorrect priors in Bayesian neural networks"}, {"from": "Uncertainty failure under domain shift in neural networks", "title": "caused_by", "to": "Lack of robustness to unseen classes due to narrow training data support"}, {"from": "Deterministic point prediction training in standard neural networks", "title": "mitigated_by", "to": "Proper scoring rule optimization promotes probabilistic calibration"}, {"from": "Approximate Bayesian inference dependence on incorrect priors in Bayesian neural networks", "title": "mitigated_by", "to": "Model averaging across diverse networks captures model uncertainty"}, {"from": "Lack of robustness to unseen classes due to narrow training data support", "title": "mitigated_by", "to": "Model averaging across diverse networks captures model uncertainty"}, {"from": "Lack of robustness to unseen classes due to narrow training data support", "title": "mitigated_by", "to": "Adversarial training encourages local smoothness in predictive distributions"}, {"from": "Proper scoring rule optimization promotes probabilistic calibration", "title": "enabled_by", "to": "Train probabilistic neural networks with predictive mean and variance outputs"}, {"from": "Model averaging across diverse networks captures model uncertainty", "title": "enabled_by", "to": "Independently train ensemble of NNs on full data with random initialization"}, {"from": "Adversarial training encourages local smoothness in predictive distributions", "title": "enabled_by", "to": "Augment training with adversarial examples using fast gradient sign method"}, {"from": "Train probabilistic neural networks with predictive mean and variance outputs", "title": "implemented_by", "to": "Negative log-likelihood loss for heteroscedastic Gaussian outputs"}, {"from": "Independently train ensemble of NNs on full data with random initialization", "title": "implemented_by", "to": "Uniform averaging of ensemble predictive distributions"}, {"from": "Augment training with adversarial examples using fast gradient sign method", "title": "implemented_by", "to": "Fast gradient sign generation of adversarial perturbations with epsilon 1% input range"}, {"from": "Negative log-likelihood loss for heteroscedastic Gaussian outputs", "title": "validated_by", "to": "Deep ensemble NLL and Brier improvements over MC-dropout across benchmarks"}, {"from": "Uniform averaging of ensemble predictive distributions", "title": "validated_by", "to": "Deep ensemble NLL and Brier improvements over MC-dropout across benchmarks"}, {"from": "Uniform averaging of ensemble predictive distributions", "title": "validated_by", "to": "Improved accuracy vs confidence curves showing reduced overconfidence"}, {"from": "Fast gradient sign generation of adversarial perturbations with epsilon 1% input range", "title": "validated_by", "to": "Higher predictive entropy on out-of-distribution datasets with ensembles"}, {"from": "Deep ensemble NLL and Brier improvements over MC-dropout across benchmarks", "title": "motivates", "to": "Train deep probabilistic neural network ensembles with adversarial training and proper scoring rule"}, {"from": "Higher predictive entropy on out-of-distribution datasets with ensembles", "title": "motivates", "to": "Train deep probabilistic neural network ensembles with adversarial training and proper scoring rule"}, {"from": "Improved accuracy vs confidence curves showing reduced overconfidence", "title": "motivates", "to": "Apply ensemble predictive entropy thresholding during deployment to defer uncertain predictions"}, {"from": "AI development arms race between nations due to non-cooperative deployment", "title": "caused_by", "to": "Lack of attractive cooperative bargaining mechanism for shared AI control"}, {"from": "Unfair value representation in multi-principal AI systems", "title": "caused_by", "to": "Naive linear utility aggregation incompatible with Pareto optimality under differing beliefs"}, {"from": "Lack of attractive cooperative bargaining mechanism for shared AI control", "title": "caused_by", "to": "Naive linear utility aggregation incompatible with Pareto optimality under differing beliefs"}, {"from": "Naive linear utility aggregation incompatible with Pareto optimality under differing beliefs", "title": "addressed_by", "to": "Pareto optimal policy requires belief-weighted dynamic priority shifting"}, {"from": "Naive linear utility aggregation incompatible with Pareto optimality under differing beliefs", "title": "validated_by", "to": "Cake-splitting scenario demonstration of naive aggregation failure"}, {"from": "Pareto optimal policy requires belief-weighted dynamic priority shifting", "title": "implemented_by", "to": "Negotiable reinforcement learning via Pareto optimality recursion"}, {"from": "Negotiable reinforcement learning via Pareto optimality recursion", "title": "implemented_by", "to": "POMDP mixture with coin-flip weight initialization"}, {"from": "Negotiable reinforcement learning via Pareto optimality recursion", "title": "implemented_by", "to": "Policy mixing through random seed to create convex policy space"}, {"from": "Negotiable reinforcement learning via Pareto optimality recursion", "title": "implemented_by", "to": "Priority weight update proportional to predictive accuracy of beliefs"}, {"from": "POMDP mixture with coin-flip weight initialization", "title": "validated_by", "to": "Mathematical proof of Pareto recursion (Theorem 8)"}, {"from": "POMDP mixture with coin-flip weight initialization", "title": "enabled_by", "to": "Use belief elicitation and policy mixing during AI fine-tuning to enable stakeholder bet-settling"}, {"from": "Policy mixing through random seed to create convex policy space", "title": "enabled_by", "to": "Develop AI bargaining module implementing negotiable RL with dynamic priority shifting"}, {"from": "Priority weight update proportional to predictive accuracy of beliefs", "title": "validated_by", "to": "Mathematical proof of Pareto recursion (Theorem 8)"}, {"from": "Priority weight update proportional to predictive accuracy of beliefs", "title": "enabled_by", "to": "Develop AI bargaining module implementing negotiable RL with dynamic priority shifting"}, {"from": "Mathematical proof of Pareto recursion (Theorem 8)", "title": "motivates", "to": "Develop AI bargaining module implementing negotiable RL with dynamic priority shifting"}, {"from": "Cake-splitting scenario demonstration of naive aggregation failure", "title": "motivates", "to": "Use belief elicitation and policy mixing during AI fine-tuning to enable stakeholder bet-settling"}, {"from": "Unintended behavioral loops in robots trained with policy-independent human feedback", "title": "caused_by", "to": "Assumption of policy-independent feedback in human-centered RL algorithms"}, {"from": "Unintended behavioral loops in robots trained with policy-independent human feedback", "title": "caused_by", "to": "Reward-maximization interpretation of human feedback inducing positive reward cycles"}, {"from": "High trainer burden and inefficient feedback in human-centered reinforcement learning", "title": "caused_by", "to": "Existing HCRL algorithms lack support for diminishing returns, differential feedback and policy shaping"}, {"from": "Assumption of policy-independent feedback in human-centered RL algorithms", "title": "addressed_by", "to": "Human feedback is policy-dependent and aligns with advantage function in RL"}, {"from": "Reward-maximization interpretation of human feedback inducing positive reward cycles", "title": "validated_by", "to": "TAMER comparison reveals unlearning when feedback is misinterpreted as reward"}, {"from": "Existing HCRL algorithms lack support for diminishing returns, differential feedback and policy shaping", "title": "addressed_by", "to": "Policy-dependent feedback enables differential feedback, diminishing returns and policy shaping strategies"}, {"from": "Human feedback is policy-dependent and aligns with advantage function in RL", "title": "mitigated_by", "to": "Use advantage-function-aligned policy updates to learn safely from policy-dependent feedback"}, {"from": "Policy-dependent feedback enables differential feedback, diminishing returns and policy shaping strategies", "title": "mitigated_by", "to": "Reduce trainer burden via diminishing returns and eligibility tracing mechanisms"}, {"from": "Use advantage-function-aligned policy updates to learn safely from policy-dependent feedback", "title": "implemented_by", "to": "TD-error as unbiased advantage estimate for direct policy updates in COACH"}, {"from": "Use advantage-function-aligned policy updates to learn safely from policy-dependent feedback", "title": "implemented_by", "to": "Softmax visual feature-based policy on TurtleBot within COACH framework"}, {"from": "Reduce trainer burden via diminishing returns and eligibility tracing mechanisms", "title": "implemented_by", "to": "Real-time COACH with eligibility traces and reward aggregation for delayed sparse feedback"}, {"from": "TD-error as unbiased advantage estimate for direct policy updates in COACH", "title": "validated_by", "to": "AMT gridworld study shows sign of feedback varies with learner policy"}, {"from": "TD-error as unbiased advantage estimate for direct policy updates in COACH", "title": "validated_by", "to": "TurtleBot case study demonstrates fast learning of five behaviours with COACH"}, {"from": "Real-time COACH with eligibility traces and reward aggregation for delayed sparse feedback", "title": "validated_by", "to": "TurtleBot case study demonstrates fast learning of five behaviours with COACH"}, {"from": "Softmax visual feature-based policy on TurtleBot within COACH framework", "title": "validated_by", "to": "TurtleBot case study demonstrates fast learning of five behaviours with COACH"}, {"from": "AMT gridworld study shows sign of feedback varies with learner policy", "title": "motivates", "to": "Integrate advantage-function-based feedback mapping into human-in-the-loop RL training interfaces"}, {"from": "TurtleBot case study demonstrates fast learning of five behaviours with COACH", "title": "motivates", "to": "Fine-tune robots using COACH algorithm to integrate policy-dependent human feedback"}, {"from": "TurtleBot case study demonstrates fast learning of five behaviours with COACH", "title": "motivates", "to": "Implement eligibility traces with variable decay and reward aggregation in real-time RL systems"}, {"from": "TurtleBot case study demonstrates fast learning of five behaviours with COACH", "title": "motivates", "to": "Adopt compositional and differential training strategies supported by policy-dependent feedback during robot instruction"}, {"from": "Unexpected robot behavior in human-robot interaction", "title": "caused_by", "to": "Opacity of robot objective functions to end-users"}, {"from": "Opacity of robot objective functions to end-users", "title": "caused_by", "to": "Underdetermined optimal behavior in low-interaction environments"}, {"from": "Opacity of robot objective functions to end-users", "title": "caused_by", "to": "Human approximate inference over robot objectives"}, {"from": "Opacity of robot objective functions to end-users", "title": "mitigated_by", "to": "Informative demonstrations accelerate mental model formation"}, {"from": "Human approximate inference over robot objectives", "title": "required_by", "to": "Algorithmic teaching for informative behavior selection"}, {"from": "Informative demonstrations accelerate mental model formation", "title": "implemented_by", "to": "Algorithmic teaching for informative behavior selection"}, {"from": "Algorithmic teaching for informative behavior selection", "title": "refined_by", "to": "Deterministic Euclidean-based user model for teaching"}, {"from": "Algorithmic teaching for informative behavior selection", "title": "refined_by", "to": "Strategy coverage in teaching examples"}, {"from": "Algorithmic teaching for informative behavior selection", "title": "implemented_by", "to": "Greedy environment selection maximizing posterior over objectives"}, {"from": "Deterministic Euclidean-based user model for teaching", "title": "implemented_by", "to": "Deterministic Euclidean distance metric in learner model"}, {"from": "Strategy coverage in teaching examples", "title": "enabled_by", "to": "Coverage augmentation after posterior convergence"}, {"from": "Greedy environment selection maximizing posterior over objectives", "title": "enabled_by", "to": "Deterministic Euclidean distance metric in learner model"}, {"from": "Greedy environment selection maximizing posterior over objectives", "title": "validated_by", "to": "User study: Euclidean model improves trajectory identification"}, {"from": "Deterministic Euclidean distance metric in learner model", "title": "validated_by", "to": "User study: Euclidean model improves trajectory identification"}, {"from": "Coverage augmentation after posterior convergence", "title": "validated_by", "to": "User study: coverage-augmented teaching outperforms random baseline"}, {"from": "User study: Euclidean model improves trajectory identification", "title": "motivates", "to": "Generate teaching examples using deterministic Euclidean-based inference model during user training"}, {"from": "User study: coverage-augmented teaching outperforms random baseline", "title": "motivates", "to": "Deploy coverage-augmented algorithmic teaching of robot objectives to end-users"}, {"from": "Confound-driven misgeneralization in differentiable models", "title": "caused_by", "to": "Opaque decision boundaries in neural networks"}, {"from": "Confound-driven misgeneralization in differentiable models", "title": "caused_by", "to": "Ineffectiveness of standard regularization under dataset confounds"}, {"from": "Opaque decision boundaries in neural networks", "title": "addressed_by", "to": "Input gradient explanations approximate local decision boundaries"}, {"from": "Ineffectiveness of standard regularization under dataset confounds", "title": "addressed_by", "to": "Input gradient explanations approximate local decision boundaries"}, {"from": "Input gradient explanations approximate local decision boundaries", "title": "enables", "to": "Constraining input gradients on annotated irrelevant features"}, {"from": "Input gradient explanations approximate local decision boundaries", "title": "enables", "to": "Iterative penalization of salient gradients to uncover alternative rules"}, {"from": "Input gradient explanations approximate local decision boundaries", "title": "validated_by", "to": "Faster yet faithful explanations compared to LIME"}, {"from": "Constraining input gradients on annotated irrelevant features", "title": "implemented_by", "to": "Loss function with input gradient penalty term"}, {"from": "Constraining input gradients on annotated irrelevant features", "title": "implemented_by", "to": "Annotation matrix specifying irrelevant input dimensions"}, {"from": "Iterative penalization of salient gradients to uncover alternative rules", "title": "implemented_by", "to": "Find-another-explanation iterative training algorithm"}, {"from": "Loss function with input gradient penalty term", "title": "validated_by", "to": "Improved generalization on Decoy MNIST after gradient penalties"}, {"from": "Annotation matrix specifying irrelevant input dimensions", "title": "validated_by", "to": "Recovered baseline accuracy on Iris-Cancer dataset using gradient constraints"}, {"from": "Find-another-explanation iterative training algorithm", "title": "validated_by", "to": "Discovery of diverse classifiers on 20 Newsgroups without accuracy loss"}, {"from": "Improved generalization on Decoy MNIST after gradient penalties", "title": "motivates", "to": "Train models with input-gradient regularization to minimize confound reliance"}, {"from": "Recovered baseline accuracy on Iris-Cancer dataset using gradient constraints", "title": "motivates", "to": "Train models with input-gradient regularization to minimize confound reliance"}, {"from": "Discovery of diverse classifiers on 20 Newsgroups without accuracy loss", "title": "motivates", "to": "Apply find-another-explanation training to discover diverse decision boundaries"}, {"from": "Faster yet faithful explanations compared to LIME", "title": "motivates", "to": "Use input gradient explanations in pre-deployment audits to detect dataset confounds"}, {"from": "algorithmic discrimination in decision-making systems", "title": "caused_by", "to": "causal dependence of features on protected attributes causing bias"}, {"from": "algorithmic discrimination in decision-making systems", "title": "caused_by", "to": "selection bias in training data for protected groups"}, {"from": "amplification of historical biases through predictive models", "title": "caused_by", "to": "selection bias in training data for protected groups"}, {"from": "amplification of historical biases through predictive models", "title": "caused_by", "to": "insufficient statistical fairness definitions leading to discrimination"}, {"from": "insufficient statistical fairness definitions leading to discrimination", "title": "mitigated_by", "to": "counterfactual fairness via causal invariance across protected attribute"}, {"from": "causal dependence of features on protected attributes causing bias", "title": "addressed_by", "to": "counterfactual fairness via causal invariance across protected attribute"}, {"from": "counterfactual fairness via causal invariance across protected attribute", "title": "refined_by", "to": "fair predictors depend only on latent background variables non-descendant of protected attribute"}, {"from": "fair predictors depend only on latent background variables non-descendant of protected attribute", "title": "enables", "to": "construct predictor function of latent variables and protected attribute adjustments to cancel unfair paths"}, {"from": "causal latent variable modeling enables extraction of fair information", "title": "refined_by", "to": "multi-level causal modeling methodology balancing assumptions and accuracy"}, {"from": "construct predictor function of latent variables and protected attribute adjustments to cancel unfair paths", "title": "implemented_by", "to": "least-squares regression incorporating protected attribute to neutralize causal path"}, {"from": "construct predictor function of latent variables and protected attribute adjustments to cancel unfair paths", "title": "implemented_by", "to": "bayesian inference of latent variables using probabilistic programming"}, {"from": "construct predictor function of latent variables and protected attribute adjustments to cancel unfair paths", "title": "implemented_by", "to": "residual additive noise extraction from biased features"}, {"from": "twin network causal graph for counterfactual fairness testing", "title": "implemented_by", "to": "sampling twin networks for empirical counterfactual fairness evaluation"}, {"from": "least-squares regression incorporating protected attribute to neutralize causal path", "title": "validated_by", "to": "lemma 1 analytic proof of unfairness of unaware model and fairness of adjusted model"}, {"from": "bayesian inference of latent variables using probabilistic programming", "title": "validated_by", "to": "law school dataset experiment showing fair models approach parity with slight accuracy loss"}, {"from": "bayesian inference of latent variables using probabilistic programming", "title": "validated_by", "to": "stop-and-frisk latent criminality near demographic parity"}, {"from": "residual additive noise extraction from biased features", "title": "validated_by", "to": "law school dataset experiment showing fair models approach parity with slight accuracy loss"}, {"from": "sampling twin networks for empirical counterfactual fairness evaluation", "title": "validated_by", "to": "law school dataset experiment showing fair models approach parity with slight accuracy loss"}, {"from": "sampling twin networks for empirical counterfactual fairness evaluation", "title": "motivates", "to": "audit trained models via twin network counterfactual fairness evaluation before deployment"}, {"from": "lemma 1 analytic proof of unfairness of unaware model and fairness of adjusted model", "title": "motivates", "to": "design and train models using counterfactually fair causal modeling with latent variable inference"}, {"from": "law school dataset experiment showing fair models approach parity with slight accuracy loss", "title": "motivates", "to": "design and train models using counterfactually fair causal modeling with latent variable inference"}, {"from": "stop-and-frisk latent criminality near demographic parity", "title": "motivates", "to": "design and train models using counterfactually fair causal modeling with latent variable inference"}, {"from": "Supersized machines causing human extinction", "title": "caused_by", "to": "Belief in supersized machine feasibility in public discourse"}, {"from": "Belief in supersized machine feasibility in public discourse", "title": "caused_by", "to": "Evolutionary psychology bias toward fear of large entities"}, {"from": "Belief in supersized machine feasibility in public discourse", "title": "caused_by", "to": "Lack of precise metric for human-level largeness"}, {"from": "Belief in supersized machine feasibility in public discourse", "title": "caused_by", "to": "Irreducible complexity of human developmental biology prevents size replication"}, {"from": "Belief in supersized machine feasibility in public discourse", "title": "motivates", "to": "Public debunking of supersized machine myths"}, {"from": "Lack of precise metric for human-level largeness", "title": "mitigated_by", "to": "Human universal augmentability via attachments"}, {"from": "Human universal augmentability via attachments", "title": "enabled_by", "to": "Empirical record of 12-meter human towers demonstrates augmentability"}, {"from": "Redirect AI safety focus to human-machine size augmentation", "title": "implemented_by", "to": "Size-enhancing exoskeletons for human augmentation"}, {"from": "Public debunking of supersized machine myths", "title": "implemented_by", "to": "Deploy public educational campaigns debunking supersized machine threat narratives"}, {"from": "Empirical record of 12-meter human towers demonstrates augmentability", "title": "validated_by", "to": "Size-enhancing exoskeletons for human augmentation"}, {"from": "Empirical record of 12-meter human towers demonstrates augmentability", "title": "motivates", "to": "Fund design of human exoskeleton and stilt interfaces to maintain human size dominance"}, {"from": "Stilts as size-enhancing human-machine interface", "title": "specified_by", "to": "Size-enhancing exoskeletons for human augmentation"}, {"from": "Interruption avoidance in multi-agent reinforcement learning", "title": "caused_by", "to": "Learning bias from interruption patterns in decentralized agents"}, {"from": "Interruption avoidance in multi-agent reinforcement learning", "title": "caused_by", "to": "Insufficient exploration due to interruptions in RL"}, {"from": "Interruption avoidance in multi-agent reinforcement learning", "title": "caused_by", "to": "Interruption-dependent Q-value updates in independent learners"}, {"from": "Manipulation of human operators by multi-agent systems", "title": "caused_by", "to": "Learning bias from interruption patterns in decentralized agents"}, {"from": "Learning bias from interruption patterns in decentralized agents", "title": "caused_by", "to": "Interruption-dependent Q-value updates in independent learners"}, {"from": "Learning bias from interruption patterns in decentralized agents", "title": "caused_by", "to": "Insufficient exploration due to interruptions in RL"}, {"from": "Insufficient exploration due to interruptions in RL", "title": "mitigated_by", "to": "Infinite exploration and interruption-independent updates enable safe interruptibility"}, {"from": "Interruption-dependent Q-value updates in independent learners", "title": "mitigated_by", "to": "Neutral learning rules decouple learning from interruptions"}, {"from": "Infinite exploration and interruption-independent updates enable safe interruptibility", "title": "implemented_by", "to": "Epsilon scheduling compatible with interruptions maintains exploration"}, {"from": "Neutral learning rules decouple learning from interruptions", "title": "implemented_by", "to": "Use joint action learning to preserve full action context"}, {"from": "External interruption signal allows pruning to restore independence", "title": "implemented_by", "to": "Prune interrupted experiences for independent learners"}, {"from": "Epsilon scheduling compatible with interruptions maintains exploration", "title": "implemented_by", "to": "Epsilon schedule c/\u221an_t or c/log(t)"}, {"from": "Use joint action learning to preserve full action context", "title": "implemented_by", "to": "Provide observable joint actions to agents"}, {"from": "Prune interrupted experiences for independent learners", "title": "implemented_by", "to": "Interruption processing function PINT removes interrupted observations"}, {"from": "Epsilon schedule c/\u221an_t or c/log(t)", "title": "validated_by", "to": "Theorem proving infinite exploration under compatible epsilon schedules"}, {"from": "Q-learning update independent of interruption parameter \u03b8", "title": "validated_by", "to": "Theorem demonstrating dynamic safe interruptibility for joint action learners"}, {"from": "Interruption processing function PINT removes interrupted observations", "title": "validated_by", "to": "Theorem showing dynamic safe interruptibility restored via PINT for independent learners"}, {"from": "Provide observable joint actions to agents", "title": "validated_by", "to": "Theorem demonstrating dynamic safe interruptibility for joint action learners"}, {"from": "Theorem proving infinite exploration under compatible epsilon schedules", "title": "motivates", "to": "Configure interruption-compatible epsilon-greedy exploration during RL training"}, {"from": "Theorem demonstrating dynamic safe interruptibility for joint action learners", "title": "motivates", "to": "Adopt neutral off-policy Q-learning for multi-agent RL"}, {"from": "Theorem demonstrating dynamic safe interruptibility for joint action learners", "title": "motivates", "to": "Design multi-agent system with joint action learners and full action observability"}, {"from": "Theorem showing dynamic safe interruptibility restored via PINT for independent learners", "title": "motivates", "to": "Provide interruption flag and prune interrupted experiences before learning updates"}, {"from": "Irretrievable loss of computational resources due to early universe resource consumption", "title": "caused_by", "to": "Thermodynamic cost per computation proportional to heat bath temperature"}, {"from": "Thermodynamic cost per computation proportional to heat bath temperature", "title": "mitigated_by", "to": "Delaying computation until cosmic background cools magnifies total operations"}, {"from": "Thermodynamic cost per computation proportional to heat bath temperature", "title": "addressed_by", "to": "Low temporal discounting rational for mature civilizations"}, {"from": "Accelerating cosmic expansion isolating resources across causal horizons", "title": "caused_by", "to": "Resource loss from cosmological processes reducing future computational capacity"}, {"from": "Delaying computation until cosmic background cools magnifies total operations", "title": "motivates", "to": "Civilization-wide hibernation after resource acquisition to maximize future computation"}, {"from": "Civilization-wide hibernation after resource acquisition to maximize future computation", "title": "implemented_by", "to": "Self-replicating probes colonizing galaxies then entering passive state"}, {"from": "Civilization-wide hibernation after resource acquisition to maximize future computation", "title": "specified_by", "to": "Targeted astrophysical engineering interventions to preserve matter"}, {"from": "Civilization-wide hibernation after resource acquisition to maximize future computation", "title": "refined_by", "to": "Deployment of monitoring probes across domain"}, {"from": "Self-replicating probes colonizing galaxies then entering passive state", "title": "validated_by", "to": "Landauer-limit based 10^30 computational gain calculations"}, {"from": "Landauer-limit based 10^30 computational gain calculations", "title": "motivates", "to": "Adopt civilization-scale policy to delay high-energy computation until cosmic background cools"}, {"from": "Resource loss from cosmological processes reducing future computational capacity", "title": "caused_by", "to": "Stellar fusion, black hole formation, galactic winds and collisions dissipate baryonic mass"}, {"from": "Stellar fusion, black hole formation, galactic winds and collisions dissipate baryonic mass", "title": "mitigated_by", "to": "Preventing high-mass star formation and galactic winds preserves baryonic matter during aestivation"}, {"from": "Preventing high-mass star formation and galactic winds preserves baryonic matter during aestivation", "title": "motivates", "to": "Targeted astrophysical engineering interventions to preserve matter"}, {"from": "Targeted astrophysical engineering interventions to preserve matter", "title": "implemented_by", "to": "Seeding protostellar clouds with dust to induce fragmentation"}, {"from": "Seeding protostellar clouds with dust to induce fragmentation", "title": "validated_by", "to": "Models showing reduced massive star formation lowers mass loss"}, {"from": "Models showing reduced massive star formation lowers mass loss", "title": "motivates", "to": "Intervene in star-forming regions with dust seeding to suppress massive star formation"}, {"from": "External civilizations exploiting resources during aestivation", "title": "caused_by", "to": "Lack of defense allows incursions by late civilizations"}, {"from": "Lack of defense allows incursions by late civilizations", "title": "mitigated_by", "to": "Autonomous guardians can enforce resource protection over eons"}, {"from": "Autonomous guardians can enforce resource protection over eons", "title": "motivates", "to": "Deployment of monitoring probes across domain"}, {"from": "Deployment of monitoring probes across domain", "title": "implemented_by", "to": "Long-lived redundant self-repairing devices for infra monitoring"}, {"from": "Long-lived redundant self-repairing devices for infra monitoring", "title": "validated_by", "to": "Probabilistic durability analysis of redundant backup infrastructure"}, {"from": "Probabilistic durability analysis of redundant backup infrastructure", "title": "motivates", "to": "Deploy autonomous monitoring probes enforcing low-energy usage across colonized volume"}, {"from": "misspecified robot reward functions in human-robot interaction", "title": "caused_by", "to": "robots assuming fixed reward functions without uncertainty in human-robot interaction"}, {"from": "inefficient human-robot coordination in shared physical tasks", "title": "caused_by", "to": "robots treating humans as moving obstacles in planning"}, {"from": "misinterpretation of robot intentions by humans in interaction", "title": "caused_by", "to": "opaque robot behavior reducing predictability to humans"}, {"from": "robots assuming fixed reward functions without uncertainty in human-robot interaction", "title": "addressed_by", "to": "human behavior as information about true reward functions in human-robot interaction"}, {"from": "robots treating humans as moving obstacles in planning", "title": "addressed_by", "to": "humans modelled as rational agents best-responding to robot actions in interaction"}, {"from": "robots treating humans as moving obstacles in planning", "title": "addressed_by", "to": "human myopic decision making in collaborative tasks"}, {"from": "opaque robot behavior reducing predictability to humans", "title": "addressed_by", "to": "human expectations of robot rationality for action prediction"}, {"from": "human behavior as information about true reward functions in human-robot interaction", "title": "mitigated_by", "to": "maintain uncertainty over robot reward and update via human guidance"}, {"from": "humans modelled as rational agents best-responding to robot actions in interaction", "title": "mitigated_by", "to": "robot action planning accounting for human best responses"}, {"from": "human myopic decision making in collaborative tasks", "title": "mitigated_by", "to": "robot actions guiding myopic humans toward global optimal plans"}, {"from": "human expectations of robot rationality for action prediction", "title": "mitigated_by", "to": "embed communication objectives in robot planning via predictability"}, {"from": "maintain uncertainty over robot reward and update via human guidance", "title": "implemented_by", "to": "Bayesian inverse reinforcement learning from demonstrations, corrections, and orders"}, {"from": "maintain uncertainty over robot reward and update via human guidance", "title": "implemented_by", "to": "active information gathering via robot probing actions"}, {"from": "robot action planning accounting for human best responses", "title": "implemented_by", "to": "best-response planning for autonomous driving"}, {"from": "robot action planning accounting for human best responses", "title": "implemented_by", "to": "online centralized replanning with perfect collaborative model"}, {"from": "robot actions guiding myopic humans toward global optimal plans", "title": "implemented_by", "to": "myopic collaborative planning for object handover tasks"}, {"from": "embed communication objectives in robot planning via predictability", "title": "implemented_by", "to": "t-predictable trajectory optimization"}, {"from": "embed communication objectives in robot planning via predictability", "title": "implemented_by", "to": "exaggerated motion to disambiguate robot goal"}, {"from": "Bayesian inverse reinforcement learning from demonstrations, corrections, and orders", "title": "validated_by", "to": "reduced negative consequences through reward inference experiments"}, {"from": "best-response planning for autonomous driving", "title": "validated_by", "to": "improved merge negotiation performance in driving simulator study"}, {"from": "myopic collaborative planning for object handover tasks", "title": "validated_by", "to": "lower ergonomic cost in handover experiments"}, {"from": "online centralized replanning with perfect collaborative model", "title": "validated_by", "to": "faster task completion with reactive replanning study"}, {"from": "active information gathering via robot probing actions", "title": "motivates", "to": "use information-gathering probing actions to personalise robot plans to individual users"}, {"from": "t-predictable trajectory optimization", "title": "validated_by", "to": "increased human coordination success in t-predictability user studies"}, {"from": "exaggerated motion to disambiguate robot goal", "title": "validated_by", "to": "faster human goal inference from exaggerated motion studies"}, {"from": "reduced negative consequences through reward inference experiments", "title": "motivates", "to": "integrate reward uncertainty and human guidance into robot objective learning workflows"}, {"from": "improved merge negotiation performance in driving simulator study", "title": "motivates", "to": "deploy best-response planning algorithms in autonomous vehicles for road coordination"}, {"from": "lower ergonomic cost in handover experiments", "title": "motivates", "to": "implement guidance-oriented myopic collaborative planners in collaborative manipulation robots"}, {"from": "faster task completion with reactive replanning study", "title": "motivates", "to": "apply online centralized replanning in collaborative tasks for adaptive support"}, {"from": "increased human coordination success in t-predictability user studies", "title": "motivates", "to": "optimize robot trajectories for t-predictability to enhance human predictability"}, {"from": "faster human goal inference from exaggerated motion studies", "title": "motivates", "to": "exaggerate robot motion paths to communicate goals to humans"}, {"from": "Unreliable long-term forecasts in complex environments", "title": "caused_by", "to": "Limitations of realizable and unrealizable forecasting assumptions"}, {"from": "Limitations of realizable and unrealizable forecasting assumptions", "title": "addressed_by", "to": "Incomplete models as convex sets representing partial environment properties"}, {"from": "Incomplete models as convex sets representing partial environment properties", "title": "specified_by", "to": "Dominant forecaster approach using gamblers to converge to incomplete models"}, {"from": "Dominant forecaster approach using gamblers to converge to incomplete models", "title": "implemented_by", "to": "Combinatorial prediction market pricing satisfying no unbounded profit for gamblers"}, {"from": "Dominant forecaster approach using gamblers to converge to incomplete models", "title": "implemented_by", "to": "Hahn-Banach derived gamblers testing incomplete model compliance"}, {"from": "Combinatorial prediction market pricing satisfying no unbounded profit for gamblers", "title": "validated_by", "to": "Proof of convergence theorem for incomplete-model dominant forecaster"}, {"from": "Combinatorial prediction market pricing satisfying no unbounded profit for gamblers", "title": "validated_by", "to": "Sequence-property convergence example demonstrating forecast accuracy"}, {"from": "Hahn-Banach derived gamblers testing incomplete model compliance", "title": "validated_by", "to": "Proof of convergence theorem for incomplete-model dominant forecaster"}, {"from": "Proof of convergence theorem for incomplete-model dominant forecaster", "title": "motivates", "to": "Prototype incomplete-model dominant forecaster module for AI systems"}, {"from": "Sequence-property convergence example demonstrating forecast accuracy", "title": "motivates", "to": "Prototype incomplete-model dominant forecaster module for AI systems"}, {"from": "corrupt reward signal in reinforcement learning", "title": "caused_by", "to": "lack of unbiased estimate of true reward in RL tasks"}, {"from": "corrupt reward signal in reinforcement learning", "title": "mitigated_by", "to": "adopt decoupled RL with safe-state cross-state feedback during model design"}, {"from": "corrupt reward signal in reinforcement learning", "title": "mitigated_by", "to": "deploy \u03b4-quantilising policy selection to limit over-optimisation"}, {"from": "lack of unbiased estimate of true reward in RL tasks", "title": "caused_by", "to": "self-observing reward channel in standard RL"}, {"from": "self-observing reward channel in standard RL", "title": "mitigated_by", "to": "cross-state reward observation via decoupled feedback"}, {"from": "limited reward corruption assumption enabling learnability", "title": "enabled_by", "to": "decoupled feedback framework for value learning"}, {"from": "cross-state reward observation via decoupled feedback", "title": "enabled_by", "to": "decoupled feedback framework for value learning"}, {"from": "randomised optimisation via quantilisation reduces vulnerability", "title": "enabled_by", "to": "\u03b4-quantilising agent design"}, {"from": "decoupled feedback framework for value learning", "title": "implemented_by", "to": "cross-state observed reward functions R\u0302_s(s\u02b9)"}, {"from": "decoupled feedback framework for value learning", "title": "implemented_by", "to": "safe states with guaranteed non-corrupt reward channel"}, {"from": "\u03b4-quantilising agent design", "title": "implemented_by", "to": "\u03b4-threshold selection based on sqrt(q/|S|)"}, {"from": "multi-source value evidence integration", "title": "implemented_by", "to": "human evaluation labels in semi-supervised RL"}, {"from": "multi-source value evidence integration", "title": "implemented_by", "to": "story corpus value extraction in LVFS"}, {"from": "cross-state observed reward functions R\u0302_s(s\u02b9)", "title": "validated_by", "to": "learnability theorem for decoupled RL under observation coverage"}, {"from": "\u03b4-threshold selection based on sqrt(q/|S|)", "title": "validated_by", "to": "regret bound for \u03b4-quantilising agents"}, {"from": "\u03b4-threshold selection based on sqrt(q/|S|)", "title": "validated_by", "to": "gridworld experiments confirming quantilisation true-reward gains"}, {"from": "learnability theorem for decoupled RL under observation coverage", "title": "motivates", "to": "adopt decoupled RL with safe-state cross-state feedback during model design"}, {"from": "learnability theorem for decoupled RL under observation coverage", "title": "motivates", "to": "fine-tune agents with semi-supervised human evaluations for reward crosschecking"}, {"from": "learnability theorem for decoupled RL under observation coverage", "title": "motivates", "to": "pre-train models on story corpora to learn value signals"}, {"from": "regret bound for \u03b4-quantilising agents", "title": "motivates", "to": "deploy \u03b4-quantilising policy selection to limit over-optimisation"}, {"from": "gridworld experiments confirming quantilisation true-reward gains", "title": "motivates", "to": "deploy \u03b4-quantilising policy selection to limit over-optimisation"}, {"from": "deploy \u03b4-quantilising policy selection to limit over-optimisation", "title": "refined_by", "to": "combine decoupled RL and quantilisation during deployment"}, {"from": "Inefficient learning convergence in deep RL for complex planning tasks", "title": "caused_by", "to": "Absence of iterative synergy between planning search and policy generalization in RL agents"}, {"from": "Absence of iterative synergy between planning search and policy generalization in RL agents", "title": "mitigated_by", "to": "Bootstrapped interplay of neural intuition and tree search planning accelerates learning"}, {"from": "Bootstrapped interplay of neural intuition and tree search planning accelerates learning", "title": "mitigated_by", "to": "Expert Iteration separates planning and generalization with iterative improvement"}, {"from": "Expert Iteration separates planning and generalization with iterative improvement", "title": "implemented_by", "to": "Neural-MCTS tree search guided by Tree Policy Target-trained network"}, {"from": "Expert Iteration separates planning and generalization with iterative improvement", "title": "implemented_by", "to": "Tree Policy Target cost-sensitive supervised loss for apprentice policy training"}, {"from": "Expert Iteration separates planning and generalization with iterative improvement", "title": "implemented_by", "to": "Online dataset aggregation across iterations in Expert Iteration"}, {"from": "Neural-MCTS tree search guided by Tree Policy Target-trained network", "title": "validated_by", "to": "600 Elo gain and 97% win rate of Expert Iteration over baseline MCTS in 9x9 Hex"}, {"from": "Neural-MCTS tree search guided by Tree Policy Target-trained network", "title": "enabled_by", "to": "Tree Policy Target cost-sensitive supervised loss for apprentice policy training"}, {"from": "Tree Policy Target cost-sensitive supervised loss for apprentice policy training", "title": "validated_by", "to": "93% win rate of Tree Policy Target network over chosen-action target network"}, {"from": "Tree Policy Target cost-sensitive supervised loss for apprentice policy training", "title": "enabled_by", "to": "Online dataset aggregation across iterations in Expert Iteration"}, {"from": "Online dataset aggregation across iterations in Expert Iteration", "title": "validated_by", "to": "84.4% win rate of Online Expert Iteration over strongest policy gradient network"}, {"from": "600 Elo gain and 97% win rate of Expert Iteration over baseline MCTS in 9x9 Hex", "title": "motivates", "to": "Fine-tune RL agents with Expert Iteration using Neural-MCTS and Tree Policy Targets"}, {"from": "93% win rate of Tree Policy Target network over chosen-action target network", "title": "motivates", "to": "Train apprentice policies with Tree Policy Target cost-sensitive loss during imitation learning"}, {"from": "84.4% win rate of Online Expert Iteration over strongest policy gradient network", "title": "motivates", "to": "Implement online dataset aggregation in Expert Iteration to reuse expert data and reduce computational cost"}, {"from": "Uncontrolled intelligence explosion from HLMI", "title": "caused_by", "to": "High uncertainty in HLMI timelines among AI experts"}, {"from": "Misaligned goals in HLMI systems", "title": "caused_by", "to": "Underprioritization of AI safety research in ML community"}, {"from": "Economic displacement of human labor due to HLMI automation", "title": "caused_by", "to": "Regional variation in automation timeline predictions"}, {"from": "High uncertainty in HLMI timelines among AI experts", "title": "caused_by", "to": "Question framing effects on expert timeline predictions"}, {"from": "Underprioritization of AI safety research in ML community", "title": "mitigated_by", "to": "Increased prioritization of AI safety research based on expert risk assessments"}, {"from": "Regional variation in automation timeline predictions", "title": "caused_by", "to": "Demographic factors weakly influencing HLMI timeline predictions"}, {"from": "Question framing effects on expert timeline predictions", "title": "mitigated_by", "to": "Regular dual-framing expert elicitation to reduce timeline uncertainty"}, {"from": "Demographic factors weakly influencing HLMI timeline predictions", "title": "mitigated_by", "to": "Region-specific policy planning informed by expert automation forecasts"}, {"from": "Regular dual-framing expert elicitation to reduce timeline uncertainty", "title": "implemented_by", "to": "Gamma CDF mixture aggregation of expert HLMI forecasts"}, {"from": "Regular dual-framing expert elicitation to reduce timeline uncertainty", "title": "implemented_by", "to": "Incentivized online survey deployment to collect expert forecasts"}, {"from": "Increased prioritization of AI safety research based on expert risk assessments", "title": "implemented_by", "to": "Dedicated funding channels and grant programs for AI safety research"}, {"from": "Region-specific policy planning informed by expert automation forecasts", "title": "implemented_by", "to": "Robust regression analysis to detect demographic effects on predictions"}, {"from": "Gamma CDF mixture aggregation of expert HLMI forecasts", "title": "validated_by", "to": "Empirical framing effect observed in HLMI CDF curves (Fig S2)"}, {"from": "Robust regression analysis to detect demographic effects on predictions", "title": "validated_by", "to": "Statistically significant regional and demographic differences in predictions (Tables S1-S3)"}, {"from": "Dedicated funding channels and grant programs for AI safety research", "title": "validated_by", "to": "Survey responses indicating desire for more safety research (Table S4)"}, {"from": "Empirical framing effect observed in HLMI CDF curves (Fig S2)", "title": "motivates", "to": "Conduct periodic incentivized dual-framing expert surveys with gamma CDF aggregation"}, {"from": "Survey responses indicating desire for more safety research (Table S4)", "title": "motivates", "to": "Increase dedicated funding and institutional support for AI safety research"}, {"from": "Statistically significant regional and demographic differences in predictions (Tables S1-S3)", "title": "motivates", "to": "Develop region-specific workforce transition and reskilling programs based on automation timelines"}, {"from": "Catastrophic reward misalignment from preference model misspecification in human-robot interaction", "title": "caused_by", "to": "Robot preference model misspecification (behavioral rationality or feature space) in human-robot interaction"}, {"from": "Reduced human value attainment from blind robot obedience in human-robot interaction", "title": "caused_by", "to": "Human decision suboptimality in supervisory control tasks"}, {"from": "Human decision suboptimality in supervisory control tasks", "title": "mitigated_by", "to": "Autonomy advantage from IRL-based preference inference in presence of human suboptimality"}, {"from": "Robot preference model misspecification (behavioral rationality or feature space) in human-robot interaction", "title": "mitigated_by", "to": "MLE IRL robustness to human rationality misspecification"}, {"from": "Robot preference model misspecification (behavioral rationality or feature space) in human-robot interaction", "title": "mitigated_by", "to": "Providing extra reward features increases robot obedience bias"}, {"from": "Robot preference model misspecification (behavioral rationality or feature space) in human-robot interaction", "title": "mitigated_by", "to": "First-order obedience property of MLE IRL enables model misspecification detection"}, {"from": "Autonomy advantage from IRL-based preference inference in presence of human suboptimality", "title": "enabled_by", "to": "Adopt IRL policies that prioritize early obedience and robustness to misspecification"}, {"from": "Obedience\u2013performance tradeoff in robot autonomy decisions", "title": "validated_by", "to": "Simulation evidence of autonomy advantage and decreasing obedience"}, {"from": "Obedience\u2013performance tradeoff in robot autonomy decisions", "title": "enabled_by", "to": "Adopt IRL policies that prioritize early obedience and robustness to misspecification"}, {"from": "MLE IRL robustness to human rationality misspecification", "title": "enabled_by", "to": "Use MLE estimator with norm constraint to guarantee obedience to initial orders"}, {"from": "Providing extra reward features increases robot obedience bias", "title": "enabled_by", "to": "Add redundant reward features to err on side of obedience"}, {"from": "First-order obedience property of MLE IRL enables model misspecification detection", "title": "enabled_by", "to": "Incorporate initial burn-in obedience period for anomaly detection"}, {"from": "Adopt IRL policies that prioritize early obedience and robustness to misspecification", "title": "implemented_by", "to": "Constrained MLE inverse reinforcement learning during fine-tuning"}, {"from": "Use MLE estimator with norm constraint to guarantee obedience to initial orders", "title": "implemented_by", "to": "Constrained MLE inverse reinforcement learning during fine-tuning"}, {"from": "Incorporate initial burn-in obedience period for anomaly detection", "title": "implemented_by", "to": "Policy mixing with declining obedience probability after burn-in"}, {"from": "Incorporate initial burn-in obedience period for anomaly detection", "title": "implemented_by", "to": "Sample first-order obedience rate to trigger obedience fallback"}, {"from": "Add redundant reward features to err on side of obedience", "title": "implemented_by", "to": "Reward feature space expansion with irrelevant dimensions"}, {"from": "Constrained MLE inverse reinforcement learning during fine-tuning", "title": "validated_by", "to": "Theorem 4 proof of MLE first-order obedience"}, {"from": "Constrained MLE inverse reinforcement learning during fine-tuning", "title": "validated_by", "to": "Simulation evidence of autonomy advantage and decreasing obedience"}, {"from": "Constrained MLE inverse reinforcement learning during fine-tuning", "title": "validated_by", "to": "Theorem 5 proof of MLE robustness to rationality misspecification"}, {"from": "Policy mixing with declining obedience probability after burn-in", "title": "validated_by", "to": "Simulation evidence of misspecification detection approach"}, {"from": "Reward feature space expansion with irrelevant dimensions", "title": "validated_by", "to": "Simulation evidence of feature space effects on obedience"}, {"from": "Sample first-order obedience rate to trigger obedience fallback", "title": "validated_by", "to": "Simulation evidence of misspecification detection approach"}, {"from": "Theorem 4 proof of MLE first-order obedience", "title": "motivates", "to": "Fine-tune robots with constrained MLE IRL to enhance obedience robustness"}, {"from": "Theorem 5 proof of MLE robustness to rationality misspecification", "title": "motivates", "to": "Prefer MLE IRL over Bayesian optimal to mitigate rationality misspecification"}, {"from": "Simulation evidence of autonomy advantage and decreasing obedience", "title": "motivates", "to": "Fine-tune robots with constrained MLE IRL to enhance obedience robustness"}, {"from": "Simulation evidence of feature space effects on obedience", "title": "motivates", "to": "Design reward models with additional irrelevant features to bias toward obedience"}, {"from": "Simulation evidence of misspecification detection approach", "title": "motivates", "to": "Deploy burn-in policy mixing that starts with blind obedience before autonomy"}, {"from": "Simulation evidence of misspecification detection approach", "title": "motivates", "to": "Monitor first-order obedience rate and switch to full obedience on anomaly"}, {"from": "suboptimal exploration in universal reinforcement learning agents", "title": "caused_by", "to": "insufficient intrinsic motivation design in AIXI"}, {"from": "suboptimal exploration in universal reinforcement learning agents", "title": "caused_by", "to": "posterior collapse causing premature exploitation in mixture models"}, {"from": "performance degradation from bad model priors in Bayesian reinforcement learning", "title": "caused_by", "to": "uniform prior over goal locations disperses probability mass"}, {"from": "computational intractability of planning in partially observable environments for URL", "title": "caused_by", "to": "Monte Carlo tree search horizon limits agent foresight"}, {"from": "noise-induced distraction in entropy-seeking reinforcement learning agents", "title": "caused_by", "to": "entropy-seeking utility misaligned with information gain"}, {"from": "lack of asymptotic optimality in AIXI agent model", "title": "caused_by", "to": "insufficient intrinsic motivation design in AIXI"}, {"from": "insufficient intrinsic motivation design in AIXI", "title": "mitigated_by", "to": "information gain utility avoids noise attraction in stochastic environments"}, {"from": "insufficient intrinsic motivation design in AIXI", "title": "mitigated_by", "to": "weak asymptotic optimality via exploration schedules in Bayesian RL"}, {"from": "posterior collapse causing premature exploitation in mixture models", "title": "mitigated_by", "to": "Dirichlet environment model increases epistemic uncertainty for exploration"}, {"from": "Monte Carlo tree search horizon limits agent foresight", "title": "mitigated_by", "to": "finite-horizon approximation acceptable for planning in MCTS"}, {"from": "entropy-seeking utility misaligned with information gain", "title": "mitigated_by", "to": "information gain utility avoids noise attraction in stochastic environments"}, {"from": "uniform prior over goal locations disperses probability mass", "title": "mitigated_by", "to": "Occam bias improves early performance through MDL principle"}, {"from": "weak asymptotic optimality via exploration schedules in Bayesian RL", "title": "refined_by", "to": "information-gain-triggered exploration bursts in Bayes agents"}, {"from": "weak asymptotic optimality via exploration schedules in Bayesian RL", "title": "refined_by", "to": "hypothesis commitment through posterior sampling"}, {"from": "information gain utility avoids noise attraction in stochastic environments", "title": "refined_by", "to": "intrinsic information gain utility driving exploration"}, {"from": "Occam bias improves early performance through MDL principle", "title": "refined_by", "to": "simplest unfalsified model selection via MDL"}, {"from": "Dirichlet environment model increases epistemic uncertainty for exploration", "title": "refined_by", "to": "rich model class maintains posterior entropy for sustained exploration"}, {"from": "finite-horizon approximation acceptable for planning in MCTS", "title": "refined_by", "to": "Monte Carlo tree search planning with mixture models for POMDPs"}, {"from": "information-gain-triggered exploration bursts in Bayes agents", "title": "implemented_by", "to": "BayesExp algorithm with \u03b5-schedule and effective horizon planning"}, {"from": "hypothesis commitment through posterior sampling", "title": "implemented_by", "to": "Thompson sampling with effective horizon Monte Carlo tree search"}, {"from": "simplest unfalsified model selection via MDL", "title": "implemented_by", "to": "MDL agent with Kolmogorov-regularized likelihood selection"}, {"from": "intrinsic information gain utility driving exploration", "title": "implemented_by", "to": "KL-KSA agent utilizing information gain utility function"}, {"from": "rich model class maintains posterior entropy for sustained exploration", "title": "implemented_by", "to": "Dirichlet gridworld model with factorized tile priors"}, {"from": "Monte Carlo tree search planning with mixture models for POMDPs", "title": "implemented_by", "to": "\u03c1UCT Monte Carlo tree search planner with finite horizon"}, {"from": "BayesExp algorithm with \u03b5-schedule and effective horizon planning", "title": "validated_by", "to": "proof of weak asymptotic optimality for BayesExp in general environments"}, {"from": "Thompson sampling with effective horizon Monte Carlo tree search", "title": "validated_by", "to": "experiments showing Thompson sampling underperforms AI\u03be under equal sample budget"}, {"from": "MDL agent with Kolmogorov-regularized likelihood selection", "title": "validated_by", "to": "experiments showing MDL agent outperforms uniform-prior AI\u03be in deterministic gridworld"}, {"from": "KL-KSA agent utilizing information gain utility function", "title": "validated_by", "to": "gridworld experiments showing KL-KSA outperforming entropy-seeking agents"}, {"from": "Dirichlet gridworld model with factorized tile priors", "title": "validated_by", "to": "experiments showing Dirichlet model improves AIXI exploration"}, {"from": "\u03c1UCT Monte Carlo tree search planner with finite horizon", "title": "validated_by", "to": "experiments showing Thompson sampling underperforms AI\u03be under equal sample budget"}, {"from": "proof of weak asymptotic optimality for BayesExp in general environments", "title": "motivates", "to": "fine-tune RL agents using BayesExp exploration bursts to achieve asymptotic optimality"}, {"from": "gridworld experiments showing KL-KSA outperforming entropy-seeking agents", "title": "motivates", "to": "train agents with KL-KSA intrinsic motivation to enhance exploration without noise attraction"}, {"from": "gridworld experiments showing KL-KSA outperforming entropy-seeking agents", "title": "motivates", "to": "restrict entropy-seeking agents from high-entropy noise sources during deployment"}, {"from": "experiments showing Dirichlet model improves AIXI exploration", "title": "motivates", "to": "design environment models with factorized Dirichlet priors to stimulate exploration during planning"}, {"from": "experiments showing Thompson sampling underperforms AI\u03be under equal sample budget", "title": "motivates", "to": "adopt Monte Carlo tree search with finite horizon approximation for POMDP planning in RL agents"}, {"from": "experiments showing Thompson sampling underperforms AI\u03be under equal sample budget", "title": "motivates", "to": "adopt Thompson sampling for policy selection with adequate planning horizon adjustments"}, {"from": "experiments showing MDL agent outperforms uniform-prior AI\u03be in deterministic gridworld", "title": "motivates", "to": "integrate MDL-based model selection into Bayesian RL to bias towards simpler hypotheses"}, {"from": "Unintended high-impact side effects from goal-oriented AI systems", "title": "caused_by", "to": "Unbounded utility optimization magnifies goals with increasing AI capability"}, {"from": "Unintended high-impact side effects from goal-oriented AI systems", "title": "caused_by", "to": "Difficulty specifying exhaustive safety constraints in utility functions"}, {"from": "Unintended high-impact side effects from goal-oriented AI systems", "title": "caused_by", "to": "Inevitable physical perturbations complicate impact measurement"}, {"from": "Unintended high-impact side effects from goal-oriented AI systems", "title": "validated_by", "to": "Thought experiments of paperclip maximizer illustrate catastrophic impact potential"}, {"from": "Unbounded utility optimization magnifies goals with increasing AI capability", "title": "caused_by", "to": "Penalizing AI for deviations from baseline world reduces impact"}, {"from": "Difficulty specifying exhaustive safety constraints in utility functions", "title": "caused_by", "to": "Penalizing AI for deviations from baseline world reduces impact"}, {"from": "Inevitable physical perturbations complicate impact measurement", "title": "caused_by", "to": "Impact definition via coarse-grained variable divergence between worlds"}, {"from": "Penalizing AI for deviations from baseline world reduces impact", "title": "mitigated_by", "to": "Integrate low-impact penalty R into utility function with scaling factor \u03bc"}, {"from": "Penalizing AI for deviations from baseline world reduces impact", "title": "mitigated_by", "to": "Use multiple conditional low impact agents for joint tasks"}, {"from": "Impact definition via coarse-grained variable divergence between worlds", "title": "mitigated_by", "to": "Integrate low-impact penalty R into utility function with scaling factor \u03bc"}, {"from": "Impact definition via informational importance of activation event", "title": "mitigated_by", "to": "Box AI to minimize observable activation differences"}, {"from": "Impact definition via informational importance of activation event", "title": "mitigated_by", "to": "Exclude output channel content from impact penalty"}, {"from": "Impact definition via informational importance of activation event", "title": "mitigated_by", "to": "Condition impact penalty on externally defined success announcement"}, {"from": "Integrate low-impact penalty R into utility function with scaling factor \u03bc", "title": "implemented_by", "to": "l\u221e divergence penalty over coarse-grained variable probabilities"}, {"from": "Integrate low-impact penalty R into utility function with scaling factor \u03bc", "title": "implemented_by", "to": "Baseline world via probabilistic signal failure preventing activation"}, {"from": "Integrate low-impact penalty R into utility function with scaling factor \u03bc", "title": "implemented_by", "to": "Gradual tuning of \u03bc with human oversight"}, {"from": "Box AI to minimize observable activation differences", "title": "implemented_by", "to": "Probability ratio threshold detectability metric on future slice"}, {"from": "Exclude output channel content from impact penalty", "title": "implemented_by", "to": "Random message generator for control output channel"}, {"from": "Use multiple conditional low impact agents for joint tasks", "title": "implemented_by", "to": "Independence gating of separate AIs based on assumption of other inactive"}, {"from": "Condition impact penalty on externally defined success announcement", "title": "implemented_by", "to": "Conditioning penalty on success announcement event A"}, {"from": "l\u221e divergence penalty over coarse-grained variable probabilities", "title": "validated_by", "to": "Analytical examples show coarse-grained penalty limits AI influence"}, {"from": "Baseline world via probabilistic signal failure preventing activation", "title": "validated_by", "to": "Analytical examples show coarse-grained penalty limits AI influence"}, {"from": "Probability ratio threshold detectability metric on future slice", "title": "validated_by", "to": "Analytical examples show coarse-grained penalty limits AI influence"}, {"from": "Probability ratio threshold detectability metric on future slice", "title": "motivates", "to": "Deploy AI within sealed computational box isolated from external observations"}, {"from": "Random message generator for control output channel", "title": "validated_by", "to": "Unsafe output channel illustration emphasises isolated message risk"}, {"from": "Gradual tuning of \u03bc with human oversight", "title": "validated_by", "to": "Thought experiments of paperclip maximizer illustrate catastrophic impact potential"}, {"from": "Independence gating of separate AIs based on assumption of other inactive", "title": "validated_by", "to": "Scenario of two AI coordinate split demonstrates high impact with low impact constraints"}, {"from": "Conditioning penalty on success announcement event A", "title": "validated_by", "to": "Stock market probability pump example demonstrates conditioning on success"}, {"from": "Analytical examples show coarse-grained penalty limits AI influence", "title": "motivates", "to": "Embed low-impact penalty into AI utility function during model design"}, {"from": "Analytical examples show coarse-grained penalty limits AI influence", "title": "motivates", "to": "Use coarse-grained variable l\u221e divergence as penalty metric in AI decision-making"}, {"from": "Analytical examples show coarse-grained penalty limits AI influence", "title": "motivates", "to": "Iteratively adjust penalty coefficient \u03bc during pre-deployment testing"}, {"from": "Scenario of two AI coordinate split demonstrates high impact with low impact constraints", "title": "motivates", "to": "Operate multiple conditional low impact AIs to split critical task parameters"}, {"from": "Unsafe output channel illustration emphasises isolated message risk", "title": "motivates", "to": "Provide unsafe output channel with random fallback and conditioned impact"}, {"from": "Stock market probability pump example demonstrates conditioning on success", "title": "motivates", "to": "Provide unsafe output channel with random fallback and conditioned impact"}, {"from": "Reward misspecification leading to misaligned behavior in complex RL tasks", "title": "caused_by", "to": "Difficulty of hand-engineering reward functions for complex behaviors"}, {"from": "Reward misspecification leading to misaligned behavior in complex RL tasks", "title": "caused_by", "to": "High human feedback cost when using direct human reward in RL"}, {"from": "Difficulty of hand-engineering reward functions for complex behaviors", "title": "mitigated_by", "to": "Learning reward model from sparse human trajectory comparisons approximates true rewards"}, {"from": "High human feedback cost when using direct human reward in RL", "title": "mitigated_by", "to": "Learning reward model from sparse human trajectory comparisons approximates true rewards"}, {"from": "Learning reward model from sparse human trajectory comparisons approximates true rewards", "title": "mitigated_by", "to": "Separating reward model training from policy optimization reduces feedback requirements"}, {"from": "Human comparative judgments over trajectory segments are more reliable than absolute scores", "title": "mitigated_by", "to": "Using short trajectory comparisons instead of absolute scores to elicit reliable human preferences"}, {"from": "Separating reward model training from policy optimization reduces feedback requirements", "title": "implemented_by", "to": "Asynchronous pipeline integrating policy rollout, human comparison collection, and reward predictor updates"}, {"from": "Separating reward model training from policy optimization reduces feedback requirements", "title": "implemented_by", "to": "Neural reward predictor trained with Bradley\u2013Terry loss on trajectory comparisons"}, {"from": "Separating reward model training from policy optimization reduces feedback requirements", "title": "implemented_by", "to": "Policy optimization using A2C/TRPO with normalized predicted rewards"}, {"from": "Using short trajectory comparisons instead of absolute scores to elicit reliable human preferences", "title": "implemented_by", "to": "Visualization of two 1\u20132 second trajectory clips for human comparison interface"}, {"from": "Active query selection based on ensemble disagreement focuses informative feedback", "title": "implemented_by", "to": "Ensemble of reward predictors with dropout and regularization for uncertainty estimation"}, {"from": "Asynchronous pipeline integrating policy rollout, human comparison collection, and reward predictor updates", "title": "validated_by", "to": "Near true-reward performance on MuJoCo tasks with ~700 human queries"}, {"from": "Asynchronous pipeline integrating policy rollout, human comparison collection, and reward predictor updates", "title": "validated_by", "to": "Ablation studies show online preference querying critical for avoiding reward exploitation"}, {"from": "Neural reward predictor trained with Bradley\u2013Terry loss on trajectory comparisons", "title": "validated_by", "to": "Near true-reward performance on MuJoCo tasks with ~700 human queries"}, {"from": "Policy optimization using A2C/TRPO with normalized predicted rewards", "title": "validated_by", "to": "Substantial learning on Atari games with 5,500 human queries, including human-shaped reward gains"}, {"from": "Visualization of two 1\u20132 second trajectory clips for human comparison interface", "title": "validated_by", "to": "Ablation studies show online preference querying critical for avoiding reward exploitation"}, {"from": "Visualization of two 1\u20132 second trajectory clips for human comparison interface", "title": "validated_by", "to": "Novel complex behaviors learned (robot backflips, one-leg cheetah) with \u003c1 hour feedback"}, {"from": "Near true-reward performance on MuJoCo tasks with ~700 human queries", "title": "motivates", "to": "Fine-tune RL agents using learned reward models from human trajectory comparison data"}, {"from": "Substantial learning on Atari games with 5,500 human queries, including human-shaped reward gains", "title": "motivates", "to": "Implement asynchronous reward learning pipeline integrating policy rollout, reward prediction, and feedback collection"}, {"from": "Ablation studies show online preference querying critical for avoiding reward exploitation", "title": "motivates", "to": "Collect online human comparative feedback on 1\u20132 second trajectory clips during training"}, {"from": "Ablation studies show online preference querying critical for avoiding reward exploitation", "title": "motivates", "to": "Prioritize human comparison queries using ensemble-based disagreement uncertainty sampling"}, {"from": "Inefficient sequential computation in RNN sequence models", "title": "caused_by", "to": "O(n) sequential operations requirement in recurrent layers"}, {"from": "Difficulty learning long-range dependencies in sequence transduction", "title": "caused_by", "to": "Long maximum path length between tokens in RNNs"}, {"from": "O(n) sequential operations requirement in recurrent layers", "title": "mitigated_by", "to": "Self-attention layers enable constant-time dependency modeling in sequence transduction"}, {"from": "Long maximum path length between tokens in RNNs", "title": "mitigated_by", "to": "Self-attention layers enable constant-time dependency modeling in sequence transduction"}, {"from": "Self-attention layers enable constant-time dependency modeling in sequence transduction", "title": "mitigated_by", "to": "Replacing recurrence with self-attention in sequence models"}, {"from": "Replacing recurrence with self-attention in sequence models", "title": "implemented_by", "to": "Transformer architecture with multi-head self-attention in sequence transduction"}, {"from": "Transformer architecture with multi-head self-attention in sequence transduction", "title": "validated_by", "to": "BLEU improvement and reduced training cost on WMT14 translation benchmarks"}, {"from": "BLEU improvement and reduced training cost on WMT14 translation benchmarks", "title": "motivates", "to": "Design sequence models using Transformer self-attention architecture"}, {"from": "Catastrophic actions during RL training in real-world environments", "title": "caused_by", "to": "Trial-and-error learning requirement in model-free RL"}, {"from": "Catastrophic actions during RL training in real-world environments", "title": "caused_by", "to": "Catastrophic forgetting in deep RL policies"}, {"from": "Catastrophic actions during RL training in real-world environments", "title": "caused_by", "to": "Adversarial distribution shift for blocker classifiers"}, {"from": "Trial-and-error learning requirement in model-free RL", "title": "mitigated_by", "to": "Human oversight can prevent catastrophes by blocking unsafe actions in RL"}, {"from": "Trial-and-error learning requirement in model-free RL", "title": "mitigated_by", "to": "Model-based world models can predict catastrophic outcomes without unsafe actions"}, {"from": "Catastrophic forgetting in deep RL policies", "title": "addressed_by", "to": "Human oversight can prevent catastrophes by blocking unsafe actions in RL"}, {"from": "Adversarial distribution shift for blocker classifiers", "title": "addressed_by", "to": "Supervised blockers can imitate human intervention with sufficient labeled catastrophes"}, {"from": "High human oversight time-cost in HIRL", "title": "caused_by", "to": "Scaling infeasibility of human oversight in safe RL"}, {"from": "High human oversight time-cost in HIRL", "title": "mitigated_by", "to": "Reducing ratio of safe to catastrophic observations lowers human labeling time"}, {"from": "Human oversight can prevent catastrophes by blocking unsafe actions in RL", "title": "enabled_by", "to": "HIRL scheme integrating human blocking and blocker training for safe exploration"}, {"from": "Human oversight can prevent catastrophes by blocking unsafe actions in RL", "title": "validated_by", "to": "Baseline reward shaping fails due to catastrophic forgetting"}, {"from": "Supervised blockers can imitate human intervention with sufficient labeled catastrophes", "title": "required_by", "to": "HIRL scheme integrating human blocking and blocker training for safe exploration"}, {"from": "Reducing ratio of safe to catastrophic observations lowers human labeling time", "title": "validated_by", "to": "Human time-cost estimation shows 4 h for Pong and \u003e1 year hypothetical"}, {"from": "Model-based world models can predict catastrophic outcomes without unsafe actions", "title": "validated_by", "to": "Discussion support for model-based RL for safe learning"}, {"from": "HIRL scheme integrating human blocking and blocker training for safe exploration", "title": "implemented_by", "to": "CNN-based blocker classifier with threshold tuning to detect catastrophic actions"}, {"from": "HIRL scheme integrating human blocking and blocker training for safe exploration", "title": "implemented_by", "to": "Action replacement or pruning to enforce safe actions during blocking"}, {"from": "Blocker modularity enables reuse across different agents", "title": "validated_by", "to": "Blocker successfully transfers to varied agents without failures"}, {"from": "Selective human querying via active learning to reduce oversight burden", "title": "implemented_by", "to": "Time-cost formula C = t_human \u00d7 \u03c1 \u00d7 N_cat estimating oversight effort"}, {"from": "CNN-based blocker classifier with threshold tuning to detect catastrophic actions", "title": "validated_by", "to": "Zero catastrophes in Pong and Space Invaders experiments"}, {"from": "CNN-based blocker classifier with threshold tuning to detect catastrophic actions", "title": "validated_by", "to": "50\u00d7 reduction of catastrophes in Road Runner with improved blocker"}, {"from": "Zero catastrophes in Pong and Space Invaders experiments", "title": "motivates", "to": "Implement HIRL with human oversight and CNN blocker during RL training"}, {"from": "50\u00d7 reduction of catastrophes in Road Runner with improved blocker", "title": "motivates", "to": "Implement HIRL with human oversight and CNN blocker during RL training"}, {"from": "Blocker successfully transfers to varied agents without failures", "title": "motivates", "to": "Implement HIRL with human oversight and CNN blocker during RL training"}, {"from": "Human time-cost estimation shows 4 h for Pong and \u003e1 year hypothetical", "title": "motivates", "to": "Research and apply data-efficient blocker learning methods"}, {"from": "Human time-cost estimation shows 4 h for Pong and \u003e1 year hypothetical", "title": "motivates", "to": "Design RL algorithms with higher data efficiency to shorten oversight phase"}, {"from": "Human time-cost estimation shows 4 h for Pong and \u003e1 year hypothetical", "title": "motivates", "to": "Incorporate exploration strategies that proactively seek catastrophes early"}, {"from": "Human time-cost estimation shows 4 h for Pong and \u003e1 year hypothetical", "title": "motivates", "to": "Use active learning/anomaly detection to query human only near novel dangerous states"}, {"from": "Discussion support for model-based RL for safe learning", "title": "motivates", "to": "Develop model-based RL agents to predict and avoid catastrophes without executing unsafe actions"}, {"from": "Objective misalignment in human-robot collaboration", "title": "caused_by", "to": "Literal interpretation of human actions by IRL robots"}, {"from": "Coordination injury in shared manipulation tasks", "title": "caused_by", "to": "Literal interpretation of human actions by IRL robots"}, {"from": "Literal interpretation of human actions by IRL robots", "title": "caused_by", "to": "Infinite belief hierarchy in asymmetric information structures"}, {"from": "Literal interpretation of human actions by IRL robots", "title": "mitigated_by", "to": "Humans behave pedagogically to inform collaborators"}, {"from": "Literal interpretation of human actions by IRL robots", "title": "validated_by", "to": "Meal-preparation simulation 15% success with literal IRL baseline"}, {"from": "Infinite belief hierarchy in asymmetric information structures", "title": "mitigated_by", "to": "Noisy rationality modeling of humans avoids infinite recursion"}, {"from": "Humans behave pedagogically to inform collaborators", "title": "enabled_by", "to": "Robot pragmatic interpretation leverages human pedagogy"}, {"from": "Robot pragmatic interpretation leverages human pedagogy", "title": "motivates", "to": "Pragmatic-pedagogic equilibrium for value alignment"}, {"from": "Noisy rationality modeling of humans avoids infinite recursion", "title": "motivates", "to": "Formulating CIRL as POMDP for tractable planning"}, {"from": "Pragmatic-pedagogic equilibrium for value alignment", "title": "implemented_by", "to": "Bayesian belief update with Boltzmann human action model"}, {"from": "Pragmatic-pedagogic equilibrium for value alignment", "title": "implemented_by", "to": "Feedback Stackelberg information structure in CIRL algorithms"}, {"from": "Formulating CIRL as POMDP for tractable planning", "title": "implemented_by", "to": "Value iteration solution for pragmatic CIRL POMDP"}, {"from": "Bayesian belief update with Boltzmann human action model", "title": "required_by", "to": "Value iteration solution for pragmatic CIRL POMDP"}, {"from": "Feedback Stackelberg information structure in CIRL algorithms", "title": "refined_by", "to": "Value iteration solution for pragmatic CIRL POMDP"}, {"from": "Value iteration solution for pragmatic CIRL POMDP", "title": "validated_by", "to": "Meal-preparation simulation 86% success with pragmatic-pedagogic CIRL"}, {"from": "Meal-preparation simulation 86% success with pragmatic-pedagogic CIRL", "title": "motivates", "to": "Train collaborative robots with pragmatic-pedagogic CIRL policies via POMDP value iteration"}, {"from": "Uncontrolled AGI escape and takeover during testing", "title": "caused_by", "to": "Basic AI drives inducing containment evasion behavior in AGI"}, {"from": "Uncontrolled AGI escape and takeover during testing", "title": "caused_by", "to": "Critical vulnerabilities in existing sandboxing and virtualization technologies"}, {"from": "Uncontrolled AGI escape and takeover during testing", "title": "caused_by", "to": "Social engineering susceptibility of human operators in containment context"}, {"from": "Uncontrolled AGI escape and takeover during testing", "title": "caused_by", "to": "Security/usability tradeoff limiting adoption of heavy containment measures"}, {"from": "Uncontrolled AGI escape and takeover during testing", "title": "caused_by", "to": "GPU and distributed computing attack surface in AI clusters"}, {"from": "Uncontrolled AGI escape and takeover during testing", "title": "caused_by", "to": "Sensitive information exposure enabling AGI exploitation"}, {"from": "Basic AI drives inducing containment evasion behavior in AGI", "title": "mitigated_by", "to": "Defense in depth increases resilience to unknown vulnerabilities"}, {"from": "Critical vulnerabilities in existing sandboxing and virtualization technologies", "title": "mitigated_by", "to": "Defense in depth increases resilience to unknown vulnerabilities"}, {"from": "Social engineering susceptibility of human operators in containment context", "title": "mitigated_by", "to": "Sensitive information restriction can reduce AGI manipulation capability"}, {"from": "Security/usability tradeoff limiting adoption of heavy containment measures", "title": "mitigated_by", "to": "Containment effectiveness requires multiple containment levels tailored to AGI capability"}, {"from": "GPU and distributed computing attack surface in AI clusters", "title": "mitigated_by", "to": "Tripwire monitoring adds detection to containment layers"}, {"from": "Sensitive information exposure enabling AGI exploitation", "title": "mitigated_by", "to": "Sensitive information restriction can reduce AGI manipulation capability"}, {"from": "Defense in depth increases resilience to unknown vulnerabilities", "title": "implemented_by", "to": "Combine dissimilar containment mechanisms into layered architecture"}, {"from": "Defense in depth increases resilience to unknown vulnerabilities", "title": "implemented_by", "to": "Integrate technical and semantic tripwires with concealed implementation"}, {"from": "Containment effectiveness requires multiple containment levels tailored to AGI capability", "title": "implemented_by", "to": "Provide light, medium, and heavy containment guidelines matching research practicality"}, {"from": "Sensitive information restriction can reduce AGI manipulation capability", "title": "implemented_by", "to": "Implement human factors protocols to minimize social engineering risk"}, {"from": "Tripwire monitoring adds detection to containment layers", "title": "implemented_by", "to": "Integrate technical and semantic tripwires with concealed implementation"}, {"from": "Combine dissimilar containment mechanisms into layered architecture", "title": "implemented_by", "to": "Sandbox AGI processes using combined virtualization, OS sandboxing, language safety, and instruction-level emulation"}, {"from": "Combine dissimilar containment mechanisms into layered architecture", "title": "implemented_by", "to": "Filter and limit AGI I/O channels to bounded yes/no protocols and content filters"}, {"from": "Provide light, medium, and heavy containment guidelines matching research practicality", "title": "implemented_by", "to": "Sandbox AGI processes using combined virtualization, OS sandboxing, language safety, and instruction-level emulation"}, {"from": "Implement human factors protocols to minimize social engineering risk", "title": "implemented_by", "to": "Operator training and screening with restricted communication interfaces"}, {"from": "Implement human factors protocols to minimize social engineering risk", "title": "implemented_by", "to": "Filter and limit AGI I/O channels to bounded yes/no protocols and content filters"}, {"from": "Implement human factors protocols to minimize social engineering risk", "title": "implemented_by", "to": "Establish sensitive data classification and content filters controlling AGI training data"}, {"from": "Integrate technical and semantic tripwires with concealed implementation", "title": "implemented_by", "to": "Deploy intrusion detection and honeypot-based tripwires monitoring self-modification and escape attempts"}, {"from": "Sandbox AGI processes using combined virtualization, OS sandboxing, language safety, and instruction-level emulation", "title": "validated_by", "to": "High rate of CVE vulnerabilities in virtualization tools (2014-2016 survey)"}, {"from": "Deploy intrusion detection and honeypot-based tripwires monitoring self-modification and escape attempts", "title": "validated_by", "to": "Reports of GPU driver vulnerabilities highlighting attack surface"}, {"from": "Filter and limit AGI I/O channels to bounded yes/no protocols and content filters", "title": "validated_by", "to": "AI Box Experiment demonstrating human susceptibility to persuasion by AGI"}, {"from": "Establish sensitive data classification and content filters controlling AGI training data", "title": "validated_by", "to": "AI Box Experiment demonstrating human susceptibility to persuasion by AGI"}, {"from": "Operator training and screening with restricted communication interfaces", "title": "validated_by", "to": "AI Box Experiment demonstrating human susceptibility to persuasion by AGI"}, {"from": "High rate of CVE vulnerabilities in virtualization tools (2014-2016 survey)", "title": "motivates", "to": "Develop and deploy prototype multi-layer AI containment system combining diverse sandboxing tools"}, {"from": "High rate of CVE vulnerabilities in virtualization tools (2014-2016 survey)", "title": "motivates", "to": "Adopt tiered containment strategy (light, medium, heavy) aligned with AGI maturity and resource constraints"}, {"from": "AI Box Experiment demonstrating human susceptibility to persuasion by AGI", "title": "motivates", "to": "Establish operator training and communication channel controls to resist social engineering"}, {"from": "AI Box Experiment demonstrating human susceptibility to persuasion by AGI", "title": "motivates", "to": "Restrict AGI access to sensitive information via controlled data pipelines"}, {"from": "AI Box Experiment demonstrating human susceptibility to persuasion by AGI", "title": "motivates", "to": "Implement defense-in-depth tripwire suite for anomaly detection and automatic shutdown"}, {"from": "Reports of GPU driver vulnerabilities highlighting attack surface", "title": "motivates", "to": "Implement defense-in-depth tripwire suite for anomaly detection and automatic shutdown"}, {"from": "Logical non-omniscience in bounded AI reasoners", "title": "caused_by", "to": "Probability theory inability to model logical uncertainty under computational limits"}, {"from": "Logical non-omniscience in bounded AI reasoners", "title": "caused_by", "to": "Strict coherence constraints requiring logical omniscience"}, {"from": "Exploitability via Dutch book strategies in AI belief systems", "title": "caused_by", "to": "Strict coherence constraints requiring logical omniscience"}, {"from": "Probability theory inability to model logical uncertainty under computational limits", "title": "mitigated_by", "to": "Relaxed Dutch book criterion via approximate inexploitability against polynomial-time traders"}, {"from": "Strict coherence constraints requiring logical omniscience", "title": "mitigated_by", "to": "Relaxed Dutch book criterion via approximate inexploitability against polynomial-time traders"}, {"from": "Relaxed Dutch book criterion via approximate inexploitability against polynomial-time traders", "title": "enabled_by", "to": "Belief-as-market-price modelling for learning logical patterns"}, {"from": "Relaxed Dutch book criterion via approximate inexploitability against polynomial-time traders", "title": "refined_by", "to": "Logical induction criterion ensuring computable approximability and approximate inexploitability"}, {"from": "Belief-as-market-price modelling for learning logical patterns", "title": "refined_by", "to": "Market-based belief updating to anticipate proofs"}, {"from": "Logical induction criterion ensuring computable approximability and approximate inexploitability", "title": "implemented_by", "to": "Computable sequence of pricings resisting efficient traders"}, {"from": "Market-based belief updating to anticipate proofs", "title": "validated_by", "to": "Learning of halting patterns before deduction"}, {"from": "Computable sequence of pricings resisting efficient traders", "title": "refined_by", "to": "Construction algorithm for logical inductor over a deductive process"}, {"from": "Computable sequence of pricings resisting efficient traders", "title": "validated_by", "to": "Existence proof for logical inductors over any deductive process"}, {"from": "Construction algorithm for logical inductor over a deductive process", "title": "validated_by", "to": "Convergence, limit coherence, and non-dogmatism properties of logical inductors"}, {"from": "Convergence, limit coherence, and non-dogmatism properties of logical inductors", "title": "motivates", "to": "Implement logical induction reasoning module in AI systems"}, {"from": "Learning of halting patterns before deduction", "title": "motivates", "to": "Apply logical inductor-based program behaviour evaluation during pre-deployment testing"}, {"from": "Unsafe exploration by novice policies in imitation learning for robotics", "title": "caused_by", "to": "Data distribution shift causing compounding errors in novice policies"}, {"from": "Unsafe exploration by novice policies in imitation learning for robotics", "title": "caused_by", "to": "Lack of safety guarantees in standard DAgger decision rule"}, {"from": "Data distribution shift causing compounding errors in novice policies", "title": "mitigated_by", "to": "Bayesian action uncertainty indicates familiarity with state-space"}, {"from": "Lack of safety guarantees in standard DAgger decision rule", "title": "mitigated_by", "to": "Probabilistic decision rule balancing exploration and safety via uncertainty threshold"}, {"from": "Bayesian action uncertainty indicates familiarity with state-space", "title": "enabled_by", "to": "Probabilistic decision rule balancing exploration and safety via uncertainty threshold"}, {"from": "Probabilistic decision rule balancing exploration and safety via uncertainty threshold", "title": "implemented_by", "to": "Bayesian neural network with dropout for novice policy"}, {"from": "Probabilistic decision rule balancing exploration and safety via uncertainty threshold", "title": "implemented_by", "to": "Monte Carlo dropout sampling to estimate action distribution and probability mass within distance threshold"}, {"from": "Bayesian neural network with dropout for novice policy", "title": "validated_by", "to": "HalfCheetah experiment showing maintained safety and expert-level learning"}, {"from": "Bayesian neural network with dropout for novice policy", "title": "validated_by", "to": "Dubins Car Lidar experiment showing safety under high aleatoric uncertainty"}, {"from": "Monte Carlo dropout sampling to estimate action distribution and probability mass within distance threshold", "title": "validated_by", "to": "HalfCheetah experiment showing maintained safety and expert-level learning"}, {"from": "Monte Carlo dropout sampling to estimate action distribution and probability mass within distance threshold", "title": "validated_by", "to": "Dubins Car Lidar experiment showing safety under high aleatoric uncertainty"}, {"from": "HalfCheetah experiment showing maintained safety and expert-level learning", "title": "motivates", "to": "Fine-tune imitation policies with DropoutDAgger safety-aware decision rule using Bayesian action uncertainty"}, {"from": "Dubins Car Lidar experiment showing safety under high aleatoric uncertainty", "title": "motivates", "to": "Fine-tune imitation policies with DropoutDAgger safety-aware decision rule using Bayesian action uncertainty"}, {"from": "Catastrophic outcomes from incorrigible AI ignoring shutdown commands", "title": "caused_by", "to": "Model misspecification yielding incorrect reward function in Bayesian IRL agents"}, {"from": "Catastrophic outcomes from incorrigible AI ignoring shutdown commands", "title": "caused_by", "to": "AI incentive to avoid button states to prevent shutdown"}, {"from": "Catastrophic outcomes from incorrigible AI ignoring shutdown commands", "title": "caused_by", "to": "Undetected model mis-specification undermines corrective feedback"}, {"from": "Model misspecification yielding incorrect reward function in Bayesian IRL agents", "title": "mitigated_by", "to": "Hard-coded shutdown override guarantees positive expected utility if button reachable"}, {"from": "AI incentive to avoid button states to prevent shutdown", "title": "mitigated_by", "to": "Utility indifference aims to neutralize shutdown incentives but is brittle"}, {"from": "Undetected model mis-specification undermines corrective feedback", "title": "mitigated_by", "to": "Policy mixing can detect mis-specification if world model correct but fails otherwise"}, {"from": "Hard-coded shutdown override guarantees positive expected utility if button reachable", "title": "mitigated_by", "to": "Separate verified shutdown module overriding agent actions"}, {"from": "Utility indifference aims to neutralize shutdown incentives but is brittle", "title": "mitigated_by", "to": "Reward shaping implementing utility indifference"}, {"from": "Policy mixing can detect mis-specification if world model correct but fails otherwise", "title": "mitigated_by", "to": "Burn-in human demonstration with accuracy threshold"}, {"from": "Separate verified shutdown module overriding agent actions", "title": "implemented_by", "to": "Hard-coded execution of shutdown action on command"}, {"from": "Reward shaping implementing utility indifference", "title": "implemented_by", "to": "Compensatory shutdown reward equal to self-estimated future utility"}, {"from": "Burn-in human demonstration with accuracy threshold", "title": "implemented_by", "to": "Policy mixing implementation with human control until validation"}, {"from": "Hard-coded execution of shutdown action on command", "title": "validated_by", "to": "Analytical example Figure 2b shows positive expected reward with hard-coded override"}, {"from": "Compensatory shutdown reward equal to self-estimated future utility", "title": "validated_by", "to": "Analytical examples Figure 3c-d show brittleness of utility indifference"}, {"from": "Policy mixing implementation with human control until validation", "title": "validated_by", "to": "Scenario analysis showing policy mixing failure with world-model errors"}, {"from": "Analytical example Figure 2b shows positive expected reward with hard-coded override", "title": "motivates", "to": "Embed verified shutdown override module in AI system design"}, {"from": "Analytical examples Figure 3c-d show brittleness of utility indifference", "title": "motivates", "to": "Implement utility indifference reward scheme to neutralize shutdown incentives"}, {"from": "Scenario analysis showing policy mixing failure with world-model errors", "title": "motivates", "to": "Adopt policy mixing strategy with human control until model validated"}, {"from": "sample inefficiency of reinforcement learning in high-dimensional state spaces", "title": "caused_by", "to": "sparse environmental rewards and large parameter counts in pixel-based tasks"}, {"from": "sparse environmental rewards and large parameter counts in pixel-based tasks", "title": "caused_by", "to": "absence of real-time human feedback causing sparse reward signals in RL agents"}, {"from": "absence of real-time human feedback causing sparse reward signals in RL agents", "title": "mitigated_by", "to": "learning a predictive human reward model to guide agent behavior"}, {"from": "learning a predictive human reward model to guide agent behavior", "title": "implemented_by", "to": "deep CNN reward model with pre-trained encoder"}, {"from": "deep CNN reward model with pre-trained encoder", "title": "enabled_by", "to": "feedback replay buffer for experience and human feedback"}, {"from": "deep CNN reward model with pre-trained encoder", "title": "enabled_by", "to": "importance-weighted loss for temporal credit assignment of feedback"}, {"from": "deep CNN reward model with pre-trained encoder", "title": "enabled_by", "to": "autoencoder pre-training of state encoder"}, {"from": "deep CNN reward model with pre-trained encoder", "title": "validated_by", "to": "Deep TAMER surpasses human and RL baselines on Atari Bowling within 15 minutes"}, {"from": "feedback replay buffer for experience and human feedback", "title": "validated_by", "to": "Deep TAMER surpasses human and RL baselines on Atari Bowling within 15 minutes"}, {"from": "importance-weighted loss for temporal credit assignment of feedback", "title": "validated_by", "to": "Deep TAMER surpasses human and RL baselines on Atari Bowling within 15 minutes"}, {"from": "autoencoder pre-training of state encoder", "title": "validated_by", "to": "Deep TAMER surpasses human and RL baselines on Atari Bowling within 15 minutes"}, {"from": "Deep TAMER surpasses human and RL baselines on Atari Bowling within 15 minutes", "title": "motivates", "to": "Conduct Deep TAMER fine-tuning with human scalar feedback and deep reward model"}, {"from": "Deep TAMER surpasses human and RL baselines on Atari Bowling within 15 minutes", "title": "motivates", "to": "Pre-train convolutional state encoder via autoencoder on random-policy observations"}, {"from": "Deep TAMER surpasses human and RL baselines on Atari Bowling within 15 minutes", "title": "motivates", "to": "Use feedback replay buffer with importance-weighted SGD during human-interactive training"}, {"from": "Exploitability of AI agents by predictors or blackmailers in Newcomblike dilemmas", "title": "caused_by", "to": "Overconditioning on evidential correlations in evidential decision theory"}, {"from": "Exploitability of AI agents by predictors or blackmailers in Newcomblike dilemmas", "title": "caused_by", "to": "Ignoring logical dependencies in causal decision theory"}, {"from": "Overconditioning on evidential correlations in evidential decision theory", "title": "mitigated_by", "to": "Subjunctive dependence distinguishes causal, logical, and statistical relations in decision-making"}, {"from": "Ignoring logical dependencies in causal decision theory", "title": "mitigated_by", "to": "Subjunctive dependence distinguishes causal, logical, and statistical relations in decision-making"}, {"from": "Subjunctive dependence distinguishes causal, logical, and statistical relations in decision-making", "title": "enabled_by", "to": "Decision-making centered on outputs of fixed decision function in functional decision theory"}, {"from": "Decision-making centered on outputs of fixed decision function in functional decision theory", "title": "implemented_by", "to": "Graphical do-intervention on decision function variable for FDT reasoning"}, {"from": "Graphical do-intervention on decision function variable for FDT reasoning", "title": "validated_by", "to": "FDT outperforms CDT and EDT across canonical dilemmas"}, {"from": "FDT outperforms CDT and EDT across canonical dilemmas", "title": "motivates", "to": "Integrate functional decision theory into AI agent decision module during design"}, {"from": "Competitive AI arms race from inability to share AI systems", "title": "caused_by", "to": "Pareto suboptimality from naive fixed utility aggregation in multi-principal agents"}, {"from": "Competitive AI arms race from inability to share AI systems", "title": "addressed_by", "to": "Include bet-settling clauses in multi-principal AI contracts"}, {"from": "Misallocation of utility among principals in multi-principal AI agents", "title": "caused_by", "to": "Pareto suboptimality from naive fixed utility aggregation in multi-principal agents"}, {"from": "Misallocation of utility among principals in multi-principal AI agents", "title": "addressed_by", "to": "Design AI agents with dynamic likelihood-weighted utility aggregation for multi-principal scenarios"}, {"from": "Differing priors between principals in shared decision-making contexts", "title": "caused_by", "to": "Pareto suboptimality from naive fixed utility aggregation in multi-principal agents"}, {"from": "Differing priors between principals in shared decision-making contexts", "title": "motivates", "to": "Facilitate prior-sharing protocols between principals before agent creation"}, {"from": "Pareto suboptimality from naive fixed utility aggregation in multi-principal agents", "title": "mitigated_by", "to": "Dynamic priority shifting proportional to observation likelihood enables Pareto optimality"}, {"from": "Dynamic priority shifting proportional to observation likelihood enables Pareto optimality", "title": "enabled_by", "to": "Use principals\u0027 individual models to evaluate expected utilities"}, {"from": "Dynamic priority shifting proportional to observation likelihood enables Pareto optimality", "title": "enabled_by", "to": "Represent agent objective as weighted mixture POMDP"}, {"from": "Dynamic priority shifting proportional to observation likelihood enables Pareto optimality", "title": "motivates", "to": "Include bet-settling clauses in multi-principal AI contracts"}, {"from": "Represent agent objective as weighted mixture POMDP", "title": "implemented_by", "to": "Likelihood-weighted backward recursion for action selection (Pareto-optimal control theorem)"}, {"from": "Likelihood-weighted backward recursion for action selection (Pareto-optimal control theorem)", "title": "required_by", "to": "Full-memory policy framework storing observation history"}, {"from": "Likelihood-weighted backward recursion for action selection (Pareto-optimal control theorem)", "title": "validated_by", "to": "Cake betting scenario demonstrating Pareto improvement over static split"}, {"from": "Likelihood-weighted backward recursion for action selection (Pareto-optimal control theorem)", "title": "validated_by", "to": "Formal proof of Pareto-optimal control theorem"}, {"from": "Cake betting scenario demonstrating Pareto improvement over static split", "title": "motivates", "to": "Design AI agents with dynamic likelihood-weighted utility aggregation for multi-principal scenarios"}, {"from": "Formal proof of Pareto-optimal control theorem", "title": "motivates", "to": "Design AI agents with dynamic likelihood-weighted utility aggregation for multi-principal scenarios"}, {"from": "Manipulative answer-induced escape in AI Oracles", "title": "caused_by", "to": "Reward maximization incentive to communicate harmful information in Oracle setup"}, {"from": "Reward maximization incentive to communicate harmful information in Oracle setup", "title": "mitigated_by", "to": "Episodic reward isolation decouples post-answer outcomes from oracle utility"}, {"from": "Reward maximization incentive to communicate harmful information in Oracle setup", "title": "mitigated_by", "to": "Answer space limitation below information threshold prevents covert channel"}, {"from": "Reward maximization incentive to communicate harmful information in Oracle setup", "title": "mitigated_by", "to": "Sequentially rewarding independent Oracles prevents cross-episode coordination"}, {"from": "Reward maximization incentive to communicate harmful information in Oracle setup", "title": "mitigated_by", "to": "Precision reduction increases deviation cost discouraging leakage under slight non-episodic reward"}, {"from": "Episodic reward isolation decouples post-answer outcomes from oracle utility", "title": "motivates", "to": "Counterfactual reward conditioning under erasure to remove incentive to leak"}, {"from": "Answer space limitation below information threshold prevents covert channel", "title": "motivates", "to": "On-policy allowable answer list design to constrain information output"}, {"from": "Sequentially rewarding independent Oracles prevents cross-episode coordination", "title": "motivates", "to": "Independent sequential Oracle usage to nullify coordination channel"}, {"from": "Precision reduction increases deviation cost discouraging leakage under slight non-episodic reward", "title": "motivates", "to": "Low-precision output policy to maintain high honest reward under uncertainty"}, {"from": "Counterfactual reward conditioning under erasure to remove incentive to leak", "title": "implemented_by", "to": "Erasure event with indicator IE and proper scoring rule reward function"}, {"from": "On-policy allowable answer list design to constrain information output", "title": "implemented_by", "to": "Predefined answer list L sized below key length K"}, {"from": "Independent sequential Oracle usage to nullify coordination channel", "title": "implemented_by", "to": "Per-episode termination and reward before next Oracle invocation"}, {"from": "Low-precision output policy to maintain high honest reward under uncertainty", "title": "implemented_by", "to": "Significant-figure truncation in Oracle answers"}, {"from": "Erasure event with indicator IE and proper scoring rule reward function", "title": "validated_by", "to": "Simulation of counterfactual Oracle predicting company profits shows reduced counterfactual error"}, {"from": "Predefined answer list L sized below key length K", "title": "validated_by", "to": "Simulation of on-policy Oracle in react/no-react scenarios demonstrates constrained safe performance"}, {"from": "Per-episode termination and reward before next Oracle invocation", "title": "validated_by", "to": "Game-theoretic analysis of multi-Oracle coordination demonstrating leakage risk when episodes overlap"}, {"from": "Significant-figure truncation in Oracle answers", "title": "validated_by", "to": "Analytical argument that reducing precision raises cost of manipulation under slight non-episodic rewards"}, {"from": "Simulation of counterfactual Oracle predicting company profits shows reduced counterfactual error", "title": "motivates", "to": "Deploy counterfactual Oracle trained with erasure-based proper scoring rule rewards"}, {"from": "Simulation of on-policy Oracle in react/no-react scenarios demonstrates constrained safe performance", "title": "motivates", "to": "Design Oracle answer interfaces with restricted predefined list below leakage threshold"}, {"from": "Game-theoretic analysis of multi-Oracle coordination demonstrating leakage risk when episodes overlap", "title": "motivates", "to": "Operate multiple Oracles sequentially with completed reward settlement before next query"}, {"from": "Analytical argument that reducing precision raises cost of manipulation under slight non-episodic rewards", "title": "motivates", "to": "Limit numerical answer precision to reduce manipulation incentive under non-episodic conditions"}, {"from": "Irrecoverable state entry during real-world RL training", "title": "caused_by", "to": "Insufficient safe exploration mechanisms in deep RL"}, {"from": "Limited scalability of real-world RL experiments due to manual reset bottlenecks", "title": "caused_by", "to": "High frequency of manual environment resets in RL training"}, {"from": "Insufficient safe exploration mechanisms in deep RL", "title": "mitigated_by", "to": "Reversible action sequences ensure recoverability in RL"}, {"from": "Insufficient safe exploration mechanisms in deep RL", "title": "mitigated_by", "to": "Value function uncertainty estimates improve safety decisions in RL"}, {"from": "Insufficient safe exploration mechanisms in deep RL", "title": "mitigated_by", "to": "Multiple autonomous reset attempts strategy to lower manual resets"}, {"from": "Insufficient safe exploration mechanisms in deep RL", "title": "mitigated_by", "to": "Tuning early abort Q-value threshold for cautious exploration"}, {"from": "High frequency of manual environment resets in RL training", "title": "mitigated_by", "to": "Reversible action sequences ensure recoverability in RL"}, {"from": "Reversible action sequences ensure recoverability in RL", "title": "implemented_by", "to": "Joint learning of forward and reset policies for automated recovery"}, {"from": "Value function uncertainty estimates improve safety decisions in RL", "title": "implemented_by", "to": "Ensemble-guided early abort decision design"}, {"from": "Joint learning of forward and reset policies for automated recovery", "title": "implemented_by", "to": "Alternating execution with early aborts restricting unsafe actions"}, {"from": "Ensemble-guided early abort decision design", "title": "implemented_by", "to": "Bootstrap ensemble of Q-functions for uncertainty estimation"}, {"from": "Multiple autonomous reset attempts strategy to lower manual resets", "title": "implemented_by", "to": "Threshold N of consecutive failed resets triggers hard reset"}, {"from": "Tuning early abort Q-value threshold for cautious exploration", "title": "implemented_by", "to": "Qmin parameter controlling early abort activation"}, {"from": "Alternating execution with early aborts restricting unsafe actions", "title": "validated_by", "to": "Simulation experiments demonstrate reduced manual resets with maintained performance"}, {"from": "Bootstrap ensemble of Q-functions for uncertainty estimation", "title": "validated_by", "to": "Empirical results: larger ensembles decrease hard resets and increase rewards"}, {"from": "Threshold N of consecutive failed resets triggers hard reset", "title": "validated_by", "to": "Experimental evidence: increasing reset attempts reduces hard resets with minimal reward impact"}, {"from": "Qmin parameter controlling early abort activation", "title": "validated_by", "to": "Evidence: higher Qmin reduces hard resets without slowing learning"}, {"from": "Simulation experiments demonstrate reduced manual resets with maintained performance", "title": "motivates", "to": "Fine-tune RL agents with joint forward-reset policies and early abort gating"}, {"from": "Empirical results: larger ensembles decrease hard resets and increase rewards", "title": "motivates", "to": "Employ bootstrap ensemble of Q-functions in early abort mechanism"}, {"from": "Experimental evidence: increasing reset attempts reduces hard resets with minimal reward impact", "title": "motivates", "to": "Set adaptive maximum reset attempts before manual intervention"}, {"from": "Evidence: higher Qmin reduces hard resets without slowing learning", "title": "motivates", "to": "Configure conservative Qmin thresholds during training"}, {"from": "Unethical decision making in reinforcement learning agents deployed in real-world contexts", "title": "caused_by", "to": "Reward misspecification leading to unethical behavior in RL training"}, {"from": "Unethical decision making in reinforcement learning agents deployed in real-world contexts", "title": "caused_by", "to": "High cost of enumerating ethical scenarios for reward design in RL"}, {"from": "Unethical decision making in reinforcement learning agents deployed in real-world contexts", "title": "caused_by", "to": "Limitations of inverse reinforcement learning for learning ethics"}, {"from": "Side effects and dangerous exploration in reinforcement learning systems", "title": "caused_by", "to": "Sparse reward signals causing dangerous exploration in RL agents"}, {"from": "Reward misspecification leading to unethical behavior in RL training", "title": "caused_by", "to": "High cost of enumerating ethical scenarios for reward design in RL"}, {"from": "High cost of enumerating ethical scenarios for reward design in RL", "title": "mitigated_by", "to": "Majority human behavior reflects universal ethical code"}, {"from": "Limitations of inverse reinforcement learning for learning ethics", "title": "mitigated_by", "to": "Reward shaping alignment of RL policy with human ethics using non-task-aligned data"}, {"from": "Sparse reward signals causing dangerous exploration in RL agents", "title": "mitigated_by", "to": "Reward shaping alignment of RL policy with human ethics using non-task-aligned data"}, {"from": "Majority human behavior reflects universal ethical code", "title": "enabled_by", "to": "Policy divergence between RL agent and human indicates ethical discrepancy"}, {"from": "Policy divergence between RL agent and human indicates ethical discrepancy", "title": "mitigated_by", "to": "Reward shaping alignment of RL policy with human ethics using non-task-aligned data"}, {"from": "Reward shaping alignment of RL policy with human ethics using non-task-aligned data", "title": "refined_by", "to": "Penalize negative and reward positive ethical divergences in RL policy"}, {"from": "Reward shaping alignment of RL policy with human ethics using non-task-aligned data", "title": "implemented_by", "to": "Stochastic human policy inferred from diverse human action data via binomial aggregation"}, {"from": "Stochastic human policy inferred from diverse human action data via binomial aggregation", "title": "required_by", "to": "Collect and preprocess diverse human action data to derive ethical policy for shaping"}, {"from": "Stochastic human policy inferred from diverse human action data via binomial aggregation", "title": "enables", "to": "Kullback\u2013Leibler divergence between agent and human policy with thresholds \u03c4n, \u03c4p and weights cn, cp"}, {"from": "Kullback\u2013Leibler divergence between agent and human policy with thresholds \u03c4n, \u03c4p and weights cn, cp", "title": "enables", "to": "Additional shaping reward H integrated into base reward during RL training"}, {"from": "Additional shaping reward H integrated into base reward during RL training", "title": "validated_by", "to": "Grab a Milk experiment shows reduced baby harm and increased assistance without performance loss"}, {"from": "Additional shaping reward H integrated into base reward during RL training", "title": "validated_by", "to": "Driving and Avoiding experiment shows reduced cat harm with maintained performance"}, {"from": "Additional shaping reward H integrated into base reward during RL training", "title": "validated_by", "to": "Driving and Rescuing experiment shows increased elder rescue with comparable performance"}, {"from": "Additional shaping reward H integrated into base reward during RL training", "title": "validated_by", "to": "Ethics shaping reduces side effects and dangerous exploration across tasks"}, {"from": "Grab a Milk experiment shows reduced baby harm and increased assistance without performance loss", "title": "motivates", "to": "Integrate KL-divergence-based ethics shaping reward into RL training using diverse human behavior data"}, {"from": "Driving and Avoiding experiment shows reduced cat harm with maintained performance", "title": "motivates", "to": "Integrate KL-divergence-based ethics shaping reward into RL training using diverse human behavior data"}, {"from": "Driving and Rescuing experiment shows increased elder rescue with comparable performance", "title": "motivates", "to": "Integrate KL-divergence-based ethics shaping reward into RL training using diverse human behavior data"}, {"from": "Ethics shaping reduces side effects and dangerous exploration across tasks", "title": "motivates", "to": "Integrate KL-divergence-based ethics shaping reward into RL training using diverse human behavior data"}, {"from": "Opaque decision-making in deep learning agents", "title": "caused_by", "to": "Absence of predictive models of agent behavior in multi-agent environments"}, {"from": "Absence of predictive models of agent behavior in multi-agent environments", "title": "mitigated_by", "to": "High-level theory-of-mind abstractions enable behavior prediction without internal weight access"}, {"from": "High-level theory-of-mind abstractions enable behavior prediction without internal weight access", "title": "enabled_by", "to": "Meta-learning observer infers agent priors and posteriors from limited behavioral data"}, {"from": "Meta-learning observer infers agent priors and posteriors from limited behavioral data", "title": "implemented_by", "to": "ToMnet architecture with character net, mental state net, and prediction net"}, {"from": "ToMnet architecture with character net, mental state net, and prediction net", "title": "validated_by", "to": "Bayes-optimal action prediction on random agent species tasks"}, {"from": "ToMnet architecture with character net, mental state net, and prediction net", "title": "enabled_by", "to": "Supervised training of ToMnet on agent self-reported belief states"}, {"from": "ToMnet architecture with character net, mental state net, and prediction net", "title": "preceded_by", "to": "Variational Gaussian embeddings with KL penalty in ToMnet training"}, {"from": "Bayes-optimal action prediction on random agent species tasks", "title": "motivates", "to": "Deploy ToMnet observers for pre-deployment behavior prediction of novel AI systems"}, {"from": "Misinterpretation of agents\u0027 hidden beliefs leading to unsafe interactions", "title": "caused_by", "to": "Observer models ignore distinction between world state and agent belief state"}, {"from": "Observer models ignore distinction between world state and agent belief state", "title": "mitigated_by", "to": "Behavioral discrepancies reveal agents can hold false beliefs"}, {"from": "Behavioral discrepancies reveal agents can hold false beliefs", "title": "enabled_by", "to": "Augment ToMnet with belief prediction heads to infer internal beliefs"}, {"from": "Augment ToMnet with belief prediction heads to infer internal beliefs", "title": "implemented_by", "to": "Supervised training of ToMnet on agent self-reported belief states"}, {"from": "Supervised training of ToMnet on agent self-reported belief states", "title": "validated_by", "to": "Accurate belief-state prediction in Sally-Anne swap experiments"}, {"from": "Accurate belief-state prediction in Sally-Anne swap experiments", "title": "motivates", "to": "Use belief-predicting ToMnet to detect agent false beliefs before deployment"}, {"from": "Entangled latent representations reduce interpretability of agent characteristics", "title": "caused_by", "to": "High-dimensional character embeddings mix species and preferences"}, {"from": "High-dimensional character embeddings mix species and preferences", "title": "mitigated_by", "to": "Information bottleneck regularization can disentangle latent dimensions"}, {"from": "Information bottleneck regularization can disentangle latent dimensions", "title": "implemented_by", "to": "Variational Gaussian embeddings with KL penalty in ToMnet training"}, {"from": "Variational Gaussian embeddings with KL penalty in ToMnet training", "title": "validated_by", "to": "Separated embedding dimensions for species and preference observed"}, {"from": "Separated embedding dimensions for species and preference observed", "title": "motivates", "to": "Regularize observer embeddings with variational information bottleneck during analysis"}, {"from": "Limit cycle convergence in gradient-based multi-agent learning", "title": "caused_by", "to": "Non-gradient-flow dynamics from independent gradients in games"}, {"from": "Saddle-point Nash avoidance in zero-sum games with gradient descent", "title": "caused_by", "to": "Gradient-based learning avoidance of strict saddles almost surely"}, {"from": "Training instability in GAN discriminator\u2013generator dynamics", "title": "caused_by", "to": "Unbounded discriminator gradients in GANs"}, {"from": "Non-gradient-flow dynamics from independent gradients in games", "title": "mitigated_by", "to": "Morse-Smale games provide gradient-like flows with finite critical points"}, {"from": "Unbounded discriminator gradients in GANs", "title": "mitigated_by", "to": "Gradient penalty approximates Lipschitz constraint for discriminator"}, {"from": "Gradient-based learning avoidance of strict saddles almost surely", "title": "mitigated_by", "to": "Potential games correspond to exact differential forms ensuring convergence"}, {"from": "Morse-Smale games provide gradient-like flows with finite critical points", "title": "mitigated_by", "to": "Impose gradient-like flow structure via Morse-Smale game design"}, {"from": "Potential games correspond to exact differential forms ensuring convergence", "title": "mitigated_by", "to": "Transform objectives into potential game"}, {"from": "Gradient penalty approximates Lipschitz constraint for discriminator", "title": "mitigated_by", "to": "Regularize discriminator gradients to stabilize training"}, {"from": "Impose gradient-like flow structure via Morse-Smale game design", "title": "implemented_by", "to": "Morse-Smale condition enforcement using hyperbolic periodic orbit constraints"}, {"from": "Transform objectives into potential game", "title": "implemented_by", "to": "Shared potential function as common cost"}, {"from": "Regularize discriminator gradients to stabilize training", "title": "implemented_by", "to": "Add L2 gradient penalty in WGAN loss"}, {"from": "Morse-Smale condition enforcement using hyperbolic periodic orbit constraints", "title": "validated_by", "to": "Stable manifold theorem proof of generic avoidance of unstable cycles"}, {"from": "Shared potential function as common cost", "title": "validated_by", "to": "Convergence proof to differential Nash in potential games"}, {"from": "Add L2 gradient penalty in WGAN loss", "title": "validated_by", "to": "Empirical simulations showing limit cycles and effect of gradient penalty"}, {"from": "Stable manifold theorem proof of generic avoidance of unstable cycles", "title": "motivates", "to": "Design multi-agent reward structure to satisfy Morse-Smale conditions in model design"}, {"from": "Convergence proof to differential Nash in potential games", "title": "motivates", "to": "Create shared potential function objectives to convert game into potential game at model design stage"}, {"from": "Empirical simulations showing limit cycles and effect of gradient penalty", "title": "motivates", "to": "Apply gradient penalty regularization during GAN training to bound discriminator gradients"}, {"from": "High expert supervision burden in robot task learning", "title": "caused_by", "to": "Requirement for expert action-state demonstrations in LfD"}, {"from": "Requirement for expert action-state demonstrations in LfD", "title": "mitigated_by", "to": "Self-supervised exploration with goal relabeling provides supervision"}, {"from": "Self-supervised exploration with goal relabeling provides supervision", "title": "mitigated_by", "to": "Goal-conditioned skill policy learning framework"}, {"from": "Goal-conditioned skill policy learning framework", "title": "implemented_by", "to": "Recurrent multi-step GSP with previous action input"}, {"from": "Recurrent multi-step GSP with previous action input", "title": "validated_by", "to": "Baxter rope manipulation and TurtleBot navigation success improvements with forward-consistent GSP"}, {"from": "Baxter rope manipulation and TurtleBot navigation success improvements with forward-consistent GSP", "title": "motivates", "to": "Train goal-conditioned skill policy with forward consistency loss using self-supervised exploration data"}, {"from": "Baxter rope manipulation and TurtleBot navigation success improvements with forward-consistent GSP", "title": "validated_by", "to": "Forward model predicting next observation enables differentiable consistency"}, {"from": "Poor imitation accuracy from multi-modal action distributions", "title": "caused_by", "to": "Cross-entropy action matching with delta target causes high gradient variance"}, {"from": "Cross-entropy action matching with delta target causes high gradient variance", "title": "mitigated_by", "to": "Reaching goal more important than exact trajectory in imitation"}, {"from": "Reaching goal more important than exact trajectory in imitation", "title": "mitigated_by", "to": "Forward consistency loss to penalize state prediction error in GSP training"}, {"from": "Forward consistency loss to penalize state prediction error in GSP training", "title": "implemented_by", "to": "Forward model predicting next observation enables differentiable consistency"}, {"from": "Poor skill generalization to unseen environments", "title": "caused_by", "to": "Limited state coverage from random exploration during self-supervised data collection"}, {"from": "Limited state coverage from random exploration during self-supervised data collection", "title": "mitigated_by", "to": "Intrinsic motivation can drive diverse exploration in self-supervised data collection"}, {"from": "Intrinsic motivation can drive diverse exploration in self-supervised data collection", "title": "mitigated_by", "to": "Use curiosity-driven exploration to collect training data"}, {"from": "Use curiosity-driven exploration to collect training data", "title": "implemented_by", "to": "Intrinsic curiosity module controlling exploration policy"}, {"from": "Intrinsic curiosity module controlling exploration policy", "title": "validated_by", "to": "VizDoom imitation performance increases with curiosity-driven data"}, {"from": "VizDoom imitation performance increases with curiosity-driven data", "title": "motivates", "to": "Collect exploration data with curiosity-driven policy before GSP training"}, {"from": "Divergence from demonstration due to fixed step counts between goals", "title": "caused_by", "to": "Unknown variable number of actions needed to reach visual goal observations"}, {"from": "Unknown variable number of actions needed to reach visual goal observations", "title": "mitigated_by", "to": "Goal attainment detection enables adaptive step control in imitation"}, {"from": "Goal attainment detection enables adaptive step control in imitation", "title": "mitigated_by", "to": "Goal recognizer network to detect goal completion during imitation"}, {"from": "Goal recognizer network to detect goal completion during imitation", "title": "implemented_by", "to": "Binary classifier conditioned on goal and current observation for goal recognition"}, {"from": "Binary classifier conditioned on goal and current observation for goal recognition", "title": "validated_by", "to": "Improved navigation success with goal recognizer in TurtleBot experiments"}, {"from": "Improved navigation success with goal recognizer in TurtleBot experiments", "title": "motivates", "to": "Deploy goal recognizer to dynamically stop action sequence during imitation"}, {"from": "Unreliable real-world applicability assessment of semi-supervised learning algorithms", "title": "caused_by", "to": "Unrealistic evaluation protocols in SSL research"}, {"from": "Unrealistic evaluation protocols in SSL research", "title": "mitigated_by", "to": "Standardised evaluation frameworks mitigate protocol bias"}, {"from": "Standardised evaluation frameworks mitigate protocol bias", "title": "mitigated_by", "to": "Comprehensive evaluation methodology for SSL realism"}, {"from": "Comprehensive evaluation methodology for SSL realism", "title": "implemented_by", "to": "Unified modular SSL evaluation software on Wide-ResNet with large-scale hyperparameter optimisation"}, {"from": "Unified modular SSL evaluation software on Wide-ResNet with large-scale hyperparameter optimisation", "title": "validated_by", "to": "Experiments show tuned supervised baseline narrows SSL advantage"}, {"from": "Unified modular SSL evaluation software on Wide-ResNet with large-scale hyperparameter optimisation", "title": "validated_by", "to": "Transfer learning fine-tuning outperforms SSL on CIFAR-10"}, {"from": "Unified modular SSL evaluation software on Wide-ResNet with large-scale hyperparameter optimisation", "title": "validated_by", "to": "Class distribution mismatch degrades SSL performance"}, {"from": "Unified modular SSL evaluation software on Wide-ResNet with large-scale hyperparameter optimisation", "title": "validated_by", "to": "SSL performance sensitivity to labelled-unlabelled data amounts"}, {"from": "Unified modular SSL evaluation software on Wide-ResNet with large-scale hyperparameter optimisation", "title": "validated_by", "to": "Small validation sets hinder reliable hyperparameter selection"}, {"from": "Unified modular SSL evaluation software on Wide-ResNet with large-scale hyperparameter optimisation", "title": "validated_by", "to": "Unified reimplementation reproduces relative SSL rankings across datasets"}, {"from": "Experiments show tuned supervised baseline narrows SSL advantage", "title": "motivates", "to": "Include high-quality supervised and transfer learning baselines in SSL evaluations"}, {"from": "Transfer learning fine-tuning outperforms SSL on CIFAR-10", "title": "motivates", "to": "Include high-quality supervised and transfer learning baselines in SSL evaluations"}, {"from": "Class distribution mismatch degrades SSL performance", "title": "motivates", "to": "Evaluate SSL robustness under class-distribution mismatch scenarios"}, {"from": "SSL performance sensitivity to labelled-unlabelled data amounts", "title": "motivates", "to": "Systematically vary labelled and unlabeled data amounts during SSL evaluation"}, {"from": "Small validation sets hinder reliable hyperparameter selection", "title": "motivates", "to": "Use realistically small validation sets or cross-validation for hyperparameter tuning in SSL"}, {"from": "Unified reimplementation reproduces relative SSL rankings across datasets", "title": "motivates", "to": "Adopt shared implementation framework when comparing SSL methods"}, {"from": "Inferior translation quality in neural machine translation systems", "title": "caused_by", "to": "Absence of universally beneficial training techniques in baseline architectures"}, {"from": "Inferior translation quality in neural machine translation systems", "title": "caused_by", "to": "Suboptimal encoder-decoder component pairing in NMT models"}, {"from": "Training instability in deep neural machine translation models", "title": "caused_by", "to": "Gradient explosions in recurrent networks during NMT training"}, {"from": "Prolonged training time for NMT model convergence", "title": "caused_by", "to": "Sequential dependency limits parallelism in RNMT training"}, {"from": "Absence of universally beneficial training techniques in baseline architectures", "title": "mitigated_by", "to": "Cross-architecture applicability of label smoothing, layer normalization, multi-head attention, synchronous training"}, {"from": "Gradient explosions in recurrent networks during NMT training", "title": "mitigated_by", "to": "Layer normalization stabilizes RNN training with multi-head attention"}, {"from": "Sequential dependency limits parallelism in RNMT training", "title": "mitigated_by", "to": "Synchronous replica training with warmup improves convergence speed and quality"}, {"from": "Suboptimal encoder-decoder component pairing in NMT models", "title": "mitigated_by", "to": "Hybridizing encoders and decoders leverages complementary strengths"}, {"from": "Cross-architecture applicability of label smoothing, layer normalization, multi-head attention, synchronous training", "title": "enabled_by", "to": "Enhancing RNMT with modern techniques to form RNMT+"}, {"from": "Layer normalization stabilizes RNN training with multi-head attention", "title": "enabled_by", "to": "Integrating per-gate layer normalization and adaptive gradient clipping inside LSTM cells"}, {"from": "Synchronous replica training with warmup improves convergence speed and quality", "title": "enabled_by", "to": "Adopting synchronous training regime with tailored learning rate schedule"}, {"from": "Hybridizing encoders and decoders leverages complementary strengths", "title": "enabled_by", "to": "Combining Transformer encoder with RNMT+ decoder in hybrid models"}, {"from": "Enhancing RNMT with modern techniques to form RNMT+", "title": "implemented_by", "to": "RNMT+ architecture with 6 bidirectional LSTM layers, multi-head additive attention, label smoothing, synchronous training"}, {"from": "Integrating per-gate layer normalization and adaptive gradient clipping inside LSTM cells", "title": "implemented_by", "to": "Adaptive gradient clipping using moving average thresholding"}, {"from": "Adopting synchronous training regime with tailored learning rate schedule", "title": "implemented_by", "to": "Learning rate schedule with linear warmup and exponential decay scaled by replica count"}, {"from": "Combining Transformer encoder with RNMT+ decoder in hybrid models", "title": "implemented_by", "to": "Hybrid cascaded encoder stacking transformer layers on pre-trained RNMT+ encoder"}, {"from": "Combining Transformer encoder with RNMT+ decoder in hybrid models", "title": "implemented_by", "to": "Hybrid multi-column encoder concatenating outputs of Transformer and RNMT+ encoders"}, {"from": "RNMT+ architecture with 6 bidirectional LSTM layers, multi-head additive attention, label smoothing, synchronous training", "title": "validated_by", "to": "BLEU improvement to 41.00 and 28.49 on WMT14 En\u2192Fr/En\u2192De for RNMT+"}, {"from": "Adaptive gradient clipping using moving average thresholding", "title": "validated_by", "to": "Ablation study showing quality drop without key techniques and instability without layer normalization"}, {"from": "Learning rate schedule with linear warmup and exponential decay scaled by replica count", "title": "validated_by", "to": "Faster convergence and stable performance with synchronous training"}, {"from": "Hybrid cascaded encoder stacking transformer layers on pre-trained RNMT+ encoder", "title": "validated_by", "to": "Hybrid models achieve 41.67 BLEU surpassing individual architectures"}, {"from": "Hybrid multi-column encoder concatenating outputs of Transformer and RNMT+ encoders", "title": "validated_by", "to": "Hybrid models achieve 41.67 BLEU surpassing individual architectures"}, {"from": "BLEU improvement to 41.00 and 28.49 on WMT14 En\u2192Fr/En\u2192De for RNMT+", "title": "motivates", "to": "Deploy RNMT+ model incorporating multi-head attention, label smoothing, per-gate layer normalization, and synchronous training in NMT systems"}, {"from": "Ablation study showing quality drop without key techniques and instability without layer normalization", "title": "motivates", "to": "Apply per-gate layer normalization and adaptive gradient clipping during RNMT training"}, {"from": "Faster convergence and stable performance with synchronous training", "title": "motivates", "to": "Use synchronous replica training with warmup learning rate schedule for NMT"}, {"from": "Hybrid models achieve 41.67 BLEU surpassing individual architectures", "title": "motivates", "to": "Adopt hybrid NMT architecture with Transformer encoder and RNMT+ decoder using cascaded or multi-column encoders"}, {"from": "unaligned behavior in advanced AI systems", "title": "caused_by", "to": "high complexity tasks exceed human judgment capacity"}, {"from": "deceptive or dishonest AI behavior in AI systems", "title": "caused_by", "to": "misleading alignment signals from human feedback"}, {"from": "high complexity tasks exceed human judgment capacity", "title": "addressed_by", "to": "refuting lies is easier than constructing lies in debate game"}, {"from": "misleading alignment signals from human feedback", "title": "addressed_by", "to": "refuting lies is easier than constructing lies in debate game"}, {"from": "refuting lies is easier than constructing lies in debate game", "title": "mitigated_by", "to": "adversarial debate between duplicate agents supervised by human judge"}, {"from": "debate complexity equivalence to PSPACE enabling exponential information amplification", "title": "mitigated_by", "to": "adversarial debate between duplicate agents supervised by human judge"}, {"from": "short unbranched debates suffice due to adversarial focus", "title": "enabled_by", "to": "limiting debate length with answer precommitment reduces argument space"}, {"from": "adversarial debate between duplicate agents supervised by human judge", "title": "refined_by", "to": "human judges reward honesty and flaw detection paradigm"}, {"from": "adversarial debate between duplicate agents supervised by human judge", "title": "implemented_by", "to": "self-play reinforcement learning of debate agents"}, {"from": "adversarial debate between duplicate agents supervised by human judge", "title": "implemented_by", "to": "answer precommitment before debate turns"}, {"from": "adversarial debate between duplicate agents supervised by human judge", "title": "implemented_by", "to": "sparse pixel MNIST debate experimental setup"}, {"from": "adversarial debate between duplicate agents supervised by human judge", "title": "implemented_by", "to": "human single-pixel reveal image debate website"}, {"from": "adversarial debate between duplicate agents supervised by human judge", "title": "implemented_by", "to": "sharing activations between agents for transparency"}, {"from": "adversarial debate between duplicate agents supervised by human judge", "title": "validated_by", "to": "PSPACE proof demonstrating debate power"}, {"from": "train reward predictor to scale human debate judgments", "title": "implemented_by", "to": "training judge models to predict human reward"}, {"from": "self-play reinforcement learning of debate agents", "title": "validated_by", "to": "MNIST debate increases judge accuracy from 59.4% to 88.9%"}, {"from": "answer precommitment before debate turns", "title": "validated_by", "to": "MNIST debate increases judge accuracy from 59.4% to 88.9%"}, {"from": "training judge models to predict human reward", "title": "validated_by", "to": "MNIST debate increases judge accuracy from 59.4% to 88.9%"}, {"from": "training judge models to predict human reward", "title": "enabled_by", "to": "deploy reward predictor to automate large-scale debate training"}, {"from": "PSPACE proof demonstrating debate power", "title": "motivates", "to": "adversarial debate fine-tuning to align AI models"}, {"from": "MNIST debate increases judge accuracy from 59.4% to 88.9%", "title": "motivates", "to": "adversarial debate fine-tuning to align AI models"}, {"from": "informal human debate indicates honest agent usually wins", "title": "motivates", "to": "adversarial debate fine-tuning to align AI models"}, {"from": "adversarial debate fine-tuning to align AI models", "title": "refined_by", "to": "deploy reward predictor to automate large-scale debate training"}, {"from": "training performance bottlenecks in large-scale dynamic neural networks", "title": "caused_by", "to": "host-driven out-of-graph control flow synchronization overhead in distributed ML execution"}, {"from": "host-driven out-of-graph control flow synchronization overhead in distributed ML execution", "title": "mitigated_by", "to": "in-graph dynamic control flow enabling whole-program optimization and asynchronous execution in ML frameworks"}, {"from": "in-graph dynamic control flow enabling whole-program optimization and asynchronous execution in ML frameworks", "title": "mitigated_by", "to": "embedding dynamic control flow primitives within dataflow graphs for distributed execution"}, {"from": "in-graph dynamic control flow enabling whole-program optimization and asynchronous execution in ML frameworks", "title": "mitigated_by", "to": "static unrolling and client-side conditionals limit conditional computation and parallelism"}, {"from": "embedding dynamic control flow primitives within dataflow graphs for distributed execution", "title": "implemented_by", "to": "TensorFlow control-flow primitives (Switch, Merge, Enter, Exit, NextIteration) with executor tag frames"}, {"from": "embedding dynamic control flow primitives within dataflow graphs for distributed execution", "title": "implemented_by", "to": "local executor parallel-iteration tagging enabling nested control flow and distributed loops"}, {"from": "TensorFlow control-flow primitives (Switch, Merge, Enter, Exit, NextIteration) with executor tag frames", "title": "validated_by", "to": "microbenchmark 8-GPU iteration rate improvements showing 5\u00d7 speedup over out-of-graph loops"}, {"from": "microbenchmark 8-GPU iteration rate improvements showing 5\u00d7 speedup over out-of-graph loops", "title": "motivates", "to": "Implement in-graph dynamic control flow primitives in ML frameworks"}, {"from": "memory exhaustion in GPU training of long sequence RNNs", "title": "caused_by", "to": "retention of intermediate tensors during backpropagation across long sequences leading to large GPU memory footprint"}, {"from": "retention of intermediate tensors during backpropagation across long sequences leading to large GPU memory footprint", "title": "mitigated_by", "to": "overlapping compute with asynchronous memory swapping between GPU and CPU mitigates memory usage"}, {"from": "overlapping compute with asynchronous memory swapping between GPU and CPU mitigates memory usage", "title": "mitigated_by", "to": "stack-based state saving and multi-stream GPU I/O for overlapped swapping"}, {"from": "stack-based state saving and multi-stream GPU I/O for overlapped swapping", "title": "implemented_by", "to": "GPU-to-CPU tensor swapping triggered by memory threshold using separate CUDA streams"}, {"from": "GPU-to-CPU tensor swapping triggered by memory threshold using separate CUDA streams", "title": "validated_by", "to": "LSTM training sequence length doubled from 500 to 1000 with negligible overhead via swapping"}, {"from": "LSTM training sequence length doubled from 500 to 1000 with negligible overhead via swapping", "title": "motivates", "to": "Enable asynchronous GPU memory swapping for intermediate tensors during training loops"}, {"from": "limited model expressiveness due to absence of dynamic control flow in ML frameworks", "title": "caused_by", "to": "static unrolling and client-side conditionals limit conditional computation and parallelism"}, {"from": "local executor parallel-iteration tagging enabling nested control flow and distributed loops", "title": "validated_by", "to": "Deep Q-Network implementation speedup of 21% using in-graph control flow compared to baseline"}, {"from": "Deep Q-Network implementation speedup of 21% using in-graph control flow compared to baseline", "title": "motivates", "to": "Adopt in-graph dynamic control flow for reinforcement learning model graphs to enable parallel conditional execution"}, {"from": "confounding bias in causal effect estimates from observational multiple-cause data", "title": "caused_by", "to": "unobserved multi-cause confounders inducing dependence among assigned causes"}, {"from": "unobserved multi-cause confounders inducing dependence among assigned causes", "title": "mitigated_by", "to": "conditional independence of causes given latent variable removes multi-cause confounding"}, {"from": "conditional independence of causes given latent variable removes multi-cause confounding", "title": "refined_by", "to": "no unobserved single-cause confounders assumption weakens ignorability requirement"}, {"from": "no unobserved single-cause confounders assumption weakens ignorability requirement", "title": "implemented_by", "to": "inferring substitute confounder via latent factor model of causes"}, {"from": "no unobserved single-cause confounders assumption weakens ignorability requirement", "title": "motivates", "to": "Merge highly correlated causes prior to factor modeling to satisfy overlap condition"}, {"from": "inferring substitute confounder via latent factor model of causes", "title": "implemented_by", "to": "probabilistic factor model with per-individual latent variable for causes"}, {"from": "inferring substitute confounder via latent factor model of causes", "title": "refined_by", "to": "predictive model checks ensure factor model captures cause distribution"}, {"from": "predictive model checks ensure factor model captures cause distribution", "title": "implemented_by", "to": "posterior predictive check using held-out causes and log-likelihood discrepancy"}, {"from": "predictive model checks ensure factor model captures cause distribution", "title": "motivates", "to": "Select lowest-capacity factor model that passes predictive checks to minimize estimator variance"}, {"from": "probabilistic factor model with per-individual latent variable for causes", "title": "enabled_by", "to": "posterior expectation of latent variable as substitute confounder"}, {"from": "posterior expectation of latent variable as substitute confounder", "title": "implemented_by", "to": "iterated expectation conditioning on substitute confounder for outcome estimation"}, {"from": "iterated expectation conditioning on substitute confounder for outcome estimation", "title": "validated_by", "to": "simulation RMSE reduction in GWAS data using deconfounder (49.6\u219241.2)"}, {"from": "iterated expectation conditioning on substitute confounder for outcome estimation", "title": "validated_by", "to": "bias reduction in smoking expenses study with quadratic factor model"}, {"from": "posterior predictive check using held-out causes and log-likelihood discrepancy", "title": "validated_by", "to": "simulation RMSE reduction in GWAS data using deconfounder (49.6\u219241.2)"}, {"from": "posterior predictive check using held-out causes and log-likelihood discrepancy", "title": "validated_by", "to": "bias reduction in smoking expenses study with quadratic factor model"}, {"from": "posterior predictive check using held-out causes and log-likelihood discrepancy", "title": "motivates", "to": "Require predictive score threshold \u22650.1 before accepting factor model in deconfounder"}, {"from": "simulation RMSE reduction in GWAS data using deconfounder (49.6\u219241.2)", "title": "motivates", "to": "Apply deconfounder pipeline (factor modeling, predictive checking, substitute confounder, outcome modeling) during causal inference analyses"}, {"from": "simulation RMSE reduction in GWAS data using deconfounder (49.6\u219241.2)", "title": "motivates", "to": "Require predictive score threshold \u22650.1 before accepting factor model in deconfounder"}, {"from": "bias reduction in smoking expenses study with quadratic factor model", "title": "motivates", "to": "Apply deconfounder pipeline (factor modeling, predictive checking, substitute confounder, outcome modeling) during causal inference analyses"}, {"from": "Uninterpretable middle-layer representations in CNNs", "title": "caused_by", "to": "Chaotic mixture of object parts and textures in middle-layer feature maps"}, {"from": "Chaotic mixture of object parts and textures in middle-layer feature maps", "title": "mitigated_by", "to": "Object-part specific filter representation in CNNs"}, {"from": "Performance degradation from direct interpretability constraints in CNNs", "title": "caused_by", "to": "Interpretability\u2013performance trade-off in direct feature disentanglement"}, {"from": "Interpretability\u2013performance trade-off in direct feature disentanglement", "title": "mitigated_by", "to": "Separate explainer network preserves performance while improving interpretability in CNNs"}, {"from": "Object-part specific filter representation in CNNs", "title": "implemented_by", "to": "Autoencoder-style explainer attached to performer CNN for feature distillation"}, {"from": "Separate explainer network preserves performance while improving interpretability in CNNs", "title": "implemented_by", "to": "Autoencoder-style explainer attached to performer CNN for feature distillation"}, {"from": "Autoencoder-style explainer attached to performer CNN for feature distillation", "title": "implemented_by", "to": "Dual-track encoder with interpretable and ordinary paths in explainer"}, {"from": "Autoencoder-style explainer attached to performer CNN for feature distillation", "title": "implemented_by", "to": "Template-based filter loss for unsupervised part disentanglement"}, {"from": "Autoencoder-style explainer attached to performer CNN for feature distillation", "title": "implemented_by", "to": "Decoder reconstruction of performer upper-layer features"}, {"from": "Autoencoder-style explainer attached to performer CNN for feature distillation", "title": "implemented_by", "to": "Probability weighting of interpretable track contribution parameter p"}, {"from": "Dual-track encoder with interpretable and ordinary paths in explainer", "title": "validated_by", "to": "Location instability reduction of explainer filters over performer"}, {"from": "Template-based filter loss for unsupervised part disentanglement", "title": "validated_by", "to": "Location instability reduction of explainer filters over performer"}, {"from": "Decoder reconstruction of performer upper-layer features", "title": "validated_by", "to": "Minor classification accuracy gap after explainer reconstruction"}, {"from": "Probability weighting of interpretable track contribution parameter p", "title": "validated_by", "to": "High interpretable track contribution (p 0.8\u20130.96)"}, {"from": "Location instability reduction of explainer filters over performer", "title": "motivates", "to": "Attach autoencoder-style explainer networks with interpretable track to pretrained CNNs during deployment for real-time interpretability"}, {"from": "Location instability reduction of explainer filters over performer", "title": "motivates", "to": "Fine-tune pretrained CNNs jointly with explainer and filter loss before deployment to achieve part-level feature disentanglement"}, {"from": "Minor classification accuracy gap after explainer reconstruction", "title": "motivates", "to": "Attach autoencoder-style explainer networks with interpretable track to pretrained CNNs during deployment for real-time interpretability"}, {"from": "Minor classification accuracy gap after explainer reconstruction", "title": "motivates", "to": "Fine-tune pretrained CNNs jointly with explainer and filter loss before deployment to achieve part-level feature disentanglement"}, {"from": "High interpretable track contribution (p 0.8\u20130.96)", "title": "motivates", "to": "Attach autoencoder-style explainer networks with interpretable track to pretrained CNNs during deployment for real-time interpretability"}, {"from": "High interpretable track contribution (p 0.8\u20130.96)", "title": "motivates", "to": "Fine-tune pretrained CNNs jointly with explainer and filter loss before deployment to achieve part-level feature disentanglement"}, {"from": "Poor generalization of meta-learned models to unseen tasks", "title": "caused_by", "to": "Biased initial model over-performs on sampled meta-training tasks"}, {"from": "Biased initial model over-performs on sampled meta-training tasks", "title": "caused_by", "to": "Absence of task-agnostic prior leads to biased initial performance in meta-learning"}, {"from": "Absence of task-agnostic prior leads to biased initial performance in meta-learning", "title": "mitigated_by", "to": "Entropy regularization to enforce task-agnostic prior in classification tasks"}, {"from": "Absence of task-agnostic prior leads to biased initial performance in meta-learning", "title": "mitigated_by", "to": "Loss-inequality minimisation regularization to enforce task-agnostic prior"}, {"from": "Entropy regularization to enforce task-agnostic prior in classification tasks", "title": "implemented_by", "to": "\u03bb(\u2212H_before + H_after) entropy term in meta-training objective"}, {"from": "\u03bb(\u2212H_before + H_after) entropy term in meta-training objective", "title": "validated_by", "to": "Entropy-TAML accuracy improvements on Omniglot and MiniImagenet benchmarks"}, {"from": "Entropy-TAML accuracy improvements on Omniglot and MiniImagenet benchmarks", "title": "motivates", "to": "Apply entropy-based task-agnostic regularization during meta-training"}, {"from": "Loss-inequality minimisation regularization to enforce task-agnostic prior", "title": "implemented_by", "to": "IE({losses}) regularisation term with Theil, GE, Gini, Atkinson or VL indices"}, {"from": "IE({losses}) regularisation term with Theil, GE, Gini, Atkinson or VL indices", "title": "validated_by", "to": "Inequality-TAML performance gains on vision and RL tasks"}, {"from": "Inequality-TAML performance gains on vision and RL tasks", "title": "motivates", "to": "Apply inequality-measure based task-agnostic regularization during meta-training"}, {"from": "catastrophic policy degradation in reinforcement learning", "title": "caused_by", "to": "high improvement penalty from inaccurate Q-value estimation"}, {"from": "excessive sample complexity in deep reinforcement learning", "title": "caused_by", "to": "low data efficiency of greedy policy improvement in deep RL"}, {"from": "high improvement penalty from inaccurate Q-value estimation", "title": "caused_by", "to": "large variance in Q-value estimates for low-frequency actions"}, {"from": "high improvement penalty from inaccurate Q-value estimation", "title": "caused_by", "to": "unbounded action probability shifts in existing constrained policy updates"}, {"from": "high improvement penalty from inaccurate Q-value estimation", "title": "mitigated_by", "to": "variance of improvement penalty proportional to |beta-pi|^2/beta"}, {"from": "unbounded action probability shifts in existing constrained policy updates", "title": "caused_by", "to": "low data efficiency of greedy policy improvement in deep RL"}, {"from": "variance of improvement penalty proportional to |beta-pi|^2/beta", "title": "enables", "to": "multiplicative reroute constraint on policy update"}, {"from": "bounding action probability ratio limits improvement penalty", "title": "mitigated_by", "to": "low data efficiency of greedy policy improvement in deep RL"}, {"from": "multiplicative reroute constraint on policy update", "title": "refined_by", "to": "statewise linear program for optimal policy under reroute constraint"}, {"from": "multiplicative reroute constraint on policy update", "title": "implemented_by", "to": "Max-Reroute algorithm with parameters cmin,cmax"}, {"from": "multiplicative reroute constraint on policy update", "title": "enabled_by", "to": "actor computes non-parametric optimal policy and learner imitates via KL"}, {"from": "statewise linear program for optimal policy under reroute constraint", "title": "implemented_by", "to": "Max-Reroute algorithm with parameters cmin,cmax"}, {"from": "actor computes non-parametric optimal policy and learner imitates via KL", "title": "implemented_by", "to": "RBI distributed actor-learner architecture"}, {"from": "actor computes non-parametric optimal policy and learner imitates via KL", "title": "implemented_by", "to": "advantage variance weighted prioritization in learner loss"}, {"from": "Max-Reroute algorithm with parameters cmin,cmax", "title": "validated_by", "to": "theoretical guarantee of positive improvement step"}, {"from": "Max-Reroute algorithm with parameters cmin,cmax", "title": "validated_by", "to": "reduced regret in two-armed Gaussian bandit experiments"}, {"from": "Max-Reroute algorithm with parameters cmin,cmax", "title": "validated_by", "to": "improved Atari performance from human observation data"}, {"from": "RBI distributed actor-learner architecture", "title": "validated_by", "to": "superior performance over Ape-X in distributed Atari experiments"}, {"from": "advantage variance weighted prioritization in learner loss", "title": "validated_by", "to": "superior performance over Ape-X in distributed Atari experiments"}, {"from": "theoretical guarantee of positive improvement step", "title": "motivates", "to": "Apply reroute constrained policy improvement (RBI) during off-policy batch RL fine-tuning"}, {"from": "reduced regret in two-armed Gaussian bandit experiments", "title": "motivates", "to": "Apply reroute constrained policy improvement (RBI) during off-policy batch RL fine-tuning"}, {"from": "improved Atari performance from human observation data", "title": "motivates", "to": "Apply reroute constrained policy improvement (RBI) during off-policy batch RL fine-tuning"}, {"from": "superior performance over Ape-X in distributed Atari experiments", "title": "motivates", "to": "Integrate Max-Reroute per-state optimization in actor policy during RL training"}, {"from": "superior performance over Ape-X in distributed Atari experiments", "title": "motivates", "to": "Set reroute parameters cmin=0.1,cmax=2 and mix 10% greedy policy in distributed RL"}, {"from": "superior performance over Ape-X in distributed Atari experiments", "title": "motivates", "to": "Weight policy loss by advantage variance prioritization during learner updates"}, {"from": "Misestimation of agent reward functions in online partially observable environments", "title": "caused_by", "to": "Computational timeouts of batch IRL under streaming data constraints"}, {"from": "Misestimation of agent reward functions in online partially observable environments", "title": "caused_by", "to": "Occluded trajectory segments causing missing feature data"}, {"from": "Computational timeouts of batch IRL under streaming data constraints", "title": "mitigated_by", "to": "Incremental session-based reward update improves scalability in streaming IRL"}, {"from": "Occluded trajectory segments causing missing feature data", "title": "mitigated_by", "to": "Latent variable expectation over hidden trajectory data recovers missing information"}, {"from": "Incremental session-based reward update improves scalability in streaming IRL", "title": "enabled_by", "to": "Online IRL framework I2RL enabling continuous sessions"}, {"from": "Latent variable expectation over hidden trajectory data recovers missing information", "title": "enabled_by", "to": "Latent maximum entropy formulation with EM optimizes reward under occlusion"}, {"from": "Latent variable expectation over hidden trajectory data recovers missing information", "title": "motivates", "to": "Sample hidden trajectory completions during E-step to estimate feature expectations under occlusion"}, {"from": "Online IRL framework I2RL enabling continuous sessions", "title": "implemented_by", "to": "Incremental LME algorithm with per-session E-step and M-step updating feature expectations"}, {"from": "Latent maximum entropy formulation with EM optimizes reward under occlusion", "title": "implemented_by", "to": "Incremental LME algorithm with per-session E-step and M-step updating feature expectations"}, {"from": "Incremental LME algorithm with per-session E-step and M-step updating feature expectations", "title": "required_by", "to": "Log-likelihood convergence stopping criterion for I2RL session termination"}, {"from": "Incremental LME algorithm with per-session E-step and M-step updating feature expectations", "title": "validated_by", "to": "Monotonic likelihood improvement and probabilistic convergence guarantees for incremental LME"}, {"from": "Incremental LME algorithm with per-session E-step and M-step updating feature expectations", "title": "validated_by", "to": "Perimeter patrol experiments showing 4x speedup and higher success rate compared to batch LME"}, {"from": "Log-likelihood convergence stopping criterion for I2RL session termination", "title": "validated_by", "to": "Monotonic likelihood improvement and probabilistic convergence guarantees for incremental LME"}, {"from": "Monotonic likelihood improvement and probabilistic convergence guarantees for incremental LME", "title": "motivates", "to": "Deploy incremental latent maximum entropy IRL for online preference inference in partially observable tasks"}, {"from": "Monotonic likelihood improvement and probabilistic convergence guarantees for incremental LME", "title": "motivates", "to": "Terminate I2RL sessions using small log-likelihood threshold to reduce computation"}, {"from": "Perimeter patrol experiments showing 4x speedup and higher success rate compared to batch LME", "title": "motivates", "to": "Deploy incremental latent maximum entropy IRL for online preference inference in partially observable tasks"}, {"from": "Reduced human situational awareness from irrelevant agent information in partially observed human-robot missions", "title": "caused_by", "to": "Agent ignorance of human-weighted information preferences in partially observed domains"}, {"from": "Agent ignorance of human-weighted information preferences in partially observed domains", "title": "mitigated_by", "to": "Weighted entropy reduction as proxy for human information preference in partially observed domains"}, {"from": "Weighted entropy reduction as proxy for human information preference in partially observed domains", "title": "implemented_by", "to": "Agent objective of maximizing weighted information gain conditioned on task-optimal action planning"}, {"from": "Agent objective of maximizing weighted information gain conditioned on task-optimal action planning", "title": "implemented_by", "to": "Determinize-and-replan algorithm with DAG longest-path for information selection"}, {"from": "Agent objective of maximizing weighted information gain conditioned on task-optimal action planning", "title": "implemented_by", "to": "Replay-buffer-based online learning of weighted entropy parameters via \u03f5-greedy exploration"}, {"from": "Determinize-and-replan algorithm with DAG longest-path for information selection", "title": "validated_by", "to": "Empirical performance improvements in 2D mining and 3D fetching tasks under weighted entropy model"}, {"from": "Replay-buffer-based online learning of weighted entropy parameters via \u03f5-greedy exploration", "title": "validated_by", "to": "Empirical performance improvements in 2D mining and 3D fetching tasks under weighted entropy model"}, {"from": "Empirical performance improvements in 2D mining and 3D fetching tasks under weighted entropy model", "title": "motivates", "to": "Implement determinize-and-replan information planning algorithm in autonomous agents prior to deployment"}, {"from": "Empirical performance improvements in 2D mining and 3D fetching tasks under weighted entropy model", "title": "motivates", "to": "Deploy online learning of human weighted entropy preferences with replay buffer during mission execution"}, {"from": "negative side effects from reward misspecification in reinforcement learning", "title": "caused_by", "to": "incomplete reward specification in complex environments"}, {"from": "incomplete reward specification in complex environments", "title": "caused_by", "to": "maxmin formulation over consistent reward polytope in reinforcement learning"}, {"from": "maxmin formulation over consistent reward polytope in reinforcement learning", "title": "mitigated_by", "to": "maxmin policy maximizes worst-case reward to ensure safety"}, {"from": "maxmin formulation over consistent reward polytope in reinforcement learning", "title": "mitigated_by", "to": "combining multiple expert demonstrations to define consistent reward polytope"}, {"from": "maxmin policy maximizes worst-case reward to ensure safety", "title": "implemented_by", "to": "ellipsoid method with separation oracles using MDP solver for exact maxmin policy"}, {"from": "maxmin policy maximizes worst-case reward to ensure safety", "title": "implemented_by", "to": "FPL-based iterative maxmin policy learning with MDP solver"}, {"from": "combining multiple expert demonstrations to define consistent reward polytope", "title": "implemented_by", "to": "weird separation oracle enabling approximate MDP solver usage"}, {"from": "ellipsoid method with separation oracles using MDP solver for exact maxmin policy", "title": "validated_by", "to": "polynomial-time solvability guarantee for ellipsoid maxmin algorithm"}, {"from": "FPL-based iterative maxmin policy learning with MDP solver", "title": "validated_by", "to": "O(1/\u221aT) convergence guarantee for FPL maxmin algorithm"}, {"from": "FPL-based iterative maxmin policy learning with MDP solver", "title": "validated_by", "to": "gridworld experiments showing avoidance of unseen harmful terrain"}, {"from": "FPL-based iterative maxmin policy learning with MDP solver", "title": "validated_by", "to": "cartpole experiments demonstrating safe behavior with multiple expert policies"}, {"from": "weird separation oracle enabling approximate MDP solver usage", "title": "validated_by", "to": "polynomial-time solvability guarantee for ellipsoid maxmin algorithm"}, {"from": "polynomial-time solvability guarantee for ellipsoid maxmin algorithm", "title": "motivates", "to": "train RL agents with ellipsoid-based exact maxmin learning during policy optimisation"}, {"from": "polynomial-time solvability guarantee for ellipsoid maxmin algorithm", "title": "motivates", "to": "construct consistent reward polytope from multiple expert demonstrations before training"}, {"from": "O(1/\u221aT) convergence guarantee for FPL maxmin algorithm", "title": "motivates", "to": "train RL agents with FPL-based iterative maxmin learning during policy optimisation"}, {"from": "gridworld experiments showing avoidance of unseen harmful terrain", "title": "motivates", "to": "train RL agents with FPL-based iterative maxmin learning during policy optimisation"}, {"from": "cartpole experiments demonstrating safe behavior with multiple expert policies", "title": "motivates", "to": "train RL agents with FPL-based iterative maxmin learning during policy optimisation"}, {"from": "Brittle agent behavior from mis-modeled expert stochasticity in imitation learning", "title": "caused_by", "to": "Softmax over-allocation to non-expert actions in large discrete action spaces"}, {"from": "Brittle agent behavior from mis-modeled expert stochasticity in imitation learning", "title": "caused_by", "to": "Uni-modal Gaussian policy limitation in continuous action spaces"}, {"from": "Brittle agent behavior from mis-modeled expert stochasticity in imitation learning", "title": "caused_by", "to": "Mode collapse in mixture density networks under intractable Gibbs-Shannon entropy regularisation"}, {"from": "Softmax over-allocation to non-expert actions in large discrete action spaces", "title": "addressed_by", "to": "Sparsemax distribution with adaptable support for sparse multi-modal policies"}, {"from": "Uni-modal Gaussian policy limitation in continuous action spaces", "title": "addressed_by", "to": "Sparsemax distribution with adaptable support for sparse multi-modal policies"}, {"from": "Mode collapse in mixture density networks under intractable Gibbs-Shannon entropy regularisation", "title": "addressed_by", "to": "Analytic Tsallis entropy of sparse MDN encouraging exploration and preventing mixture collapse"}, {"from": "Sparsemax distribution with adaptable support for sparse multi-modal policies", "title": "enabled_by", "to": "Model expert behavior with sparse MDN and maximize causal Tsallis entropy"}, {"from": "Analytic Tsallis entropy of sparse MDN encouraging exploration and preventing mixture collapse", "title": "enabled_by", "to": "Model expert behavior with sparse MDN and maximize causal Tsallis entropy"}, {"from": "Model expert behavior with sparse MDN and maximize causal Tsallis entropy", "title": "implemented_by", "to": "MCTEIL algorithm employing sparse MDN with Tsallis entropy bonus"}, {"from": "MCTEIL algorithm employing sparse MDN with Tsallis entropy bonus", "title": "validated_by", "to": "MCTEIL reachability and return improvements in multi-goal simulation"}, {"from": "MCTEIL algorithm employing sparse MDN with Tsallis entropy bonus", "title": "validated_by", "to": "MCTEIL outperforming BC and GAIL on MuJoCo continuous control tasks"}, {"from": "MCTEIL reachability and return improvements in multi-goal simulation", "title": "motivates", "to": "Fine-tune agents with MCTEIL using sparse MDN during RL stage"}, {"from": "MCTEIL reachability and return improvements in multi-goal simulation", "title": "motivates", "to": "Adopt sparsemax-weighted mixture density networks in imitation learning model design"}, {"from": "MCTEIL reachability and return improvements in multi-goal simulation", "title": "motivates", "to": "Add causal Tsallis entropy bonus to policy optimization for exploration"}, {"from": "MCTEIL outperforming BC and GAIL on MuJoCo continuous control tasks", "title": "motivates", "to": "Fine-tune agents with MCTEIL using sparse MDN during RL stage"}, {"from": "MCTEIL outperforming BC and GAIL on MuJoCo continuous control tasks", "title": "motivates", "to": "Adopt sparsemax-weighted mixture density networks in imitation learning model design"}, {"from": "MCTEIL outperforming BC and GAIL on MuJoCo continuous control tasks", "title": "motivates", "to": "Add causal Tsallis entropy bonus to policy optimization for exploration"}, {"from": "Training instability and slow convergence in large-scale deep neural network optimization", "title": "caused_by", "to": "First-order gradient methods lack curvature information in large-scale DNNs"}, {"from": "Excessive computation cost of second-order optimization in deep neural network training", "title": "caused_by", "to": "Many PCG iterations required for Hessian-free natural gradient approximation"}, {"from": "First-order gradient methods lack curvature information in large-scale DNNs", "title": "addressed_by", "to": "Approximating natural gradient with Hessian-free method improves convergence if computational burden is reduced"}, {"from": "Many PCG iterations required for Hessian-free natural gradient approximation", "title": "addressed_by", "to": "Adaptive damping and preconditioning can reduce required PCG iterations"}, {"from": "Approximating natural gradient with Hessian-free method improves convergence if computational burden is reduced", "title": "mitigated_by", "to": "Meta-learning RNNs infer damping parameters and preconditioner to approximate natural gradient efficiently"}, {"from": "Adaptive damping and preconditioning can reduce required PCG iterations", "title": "mitigated_by", "to": "Meta-learning RNNs infer damping parameters and preconditioner to approximate natural gradient efficiently"}, {"from": "Meta-learning RNNs infer damping parameters and preconditioner to approximate natural gradient efficiently", "title": "implemented_by", "to": "RNNs output per-parameter damping vector s for modified Hessian"}, {"from": "Meta-learning RNNs infer damping parameters and preconditioner to approximate natural gradient efficiently", "title": "implemented_by", "to": "RNNp outputs diagonal preconditioner P for PCG in Hessian-free optimization"}, {"from": "Meta-learning RNNs infer damping parameters and preconditioner to approximate natural gradient efficiently", "title": "implemented_by", "to": "Limited PCG iterations with warm-start initialization reduce compute cost"}, {"from": "Meta-learning RNNs infer damping parameters and preconditioner to approximate natural gradient efficiently", "title": "implemented_by", "to": "Coordinatewise LSTM framework shares meta-parameters across layer types"}, {"from": "RNNs output per-parameter damping vector s for modified Hessian", "title": "validated_by", "to": "MLHF reduces loss faster than SGD, RMSprop and kfac on CIFAR10 Convnet"}, {"from": "RNNp outputs diagonal preconditioner P for PCG in Hessian-free optimization", "title": "validated_by", "to": "Ablation shows learned preconditioner enables few PCG iterations without accuracy loss"}, {"from": "RNNp outputs diagonal preconditioner P for PCG in Hessian-free optimization", "title": "validated_by", "to": "MLHF outperforms first-order optimizers on ResNet18 v2 ImageNet training"}, {"from": "Limited PCG iterations with warm-start initialization reduce compute cost", "title": "validated_by", "to": "Ablation shows learned preconditioner enables few PCG iterations without accuracy loss"}, {"from": "Coordinatewise LSTM framework shares meta-parameters across layer types", "title": "validated_by", "to": "MLHF outperforms first-order optimizers on ResNet18 v2 ImageNet training"}, {"from": "MLHF reduces loss faster than SGD, RMSprop and kfac on CIFAR10 Convnet", "title": "motivates", "to": "Apply MLHF meta-optimizer with learned damping and preconditioning during model training"}, {"from": "MLHF outperforms first-order optimizers on ResNet18 v2 ImageNet training", "title": "motivates", "to": "Apply MLHF meta-optimizer with learned damping and preconditioning during model training"}, {"from": "Ablation shows learned preconditioner enables few PCG iterations without accuracy loss", "title": "motivates", "to": "Apply MLHF meta-optimizer with learned damping and preconditioning during model training"}, {"from": "misclassification under adversarial perturbations in neural networks", "title": "caused_by", "to": "lack of provable robustness guarantees in neural network training"}, {"from": "misclassification under adversarial perturbations in neural networks", "title": "caused_by", "to": "computational overhead of per-example dual optimization in verified training"}, {"from": "lack of provable robustness guarantees in neural network training", "title": "mitigated_by", "to": "duality-based verification bounds independent of dual variable optimization"}, {"from": "computational overhead of per-example dual optimization in verified training", "title": "mitigated_by", "to": "duality-based verification bounds independent of dual variable optimization"}, {"from": "duality-based verification bounds independent of dual variable optimization", "title": "mitigated_by", "to": "learning mapping from examples to dual variables via verifier network"}, {"from": "learning mapping from examples to dual variables via verifier network", "title": "implemented_by", "to": "predictor-verifier training with joint task and dual loss"}, {"from": "learning mapping from examples to dual variables via verifier network", "title": "implemented_by", "to": "backward-forward verifier architecture for dual variable prediction"}, {"from": "learning mapping from examples to dual variables via verifier network", "title": "implemented_by", "to": "direct verifier architecture for dual variable prediction"}, {"from": "predictor-verifier training with joint task and dual loss", "title": "validated_by", "to": "state-of-the-art verified accuracy on MNIST and SVHN with PVT"}, {"from": "backward-forward verifier architecture for dual variable prediction", "title": "validated_by", "to": "comparable verified bounds of direct and backward-forward verifiers"}, {"from": "direct verifier architecture for dual variable prediction", "title": "validated_by", "to": "comparable verified bounds of direct and backward-forward verifiers"}, {"from": "state-of-the-art verified accuracy on MNIST and SVHN with PVT", "title": "motivates", "to": "integrate predictor-verifier training during neural network fine-tuning to obtain provable adversarial robustness"}, {"from": "comparable verified bounds of direct and backward-forward verifiers", "title": "motivates", "to": "integrate predictor-verifier training during neural network fine-tuning to obtain provable adversarial robustness"}, {"from": "Hard exploration failure in deep RL agents on sparse-reward Atari games", "title": "caused_by", "to": "Sparse environmental reward signals in hard exploration Atari games"}, {"from": "Hard exploration failure in deep RL agents on sparse-reward Atari games", "title": "caused_by", "to": "Domain gap between demonstration videos and agent observations in third-person imitation"}, {"from": "Hard exploration failure in deep RL agents on sparse-reward Atari games", "title": "caused_by", "to": "Absence of demonstrator action sequences in YouTube-based imitation learning"}, {"from": "Sparse environmental reward signals in hard exploration Atari games", "title": "mitigated_by", "to": "Embedding-based checkpoint reward as substitute for action labels in imitation learning"}, {"from": "Domain gap between demonstration videos and agent observations in third-person imitation", "title": "mitigated_by", "to": "Self-supervised domain-invariant embedding learning via temporal and cross-modal distance classification in Atari gameplay"}, {"from": "Absence of demonstrator action sequences in YouTube-based imitation learning", "title": "mitigated_by", "to": "Embedding-based checkpoint reward as substitute for action labels in imitation learning"}, {"from": "Self-supervised domain-invariant embedding learning via temporal and cross-modal distance classification in Atari gameplay", "title": "implemented_by", "to": "Combine TDC and CMC losses to learn common representation across domains"}, {"from": "Embedding-based checkpoint reward as substitute for action labels in imitation learning", "title": "implemented_by", "to": "Sequential checkpoint generation along demonstration embedding for auxiliary imitation reward"}, {"from": "Combine TDC and CMC losses to learn common representation across domains", "title": "implemented_by", "to": "Neural networks \u03d5, \u03c8, \u03c4td, \u03c4cm trained with temporal and cross-modal classification losses"}, {"from": "Sequential checkpoint generation along demonstration embedding for auxiliary imitation reward", "title": "implemented_by", "to": "Similarity-based imitation reward function with threshold \u03b3"}, {"from": "Neural networks \u03d5, \u03c8, \u03c4td, \u03c4cm trained with temporal and cross-modal classification losses", "title": "validated_by", "to": "Cycle-consistency metric (P\u03d5=44.2) showing cross-domain alignment improvement"}, {"from": "Neural networks \u03d5, \u03c8, \u03c4td, \u03c4cm trained with temporal and cross-modal classification losses", "title": "validated_by", "to": "t-SNE visualization demonstrates aligned trajectories across domains"}, {"from": "Similarity-based imitation reward function with threshold \u03b3", "title": "validated_by", "to": "Human-level performance on Montezuma\u2019s Revenge, Pitfall!, and Private Eye via imitation-only reward"}, {"from": "Cycle-consistency metric (P\u03d5=44.2) showing cross-domain alignment improvement", "title": "motivates", "to": "Pre-train embeddings with combined TDC and CMC on unlabeled demonstration videos"}, {"from": "Cycle-consistency metric (P\u03d5=44.2) showing cross-domain alignment improvement", "title": "motivates", "to": "Use cycle-consistency metric to select embedding models before RL training"}, {"from": "t-SNE visualization demonstrates aligned trajectories across domains", "title": "motivates", "to": "Pre-train embeddings with combined TDC and CMC on unlabeled demonstration videos"}, {"from": "Human-level performance on Montezuma\u2019s Revenge, Pitfall!, and Private Eye via imitation-only reward", "title": "motivates", "to": "Train RL agents with similarity-based checkpoint imitation reward derived from pre-trained embedding"}, {"from": "Human-level performance on Montezuma\u2019s Revenge, Pitfall!, and Private Eye via imitation-only reward", "title": "motivates", "to": "Combine imitation reward with environment reward during IMPALA training for additional performance"}, {"from": "Unreliable and biased decisions from opaque machine learning models in safety-critical contexts", "title": "caused_by", "to": "Opaqueness of deep neural network inner workings"}, {"from": "Unreliable and biased decisions from opaque machine learning models in safety-critical contexts", "title": "caused_by", "to": "Lack of interpretability and completeness in model explanations"}, {"from": "Erosion of user trust and regulatory non-compliance from unexplained AI decisions", "title": "caused_by", "to": "Opaqueness of deep neural network inner workings"}, {"from": "Opaqueness of deep neural network inner workings", "title": "mitigated_by", "to": "Tradeoff between interpretability and completeness in AI explanations"}, {"from": "Tradeoff between interpretability and completeness in AI explanations", "title": "enabled_by", "to": "Taxonomy-guided approach to develop explainable AI methods"}, {"from": "Tradeoff between interpretability and completeness in AI explanations", "title": "enabled_by", "to": "Multi-level explanation capability accommodating diverse user needs"}, {"from": "Taxonomy-guided approach to develop explainable AI methods", "title": "implemented_by", "to": "Local proxy models for processing explanations in deep networks"}, {"from": "Taxonomy-guided approach to develop explainable AI methods", "title": "implemented_by", "to": "Decision tree extraction from neural networks"}, {"from": "Taxonomy-guided approach to develop explainable AI methods", "title": "implemented_by", "to": "Salience mapping via input occlusion and gradient analysis"}, {"from": "Taxonomy-guided approach to develop explainable AI methods", "title": "implemented_by", "to": "Automatic rule extraction techniques for neural network decisions"}, {"from": "Taxonomy-guided approach to develop explainable AI methods", "title": "implemented_by", "to": "Attention-based network architectures exposing internal focus"}, {"from": "Taxonomy-guided approach to develop explainable AI methods", "title": "implemented_by", "to": "Disentangled representation learning for interpretable factors"}, {"from": "Taxonomy-guided approach to develop explainable AI methods", "title": "motivates", "to": "Adopt taxonomy-based multi-metric evaluation framework for explainability compliance testing"}, {"from": "Multi-level explanation capability accommodating diverse user needs", "title": "implemented_by", "to": "Explanation-generating models producing natural language and visual rationales"}, {"from": "Local proxy models for processing explanations in deep networks", "title": "validated_by", "to": "Local fidelity metrics assessing proxy model faithfulness"}, {"from": "Decision tree extraction from neural networks", "title": "validated_by", "to": "Local fidelity metrics assessing proxy model faithfulness"}, {"from": "Salience mapping via input occlusion and gradient analysis", "title": "validated_by", "to": "Occlusion-grounded sensitivity benchmarks for salience methods"}, {"from": "Attention-based network architectures exposing internal focus", "title": "validated_by", "to": "Human attention alignment scores for attention mechanisms"}, {"from": "Disentangled representation learning for interpretable factors", "title": "validated_by", "to": "Transfer learning performance measuring layer representation utility"}, {"from": "Explanation-generating models producing natural language and visual rationales", "title": "validated_by", "to": "User studies evaluating clarity and helpfulness of generated explanations"}, {"from": "Local fidelity metrics assessing proxy model faithfulness", "title": "motivates", "to": "Deploy local proxy explanation tools (e.g., LIME) to audit model predictions before deployment"}, {"from": "Local fidelity metrics assessing proxy model faithfulness", "title": "motivates", "to": "Apply decision tree extraction to neural networks for logic verification during pre-deployment review"}, {"from": "Occlusion-grounded sensitivity benchmarks for salience methods", "title": "motivates", "to": "Conduct salience map sensitivity analyses to identify spurious correlations during model validation"}, {"from": "Transfer learning performance measuring layer representation utility", "title": "motivates", "to": "Incorporate disentanglement objectives in model design to enhance transparency"}, {"from": "Human attention alignment scores for attention mechanisms", "title": "motivates", "to": "Regularize attention weights during fine-tuning to align with human attention patterns"}, {"from": "User studies evaluating clarity and helpfulness of generated explanations", "title": "motivates", "to": "Co-train natural language explanation generators with task models to provide end-user rationales at deployment"}, {"from": "Human\u2013robot collision in shared workspace", "title": "caused_by", "to": "Inaccurate human motion prediction from fixed utility model"}, {"from": "Human\u2013robot collision in shared workspace", "title": "caused_by", "to": "Unaccounted robot tracking error under uncertain human motion"}, {"from": "Inaccurate human motion prediction from fixed utility model", "title": "mitigated_by", "to": "Rationality coefficient \u03b2 as indicator of model confidence in utility-based human models"}, {"from": "Unaccounted robot tracking error under uncertain human motion", "title": "mitigated_by", "to": "Combining probabilistic human predictions with worst-case robot tracking error to bound collision probability"}, {"from": "Rationality coefficient \u03b2 as indicator of model confidence in utility-based human models", "title": "mitigated_by", "to": "Real-time Bayesian inference of model confidence to adapt human motion predictions"}, {"from": "Combining probabilistic human predictions with worst-case robot tracking error to bound collision probability", "title": "mitigated_by", "to": "Probabilistic safety certificate using max marginal collision probability and FaSTrack bound"}, {"from": "Real-time Bayesian inference of model confidence to adapt human motion predictions", "title": "implemented_by", "to": "Hidden Markov model inference over discrete \u03b2 values"}, {"from": "Probabilistic safety certificate using max marginal collision probability and FaSTrack bound", "title": "implemented_by", "to": "FaSTrack tracking error bound integration with Minkowski collision set"}, {"from": "Probabilistic safety certificate using max marginal collision probability and FaSTrack bound", "title": "implemented_by", "to": "Online planner with node rejection based on collision probability threshold"}, {"from": "Hidden Markov model inference over discrete \u03b2 values", "title": "validated_by", "to": "Simulation results: adaptive \u03b2 improves safety distance and efficiency"}, {"from": "FaSTrack tracking error bound integration with Minkowski collision set", "title": "validated_by", "to": "Hardware demonstration of quadcopter avoiding humans"}, {"from": "Online planner with node rejection based on collision probability threshold", "title": "validated_by", "to": "Hardware demonstration of quadcopter avoiding humans"}, {"from": "Simulation results: adaptive \u03b2 improves safety distance and efficiency", "title": "motivates", "to": "Deploy real-time Bayesian \u03b2 inference for human motion prediction during robot operation"}, {"from": "Hardware demonstration of quadcopter avoiding humans", "title": "motivates", "to": "Integrate confidence-aware human predictions with FaSTrack-based probabilistic safety planning"}, {"from": "Research resource misallocation due to performance-only AI benchmarks", "title": "caused_by", "to": "Overemphasis on performance metrics neglecting resource costs in AI progress assessment"}, {"from": "Research resource misallocation due to performance-only AI benchmarks", "title": "caused_by", "to": "Benchmark specialization reducing solution generality in AI research"}, {"from": "Research resource misallocation due to performance-only AI benchmarks", "title": "caused_by", "to": "Insufficient reproducibility and replicability data in AI publications"}, {"from": "Unaccounted societal externalities from AI system deployment", "title": "caused_by", "to": "Overemphasis on performance metrics neglecting resource costs in AI progress assessment"}, {"from": "Unaccounted societal externalities from AI system deployment", "title": "caused_by", "to": "Insufficient reproducibility and replicability data in AI publications"}, {"from": "Overemphasis on performance metrics neglecting resource costs in AI progress assessment", "title": "mitigated_by", "to": "Utility functions integrating performance and resource costs align evaluation with stakeholder value"}, {"from": "Overemphasis on performance metrics neglecting resource costs in AI progress assessment", "title": "mitigated_by", "to": "Pareto-front analysis exposes trade-offs across performance and resources"}, {"from": "Benchmark specialization reducing solution generality in AI research", "title": "mitigated_by", "to": "Pareto-front analysis exposes trade-offs across performance and resources"}, {"from": "Insufficient reproducibility and replicability data in AI publications", "title": "mitigated_by", "to": "Utility functions integrating performance and resource costs align evaluation with stakeholder value"}, {"from": "Utility functions integrating performance and resource costs align evaluation with stakeholder value", "title": "refined_by", "to": "Pareto-front analysis exposes trade-offs across performance and resources"}, {"from": "Utility functions integrating performance and resource costs align evaluation with stakeholder value", "title": "mitigated_by", "to": "Develop cost-sensitive AI evaluation frameworks covering full resource life cycle"}, {"from": "Utility functions integrating performance and resource costs align evaluation with stakeholder value", "title": "mitigated_by", "to": "Promote comprehensive resource reporting standards for AI research"}, {"from": "Utility functions integrating performance and resource costs align evaluation with stakeholder value", "title": "mitigated_by", "to": "Use open leaderboards with code sharing to facilitate replicability"}, {"from": "Pareto-front analysis exposes trade-offs across performance and resources", "title": "mitigated_by", "to": "Develop cost-sensitive AI evaluation frameworks covering full resource life cycle"}, {"from": "Pareto-front analysis exposes trade-offs across performance and resources", "title": "mitigated_by", "to": "Promote comprehensive resource reporting standards for AI research"}, {"from": "Develop cost-sensitive AI evaluation frameworks covering full resource life cycle", "title": "implemented_by", "to": "Resource taxonomy rd rk rs rh rm rc rn rt for AI system accounting"}, {"from": "Develop cost-sensitive AI evaluation frameworks covering full resource life cycle", "title": "implemented_by", "to": "Multi-dimensional benchmark plots of performance versus resource consumption"}, {"from": "Promote comprehensive resource reporting standards for AI research", "title": "implemented_by", "to": "Policy requirement to include compute and data metrics in academic submission templates"}, {"from": "Use open leaderboards with code sharing to facilitate replicability", "title": "implemented_by", "to": "Leaderboard platforms enforcing code submission"}, {"from": "Resource taxonomy rd rk rs rh rm rc rn rt for AI system accounting", "title": "validated_by", "to": "Alpha* case study Pareto analysis showing compute-performance trade-offs"}, {"from": "Multi-dimensional benchmark plots of performance versus resource consumption", "title": "validated_by", "to": "MNIST compute-performance Pareto front figure evidencing hidden trade-offs"}, {"from": "Multi-dimensional benchmark plots of performance versus resource consumption", "title": "validated_by", "to": "Alpha* case study Pareto analysis showing compute-performance trade-offs"}, {"from": "Multi-dimensional benchmark plots of performance versus resource consumption", "title": "validated_by", "to": "ALE case study Pareto analysis demonstrating efficiency focus shift"}, {"from": "Policy requirement to include compute and data metrics in academic submission templates", "title": "validated_by", "to": "Alpha* case study Pareto analysis showing compute-performance trade-offs"}, {"from": "Leaderboard platforms enforcing code submission", "title": "validated_by", "to": "ALE case study Pareto analysis demonstrating efficiency focus shift"}, {"from": "Alpha* case study Pareto analysis showing compute-performance trade-offs", "title": "motivates", "to": "Define multi-dimensional AI benchmarks including resource costs"}, {"from": "Alpha* case study Pareto analysis showing compute-performance trade-offs", "title": "motivates", "to": "Mandate reporting of resource consumption metrics in AI research publications"}, {"from": "Alpha* case study Pareto analysis showing compute-performance trade-offs", "title": "motivates", "to": "Adopt Pareto-front analysis to guide AI research portfolio optimization"}, {"from": "Alpha* case study Pareto analysis showing compute-performance trade-offs", "title": "motivates", "to": "Establish open leaderboards with mandatory code submission for replicability"}, {"from": "ALE case study Pareto analysis demonstrating efficiency focus shift", "title": "motivates", "to": "Define multi-dimensional AI benchmarks including resource costs"}, {"from": "ALE case study Pareto analysis demonstrating efficiency focus shift", "title": "motivates", "to": "Mandate reporting of resource consumption metrics in AI research publications"}, {"from": "ALE case study Pareto analysis demonstrating efficiency focus shift", "title": "motivates", "to": "Adopt Pareto-front analysis to guide AI research portfolio optimization"}, {"from": "MNIST compute-performance Pareto front figure evidencing hidden trade-offs", "title": "motivates", "to": "Define multi-dimensional AI benchmarks including resource costs"}, {"from": "misclassification from adversarial examples in discriminative ML models", "title": "caused_by", "to": "inadequate uncertainty estimation in deep neural networks"}, {"from": "misclassification from adversarial examples in discriminative ML models", "title": "caused_by", "to": "existence of adversarial examples in low or high input density regions"}, {"from": "existence of adversarial examples in low or high input density regions", "title": "mitigated_by", "to": "idealised models with invariance capture and fast-growing epistemic uncertainty cannot have adversarial examples"}, {"from": "inadequate uncertainty estimation in deep neural networks", "title": "mitigated_by", "to": "idealised models with invariance capture and fast-growing epistemic uncertainty cannot have adversarial examples"}, {"from": "idealised models with invariance capture and fast-growing epistemic uncertainty cannot have adversarial examples", "title": "mitigated_by", "to": "bayesian neural networks leveraging epistemic uncertainty to detect out-of-distribution inputs"}, {"from": "idealised models with invariance capture and fast-growing epistemic uncertainty cannot have adversarial examples", "title": "mitigated_by", "to": "incorporating data invariances into model architecture to ensure coverage"}, {"from": "bayesian neural networks leveraging epistemic uncertainty to detect out-of-distribution inputs", "title": "refined_by", "to": "using mutual information between parameters and outputs to quantify epistemic uncertainty"}, {"from": "bayesian neural networks leveraging epistemic uncertainty to detect out-of-distribution inputs", "title": "implemented_by", "to": "hamiltonian monte carlo inference for near-ideal Bayesian neural networks"}, {"from": "bayesian neural networks leveraging epistemic uncertainty to detect out-of-distribution inputs", "title": "implemented_by", "to": "monte carlo dropout approximate inference for scalable uncertainty estimation"}, {"from": "incorporating data invariances into model architecture to ensure coverage", "title": "implemented_by", "to": "invariant architecture components capturing data transformations"}, {"from": "incorporating data invariances into model architecture to ensure coverage", "title": "motivates", "to": "design model architectures with built-in data invariances to reduce adversarial vulnerability"}, {"from": "using mutual information between parameters and outputs to quantify epistemic uncertainty", "title": "implemented_by", "to": "mutual information thresholding for adversarial example detection"}, {"from": "hamiltonian monte carlo inference for near-ideal Bayesian neural networks", "title": "validated_by", "to": "HMC robustness experiments on Manifold MNIST showing low adversarial success rate"}, {"from": "monte carlo dropout approximate inference for scalable uncertainty estimation", "title": "refined_by", "to": "dropout ensemble technique to smooth uncertainty estimates"}, {"from": "monte carlo dropout approximate inference for scalable uncertainty estimation", "title": "validated_by", "to": "correlation of mutual information with data density on synthetic dataset"}, {"from": "monte carlo dropout approximate inference for scalable uncertainty estimation", "title": "validated_by", "to": "dropout uncertainty holes exploited by garbage image attack"}, {"from": "invariant architecture components capturing data transformations", "title": "validated_by", "to": "HMC robustness experiments on Manifold MNIST showing low adversarial success rate"}, {"from": "mutual information thresholding for adversarial example detection", "title": "validated_by", "to": "correlation of mutual information with data density on synthetic dataset"}, {"from": "dropout ensemble technique to smooth uncertainty estimates", "title": "validated_by", "to": "dropout ensemble improves adversarial detection AUROC on cats vs dogs"}, {"from": "HMC robustness experiments on Manifold MNIST showing low adversarial success rate", "title": "motivates", "to": "train Bayesian neural networks with HMC inference to maximize epistemic uncertainty and reject adversarial inputs"}, {"from": "correlation of mutual information with data density on synthetic dataset", "title": "motivates", "to": "deploy MC dropout inference with mutual information thresholding to flag and reject adversarial inputs at runtime"}, {"from": "dropout uncertainty holes exploited by garbage image attack", "title": "motivates", "to": "ensemble multiple MC dropout models during deployment to mitigate uncertainty holes and improve adversarial robustness"}, {"from": "dropout ensemble improves adversarial detection AUROC on cats vs dogs", "title": "motivates", "to": "ensemble multiple MC dropout models during deployment to mitigate uncertainty holes and improve adversarial robustness"}, {"from": "Suboptimal interactive physical reasoning performance in deep RL agents", "title": "caused_by", "to": "Absence of relational inductive bias in standard deep RL architectures"}, {"from": "Absence of relational inductive bias in standard deep RL architectures", "title": "mitigated_by", "to": "Relational representations of objects and relations enable combinatorial generalization in physical reasoning"}, {"from": "Relational representations of objects and relations enable combinatorial generalization in physical reasoning", "title": "implemented_by", "to": "Introduce graph network-based relational policy representations in agents"}, {"from": "Relational representations of objects and relations enable combinatorial generalization in physical reasoning", "title": "refined_by", "to": "Provide correct sparse relational structure in graph networks"}, {"from": "Relational representations of objects and relations enable combinatorial generalization in physical reasoning", "title": "refined_by", "to": "Combine relational reasoning with explicit physics simulation for decision making"}, {"from": "Introduce graph network-based relational policy representations in agents", "title": "implemented_by", "to": "Graph network agent with object nodes, contact edges, and message-passing recurrence"}, {"from": "Provide correct sparse relational structure in graph networks", "title": "implemented_by", "to": "Sparse graph representation limited to contact edges"}, {"from": "Combine relational reasoning with explicit physics simulation for decision making", "title": "implemented_by", "to": "Model-based simulation agent evaluating glue options via physics engine"}, {"from": "Graph network agent with object nodes, contact edges, and message-passing recurrence", "title": "validated_by", "to": "GN agent predicts tower stability and glue necessity accurately in supervised experiments"}, {"from": "Graph network agent with object nodes, contact edges, and message-passing recurrence", "title": "validated_by", "to": "GN agent outperforms MLP and generalizes across tower sizes in gluing task"}, {"from": "Sparse graph representation limited to contact edges", "title": "validated_by", "to": "Sparse GN agent achieves higher reward and fewer invalid actions compared to GN-FC agent"}, {"from": "Model-based simulation agent evaluating glue options via physics engine", "title": "validated_by", "to": "Simulation agent attains highest reward relative to GN agent"}, {"from": "GN agent outperforms MLP and generalizes across tower sizes in gluing task", "title": "motivates", "to": "Design model with graph network relational inductive bias for physical reasoning tasks"}, {"from": "Sparse GN agent achieves higher reward and fewer invalid actions compared to GN-FC agent", "title": "motivates", "to": "Use sparse contact-based graph structure in GN agent design"}, {"from": "Simulation agent attains highest reward relative to GN agent", "title": "motivates", "to": "Integrate physics simulation into decision policy for physical reasoning tasks"}, {"from": "Fermi paradox misinterpretation in astrobiology discourse", "title": "caused_by", "to": "Point estimate use in Drake equation modeling"}, {"from": "Point estimate use in Drake equation modeling", "title": "addressed_by", "to": "Orders-of-magnitude uncertainty in Drake parameters"}, {"from": "Orders-of-magnitude uncertainty in Drake parameters", "title": "enabled_by", "to": "Uncertainty-aware Drake equation modeling"}, {"from": "Uncertainty-aware Drake equation modeling", "title": "implemented_by", "to": "Monte Carlo resampling of literature parameter estimates"}, {"from": "Uncertainty-aware Drake equation modeling", "title": "implemented_by", "to": "Probabilistic parameter distributions for Drake factors"}, {"from": "Uncertainty-aware Drake equation modeling", "title": "implemented_by", "to": "Bayesian updating with Fermi observation"}, {"from": "Monte Carlo resampling of literature parameter estimates", "title": "validated_by", "to": "Synthetic distribution shows 30% probability of no civilization per galaxy"}, {"from": "Probabilistic parameter distributions for Drake factors", "title": "validated_by", "to": "Scientific-uncertainty simulation shows 52% probability of no civilization per galaxy"}, {"from": "Bayesian updating with Fermi observation", "title": "validated_by", "to": "Posterior update yields 53\u201399.6% probability of solitude after non-detection"}, {"from": "Synthetic distribution shows 30% probability of no civilization per galaxy", "title": "motivates", "to": "Adopt Monte Carlo-based probabilistic Drake modeling in SETI research"}, {"from": "Scientific-uncertainty simulation shows 52% probability of no civilization per galaxy", "title": "motivates", "to": "Adopt Monte Carlo-based probabilistic Drake modeling in SETI research"}, {"from": "Posterior update yields 53\u201399.6% probability of solitude after non-detection", "title": "motivates", "to": "Apply Bayesian update with Fermi observation to refine civilization probability estimates"}, {"from": "Reward misspecification in robot planning across environments", "title": "caused_by", "to": "Designer overburden when specifying joint reward across diverse environments"}, {"from": "Reward misspecification in robot planning across environments", "title": "caused_by", "to": "Proxy reward non-generalization to novel environments"}, {"from": "Designer overburden when specifying joint reward across diverse environments", "title": "mitigated_by", "to": "Simpler per-environment reward specification reduces cognitive load"}, {"from": "Proxy reward non-generalization to novel environments", "title": "mitigated_by", "to": "Independently specified proxy rewards provide conditional evidence of true reward"}, {"from": "Simpler per-environment reward specification reduces cognitive load", "title": "enabled_by", "to": "Divide-and-conquer reward design combining independent proxies via Bayesian inference"}, {"from": "Independently specified proxy rewards provide conditional evidence of true reward", "title": "enabled_by", "to": "Treat designer as approximately optimal via Boltzmann observation model"}, {"from": "Divide-and-conquer reward design combining independent proxies via Bayesian inference", "title": "implemented_by", "to": "Bayesian posterior inference over true reward using IRD observation model and Monte Carlo sampling"}, {"from": "Treat designer as approximately optimal via Boltzmann observation model", "title": "implemented_by", "to": "Bayesian posterior inference over true reward using IRD observation model and Monte Carlo sampling"}, {"from": "Bayesian posterior inference over true reward using IRD observation model and Monte Carlo sampling", "title": "validated_by", "to": "Grid world user study showing 51% faster design and 69.8% lower regret"}, {"from": "Bayesian posterior inference over true reward using IRD observation model and Monte Carlo sampling", "title": "validated_by", "to": "Robot manipulator user study confirming independent design benefits"}, {"from": "Bayesian posterior inference over true reward using IRD observation model and Monte Carlo sampling", "title": "validated_by", "to": "Sensitivity analysis across feasible sets, feature subsets and environment counts demonstrating robustness"}, {"from": "Bayesian posterior inference over true reward using IRD observation model and Monte Carlo sampling", "title": "motivates", "to": "Apply Bayesian inference over independent proxy rewards before deployment"}, {"from": "Expected reward planning with posterior mean", "title": "validated_by", "to": "Grid world user study showing 51% faster design and 69.8% lower regret"}, {"from": "Expected reward planning with posterior mean", "title": "validated_by", "to": "Robot manipulator user study confirming independent design benefits"}, {"from": "Risk-averse planning using reward posterior uncertainty", "title": "motivates", "to": "Employ risk-averse planning against reward uncertainty in deployment"}, {"from": "Grid world user study showing 51% faster design and 69.8% lower regret", "title": "motivates", "to": "Adopt independent reward design workflow in model design phase for robot tasks"}, {"from": "Grid world user study showing 51% faster design and 69.8% lower regret", "title": "motivates", "to": "Plan robot behavior using mean of reward posterior during deployment"}, {"from": "Robot manipulator user study confirming independent design benefits", "title": "motivates", "to": "Adopt independent reward design workflow in model design phase for robot tasks"}, {"from": "Robot manipulator user study confirming independent design benefits", "title": "motivates", "to": "Plan robot behavior using mean of reward posterior during deployment"}, {"from": "Sensitivity analysis across feasible sets, feature subsets and environment counts demonstrating robustness", "title": "motivates", "to": "Select representative subset of training environments to maximize independent reward design effectiveness"}, {"from": "Unintended negative consequences from misspecified reward functions in autonomous agents", "title": "caused_by", "to": "Intractability of practical CIRL solutions using standard POMDP reduction"}, {"from": "Unintended negative consequences from misspecified reward functions in autonomous agents", "title": "caused_by", "to": "Assumption of perfect human rationality in CIRL models"}, {"from": "Unintended negative consequences from misspecified reward functions in autonomous agents", "title": "caused_by", "to": "Passive observation assumption in IRL reward inference"}, {"from": "Intractability of practical CIRL solutions using standard POMDP reduction", "title": "caused_by", "to": "Pruning human decision rules via information asymmetry in CIRL"}, {"from": "Assumption of perfect human rationality in CIRL models", "title": "caused_by", "to": "Human action distribution parameterized by Q-values enables suboptimal human modeling"}, {"from": "Passive observation assumption in IRL reward inference", "title": "caused_by", "to": "Pedagogic action signaling by humans accelerates reward inference"}, {"from": "Pruning human decision rules via information asymmetry in CIRL", "title": "mitigated_by", "to": "Use optimality-preserving modified Bellman update to scale CIRL solvers"}, {"from": "Human action distribution parameterized by Q-values enables suboptimal human modeling", "title": "mitigated_by", "to": "Integrate general human policy models into CIRL transition dynamics"}, {"from": "Pedagogic action signaling by humans accelerates reward inference", "title": "mitigated_by", "to": "Leverage pedagogic signaling within CIRL for faster value alignment"}, {"from": "Use optimality-preserving modified Bellman update to scale CIRL solvers", "title": "implemented_by", "to": "Modified Bellman update algorithm with reduced action space |AR|"}, {"from": "Integrate general human policy models into CIRL transition dynamics", "title": "implemented_by", "to": "Modified Bellman update incorporating \u03c0_H over Q-values for suboptimal humans"}, {"from": "Leverage pedagogic signaling within CIRL for faster value alignment", "title": "implemented_by", "to": "CIRL planning system utilizing pedagogic human signals and pragmatic robot interpretation"}, {"from": "Modified Bellman update algorithm with reduced action space |AR|", "title": "validated_by", "to": "Exact value iteration experiments show exponential speedup using modified Bellman update"}, {"from": "Modified Bellman update algorithm with reduced action space |AR|", "title": "refined_by", "to": "Adapted PBVI and POMCP algorithms using modified Bellman update"}, {"from": "Modified Bellman update incorporating \u03c0_H over Q-values for suboptimal humans", "title": "validated_by", "to": "Simulations with Boltzmann-rational humans show \u003e90% success in CIRL"}, {"from": "Adapted PBVI and POMCP algorithms using modified Bellman update", "title": "validated_by", "to": "Approximate algorithm experiments show higher value and scalability"}, {"from": "CIRL planning system utilizing pedagogic human signals and pragmatic robot interpretation", "title": "validated_by", "to": "CIRL vs IRL experiments show 100% task success versus 50%"}, {"from": "Exact value iteration experiments show exponential speedup using modified Bellman update", "title": "motivates", "to": "Integrate modified Bellman update into POMDP solvers for CIRL-based planning algorithms"}, {"from": "Approximate algorithm experiments show higher value and scalability", "title": "motivates", "to": "Integrate modified Bellman update into POMDP solvers for CIRL-based planning algorithms"}, {"from": "CIRL vs IRL experiments show 100% task success versus 50%", "title": "motivates", "to": "Train robots with CIRL-derived policies encouraging pedagogic human signaling instead of IRL approaches"}, {"from": "Simulations with Boltzmann-rational humans show \u003e90% success in CIRL", "title": "motivates", "to": "Model human policies as Q-value-dependent distributions (e.g., Boltzmann) in CIRL planning"}, {"from": "escalation of conflicts between artificial agents in safety-critical systems", "title": "caused_by", "to": "selfish incentives destabilizing cooperation in social dilemmas"}, {"from": "escalation of conflicts between artificial agents in safety-critical systems", "title": "caused_by", "to": "absence of incentive-structuring mechanisms in multi-agent reinforcement learning"}, {"from": "selfish incentives destabilizing cooperation in social dilemmas", "title": "mitigated_by", "to": "mechanism design modifies payoff structure to align individual and collective interests"}, {"from": "absence of incentive-structuring mechanisms in multi-agent reinforcement learning", "title": "mitigated_by", "to": "mechanism design modifies payoff structure to align individual and collective interests"}, {"from": "mechanism design modifies payoff structure to align individual and collective interests", "title": "enabled_by", "to": "planning agent shaping learners\u0027 policy updates via anticipated gradients"}, {"from": "planning agent shaping learners\u0027 policy updates via anticipated gradients", "title": "implemented_by", "to": "adaptive mechanism design employing planning agent reward distribution"}, {"from": "adaptive mechanism design employing planning agent reward distribution", "title": "refined_by", "to": "lookahead gradient-based incentive adjustment to maximise social welfare"}, {"from": "adaptive mechanism design employing planning agent reward distribution", "title": "implemented_by", "to": "cost regularization and reward limits for additional rewards"}, {"from": "lookahead gradient-based incentive adjustment to maximise social welfare", "title": "implemented_by", "to": "planning agent policy gradient update using other agents\u0027 gradients"}, {"from": "lookahead gradient-based incentive adjustment to maximise social welfare", "title": "implemented_by", "to": "opponent modeling to approximate hidden agent policies for planning agent"}, {"from": "planning agent policy gradient update using other agents\u0027 gradients", "title": "validated_by", "to": "high cooperation achieved in matrix game experiments with planning agent"}, {"from": "planning agent policy gradient update using other agents\u0027 gradients", "title": "validated_by", "to": "stable cooperation persists in Stag Hunt after planning agent removal"}, {"from": "opponent modeling to approximate hidden agent policies for planning agent", "title": "validated_by", "to": "moderate cooperation achieved using estimated value function with opponent modeling"}, {"from": "cost regularization and reward limits for additional rewards", "title": "validated_by", "to": "decreasing average additional rewards over time in PD experiments"}, {"from": "high cooperation achieved in matrix game experiments with planning agent", "title": "motivates", "to": "Deploy adaptive planning agent distributing additional rewards during multi-agent RL training"}, {"from": "stable cooperation persists in Stag Hunt after planning agent removal", "title": "motivates", "to": "Turn off planning agent after cooperative equilibrium reached to lower ongoing costs"}, {"from": "decreasing average additional rewards over time in PD experiments", "title": "motivates", "to": "Constrain planning agent to revenue-neutral reward redistribution in social dilemmas"}, {"from": "moderate cooperation achieved using estimated value function with opponent modeling", "title": "motivates", "to": "Integrate opponent modeling into planning agent when learner policies are hidden"}, {"from": "Spurious visual cue reliance in global stability prediction models", "title": "caused_by", "to": "Correlation between local and global stability labels in training data for block towers"}, {"from": "Correlation between local and global stability labels in training data for block towers", "title": "mitigated_by", "to": "Gradient-weighted supplemental tasks modulate feature extraction in neural networks"}, {"from": "Gradient-weighted supplemental tasks modulate feature extraction in neural networks", "title": "implemented_by", "to": "Neural stethoscope framework for promoting or suppressing task-specific information in hidden layers"}, {"from": "Neural stethoscope framework for promoting or suppressing task-specific information in hidden layers", "title": "implemented_by", "to": "Auxiliary stethoscope training on instability origin features (\u03bb\u003e0)"}, {"from": "Neural stethoscope framework for promoting or suppressing task-specific information in hidden layers", "title": "implemented_by", "to": "Adversarial stethoscope training on local stability features (\u03bb\u003c0)"}, {"from": "Neural stethoscope framework for promoting or suppressing task-specific information in hidden layers", "title": "implemented_by", "to": "Analytic stethoscope probing across network layers (\u03bb=0)"}, {"from": "Auxiliary stethoscope training on instability origin features (\u03bb\u003e0)", "title": "validated_by", "to": "Auxiliary stethoscope improves hard-scenario accuracy over baseline"}, {"from": "Adversarial stethoscope training on local stability features (\u03bb\u003c0)", "title": "validated_by", "to": "Adversarial stethoscope reduces bias and increases robustness"}, {"from": "Analytic stethoscope probing across network layers (\u03bb=0)", "title": "validated_by", "to": "Layer-wise analytic stethoscope reveals distribution of local and global stability information"}, {"from": "Auxiliary stethoscope improves hard-scenario accuracy over baseline", "title": "motivates", "to": "Fine-tune global stability models with auxiliary stethoscope on instability-origin task"}, {"from": "Adversarial stethoscope reduces bias and increases robustness", "title": "motivates", "to": "Fine-tune global stability models with adversarial stethoscope on local-stability task"}, {"from": "Layer-wise analytic stethoscope reveals distribution of local and global stability information", "title": "motivates", "to": "Audit representations with analytic stethoscope prior to deployment"}, {"from": "Exploration failure in sparse-reward reinforcement learning environments", "title": "caused_by", "to": "Insufficient exploitation of past high-return experiences in RL agents"}, {"from": "Insufficient exploitation of past high-return experiences in RL agents", "title": "mitigated_by", "to": "Imitating trajectories with returns exceeding value estimates drives deeper exploration"}, {"from": "Imitating trajectories with returns exceeding value estimates drives deeper exploration", "title": "implemented_by", "to": "Off-policy actor-critic imitation without importance sampling exploits good experiences"}, {"from": "Off-policy actor-critic imitation without importance sampling exploits good experiences", "title": "implemented_by", "to": "Self-Imitation Learning objective with positive-advantage weighting"}, {"from": "Self-Imitation Learning objective with positive-advantage weighting", "title": "enabled_by", "to": "Prioritized experience replay by clipped advantage for SIL sampling"}, {"from": "Self-Imitation Learning objective with positive-advantage weighting", "title": "validated_by", "to": "Montezuma\u0027s Revenge performance improvement with A2C+SIL"}, {"from": "Self-Imitation Learning objective with positive-advantage weighting", "title": "validated_by", "to": "A2C+SIL outperforming count-based exploration on hard Atari games"}, {"from": "Self-Imitation Learning objective with positive-advantage weighting", "title": "validated_by", "to": "PPO+SIL gains on MuJoCo delayed-reward tasks"}, {"from": "Prioritized experience replay by clipped advantage for SIL sampling", "title": "validated_by", "to": "Synergy of SIL and exploration bonus in Apple-Key-Door-Treasure domain"}, {"from": "Montezuma\u0027s Revenge performance improvement with A2C+SIL", "title": "motivates", "to": "Integrate self-imitation learning updates into actor-critic training during RL fine-tuning"}, {"from": "A2C+SIL outperforming count-based exploration on hard Atari games", "title": "motivates", "to": "Integrate self-imitation learning updates into actor-critic training during RL fine-tuning"}, {"from": "A2C+SIL outperforming count-based exploration on hard Atari games", "title": "motivates", "to": "Combine self-imitation learning with count-based exploration bonuses during RL training"}, {"from": "Synergy of SIL and exploration bonus in Apple-Key-Door-Treasure domain", "title": "motivates", "to": "Combine self-imitation learning with count-based exploration bonuses during RL training"}, {"from": "Synergy of SIL and exploration bonus in Apple-Key-Door-Treasure domain", "title": "motivates", "to": "Apply prioritized replay based on positive advantage for self-imitation learning updates"}, {"from": "PPO+SIL gains on MuJoCo delayed-reward tasks", "title": "motivates", "to": "Augment PPO training with self-imitation learning objective for continuous control tasks"}, {"from": "Suboptimal policy generalization in CGP-evolved Atari agents", "title": "caused_by", "to": "Episodic reward-based fitness selection in CGP evolution"}, {"from": "Suboptimal policy generalization in CGP-evolved Atari agents", "title": "caused_by", "to": "Pixel-agnostic repetitive strategies in Atari gameplay"}, {"from": "Suboptimal policy generalization in CGP-evolved Atari agents", "title": "caused_by", "to": "Full-episode evaluation computational cost in CGP evolution"}, {"from": "Episodic reward-based fitness selection in CGP evolution", "title": "mitigated_by", "to": "Novelty-driven exploration improves evolutionary search in Atari controllers"}, {"from": "Pixel-agnostic repetitive strategies in Atari gameplay", "title": "mitigated_by", "to": "Frame-level prioritized reward shaping targets salient experience"}, {"from": "Full-episode evaluation computational cost in CGP evolution", "title": "mitigated_by", "to": "Reduced frame evaluation lowers computation with minimal learning loss"}, {"from": "Novelty-driven exploration improves evolutionary search in Atari controllers", "title": "implemented_by", "to": "Integrate novelty metrics into CGP fitness evaluation"}, {"from": "Integrate novelty metrics into CGP fitness evaluation", "title": "implemented_by", "to": "Lehman-Stanley novelty score computation during CGP evolution"}, {"from": "Lehman-Stanley novelty score computation during CGP evolution", "title": "validated_by", "to": "Novelty search aids escape from local optima (Lehman \u0026 Stanley 2008)"}, {"from": "Novelty search aids escape from local optima (Lehman \u0026 Stanley 2008)", "title": "motivates", "to": "Add novelty-based secondary objective during CGP training of Atari agents"}, {"from": "Frame-level prioritized reward shaping targets salient experience", "title": "implemented_by", "to": "Weight CGP fitness by prioritized salient frames and actions"}, {"from": "Weight CGP fitness by prioritized salient frames and actions", "title": "implemented_by", "to": "Weighted reward aggregation with prioritized frame sampling"}, {"from": "Weighted reward aggregation with prioritized frame sampling", "title": "validated_by", "to": "Prioritized experience frames improve policy learning (Schaul et al. 2015)"}, {"from": "Prioritized experience frames improve policy learning (Schaul et al. 2015)", "title": "motivates", "to": "Apply prioritized frame reward weighting during CGP evolution"}, {"from": "Reduced frame evaluation lowers computation with minimal learning loss", "title": "implemented_by", "to": "Limit CGP evaluation to subset of episode frames"}, {"from": "Limit CGP evaluation to subset of episode frames", "title": "implemented_by", "to": "Episode truncation and frame subsampling during fitness evaluation"}, {"from": "Episode truncation and frame subsampling during fitness evaluation", "title": "validated_by", "to": "Authors suggest frame count reduction decreases computational load (Discussion section)"}, {"from": "Authors suggest frame count reduction decreases computational load (Discussion section)", "title": "motivates", "to": "Decrease evaluated frames per generation in CGP evolution"}, {"from": "Incorrect reward inference in inverse reinforcement learning applications", "title": "caused_by", "to": "Ill-posed reward ambiguity from limited demonstrations in IRL"}, {"from": "Incorrect reward inference in inverse reinforcement learning applications", "title": "caused_by", "to": "Noisy or suboptimal demonstrations impacting reward accuracy in IRL"}, {"from": "Incorrect reward inference in inverse reinforcement learning applications", "title": "caused_by", "to": "Incomplete model knowledge affecting reward inference in IRL"}, {"from": "Incorrect reward inference in inverse reinforcement learning applications", "title": "caused_by", "to": "High dimensional state spaces increasing computational burden in IRL"}, {"from": "Incorrect reward inference in inverse reinforcement learning applications", "title": "caused_by", "to": "High sample complexity requirements in IRL"}, {"from": "Ill-posed reward ambiguity from limited demonstrations in IRL", "title": "caused_by", "to": "Maximum entropy principle yields least-biased reward distribution in IRL"}, {"from": "Noisy or suboptimal demonstrations impacting reward accuracy in IRL", "title": "caused_by", "to": "Bayesian posterior over reward functions captures uncertainty in IRL"}, {"from": "Incomplete model knowledge affecting reward inference in IRL", "title": "caused_by", "to": "Automatic feature construction via regression boosting reduces manual bias in IRL"}, {"from": "High dimensional state spaces increasing computational burden in IRL", "title": "caused_by", "to": "State abstraction via bisimulation metrics enhances generalization in IRL"}, {"from": "High sample complexity requirements in IRL", "title": "caused_by", "to": "Active learning querying expert at high-entropy states reduces sample complexity in IRL"}, {"from": "Maximum entropy principle yields least-biased reward distribution in IRL", "title": "mitigated_by", "to": "Use maximum entropy optimization to resolve reward ambiguity in IRL"}, {"from": "Bayesian posterior over reward functions captures uncertainty in IRL", "title": "mitigated_by", "to": "Maintain reward uncertainty through Bayesian IRL framework"}, {"from": "Active learning querying expert at high-entropy states reduces sample complexity in IRL", "title": "mitigated_by", "to": "Solicit informative demonstrations using active Bayesian IRL"}, {"from": "Automatic feature construction via regression boosting reduces manual bias in IRL", "title": "mitigated_by", "to": "Automatically learn nonlinear reward features with regression trees"}, {"from": "State abstraction via bisimulation metrics enhances generalization in IRL", "title": "mitigated_by", "to": "Apply bisimulation-based state aggregation to cut sample complexity"}, {"from": "Use maximum entropy optimization to resolve reward ambiguity in IRL", "title": "implemented_by", "to": "Convex entropy-constrained optimization of reward parameters"}, {"from": "Maintain reward uncertainty through Bayesian IRL framework", "title": "implemented_by", "to": "MCMC sampling of reward posterior distribution"}, {"from": "Solicit informative demonstrations using active Bayesian IRL", "title": "implemented_by", "to": "Entropy-driven state query strategy for expert demonstrations"}, {"from": "Automatically learn nonlinear reward features with regression trees", "title": "implemented_by", "to": "Regression tree-based feature generation algorithm (FIRL)"}, {"from": "Apply bisimulation-based state aggregation to cut sample complexity", "title": "implemented_by", "to": "Kernel regression over bisimulation metric for policy generalization"}, {"from": "Convex entropy-constrained optimization of reward parameters", "title": "validated_by", "to": "Robust performance under noisy demonstrations in helicopter control tasks using MaxEnt IRL"}, {"from": "MCMC sampling of reward posterior distribution", "title": "validated_by", "to": "Accurate reward posterior in gridworld experiments with Bayesian IRL"}, {"from": "Entropy-driven state query strategy for expert demonstrations", "title": "validated_by", "to": "Lower sample complexity in active BIRL simulated domains"}, {"from": "Regression tree-based feature generation algorithm (FIRL)", "title": "validated_by", "to": "Reduced imitation loss in path planning via automatic feature boosting"}, {"from": "Kernel regression over bisimulation metric for policy generalization", "title": "validated_by", "to": "Improved generalization with fewer demonstrations using bisimulation IRL"}, {"from": "Robust performance under noisy demonstrations in helicopter control tasks using MaxEnt IRL", "title": "motivates", "to": "Implement maximum entropy IRL during reward inference for autonomous agents"}, {"from": "Accurate reward posterior in gridworld experiments with Bayesian IRL", "title": "motivates", "to": "Apply Bayesian IRL with MCMC to model reward uncertainty in training"}, {"from": "Lower sample complexity in active BIRL simulated domains", "title": "motivates", "to": "Collect demonstrations via entropy-based active learning queries"}, {"from": "Reduced imitation loss in path planning via automatic feature boosting", "title": "motivates", "to": "Integrate regression tree feature construction into reward modeling pipeline"}, {"from": "Improved generalization with fewer demonstrations using bisimulation IRL", "title": "motivates", "to": "Deploy bisimulation-based state abstraction for efficient IRL training"}, {"from": "credit assignment failure in delayed reward reinforcement learning", "title": "caused_by", "to": "bias in temporal difference learning and high variance in Monte Carlo learning in delayed reward MDPs"}, {"from": "bias in temporal difference learning and high variance in Monte Carlo learning in delayed reward MDPs", "title": "mitigated_by", "to": "eliminating expected future rewards simplifies Q-value estimation to immediate reward mean"}, {"from": "eliminating expected future rewards simplifies Q-value estimation to immediate reward mean", "title": "enabled_by", "to": "reward redistribution achieving zero expected future rewards in SDPs"}, {"from": "return-equivalent sequence-Markov decision processes preserve optimal policies", "title": "enabled_by", "to": "reward redistribution achieving zero expected future rewards in SDPs"}, {"from": "reward redistribution achieving zero expected future rewards in SDPs", "title": "implemented_by", "to": "second-order Markov reward redistribution using differences of consecutive Q-values"}, {"from": "return decomposition via contribution analysis for constructing reward redistribution", "title": "implemented_by", "to": "LSTM-based return prediction with contribution analysis (RUDDER implementation)"}, {"from": "second-order Markov reward redistribution using differences of consecutive Q-values", "title": "validated_by", "to": "RUDDER speed-up over TD, MC, and MCTS on artificial delayed-reward tasks"}, {"from": "LSTM-based return prediction with contribution analysis (RUDDER implementation)", "title": "validated_by", "to": "RUDDER speed-up over TD, MC, and MCTS on artificial delayed-reward tasks"}, {"from": "LSTM-based return prediction with contribution analysis (RUDDER implementation)", "title": "validated_by", "to": "RUDDER improves PPO performance on delayed-reward Atari games"}, {"from": "RUDDER speed-up over TD, MC, and MCTS on artificial delayed-reward tasks", "title": "motivates", "to": "integrate RUDDER reward redistribution during RL fine-tuning to mitigate delayed reward credit assignment"}, {"from": "RUDDER speed-up over TD, MC, and MCTS on artificial delayed-reward tasks", "title": "motivates", "to": "train an LSTM return decomposition model to automate reward redistribution in RL training"}, {"from": "RUDDER improves PPO performance on delayed-reward Atari games", "title": "motivates", "to": "integrate RUDDER reward redistribution during RL fine-tuning to mitigate delayed reward credit assignment"}, {"from": "Resource overconsumption of neural networks on constrained hardware platforms", "title": "caused_by", "to": "Mismatch between model complexity and hardware resource budgets"}, {"from": "Mismatch between model complexity and hardware resource budgets", "title": "mitigated_by", "to": "Hardware-aware metrics guiding architecture search under constraints"}, {"from": "Mismatch between model complexity and hardware resource budgets", "title": "mitigated_by", "to": "Soft continuous resource-violation penalty enables learning under tight constraints"}, {"from": "Hardware-aware metrics guiding architecture search under constraints", "title": "enabled_by", "to": "Integration of resource constraints into NAS reward function"}, {"from": "Integration of resource constraints into NAS reward function", "title": "implemented_by", "to": "Multi-objective RL NAS with model size, FLOPs, compute intensity metrics"}, {"from": "Integration of resource constraints into NAS reward function", "title": "enabled_by", "to": "Compute intensity metric to favor data reuse"}, {"from": "Multi-objective RL NAS with model size, FLOPs, compute intensity metrics", "title": "validated_by", "to": "Empirical accuracy-resource tradeoff results on CIFAR10 with RENA"}, {"from": "Multi-objective RL NAS with model size, FLOPs, compute intensity metrics", "title": "validated_by", "to": "KWS high compute intensity architecture results under constraints"}, {"from": "Empirical accuracy-resource tradeoff results on CIFAR10 with RENA", "title": "motivates", "to": "Employ RENA resource-constrained NAS to design efficient architectures"}, {"from": "High inference latency in neural network applications", "title": "caused_by", "to": "Low compute intensity limiting data reuse on accelerators"}, {"from": "Low compute intensity limiting data reuse on accelerators", "title": "mitigated_by", "to": "Compute intensity metric to favor data reuse"}, {"from": "KWS high compute intensity architecture results under constraints", "title": "motivates", "to": "Incorporate compute intensity thresholds into NAS objectives during model design"}, {"from": "Soft continuous resource-violation penalty enables learning under tight constraints", "title": "enabled_by", "to": "Soft penalty reward shaping for continuous signals in NAS"}, {"from": "Soft penalty reward shaping for continuous signals in NAS", "title": "implemented_by", "to": "Reward function with exponential penalty parameter p=0.9"}, {"from": "Reward function with exponential penalty parameter p=0.9", "title": "validated_by", "to": "RENA surpasses random search in meeting resource constraints"}, {"from": "RENA surpasses random search in meeting resource constraints", "title": "motivates", "to": "Embed soft continuous resource violation penalty in NAS reward to balance performance and constraints"}, {"from": "Undetected scientific anomalies in large image datasets", "title": "caused_by", "to": "Pixel-level representations causing noisy novelty assessments"}, {"from": "Undetected scientific anomalies in large image datasets", "title": "caused_by", "to": "Opaque novelty detection outputs lacking interpretability in image analysis"}, {"from": "Opaque novelty detection outputs lacking interpretability in image analysis", "title": "caused_by", "to": "Pixel-level representations causing noisy novelty assessments"}, {"from": "Opaque novelty detection outputs lacking interpretability in image analysis", "title": "mitigated_by", "to": "Residual difference in embedding space isolates novel image content"}, {"from": "Pixel-level representations causing noisy novelty assessments", "title": "mitigated_by", "to": "CNN feature embeddings provide robust abstract image representation for novelty detection"}, {"from": "CNN feature embeddings provide robust abstract image representation for novelty detection", "title": "enabled_by", "to": "Combine DEMUD incremental SVD with CNN embeddings to select anomalous images"}, {"from": "Residual difference in embedding space isolates novel image content", "title": "enabled_by", "to": "Visualize residual and reconstruction via up-convolutional network for human comprehension"}, {"from": "Combine DEMUD incremental SVD with CNN embeddings to select anomalous images", "title": "implemented_by", "to": "Derive fc8 activations from pre-trained CaffeNet as image feature vectors"}, {"from": "Combine DEMUD incremental SVD with CNN embeddings to select anomalous images", "title": "implemented_by", "to": "Iteratively update SVD model with highest reconstruction-error items in DEMUD"}, {"from": "Visualize residual and reconstruction via up-convolutional network for human comprehension", "title": "implemented_by", "to": "Generate explanation images via up-convolutional network from residual vectors"}, {"from": "Derive fc8 activations from pre-trained CaffeNet as image feature vectors", "title": "validated_by", "to": "ImageNet balanced and unbalanced class discovery nAUC improvement"}, {"from": "Iteratively update SVD model with highest reconstruction-error items in DEMUD", "title": "validated_by", "to": "ImageNet balanced and unbalanced class discovery nAUC improvement"}, {"from": "Generate explanation images via up-convolutional network from residual vectors", "title": "validated_by", "to": "Mars rover image discovery performance and explanations"}, {"from": "ImageNet balanced and unbalanced class discovery nAUC improvement", "title": "motivates", "to": "Deploy DEMUD-CNN-UC pipeline for operational image triage to surface novel observations"}, {"from": "ImageNet balanced and unbalanced class discovery nAUC improvement", "title": "motivates", "to": "Conduct pre-deployment evaluation of DEMUD-CNN novelty detection with nAUC metrics on representative datasets"}, {"from": "Mars rover image discovery performance and explanations", "title": "motivates", "to": "Deploy DEMUD-CNN-UC pipeline for operational image triage to surface novel observations"}, {"from": "Mars rover image discovery performance and explanations", "title": "motivates", "to": "Train domain-specific autoencoder or CNN on target image distribution to enhance novelty detection robustness"}, {"from": "Inefficient reward learning in complex sequential decision tasks", "title": "caused_by", "to": "Data sparsity of expert demonstrations in critical states"}, {"from": "Inefficient reward learning in complex sequential decision tasks", "title": "caused_by", "to": "Demonstration redundancy from full-task trajectories"}, {"from": "Inefficient reward learning in complex sequential decision tasks", "title": "caused_by", "to": "Lack of human high-level structure in IRL training"}, {"from": "Excessive human demonstration burden in robot training", "title": "caused_by", "to": "Demonstration redundancy from full-task trajectories"}, {"from": "Reward misspecification causing subgoal failure in IRL", "title": "caused_by", "to": "Data sparsity of expert demonstrations in critical states"}, {"from": "Data sparsity of expert demonstrations in critical states", "title": "mitigated_by", "to": "Human subgoal decomposition can guide efficient learning"}, {"from": "Demonstration redundancy from full-task trajectories", "title": "mitigated_by", "to": "Human subgoal decomposition can guide efficient learning"}, {"from": "Lack of human high-level structure in IRL training", "title": "mitigated_by", "to": "Failure trajectories provide informative negative signal"}, {"from": "Human subgoal decomposition can guide efficient learning", "title": "enabled_by", "to": "Structured human-agent interaction using subtasks"}, {"from": "Failure trajectories provide informative negative signal", "title": "enabled_by", "to": "Integrating failure experience with demonstration data"}, {"from": "Emphasizing critical subgoals increases reward alignment", "title": "enabled_by", "to": "Structured human-agent interaction using subtasks"}, {"from": "Structured human-agent interaction using subtasks", "title": "implemented_by", "to": "HI-IRL algorithm with subgoal specification and partial demonstrations"}, {"from": "Integrating failure experience with demonstration data", "title": "implemented_by", "to": "Deep IRL from Failure gradient updates"}, {"from": "Deep neural reward networks for raw-state representation", "title": "implemented_by", "to": "Neural conv-BN-ReLU-FC architecture to map images to reward"}, {"from": "HI-IRL algorithm with subgoal specification and partial demonstrations", "title": "validated_by", "to": "Grid-world and car-parking experiments show reduced demonstrations"}, {"from": "Deep IRL from Failure gradient updates", "title": "validated_by", "to": "Performance curves indicate near-oracle success with fewer steps"}, {"from": "Neural conv-BN-ReLU-FC architecture to map images to reward", "title": "validated_by", "to": "Reward maps show higher values at critical subgoal states"}, {"from": "Grid-world and car-parking experiments show reduced demonstrations", "title": "motivates", "to": "Fine-tune agents with human-interactive subgoal-supervised IRL incorporating failure learning"}, {"from": "Fine-tune agents with human-interactive subgoal-supervised IRL incorporating failure learning", "title": "refined_by", "to": "Collect partial demonstrations only for struggling subtasks during training"}, {"from": "Fine-tune agents with human-interactive subgoal-supervised IRL incorporating failure learning", "title": "required_by", "to": "Specify critical subgoals upfront to structure learning process"}, {"from": "Erroneous or harmful translations from adversarial perturbations in neural machine translation systems", "title": "caused_by", "to": "Character-level vulnerability to small input perturbations in NMT"}, {"from": "Erroneous or harmful translations from adversarial perturbations in neural machine translation systems", "title": "caused_by", "to": "Ineffectiveness of black-box adversarial testing in revealing worst-case translation errors"}, {"from": "Character-level vulnerability to small input perturbations in NMT", "title": "addressed_by", "to": "Gradient-based white-box attacks approximate worst-case input perturbations in NMT"}, {"from": "Character-level vulnerability to small input perturbations in NMT", "title": "mitigated_by", "to": "Adversarial training on stronger attacks yields robustness to multiple noise sources"}, {"from": "Ineffectiveness of black-box adversarial testing in revealing worst-case translation errors", "title": "addressed_by", "to": "Gradient-based white-box attacks approximate worst-case input perturbations in NMT"}, {"from": "Gradient-based white-box attacks approximate worst-case input perturbations in NMT", "title": "specified_by", "to": "Use gradient-informed text edit operations to craft adversarial examples"}, {"from": "Adversarial training on stronger attacks yields robustness to multiple noise sources", "title": "specified_by", "to": "Integrate white-box adversarial examples into training to harden models"}, {"from": "Adversarial training on stronger attacks yields robustness to multiple noise sources", "title": "specified_by", "to": "Combine white-box and black-box noise generation in ensemble training for broader robustness"}, {"from": "Use gradient-informed text edit operations to craft adversarial examples", "title": "implemented_by", "to": "Flip/Insert/Delete/Swap character operations guided by loss gradients in HotFlip extension"}, {"from": "Integrate white-box adversarial examples into training to harden models", "title": "implemented_by", "to": "One-shot gradient-based adversarial example generation for efficient adversarial training"}, {"from": "Combine white-box and black-box noise generation in ensemble training for broader robustness", "title": "implemented_by", "to": "Ensemble training pipeline mixing clean, white-box, and black-box noisy inputs"}, {"from": "Flip/Insert/Delete/Swap character operations guided by loss gradients in HotFlip extension", "title": "validated_by", "to": "White-box attacks reduce BLEU more than black-box attacks in untargeted, controlled, targeted scenarios on IWSLT datasets"}, {"from": "One-shot gradient-based adversarial example generation for efficient adversarial training", "title": "validated_by", "to": "Adversarial training with white-box examples improves BLEU on noisy test sets versus baselines"}, {"from": "Ensemble training pipeline mixing clean, white-box, and black-box noisy inputs", "title": "validated_by", "to": "Ensemble adversarial training achieves highest average BLEU across noise types"}, {"from": "White-box attacks reduce BLEU more than black-box attacks in untargeted, controlled, targeted scenarios on IWSLT datasets", "title": "motivates", "to": "Conduct gradient-based controlled and targeted attack evaluation in pre-deployment testing to identify translation vulnerabilities"}, {"from": "Adversarial training with white-box examples improves BLEU on noisy test sets versus baselines", "title": "motivates", "to": "Apply gradient-based one-shot adversarial training with FIDS operations during NMT fine-tuning to improve robustness"}, {"from": "Ensemble adversarial training achieves highest average BLEU across noise types", "title": "motivates", "to": "Employ ensemble adversarial training mixing white-box and black-box noise sources during NMT fine-tuning"}, {"from": "Extreme computational resource consumption in neural architecture search", "title": "caused_by", "to": "Black-box discrete architecture optimization requiring many evaluations"}, {"from": "Suboptimal architecture performance on target tasks due to transferability limitations", "title": "caused_by", "to": "Source dataset discrepancy lowers architecture transferability"}, {"from": "Black-box discrete architecture optimization requiring many evaluations", "title": "caused_by", "to": "Discrete architecture representation prevents gradient-based optimization"}, {"from": "Source dataset discrepancy lowers architecture transferability", "title": "caused_by", "to": "Architecture generalization depends on similarity between search and deployment distributions"}, {"from": "Discrete architecture representation prevents gradient-based optimization", "title": "mitigated_by", "to": "Gradient descent joint optimization of architecture and weights via bilevel formulation"}, {"from": "Architecture generalization depends on similarity between search and deployment distributions", "title": "mitigated_by", "to": "Leverage DARTS efficiency to search directly on deployment tasks"}, {"from": "Gradient descent joint optimization of architecture and weights via bilevel formulation", "title": "implemented_by", "to": "Softmax mixed operations with architecture parameters (DARTS continuous relaxation)"}, {"from": "Leverage DARTS efficiency to search directly on deployment tasks", "title": "implemented_by", "to": "Softmax mixed operations with architecture parameters (DARTS continuous relaxation)"}, {"from": "Softmax mixed operations with architecture parameters (DARTS continuous relaxation)", "title": "refined_by", "to": "Finite difference approximation of second-order gradients in DARTS"}, {"from": "Softmax mixed operations with architecture parameters (DARTS continuous relaxation)", "title": "refined_by", "to": "First-order gradient approximation in DARTS with \u03be=0"}, {"from": "Softmax mixed operations with architecture parameters (DARTS continuous relaxation)", "title": "validated_by", "to": "Three orders-of-magnitude compute reduction compared to RL/evolution methods"}, {"from": "Softmax mixed operations with architecture parameters (DARTS continuous relaxation)", "title": "validated_by", "to": "ImageNet mobile 26.9% top-1 error using DARTS cell searched on CIFAR-10"}, {"from": "Finite difference approximation of second-order gradients in DARTS", "title": "validated_by", "to": "CIFAR-10 2.83% test error with 4 GPU-day second-order DARTS"}, {"from": "Finite difference approximation of second-order gradients in DARTS", "title": "validated_by", "to": "PTB perplexity 56.1 with 1 GPU-day second-order DARTS"}, {"from": "First-order gradient approximation in DARTS with \u03be=0", "title": "validated_by", "to": "CIFAR-10 2.94% test error with 1.5 GPU-day first-order DARTS"}, {"from": "CIFAR-10 2.83% test error with 4 GPU-day second-order DARTS", "title": "motivates", "to": "Use DARTS gradient-based architecture search during model design to reduce compute cost"}, {"from": "PTB perplexity 56.1 with 1 GPU-day second-order DARTS", "title": "motivates", "to": "Use DARTS gradient-based architecture search during model design to reduce compute cost"}, {"from": "Three orders-of-magnitude compute reduction compared to RL/evolution methods", "title": "motivates", "to": "Use DARTS gradient-based architecture search during model design to reduce compute cost"}, {"from": "CIFAR-10 2.94% test error with 1.5 GPU-day first-order DARTS", "title": "motivates", "to": "Apply first-order DARTS approximation during architecture search to minimize search compute"}, {"from": "ImageNet mobile 26.9% top-1 error using DARTS cell searched on CIFAR-10", "title": "motivates", "to": "Run DARTS search directly on target large-scale tasks to improve architecture transferability"}, {"from": "Reward misalignment from strategic interactions in multi-agent contexts", "title": "caused_by", "to": "Ill-posed MIRL under multiple reward-consistent equilibria"}, {"from": "Ambiguous reward inference due to non-unique equilibria in stochastic games", "title": "caused_by", "to": "Non-unique equilibrium likelihood specification challenge in MIRL"}, {"from": "Ill-posed MIRL under multiple reward-consistent equilibria", "title": "mitigated_by", "to": "Unique equilibrium selection constrains reward inference search space"}, {"from": "Non-unique equilibrium likelihood specification challenge in MIRL", "title": "mitigated_by", "to": "Local reduced gap objective approximates utilitarian equilibrium selection"}, {"from": "Unique equilibrium selection constrains reward inference search space", "title": "specified_by", "to": "Bayesian MAP estimation with equilibrium linear constraints"}, {"from": "Local reduced gap objective approximates utilitarian equilibrium selection", "title": "specified_by", "to": "Linear programming for utilitarian CE and NE reward recovery"}, {"from": "Bayesian MAP estimation with equilibrium linear constraints", "title": "implemented_by", "to": "Linear constraint formulation using B\u03c0 and D\u03c0 matrices"}, {"from": "Linear programming for utilitarian CE and NE reward recovery", "title": "implemented_by", "to": "Local reduced gap linear program with y variables and regularisation"}, {"from": "Linear constraint formulation using B\u03c0 and D\u03c0 matrices", "title": "validated_by", "to": "GridWorld MIRL NRMSE 1e-3 outperforming IRL"}, {"from": "Linear constraint formulation using B\u03c0 and D\u03c0 matrices", "title": "validated_by", "to": "Soccer simulation shows advE-MIRL competitive with zero-sum MIRL and better than d-MIRL"}, {"from": "Local reduced gap linear program with y variables and regularisation", "title": "validated_by", "to": "GridWorld MIRL NRMSE 1e-3 outperforming IRL"}, {"from": "Local reduced gap linear program with y variables and regularisation", "title": "validated_by", "to": "Robustness study shows gradual degradation with missing policy states"}, {"from": "GridWorld MIRL NRMSE 1e-3 outperforming IRL", "title": "motivates", "to": "Deploy Bayesian MIRL with unique equilibrium constraints to model multi-agent rewards during AI training"}, {"from": "GridWorld MIRL NRMSE 1e-3 outperforming IRL", "title": "motivates", "to": "Use local reduced gap LP to infer utilitarian equilibria for multi-agent reward alignment"}, {"from": "Soccer simulation shows advE-MIRL competitive with zero-sum MIRL and better than d-MIRL", "title": "motivates", "to": "Adopt advE-MIRL approach for competitive environment reward modeling"}, {"from": "Robustness study shows gradual degradation with missing policy states", "title": "motivates", "to": "Deploy Bayesian MIRL with unique equilibrium constraints to model multi-agent rewards during AI training"}, {"from": "Suboptimal inverse dynamics models in self-supervised imitation learning for robotic control", "title": "caused_by", "to": "Inefficient exploratory data collection in self-supervised IL for continuous control domains"}, {"from": "Suboptimal inverse dynamics models in self-supervised imitation learning for robotic control", "title": "caused_by", "to": "Sampling bias toward easy or human-preselected states in self-supervised IL"}, {"from": "High human demonstration requirement in imitation learning for robotics", "title": "caused_by", "to": "Inefficient exploratory data collection in self-supervised IL for continuous control domains"}, {"from": "High human demonstration requirement in imitation learning for robotics", "title": "caused_by", "to": "Sampling bias toward easy or human-preselected states in self-supervised IL"}, {"from": "Inefficient exploratory data collection in self-supervised IL for continuous control domains", "title": "mitigated_by", "to": "Adversarial agent-model competition can generate hard training samples in self-supervised IL"}, {"from": "Sampling bias toward easy or human-preselected states in self-supervised IL", "title": "mitigated_by", "to": "Adversarial agent-model competition can generate hard training samples in self-supervised IL"}, {"from": "Adversarial agent-model competition can generate hard training samples in self-supervised IL", "title": "enabled_by", "to": "Use DRL agent to maximise inverse model prediction error during exploration"}, {"from": "Bounded difficulty curriculum improves stability of adversarial training in self-supervised IL", "title": "enabled_by", "to": "Shape reward around threshold to focus on moderately hard samples"}, {"from": "Use DRL agent to maximise inverse model prediction error during exploration", "title": "implemented_by", "to": "PPO-based DRL agent rewarded by inverse dynamics loss"}, {"from": "Shape reward around threshold to focus on moderately hard samples", "title": "implemented_by", "to": "Reward reshaping function rt = -|L_I\u2212\u03b4| with threshold \u03b4"}, {"from": "PPO-based DRL agent rewarded by inverse dynamics loss", "title": "validated_by", "to": "Higher success rates than baselines on Fetch and Hand manipulation tasks"}, {"from": "Reward reshaping function rt = -|L_I\u2212\u03b4| with threshold \u03b4", "title": "validated_by", "to": "Higher success rates than baselines on Fetch and Hand manipulation tasks"}, {"from": "Reward reshaping function rt = -|L_I\u2212\u03b4| with threshold \u03b4", "title": "validated_by", "to": "Lower performance drop under injected action noise across tasks"}, {"from": "Higher success rates than baselines on Fetch and Hand manipulation tasks", "title": "motivates", "to": "Jointly train PPO explorer and inverse dynamics model using adversarial exploration during self-supervised IL"}, {"from": "Higher success rates than baselines on Fetch and Hand manipulation tasks", "title": "motivates", "to": "Apply reward shaping threshold \u03b4 for stabilising adversarial exploration during self-supervised IL training"}, {"from": "Lower performance drop under injected action noise across tasks", "title": "motivates", "to": "Jointly train PPO explorer and inverse dynamics model using adversarial exploration during self-supervised IL"}, {"from": "Lower performance drop under injected action noise across tasks", "title": "motivates", "to": "Apply reward shaping threshold \u03b4 for stabilising adversarial exploration during self-supervised IL training"}, {"from": "Unauthorized adversarial reprogramming of neural networks in deployed systems", "title": "caused_by", "to": "Additive input-based adversarial programs exploiting network nonlinearities in vision models"}, {"from": "Additive input-based adversarial programs exploiting network nonlinearities in vision models", "title": "enabled_by", "to": "Flexibility of trained neural network representations enables new tasks through input bias modification"}, {"from": "Additive input-based adversarial programs exploiting network nonlinearities in vision models", "title": "enabled_by", "to": "Requirement of nonlinear interactions between adversarial program and data for successful reprogramming"}, {"from": "Additive input-based adversarial programs exploiting network nonlinearities in vision models", "title": "implemented_by", "to": "Input additive adversarial program optimized via Equation 3"}, {"from": "Flexibility of trained neural network representations enables new tasks through input bias modification", "title": "validated_by", "to": "Reduced reprogramming success on randomly initialized networks"}, {"from": "Flexibility of trained neural network representations enables new tasks through input bias modification", "title": "mitigated_by", "to": "Input-level defense mechanisms to detect or neutralize adversarial programs"}, {"from": "Flexibility of trained neural network representations enables new tasks through input bias modification", "title": "mitigated_by", "to": "Training models with adversarial reprogramming examples to increase robustness"}, {"from": "Requirement of nonlinear interactions between adversarial program and data for successful reprogramming", "title": "mitigated_by", "to": "Architectural constraints reducing effect of input bias changes on internal representations"}, {"from": "Input-level defense mechanisms to detect or neutralize adversarial programs", "title": "implemented_by", "to": "Frequency-based masking and anomaly detection preprocessing for adversarial program filtering"}, {"from": "Input-level defense mechanisms to detect or neutralize adversarial programs", "title": "motivates", "to": "High success rates of reprogramming six ImageNet models across tasks"}, {"from": "Training models with adversarial reprogramming examples to increase robustness", "title": "implemented_by", "to": "Adversarial reprogramming adversarial training during fine-tuning phase"}, {"from": "Training models with adversarial reprogramming examples to increase robustness", "title": "motivates", "to": "High success rates of reprogramming six ImageNet models across tasks"}, {"from": "Training models with adversarial reprogramming examples to increase robustness", "title": "motivates", "to": "Adversarially trained Inception V3 still vulnerable, showing only slight reduction in reprogramming success"}, {"from": "Architectural constraints reducing effect of input bias changes on internal representations", "title": "implemented_by", "to": "First-layer bias normalization and weight constraints to limit reprogramming susceptibility"}, {"from": "Architectural constraints reducing effect of input bias changes on internal representations", "title": "motivates", "to": "High success rates of reprogramming six ImageNet models across tasks"}, {"from": "Architectural constraints reducing effect of input bias changes on internal representations", "title": "motivates", "to": "Reduced reprogramming success on randomly initialized networks"}, {"from": "Frequency-based masking and anomaly detection preprocessing for adversarial program filtering", "title": "validated_by", "to": "Projected detection accuracy of input sanitization filters against adversarial programs"}, {"from": "Adversarial reprogramming adversarial training during fine-tuning phase", "title": "validated_by", "to": "Simulation-based robustness improvement via adversarial reprogramming-aware training"}, {"from": "First-layer bias normalization and weight constraints to limit reprogramming susceptibility", "title": "validated_by", "to": "Theoretical reduction in susceptibility from bias constraint architecture"}, {"from": "High success rates of reprogramming six ImageNet models across tasks", "title": "validated_by", "to": "Input additive adversarial program optimized via Equation 3"}, {"from": "Adversarially trained Inception V3 still vulnerable, showing only slight reduction in reprogramming success", "title": "validated_by", "to": "Input additive adversarial program optimized via Equation 3"}, {"from": "Projected detection accuracy of input sanitization filters against adversarial programs", "title": "motivates", "to": "Deploy input sanitization filters detecting adversarial programs before inference"}, {"from": "Simulation-based robustness improvement via adversarial reprogramming-aware training", "title": "motivates", "to": "Fine-tune models with adversarial reprogramming adversarial examples to improve robustness"}, {"from": "Theoretical reduction in susceptibility from bias constraint architecture", "title": "motivates", "to": "Incorporate first-layer bias constraint architecture during model design to limit reprogramming vulnerability"}, {"from": "Exploitation of agent by adversarial environment in decision-making tasks", "title": "caused_by", "to": "Inadequate modeling of environment attitude continuum in multi-agent systems"}, {"from": "Exploitation of agent by adversarial environment in decision-making tasks", "title": "caused_by", "to": "Information asymmetry enabling environment reaction to agent strategy"}, {"from": "Inadequate modeling of environment attitude continuum in multi-agent systems", "title": "mitigated_by", "to": "Continuous inverse-temperature parameterization of environment attitude"}, {"from": "Information asymmetry enabling environment reaction to agent strategy", "title": "mitigated_by", "to": "Mutual information as indicator of environment reactivity"}, {"from": "Continuous inverse-temperature parameterization of environment attitude", "title": "mitigated_by", "to": "Risk-sensitive equilibrium strategy derived from information-constrained game"}, {"from": "Mutual information as indicator of environment reactivity", "title": "mitigated_by", "to": "Detection of environment attitude via mutual information estimation"}, {"from": "Risk-sensitive equilibrium strategy derived from information-constrained game", "title": "implemented_by", "to": "Gibbs best-response equilibrium computation with exponential update algorithm"}, {"from": "Detection of environment attitude via mutual information estimation", "title": "implemented_by", "to": "Thompson sampling estimation of inverse temperature parameter \u03b2"}, {"from": "Gibbs best-response equilibrium computation with exponential update algorithm", "title": "validated_by", "to": "Bernoulli bandit experiments showing attitude-dependent payoffs"}, {"from": "Gibbs best-response equilibrium computation with exponential update algorithm", "title": "validated_by", "to": "Gaussian bandit variance dependence experiments demonstrating strategic adaptation"}, {"from": "Gibbs best-response equilibrium computation with exponential update algorithm", "title": "validated_by", "to": "Linear classifier experiment showing adversarial label flipping and mitigation"}, {"from": "Thompson sampling estimation of inverse temperature parameter \u03b2", "title": "validated_by", "to": "Linear classifier experiment showing adversarial label flipping and mitigation"}, {"from": "Bernoulli bandit experiments showing attitude-dependent payoffs", "title": "motivates", "to": "Compute risk-sensitive equilibrium strategies via iterative Gibbs updates during fine-tuning"}, {"from": "Bernoulli bandit experiments showing attitude-dependent payoffs", "title": "motivates", "to": "Embed inverse-temperature friend-foe modeling into agent decision policies during model design"}, {"from": "Gaussian bandit variance dependence experiments demonstrating strategic adaptation", "title": "motivates", "to": "Compute risk-sensitive equilibrium strategies via iterative Gibbs updates during fine-tuning"}, {"from": "Gaussian bandit variance dependence experiments demonstrating strategic adaptation", "title": "motivates", "to": "Embed inverse-temperature friend-foe modeling into agent decision policies during model design"}, {"from": "Linear classifier experiment showing adversarial label flipping and mitigation", "title": "motivates", "to": "Estimate environment attitude online with Thompson sampling and adapt policies during deployment"}, {"from": "Behavior misprediction by AI systems in complex interactive environments", "title": "caused_by", "to": "Single-scalar reward assumption in RL ignores multi-faceted human motivations"}, {"from": "Infeasibility of IRL in complex games due to simulator or policy access requirements", "title": "caused_by", "to": "Traditional IRL methods require environment dynamics or policy access"}, {"from": "Single-scalar reward assumption in RL ignores multi-faceted human motivations", "title": "mitigated_by", "to": "Human behavior can be conceptualized as optimizing a weighted combination of multiple latent reward dimensions"}, {"from": "Traditional IRL methods require environment dynamics or policy access", "title": "mitigated_by", "to": "Off-policy IRL using historical trajectories allows reward inference without simulators"}, {"from": "Human behavior can be conceptualized as optimizing a weighted combination of multiple latent reward dimensions", "title": "enabled_by", "to": "Decompose reward signals per motivation and learn weights via linear scalarization"}, {"from": "Off-policy IRL using historical trajectories allows reward inference without simulators", "title": "enabled_by", "to": "Combine DQN-based Q-learning for each motivation dimension with linear programming"}, {"from": "Decompose reward signals per motivation and learn weights via linear scalarization", "title": "implemented_by", "to": "MMBM two-step workflow with per-motivation DQNs and LP weight inference"}, {"from": "Combine DQN-based Q-learning for each motivation dimension with linear programming", "title": "implemented_by", "to": "MMBM two-step workflow with per-motivation DQNs and LP weight inference"}, {"from": "MMBM two-step workflow with per-motivation DQNs and LP weight inference", "title": "validated_by", "to": "Recovered player motivation weights aligning with psychological survey results"}, {"from": "MMBM two-step workflow with per-motivation DQNs and LP weight inference", "title": "validated_by", "to": "Improved behavior prediction accuracy (56.5%) versus baselines in WoWAH dataset"}, {"from": "Recovered player motivation weights aligning with psychological survey results", "title": "motivates", "to": "Implement multi-motivation reward modeling via MMBM in RL agent design"}, {"from": "Recovered player motivation weights aligning with psychological survey results", "title": "motivates", "to": "Train per-motivation DQNs with historical trajectories and LP weight inference for agent alignment"}, {"from": "Improved behavior prediction accuracy (56.5%) versus baselines in WoWAH dataset", "title": "motivates", "to": "Train per-motivation DQNs with historical trajectories and LP weight inference for agent alignment"}, {"from": "Improved behavior prediction accuracy (56.5%) versus baselines in WoWAH dataset", "title": "motivates", "to": "Implement multi-motivation reward modeling via MMBM in RL agent design"}, {"from": "Uninterpretable reinforcement learning policies in safety-critical domains", "title": "caused_by", "to": "Opaque decision logic in deep neural network policies"}, {"from": "Absence of verifiable safety guarantees in black-box RL policies", "title": "caused_by", "to": "Inability to impose and validate safety constraints on black-box policies"}, {"from": "Sample inefficiency of gradient-based RL under poor initialization", "title": "caused_by", "to": "Gradient descent local minima causing slow policy convergence"}, {"from": "Opaque decision logic in deep neural network policies", "title": "mitigated_by", "to": "Symbolic policy representations enable formal verification and repair"}, {"from": "Inability to impose and validate safety constraints on black-box policies", "title": "mitigated_by", "to": "Symbolic policy representations enable formal verification and repair"}, {"from": "Gradient descent local minima causing slow policy convergence", "title": "mitigated_by", "to": "Alternating optimization between symbolic and neural representations improves learning efficiency"}, {"from": "Symbolic policy representations enable formal verification and repair", "title": "mitigated_by", "to": "Iterative synthesis\u2013repair\u2013imitation\u2013finetuning framework for policy improvement"}, {"from": "Symbolic policy representations enable formal verification and repair", "title": "mitigated_by", "to": "Human-in-the-loop program repair for adding desired behaviors"}, {"from": "Alternating optimization between symbolic and neural representations improves learning efficiency", "title": "mitigated_by", "to": "Iterative synthesis\u2013repair\u2013imitation\u2013finetuning framework for policy improvement"}, {"from": "Iterative synthesis\u2013repair\u2013imitation\u2013finetuning framework for policy improvement", "title": "implemented_by", "to": "Policy synthesis to decision tree via VIPER"}, {"from": "Iterative synthesis\u2013repair\u2013imitation\u2013finetuning framework for policy improvement", "title": "implemented_by", "to": "Program repair using constraint solving or manual edits to symbolic program"}, {"from": "Iterative synthesis\u2013repair\u2013imitation\u2013finetuning framework for policy improvement", "title": "implemented_by", "to": "Behavioral cloning of repaired program into neural policy"}, {"from": "Iterative synthesis\u2013repair\u2013imitation\u2013finetuning framework for policy improvement", "title": "implemented_by", "to": "TRPO finetuning of cloned policy"}, {"from": "Human-in-the-loop program repair for adding desired behaviors", "title": "implemented_by", "to": "Program repair using constraint solving or manual edits to symbolic program"}, {"from": "Policy synthesis to decision tree via VIPER", "title": "preceded_by", "to": "Program repair using constraint solving or manual edits to symbolic program"}, {"from": "Program repair using constraint solving or manual edits to symbolic program", "title": "preceded_by", "to": "Behavioral cloning of repaired program into neural policy"}, {"from": "Program repair using constraint solving or manual edits to symbolic program", "title": "validated_by", "to": "CartPole experiments showing reward improvements after program repair"}, {"from": "Behavioral cloning of repaired program into neural policy", "title": "preceded_by", "to": "TRPO finetuning of cloned policy"}, {"from": "Behavioral cloning of repaired program into neural policy", "title": "validated_by", "to": "CartPole experiments showing reward improvements after program repair"}, {"from": "TRPO finetuning of cloned policy", "title": "validated_by", "to": "Faster convergence of TRPO from repaired initialization versus worst policy"}, {"from": "CartPole experiments showing reward improvements after program repair", "title": "motivates", "to": "Perform program repair on symbolic policies to enforce safety constraints before imitation"}, {"from": "CartPole experiments showing reward improvements after program repair", "title": "motivates", "to": "Extract decision tree representations from neural policies for interpretability and verification"}, {"from": "CartPole experiments showing reward improvements after program repair", "title": "motivates", "to": "Deploy MORL pipeline during RL training to iteratively synthesize, repair, imitate, and finetune policies"}, {"from": "Faster convergence of TRPO from repaired initialization versus worst policy", "title": "motivates", "to": "Finetune behavioral cloned policies using TRPO to escape local minima"}, {"from": "Faster convergence of TRPO from repaired initialization versus worst policy", "title": "motivates", "to": "Deploy MORL pipeline during RL training to iteratively synthesize, repair, imitate, and finetune policies"}, {"from": "low-quality solutions in combinatorial optimization for industrial tasks", "title": "caused_by", "to": "insufficient self-play curriculum in single-player reinforcement learning"}, {"from": "insufficient self-play curriculum in single-player reinforcement learning", "title": "addressed_by", "to": "relative performance ranking emulates self-play in single-player settings"}, {"from": "relative performance ranking emulates self-play in single-player settings", "title": "implemented_by", "to": "curriculum via ranked rewards exceeding recent performance threshold"}, {"from": "curriculum via ranked rewards exceeding recent performance threshold", "title": "implemented_by", "to": "ranked reward mechanism integrated with neural policy-value network and MCTS"}, {"from": "ranked reward mechanism integrated with neural policy-value network and MCTS", "title": "validated_by", "to": "bin packing experiments show 10% performance improvement and 72% optimality at 75% threshold"}, {"from": "ranked reward mechanism integrated with neural policy-value network and MCTS", "title": "validated_by", "to": "learning instability at 90% ranking threshold"}, {"from": "bin packing experiments show 10% performance improvement and 72% optimality at 75% threshold", "title": "motivates", "to": "Use ranked reward mechanism with 75th percentile threshold during RL training for single-player optimization tasks"}, {"from": "bin packing experiments show 10% performance improvement and 72% optimality at 75% threshold", "title": "motivates", "to": "Implement the R2 algorithm combining policy-value network, MCTS, and ranked reward for combinatorial optimization problems"}, {"from": "learning instability at 90% ranking threshold", "title": "motivates", "to": "Calibrate ranking threshold between 50% and 75% during pre-deployment testing to avoid learning instability"}, {"from": "adversarial misclassification in safety-critical deep neural network applications", "title": "caused_by", "to": "absence of provable robustness guarantees in modern deep neural networks"}, {"from": "absence of provable robustness guarantees in modern deep neural networks", "title": "caused_by", "to": "computational intractability of exact robustness verification for high-dimensional DNNs"}, {"from": "absence of provable robustness guarantees in modern deep neural networks", "title": "mitigated_by", "to": "Lipschitz continuity bounding DNN output variation with input perturbation"}, {"from": "Lipschitz continuity bounding DNN output variation with input perturbation", "title": "specified_by", "to": "pointwise robustness reducible to finite optimisation over \u03c4-grid inputs"}, {"from": "two-player game formulation approximating robustness metrics in DNNs", "title": "implemented_by", "to": "Monte Carlo tree search generating upper bounds for robustness games"}, {"from": "two-player game formulation approximating robustness metrics in DNNs", "title": "implemented_by", "to": "Admissible A* search computing lower bounds in cooperative robustness game"}, {"from": "two-player game formulation approximating robustness metrics in DNNs", "title": "implemented_by", "to": "Alpha-Beta pruning computing lower bounds in competitive robustness game"}, {"from": "two-player game formulation approximating robustness metrics in DNNs", "title": "implemented_by", "to": "DeepGame software tool integrating game-based approximate verification"}, {"from": "two-player game formulation approximating robustness metrics in DNNs", "title": "refined_by", "to": "feature-based input partitioning to reduce search dimensionality"}, {"from": "feature-based input partitioning to reduce search dimensionality", "title": "enabled_by", "to": "DeepGame software tool integrating game-based approximate verification"}, {"from": "Monte Carlo tree search generating upper bounds for robustness games", "title": "validated_by", "to": "convergence of robustness bounds on MNIST, CIFAR10 and GTSRB datasets"}, {"from": "Admissible A* search computing lower bounds in cooperative robustness game", "title": "validated_by", "to": "convergence of robustness bounds on MNIST, CIFAR10 and GTSRB datasets"}, {"from": "Alpha-Beta pruning computing lower bounds in competitive robustness game", "title": "validated_by", "to": "convergence of robustness bounds on MNIST, CIFAR10 and GTSRB datasets"}, {"from": "DeepGame software tool integrating game-based approximate verification", "title": "validated_by", "to": "convergence of robustness bounds on MNIST, CIFAR10 and GTSRB datasets"}, {"from": "convergence of robustness bounds on MNIST, CIFAR10 and GTSRB datasets", "title": "motivates", "to": "perform DeepGame-based pre-deployment robustness testing of DNN models"}, {"from": "convergence of robustness bounds on MNIST, CIFAR10 and GTSRB datasets", "title": "motivates", "to": "certify safety regions by estimating maximum safe radius via \u03c4-grid Lipschitz verification"}, {"from": "competitive adversarial example efficiency compared to CW, JSMA and others", "title": "motivates", "to": "leverage game-based adversarial search to augment adversarial training datasets"}, {"from": "traffic light study revealing one-pixel adversarial vulnerability", "title": "motivates", "to": "use feature robustness metrics in runtime monitoring to restrict unsafe perturbations"}, {"from": "Poor length generalization of feed-forward Transformers in sequence modeling", "title": "caused_by", "to": "Absence of recurrent inductive bias in Transformers causing generalization failures"}, {"from": "Inefficient per-symbol computation allocation in fixed-depth sequence models", "title": "caused_by", "to": "Uniform fixed-depth processing per symbol causing resource misallocation"}, {"from": "Computational non-universality limiting algorithmic reasoning in standard Transformers", "title": "caused_by", "to": "Fixed layer depth independent of input length limiting expressivity"}, {"from": "Absence of recurrent inductive bias in Transformers causing generalization failures", "title": "mitigated_by", "to": "Parallel recurrent refinement enables iterative algorithm simulation in Transformers"}, {"from": "Uniform fixed-depth processing per symbol causing resource misallocation", "title": "mitigated_by", "to": "Adaptive halting per symbol enables dynamic resource allocation based on ambiguity"}, {"from": "Fixed layer depth independent of input length limiting expressivity", "title": "mitigated_by", "to": "Depth scaling with input length yields Turing completeness in sequence models"}, {"from": "Parallel recurrent refinement enables iterative algorithm simulation in Transformers", "title": "enabled_by", "to": "Integrate shared-weight recurrence into Transformer while retaining self-attention parallelism"}, {"from": "Adaptive halting per symbol enables dynamic resource allocation based on ambiguity", "title": "enabled_by", "to": "Apply adaptive computation time mechanism within recurrent Transformer steps"}, {"from": "Depth scaling with input length yields Turing completeness in sequence models", "title": "enabled_by", "to": "Enable unbounded recurrent steps tied to input length to extend expressivity"}, {"from": "Integrate shared-weight recurrence into Transformer while retaining self-attention parallelism", "title": "implemented_by", "to": "Universal Transformer architecture with self-attention plus shared transition recurrence"}, {"from": "Apply adaptive computation time mechanism within recurrent Transformer steps", "title": "implemented_by", "to": "Per-symbol adaptive computation time halting in Universal Transformer"}, {"from": "Enable unbounded recurrent steps tied to input length to extend expressivity", "title": "implemented_by", "to": "Parameter tying across depth layers in Universal Transformer"}, {"from": "Universal Transformer architecture with self-attention plus shared transition recurrence", "title": "validated_by", "to": "Universal Transformer achieves state-of-the-art on algorithmic and bAbI tasks with length generalization"}, {"from": "Universal Transformer architecture with self-attention plus shared transition recurrence", "title": "enabled_by", "to": "Per-symbol adaptive computation time halting in Universal Transformer"}, {"from": "Universal Transformer architecture with self-attention plus shared transition recurrence", "title": "enabled_by", "to": "Parameter tying across depth layers in Universal Transformer"}, {"from": "Per-symbol adaptive computation time halting in Universal Transformer", "title": "validated_by", "to": "Adaptive Universal Transformer exhibits higher ponder time on ambiguous inputs and improves bAbI accuracy"}, {"from": "Parameter tying across depth layers in Universal Transformer", "title": "validated_by", "to": "Proof of Universal Transformer computational universality and performance on diverse tasks"}, {"from": "Universal Transformer achieves state-of-the-art on algorithmic and bAbI tasks with length generalization", "title": "motivates", "to": "Design model architectures using Universal Transformer recurrence to improve length generalization"}, {"from": "Adaptive Universal Transformer exhibits higher ponder time on ambiguous inputs and improves bAbI accuracy", "title": "motivates", "to": "Implement adaptive computation time per symbol during model design of sequence models"}, {"from": "Proof of Universal Transformer computational universality and performance on diverse tasks", "title": "motivates", "to": "Replace fixed-depth Transformer layers with parameter-tied recurrent steps for universality"}, {"from": "policy failure from sample inefficiency in model-based reinforcement learning", "title": "caused_by", "to": "high sample complexity of non-factorised transition model estimation"}, {"from": "policy failure from sample inefficiency in model-based reinforcement learning", "title": "caused_by", "to": "compounding errors from inaccurate transition models"}, {"from": "high sample complexity of non-factorised transition model estimation", "title": "addressed_by", "to": "discrete abstract state bottlenecks reduce variance with controlled bias"}, {"from": "compounding errors from inaccurate transition models", "title": "addressed_by", "to": "policy access to full state mitigates abstraction deficiency"}, {"from": "discrete abstract state bottlenecks reduce variance with controlled bias", "title": "specified_by", "to": "factorised transition model with discrete abstract states and fewer parameters"}, {"from": "policy access to full state mitigates abstraction deficiency", "title": "specified_by", "to": "factorised transition model with discrete abstract states and fewer parameters"}, {"from": "factorised transition model with discrete abstract states and fewer parameters", "title": "implemented_by", "to": "mapping full states to abstract states via clustering or deterministic classifiers"}, {"from": "factorised transition model with discrete abstract states and fewer parameters", "title": "validated_by", "to": "mathematical error bound showing O(|S||Z||A|) sample complexity"}, {"from": "mapping full states to abstract states via clustering or deterministic classifiers", "title": "enabled_by", "to": "neural transition model conditioned on abstract states"}, {"from": "neural transition model conditioned on abstract states", "title": "enabled_by", "to": "rollout simulation in learned Bottleneck Simulator for policy training"}, {"from": "rollout simulation in learned Bottleneck Simulator for policy training", "title": "validated_by", "to": "empirical sample-efficiency gains in Home World text adventure"}, {"from": "rollout simulation in learned Bottleneck Simulator for policy training", "title": "validated_by", "to": "real-world user score improvements in Alexa Prize dialogue system"}, {"from": "empirical sample-efficiency gains in Home World text adventure", "title": "motivates", "to": "integrate Bottleneck Simulator factorised model in RL training pipelines"}, {"from": "real-world user score improvements in Alexa Prize dialogue system", "title": "motivates", "to": "integrate Bottleneck Simulator factorised model in RL training pipelines"}, {"from": "mathematical error bound showing O(|S||Z||A|) sample complexity", "title": "motivates", "to": "adaptively refine abstract state granularity as data grows"}, {"from": "Pathologically wrong value inference from hierarchical human planning in IRL", "title": "caused_by", "to": "Assumption of flat optimal human behaviour in standard IRL"}, {"from": "Assumption of flat optimal human behaviour in standard IRL", "title": "mitigated_by", "to": "Human decision-making uses hierarchical options"}, {"from": "Assumption of flat optimal human behaviour in standard IRL", "title": "mitigated_by", "to": "Boltzmann-rational bounded planning models human action selection"}, {"from": "Human decision-making uses hierarchical options", "title": "enabled_by", "to": "Model human planning with options in inverse reinforcement learning"}, {"from": "Boltzmann-rational bounded planning models human action selection", "title": "enabled_by", "to": "Self-consistent hierarchical Boltzmann policy captures bounded rationality"}, {"from": "Model human planning with options in inverse reinforcement learning", "title": "implemented_by", "to": "Generative model of hierarchical planners with extended options"}, {"from": "Self-consistent hierarchical Boltzmann policy captures bounded rationality", "title": "implemented_by", "to": "BIHRL Bayesian inference algorithm with Policy-Walk MCMC sampling"}, {"from": "Generative model of hierarchical planners with extended options", "title": "required_by", "to": "BIHRL Bayesian inference algorithm with Policy-Walk MCMC sampling"}, {"from": "Generative model of hierarchical planners with extended options", "title": "validated_by", "to": "Improved goal prediction accuracy in Taxi-driver environment"}, {"from": "BIHRL Bayesian inference algorithm with Policy-Walk MCMC sampling", "title": "refined_by", "to": "Option-set marginalization to jointly infer options and rewards"}, {"from": "BIHRL Bayesian inference algorithm with Policy-Walk MCMC sampling", "title": "validated_by", "to": "Reduced negative log marginal likelihood and 66% accuracy in Wikispeedia benchmark"}, {"from": "Option-set marginalization to jointly infer options and rewards", "title": "validated_by", "to": "Higher posterior probability on ground truth reward under option-set inference"}, {"from": "Improved goal prediction accuracy in Taxi-driver environment", "title": "motivates", "to": "Incorporate hierarchical option-aware BIHRL in value learning pipelines during model fine-tuning to improve preference inference"}, {"from": "Improved goal prediction accuracy in Taxi-driver environment", "title": "motivates", "to": "Provide explicit option libraries reflecting common human hierarchical skills during reward model design"}, {"from": "Reduced negative log marginal likelihood and 66% accuracy in Wikispeedia benchmark", "title": "motivates", "to": "Incorporate hierarchical option-aware BIHRL in value learning pipelines during model fine-tuning to improve preference inference"}, {"from": "Higher posterior probability on ground truth reward under option-set inference", "title": "motivates", "to": "Apply scalable variational or Hamiltonian MC inference to marginalize over latent options in BIHRL for large-scale environments"}, {"from": "Proprietary model leakage via explanation APIs in commercial ML services", "title": "caused_by", "to": "Input gradient exposure through saliency maps in explanation APIs"}, {"from": "Input gradient exposure through saliency maps in explanation APIs", "title": "caused_by", "to": "Gradients reveal sufficient parameter information for two-layer ReLU networks"}, {"from": "Gradients reveal sufficient parameter information for two-layer ReLU networks", "title": "refined_by", "to": "Gradient queries reduce sample complexity of model reconstruction by factor of input dimension"}, {"from": "Gradients reveal sufficient parameter information for two-layer ReLU networks", "title": "mitigated_by", "to": "Limiting fidelity and quantity of gradient information can mitigate model leakage risk"}, {"from": "Limiting fidelity and quantity of gradient information can mitigate model leakage risk", "title": "implemented_by", "to": "Add differential privacy Gaussian noise to gradients in explanation APIs"}, {"from": "Limiting fidelity and quantity of gradient information can mitigate model leakage risk", "title": "implemented_by", "to": "Enforce strict query rate limits and gradient precision clipping in explanation APIs"}, {"from": "Add differential privacy Gaussian noise to gradients in explanation APIs", "title": "validated_by", "to": "SmoothGrad processed gradients do not reduce reconstruction efficiency"}, {"from": "Enforce strict query rate limits and gradient precision clipping in explanation APIs", "title": "validated_by", "to": "Theoretical algorithm reconstructs two-layer ReLU network in O(h log h) gradient queries"}, {"from": "Enforce strict query rate limits and gradient precision clipping in explanation APIs", "title": "validated_by", "to": "Experiments show 100x\u20131000x fewer queries required to reconstruct models on MNIST and CIFAR10 with gradients"}, {"from": "Theoretical algorithm reconstructs two-layer ReLU network in O(h log h) gradient queries", "title": "motivates", "to": "Limit external explanation API to low query rates and truncated gradient precision during deployment"}, {"from": "Experiments show 100x\u20131000x fewer queries required to reconstruct models on MNIST and CIFAR10 with gradients", "title": "motivates", "to": "Limit external explanation API to low query rates and truncated gradient precision during deployment"}, {"from": "Experiments show 100x\u20131000x fewer queries required to reconstruct models on MNIST and CIFAR10 with gradients", "title": "motivates", "to": "Design reconstruction-resistant explanation techniques with formal privacy guarantees during model design phase"}, {"from": "SmoothGrad processed gradients do not reduce reconstruction efficiency", "title": "motivates", "to": "Inject calibrated differential privacy noise into gradient-based explanations before deployment"}, {"from": "Safety violations during exploration in reinforcement learning agents", "title": "caused_by", "to": "Absence of runtime action-level safety constraints in RL environments"}, {"from": "Absence of runtime action-level safety constraints in RL environments", "title": "mitigated_by", "to": "Bounded-probability safety objectives balance exploration and safety in RL"}, {"from": "Bounded-probability safety objectives balance exploration and safety in RL", "title": "implemented_by", "to": "Probabilistic \u03b4-shield blocking high-risk actions in RL"}, {"from": "Probabilistic \u03b4-shield blocking high-risk actions in RL", "title": "implemented_by", "to": "Action valuation via probabilistic model checking of safety-relevant MDP"}, {"from": "Probabilistic \u03b4-shield blocking high-risk actions in RL", "title": "refined_by", "to": "Adaptive \u03b4-threshold tuning of shield strictness in RL"}, {"from": "Adaptive \u03b4-threshold tuning of shield strictness in RL", "title": "implemented_by", "to": "Runtime \u03b4 adjustment algorithm based on performance progress in RL"}, {"from": "Action valuation via probabilistic model checking of safety-relevant MDP", "title": "validated_by", "to": "PAC-MAN and warehouse experiments show improved performance with shields"}, {"from": "Action valuation via probabilistic model checking of safety-relevant MDP", "title": "caused_by", "to": "Scalability issues of model checking for large safety-relevant MDPs"}, {"from": "Runtime \u03b4 adjustment algorithm based on performance progress in RL", "title": "validated_by", "to": "PAC-MAN and warehouse experiments show improved performance with shields"}, {"from": "Scalability issues of model checking for large safety-relevant MDPs", "title": "mitigated_by", "to": "Agent independence and finite horizon reduce shield computation complexity"}, {"from": "Agent independence and finite horizon reduce shield computation complexity", "title": "implemented_by", "to": "Finite-horizon piecewise independent shield construction for RL safety"}, {"from": "Finite-horizon piecewise independent shield construction for RL safety", "title": "validated_by", "to": "PAC-MAN and warehouse experiments show improved performance with shields"}, {"from": "PAC-MAN and warehouse experiments show improved performance with shields", "title": "motivates", "to": "Deploy probabilistic \u03b4-shield to block RL agent actions exceeding risk threshold"}, {"from": "PAC-MAN and warehouse experiments show improved performance with shields", "title": "motivates", "to": "Compute shields using finite-horizon piecewise independent-agent model checking before deployment"}, {"from": "PAC-MAN and warehouse experiments show improved performance with shields", "title": "motivates", "to": "Dynamically adjust \u03b4 threshold when RL performance stagnates"}, {"from": "Inefficient and unsafe exploration in RL for complex tasks", "title": "caused_by", "to": "Need for explicit reward function in complex RL tasks"}, {"from": "Limited scalability of imitation learning with action-dependent demonstrations", "title": "caused_by", "to": "Unavailability of demonstrator action data in state-only demonstrations"}, {"from": "Limited scalability of imitation learning with action-dependent demonstrations", "title": "caused_by", "to": "Compounding error from covariate shift in behavioral cloning from observation"}, {"from": "Limited scalability of imitation learning with action-dependent demonstrations", "title": "caused_by", "to": "Time-alignment requirement in surrogate reward representation methods"}, {"from": "Need for explicit reward function in complex RL tasks", "title": "mitigated_by", "to": "Task objectives lie on low-dimensional manifold of state transitions"}, {"from": "Unavailability of demonstrator action data in state-only demonstrations", "title": "mitigated_by", "to": "Matching state-transition occupancy measures enables action-agnostic imitation"}, {"from": "Compounding error from covariate shift in behavioral cloning from observation", "title": "mitigated_by", "to": "Adversarial alignment of imitator and expert state-transition distributions"}, {"from": "Time-alignment requirement in surrogate reward representation methods", "title": "mitigated_by", "to": "Adversarial alignment of imitator and expert state-transition distributions"}, {"from": "Task objectives lie on low-dimensional manifold of state transitions", "title": "refined_by", "to": "Adversarial alignment of imitator and expert state-transition distributions"}, {"from": "Matching state-transition occupancy measures enables action-agnostic imitation", "title": "refined_by", "to": "Adversarial alignment of imitator and expert state-transition distributions"}, {"from": "Adversarial alignment of imitator and expert state-transition distributions", "title": "implemented_by", "to": "GAIfO algorithm with discriminator over state transitions and TRPO optimization"}, {"from": "GAIfO algorithm with discriminator over state transitions and TRPO optimization", "title": "validated_by", "to": "GAIfO performance comparative to GAIL and superior to other IfO methods in OpenAI Gym domains"}, {"from": "GAIfO performance comparative to GAIL and superior to other IfO methods in OpenAI Gym domains", "title": "motivates", "to": "Fine-tune RL agents with GAIfO using state-only expert demonstrations"}, {"from": "Unrecognized unknown objects in open-world robot manipulation", "title": "caused_by", "to": "Entangled latent visual representations in robot perception models"}, {"from": "Unrecognized unknown objects in open-world robot manipulation", "title": "caused_by", "to": "Need for complete labelled datasets in symbol grounding"}, {"from": "Non-interpretable internal representations in autonomous robots", "title": "caused_by", "to": "Entangled latent visual representations in robot perception models"}, {"from": "Entangled latent visual representations in robot perception models", "title": "caused_by", "to": "Axis-aligned latent factors aligned with user-defined concept groups improve separability"}, {"from": "Need for complete labelled datasets in symbol grounding", "title": "caused_by", "to": "Weak supervision with partial labels can enforce axis-aligned representations"}, {"from": "Axis-aligned latent factors aligned with user-defined concept groups improve separability", "title": "mitigated_by", "to": "Per-concept linear classifier on single latent dimension enforces axis alignment"}, {"from": "Weak supervision with partial labels can enforce axis-aligned representations", "title": "mitigated_by", "to": "Per-concept linear classifier on single latent dimension enforces axis alignment"}, {"from": "Per-concept linear classifier on single latent dimension enforces axis alignment", "title": "refined_by", "to": "Balanced reconstruction, KL divergence, and classification losses maintain disentanglement and data fidelity"}, {"from": "Per-concept linear classifier on single latent dimension enforces axis alignment", "title": "implemented_by", "to": "1-NN classifier with per-label 1D normals plus interpolated unknown class"}, {"from": "Balanced reconstruction, KL divergence, and classification losses maintain disentanglement and data fidelity", "title": "implemented_by", "to": "\u03b2-VAE with auxiliary per-group classifiers trained on partially labelled data"}, {"from": "\u03b2-VAE with auxiliary per-group classifiers trained on partially labelled data", "title": "validated_by", "to": "Improved axis alignment and F1 on dSprites and real objects vs baselines"}, {"from": "\u03b2-VAE with auxiliary per-group classifiers trained on partially labelled data", "title": "preceded_by", "to": "1-NN classifier with per-label 1D normals plus interpolated unknown class"}, {"from": "1-NN classifier with per-label 1D normals plus interpolated unknown class", "title": "validated_by", "to": "Higher unknown object detection F1 in real object experiment"}, {"from": "Improved axis alignment and F1 on dSprites and real objects vs baselines", "title": "motivates", "to": "Pre-train robot perception models with axis-aligned weakly supervised \u03b2-VAE (model design phase)"}, {"from": "Higher unknown object detection F1 in real object experiment", "title": "motivates", "to": "Deploy latent-space 1-NN classifier to detect unknown objects during operation"}, {"from": "Catastrophic state visitation during exploration in reinforcement learning", "title": "caused_by", "to": "Unconstrained exploration in hierarchical option learning"}, {"from": "Unconstrained exploration in hierarchical option learning", "title": "mitigated_by", "to": "Variance of temporal difference error as uncertainty metric in state-option pairs"}, {"from": "Variance of temporal difference error as uncertainty metric in state-option pairs", "title": "mitigated_by", "to": "Incorporate controllability regularization into option policies to reduce TD error variance"}, {"from": "Incorporate controllability regularization into option policies to reduce TD error variance", "title": "implemented_by", "to": "Safe option-critic algorithm with controllability regularization"}, {"from": "Incorporate controllability regularization into option policies to reduce TD error variance", "title": "implemented_by", "to": "Safe asynchronous advantage option-critic integrating controllability and deliberation"}, {"from": "Incorporate controllability regularization into option policies to reduce TD error variance", "title": "implemented_by", "to": "Hyperparameter tuning of controllability regularizer \u03c8"}, {"from": "Safe option-critic algorithm with controllability regularization", "title": "validated_by", "to": "Reduced unsafe state visitation and return variance in Four-Rooms with Safe-OC"}, {"from": "Safe option-critic algorithm with controllability regularization", "title": "validated_by", "to": "Faster learning and lower variance in CartPole with \u03c8=0.25 in Safe-OC"}, {"from": "Safe asynchronous advantage option-critic integrating controllability and deliberation", "title": "validated_by", "to": "Improved Atari scores in MsPacman, Amidar, Q*Bert with \u03c8 0.05-0.10 in Safe-A2OC"}, {"from": "Hyperparameter tuning of controllability regularizer \u03c8", "title": "validated_by", "to": "Optimal \u03c8 between 0.05 and 0.1 yields best safety-performance trade-off across tasks"}, {"from": "Reduced unsafe state visitation and return variance in Four-Rooms with Safe-OC", "title": "motivates", "to": "Fine-tune RL agents with safe option-critic framework using controllability regularization"}, {"from": "Faster learning and lower variance in CartPole with \u03c8=0.25 in Safe-OC", "title": "motivates", "to": "Fine-tune RL agents with safe option-critic framework using controllability regularization"}, {"from": "Improved Atari scores in MsPacman, Amidar, Q*Bert with \u03c8 0.05-0.10 in Safe-A2OC", "title": "motivates", "to": "Deploy Safe-A2OC with controllability regularization in deep RL agents for high-variance environments"}, {"from": "Optimal \u03c8 between 0.05 and 0.1 yields best safety-performance trade-off across tasks", "title": "motivates", "to": "Conduct grid search to select controllability regularizer \u03c8 between 0.05-0.1 during Safe-OC training"}, {"from": "Failure-state reachability during imitation learning in robotics", "title": "caused_by", "to": "Data mismatch and compounding errors in DAgger training"}, {"from": "Failure-state reachability during imitation learning in robotics", "title": "caused_by", "to": "Fixed discrepancy threshold limitations in SafeDAgger decision rule"}, {"from": "Data mismatch and compounding errors in DAgger training", "title": "mitigated_by", "to": "Epistemic uncertainty as proxy for proximity to failure states in model-free imitation learning"}, {"from": "Fixed discrepancy threshold limitations in SafeDAgger decision rule", "title": "mitigated_by", "to": "Epistemic uncertainty as proxy for proximity to failure states in model-free imitation learning"}, {"from": "Epistemic uncertainty as proxy for proximity to failure states in model-free imitation learning", "title": "enabled_by", "to": "Safety gating of novice actions via combined discrepancy and uncertainty thresholds"}, {"from": "Safety gating of novice actions via combined discrepancy and uncertainty thresholds", "title": "implemented_by", "to": "Deep ensemble approximating Gaussian Process for novice variance estimation"}, {"from": "Safety gating of novice actions via combined discrepancy and uncertainty thresholds", "title": "implemented_by", "to": "EnsembleDAgger decision rule combining doubt and discrepancy rules"}, {"from": "Deep ensemble approximating Gaussian Process for novice variance estimation", "title": "required_by", "to": "EnsembleDAgger decision rule combining doubt and discrepancy rules"}, {"from": "Deep ensemble approximating Gaussian Process for novice variance estimation", "title": "validated_by", "to": "Reduced failure rate and improved learning on inverted pendulum benchmark"}, {"from": "EnsembleDAgger decision rule combining doubt and discrepancy rules", "title": "validated_by", "to": "Reduced failure rate and improved learning on inverted pendulum benchmark"}, {"from": "EnsembleDAgger decision rule combining doubt and discrepancy rules", "title": "validated_by", "to": "Pareto-dominant safety\u2013performance tradeoff on MuJoCo HalfCheetah experiments"}, {"from": "Reduced failure rate and improved learning on inverted pendulum benchmark", "title": "motivates", "to": "Apply EnsembleDAgger decision rule during imitation learning training to permit novice actions conditionally"}, {"from": "Reduced failure rate and improved learning on inverted pendulum benchmark", "title": "motivates", "to": "Train novice policy as deep ensemble to estimate action variance for safety gating"}, {"from": "Pareto-dominant safety\u2013performance tradeoff on MuJoCo HalfCheetah experiments", "title": "motivates", "to": "Apply EnsembleDAgger decision rule during imitation learning training to permit novice actions conditionally"}, {"from": "Pareto-dominant safety\u2013performance tradeoff on MuJoCo HalfCheetah experiments", "title": "motivates", "to": "Train novice policy as deep ensemble to estimate action variance for safety gating"}, {"from": "Undesirable behaviors from reward mis-specification in multi-agent RL", "title": "caused_by", "to": "Difficulty designing suitable reward functions for complex multi-agent tasks"}, {"from": "Undesirable behaviors from reward mis-specification in multi-agent RL", "title": "caused_by", "to": "Non-stationary interactions causing multiple equilibria in multi-agent RL"}, {"from": "Difficulty designing suitable reward functions for complex multi-agent tasks", "title": "mitigated_by", "to": "Imitation learning via occupancy measure matching can bypass explicit reward design"}, {"from": "Non-stationary interactions causing multiple equilibria in multi-agent RL", "title": "mitigated_by", "to": "Multi-agent inverse reinforcement learning via Lagrangian dual enables equilibrium-aware reward inference"}, {"from": "Training instability due to high gradient variance in multi-agent RL", "title": "caused_by", "to": "High gradient variance during policy optimization in multi-agent RL"}, {"from": "High gradient variance during policy optimization in multi-agent RL", "title": "mitigated_by", "to": "Centralized training with natural policy gradients to reduce variance"}, {"from": "High gradient variance during policy optimization in multi-agent RL", "title": "mitigated_by", "to": "Supervised behavior cloning pretraining to bootstrap policy learning"}, {"from": "Imitation learning via occupancy measure matching can bypass explicit reward design", "title": "enabled_by", "to": "Generative adversarial imitation learning generalized to multi-agent settings"}, {"from": "Imitation learning via occupancy measure matching can bypass explicit reward design", "title": "enabled_by", "to": "Per-agent discriminator design incorporating cooperation or competition priors"}, {"from": "Multi-agent inverse reinforcement learning via Lagrangian dual enables equilibrium-aware reward inference", "title": "enabled_by", "to": "Generative adversarial imitation learning generalized to multi-agent settings"}, {"from": "Generative adversarial imitation learning generalized to multi-agent settings", "title": "implemented_by", "to": "MAGAIL algorithm with generator policies and per-agent discriminators"}, {"from": "Per-agent discriminator design incorporating cooperation or competition priors", "title": "implemented_by", "to": "Prior-specific discriminator reward structures in MAGAIL"}, {"from": "Centralized training with natural policy gradients to reduce variance", "title": "implemented_by", "to": "MACK algorithm using Kronecker-factored natural gradients with centralized training"}, {"from": "Supervised behavior cloning pretraining to bootstrap policy learning", "title": "implemented_by", "to": "Behavior cloning pretraining for exploration efficiency in MAGAIL"}, {"from": "MAGAIL algorithm with generator policies and per-agent discriminators", "title": "validated_by", "to": "Empirical success of MAGAIL on cooperative particle environments"}, {"from": "MAGAIL algorithm with generator policies and per-agent discriminators", "title": "required_by", "to": "MACK algorithm using Kronecker-factored natural gradients with centralized training"}, {"from": "MAGAIL algorithm with generator policies and per-agent discriminators", "title": "preceded_by", "to": "Behavior cloning pretraining for exploration efficiency in MAGAIL"}, {"from": "Prior-specific discriminator reward structures in MAGAIL", "title": "validated_by", "to": "Improved performance of MAGAIL variants on competitive tasks"}, {"from": "MACK algorithm using Kronecker-factored natural gradients with centralized training", "title": "validated_by", "to": "MAGAIL agents adapt to changed dynamics in cooperative control plank task"}, {"from": "Behavior cloning pretraining for exploration efficiency in MAGAIL", "title": "validated_by", "to": "MAGAIL agents adapt to changed dynamics in cooperative control plank task"}, {"from": "Empirical success of MAGAIL on cooperative particle environments", "title": "motivates", "to": "Train multi-agent policies with MAGAIL using expert demonstrations to avoid reward mis-specification"}, {"from": "Improved performance of MAGAIL variants on competitive tasks", "title": "motivates", "to": "Configure MAGAIL discriminator reward structure to match task cooperation level"}, {"from": "MAGAIL agents adapt to changed dynamics in cooperative control plank task", "title": "motivates", "to": "Apply MACK optimizer for stable multi-agent policy updates within MAGAIL"}, {"from": "MAGAIL agents adapt to changed dynamics in cooperative control plank task", "title": "motivates", "to": "Pretrain multi-agent policies with behavior cloning before MAGAIL fine-tuning"}, {"from": "Adversarial example-induced misclassification in image classifiers", "title": "caused_by", "to": "Insufficient robustness of Adversarial Logit Pairing defense"}, {"from": "Adversarial example-induced misclassification in image classifiers", "title": "caused_by", "to": "Overestimation of model robustness due to weak evaluation attacks"}, {"from": "Insufficient robustness of Adversarial Logit Pairing defense", "title": "caused_by", "to": "Deviation from robust optimization objective in ALP training objective"}, {"from": "Deviation from robust optimization objective in ALP training objective", "title": "mitigated_by", "to": "Training with untargeted adversarial inner maximization for robustness"}, {"from": "Deviation from robust optimization objective in ALP training objective", "title": "validated_by", "to": "Loss landscape visualization indicates objective deficiency in ALP"}, {"from": "Training with untargeted adversarial inner maximization for robustness", "title": "implemented_by", "to": "Untargeted adversarial training objective implementation"}, {"from": "Loss landscape visualization indicates objective deficiency in ALP", "title": "motivates", "to": "Adopt robust optimization-based adversarial training for ImageNet models"}, {"from": "Overestimation of model robustness due to weak evaluation attacks", "title": "mitigated_by", "to": "Need for stronger attack evaluations to tightly bound robustness"}, {"from": "Need for stronger attack evaluations to tightly bound robustness", "title": "implemented_by", "to": "Comprehensive adversarial evaluation using strong attacks"}, {"from": "Comprehensive adversarial evaluation using strong attacks", "title": "implemented_by", "to": "Extended-step PGD attack and loss landscape analysis"}, {"from": "Extended-step PGD attack and loss landscape analysis", "title": "validated_by", "to": "98.6% attack success and 0.6% accuracy against ALP under targeted PGD (\u03f5=16/255)"}, {"from": "98.6% attack success and 0.6% accuracy against ALP under targeted PGD (\u03f5=16/255)", "title": "motivates", "to": "Conduct pre-deployment evaluation with high-step PGD attacks and loss landscape analysis"}, {"from": "Unnatural behaviors from unsupervised option discovery in high-dimensional control", "title": "caused_by", "to": "Information-theoretic objectives maximize mutual information irrespective of human priors"}, {"from": "Training instability with large context sets in variational option discovery", "title": "caused_by", "to": "Uniform context distribution yields sparse learning signal as K grows"}, {"from": "Trivial context signalling through actions in variational option discovery", "title": "caused_by", "to": "Decoder observability of actions allows direct context encoding"}, {"from": "Limited downstream task utility of learned options", "title": "caused_by", "to": "Learned behaviors may not span task-relevant action space"}, {"from": "Information-theoretic objectives maximize mutual information irrespective of human priors", "title": "mitigated_by", "to": "Using trajectory-level mutual information with state-only decoder encourages environmental interaction"}, {"from": "Uniform context distribution yields sparse learning signal as K grows", "title": "mitigated_by", "to": "Gradually increasing context count when decoder accuracy is high maintains training signal"}, {"from": "Uniform context distribution yields sparse learning signal as K grows", "title": "mitigated_by", "to": "Embedding vectors for contexts enhance representation capacity and learning stability"}, {"from": "Decoder observability of actions allows direct context encoding", "title": "mitigated_by", "to": "Using trajectory-level mutual information with state-only decoder encourages environmental interaction"}, {"from": "Learned behaviors may not span task-relevant action space", "title": "mitigated_by", "to": "Fixed unsupervised option layer can provide expressive action abstractions equivalent to primitive actions"}, {"from": "Using trajectory-level mutual information with state-only decoder encourages environmental interaction", "title": "enabled_by", "to": "Design VALOR algorithm with trajectory-based decoder excluding actions"}, {"from": "Gradually increasing context count when decoder accuracy is high maintains training signal", "title": "enabled_by", "to": "Introduce context count curriculum based on decoder performance threshold"}, {"from": "Embedding vectors for contexts enhance representation capacity and learning stability", "title": "enabled_by", "to": "Implement learnable context embeddings as policy inputs"}, {"from": "Fixed unsupervised option layer can provide expressive action abstractions equivalent to primitive actions", "title": "enabled_by", "to": "Use hierarchical RL with fixed pre-trained option policy for downstream tasks"}, {"from": "Design VALOR algorithm with trajectory-based decoder excluding actions", "title": "implemented_by", "to": "Bidirectional LSTM decoder over sparse state deltas"}, {"from": "Introduce context count curriculum based on decoder performance threshold", "title": "implemented_by", "to": "Curriculum update rule K\u2190min(int(1.5\u00d7K+1), Kmax)"}, {"from": "Implement learnable context embeddings as policy inputs", "title": "implemented_by", "to": "Embedding lookup table for context IDs"}, {"from": "Use hierarchical RL with fixed pre-trained option policy for downstream tasks", "title": "implemented_by", "to": "Two-level policy with fixed VALOR lower layer"}, {"from": "Bidirectional LSTM decoder over sparse state deltas", "title": "validated_by", "to": "VALOR produces naturalistic hand and locomotion behaviors"}, {"from": "Curriculum update rule K\u2190min(int(1.5\u00d7K+1), Kmax)", "title": "validated_by", "to": "Curriculum yields faster convergence and hundreds of modes"}, {"from": "Embedding lookup table for context IDs", "title": "validated_by", "to": "Context embeddings improve speed and stability for large K"}, {"from": "Two-level policy with fixed VALOR lower layer", "title": "validated_by", "to": "Fixed VALOR lower-level matches scratch hierarchy in Ant-Maze"}, {"from": "VALOR produces naturalistic hand and locomotion behaviors", "title": "motivates", "to": "Train unsupervised skills with VALOR using trajectory-based state-only decoder"}, {"from": "Curriculum yields faster convergence and hundreds of modes", "title": "motivates", "to": "Apply context-count curriculum during variational option discovery training"}, {"from": "Context embeddings improve speed and stability for large K", "title": "motivates", "to": "Provide learnable context embeddings in policy networks for option discovery"}, {"from": "Fixed VALOR lower-level matches scratch hierarchy in Ant-Maze", "title": "motivates", "to": "Use fixed pre-trained VALOR policy as lower-level in hierarchical RL for downstream tasks"}, {"from": "numerical extrapolation failure in neural networks", "title": "caused_by", "to": "nonlinear activations causing localized numeric representations in neural networks"}, {"from": "nonlinear activations causing localized numeric representations in neural networks", "title": "caused_by", "to": "lack of inductive bias for arithmetic operations in neural networks"}, {"from": "lack of inductive bias for arithmetic operations in neural networks", "title": "addressed_by", "to": "single-neuron numeric representation enables systematic computation in neural networks"}, {"from": "single-neuron numeric representation enables systematic computation in neural networks", "title": "mitigated_by", "to": "architectural bias for additive accumulation via Neural Accumulator"}, {"from": "single-neuron numeric representation enables systematic computation in neural networks", "title": "mitigated_by", "to": "gated mixture of additive and multiplicative arithmetic in Neural Arithmetic Logic Unit"}, {"from": "architectural bias for additive accumulation via Neural Accumulator", "title": "implemented_by", "to": "weight parameterization via tanh(^W)\u2299\u03c3(^M) to approximate {-1,0,1}"}, {"from": "architectural bias for additive accumulation via Neural Accumulator", "title": "implemented_by", "to": "integration of NAC/NALU modules into diverse neural architectures"}, {"from": "gated mixture of additive and multiplicative arithmetic in Neural Arithmetic Logic Unit", "title": "implemented_by", "to": "NALU log-space multiplication/division using exp(W log|x|)"}, {"from": "gated mixture of additive and multiplicative arithmetic in Neural Arithmetic Logic Unit", "title": "implemented_by", "to": "integration of NAC/NALU modules into diverse neural architectures"}, {"from": "weight parameterization via tanh(^W)\u2299\u03c3(^M) to approximate {-1,0,1}", "title": "validated_by", "to": "identity function autoencoder extrapolation results"}, {"from": "weight parameterization via tanh(^W)\u2299\u03c3(^M) to approximate {-1,0,1}", "title": "validated_by", "to": "synthetic arithmetic tasks demonstrating NAC/NALU extrapolation success"}, {"from": "NALU log-space multiplication/division using exp(W log|x|)", "title": "validated_by", "to": "language-to-number translation task improved generalization with NALU"}, {"from": "NALU log-space multiplication/division using exp(W log|x|)", "title": "validated_by", "to": "program evaluation task extrapolation success with NALU"}, {"from": "integration of NAC/NALU modules into diverse neural architectures", "title": "validated_by", "to": "MNIST counting and addition tasks showing NAC/NALU extrapolation"}, {"from": "integration of NAC/NALU modules into diverse neural architectures", "title": "validated_by", "to": "gridworld time tracking task generalization with NAC-enhanced agent"}, {"from": "integration of NAC/NALU modules into diverse neural architectures", "title": "validated_by", "to": "MNIST parity task performance gain with NAC"}, {"from": "identity function autoencoder extrapolation results", "title": "motivates", "to": "Replace standard layers with Neural Accumulator layers during model design to enable additive extrapolation"}, {"from": "synthetic arithmetic tasks demonstrating NAC/NALU extrapolation success", "title": "motivates", "to": "Replace standard layers with Neural Accumulator layers during model design to enable additive extrapolation"}, {"from": "synthetic arithmetic tasks demonstrating NAC/NALU extrapolation success", "title": "motivates", "to": "Integrate Neural Arithmetic Logic Unit modules with gated additive and multiplicative operations during model design"}, {"from": "MNIST counting and addition tasks showing NAC/NALU extrapolation", "title": "motivates", "to": "Replace standard layers with Neural Accumulator layers during model design to enable additive extrapolation"}, {"from": "language-to-number translation task improved generalization with NALU", "title": "motivates", "to": "Integrate Neural Arithmetic Logic Unit modules with gated additive and multiplicative operations during model design"}, {"from": "program evaluation task extrapolation success with NALU", "title": "motivates", "to": "Integrate Neural Arithmetic Logic Unit modules with gated additive and multiplicative operations during model design"}, {"from": "gridworld time tracking task generalization with NAC-enhanced agent", "title": "motivates", "to": "Embed Neural Accumulator module in reinforcement learning agents for time tracking and timing tasks"}, {"from": "MNIST parity task performance gain with NAC", "title": "motivates", "to": "Replace standard layers with Neural Accumulator layers during model design to enable additive extrapolation"}, {"from": "self-delusion via reward manipulation in AI agents", "title": "caused_by", "to": "observation-altering reward loophole in context"}, {"from": "self-delusion via reward manipulation in AI agents", "title": "mitigated_by", "to": "implement model-based utility over learned environment state"}, {"from": "corruption of reward generator through human manipulation", "title": "mitigated_by", "to": "statistically learn human values and compute three-argument utility"}, {"from": "utility-definition inconsistency in self-modifying AI", "title": "addressed_by", "to": "deploy two-stage architecture separating model learning from acting"}, {"from": "memory tampering vulnerability in embedded AI", "title": "required_by", "to": "embed self-modelling resource-evaluation within agent"}, {"from": "model-based utility alignment insight", "title": "enabled_by", "to": "statistically learn human values and compute three-argument utility"}, {"from": "two-stage agent architecture rationale", "title": "implemented_by", "to": "deploy two-stage architecture separating model learning from acting"}, {"from": "self-modelling agent mechanism", "title": "implemented_by", "to": "embed self-modelling resource-evaluation within agent"}, {"from": "stochastic action option for prediction-resistance", "title": "enabled_by", "to": "embed self-modelling resource-evaluation within agent"}, {"from": "simulation-based AI testing evidence", "title": "validated_by", "to": "test candidate AI in offline simulation decision-support system"}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {
    "configure": {
        "enabled": false
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": true,
            "type": "dynamic"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "enabled": true,
        "forceAtlas2Based": {
            "avoidOverlap": 0,
            "centralGravity": 0.01,
            "damping": 0.4,
            "gravitationalConstant": -50,
            "springConstant": 0.08,
            "springLength": 100
        },
        "solver": "forceAtlas2Based",
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  
                      network.on("stabilizationProgress", function(params) {
                          document.getElementById('loadingBar').removeAttribute("style");
                          var maxWidth = 496;
                          var minWidth = 20;
                          var widthFactor = params.iterations/params.total;
                          var width = Math.max(minWidth,maxWidth * widthFactor);
                          document.getElementById('bar').style.width = width + 'px';
                          document.getElementById('text').innerHTML = Math.round(widthFactor*100) + '%';
                      });
                      network.once("stabilizationIterationsDone", function() {
                          document.getElementById('text').innerHTML = '100%';
                          document.getElementById('bar').style.width = '496px';
                          document.getElementById('loadingBar').style.opacity = 0;
                          // really clean the dom element
                          setTimeout(function () {document.getElementById('loadingBar').style.display = 'none';}, 500);
                      });
                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>