<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 800px;
                 background-color: #222222;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             
             #loadingBar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width: 100%;
                 height: 800px;
                 background-color:rgba(200,200,200,0.8);
                 -webkit-transition: all 0.5s ease;
                 -moz-transition: all 0.5s ease;
                 -ms-transition: all 0.5s ease;
                 -o-transition: all 0.5s ease;
                 transition: all 0.5s ease;
                 opacity:1;
             }

             #bar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width:20px;
                 height:20px;
                 margin:auto auto auto auto;
                 border-radius:11px;
                 border:2px solid rgba(30,30,30,0.05);
                 background: rgb(0, 173, 246); /* Old browsers */
                 box-shadow: 2px 0px 4px rgba(0,0,0,0.4);
             }

             #border {
                 position:absolute;
                 top:10px;
                 left:10px;
                 width:500px;
                 height:23px;
                 margin:auto auto auto auto;
                 box-shadow: 0px 0px 4px rgba(0,0,0,0.2);
                 border-radius:10px;
             }

             #text {
                 position:absolute;
                 top:8px;
                 left:530px;
                 width:30px;
                 height:50px;
                 margin:auto auto auto auto;
                 font-size:22px;
                 color: #000000;
             }

             div.outerBorder {
                 position:relative;
                 top:400px;
                 width:600px;
                 height:44px;
                 margin:auto auto auto auto;
                 border:8px solid rgba(0,0,0,0.1);
                 background: rgb(252,252,252); /* Old browsers */
                 background: -moz-linear-gradient(top,  rgba(252,252,252,1) 0%, rgba(237,237,237,1) 100%); /* FF3.6+ */
                 background: -webkit-gradient(linear, left top, left bottom, color-stop(0%,rgba(252,252,252,1)), color-stop(100%,rgba(237,237,237,1))); /* Chrome,Safari4+ */
                 background: -webkit-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Chrome10+,Safari5.1+ */
                 background: -o-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Opera 11.10+ */
                 background: -ms-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* IE10+ */
                 background: linear-gradient(to bottom,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* W3C */
                 filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#fcfcfc', endColorstr='#ededed',GradientType=0 ); /* IE6-9 */
                 border-radius:72px;
                 box-shadow: 0px 0px 10px rgba(0,0,0,0.2);
             }
             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
            <div id="loadingBar">
              <div class="outerBorder">
                <div id="text">0%</div>
                <div id="border">
                  <div id="bar"></div>
                </div>
              </div>
            </div>
        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"font": {"color": "white"}, "group": 106, "id": "Analytical examples show coarse-grained penalty limits AI influence", "label": "Analytical examples show coarse-grained penalty limits AI influence (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "unsupervised risk estimation signals out-of-distribution inputs", "label": "unsupervised risk estimation signals out-of-distribution inputs (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Suboptimal translation quality and scalability in neural machine translation production", "label": "Suboptimal translation quality and scalability in neural machine translation production (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "Thompson sampling with effective horizon Monte Carlo tree search", "label": "Thompson sampling with effective horizon Monte Carlo tree search (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "use error distribution modeling to predict performance", "label": "use error distribution modeling to predict performance (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Deploy quantised GNMT models on TPUs with 8-bit weights", "label": "Deploy quantised GNMT models on TPUs with 8-bit weights (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "adopt Thompson sampling for policy selection with adequate planning horizon adjustments", "label": "adopt Thompson sampling for policy selection with adequate planning horizon adjustments (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Box AI to minimize observable activation differences", "label": "Box AI to minimize observable activation differences (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Condition impact penalty on externally defined success announcement", "label": "Condition impact penalty on externally defined success announcement (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Regression tree-based feature generation algorithm (FIRL)", "label": "Regression tree-based feature generation algorithm (FIRL) (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Solicit informative demonstrations using active Bayesian IRL", "label": "Solicit informative demonstrations using active Bayesian IRL (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Reduced imitation loss in path planning via automatic feature boosting", "label": "Reduced imitation loss in path planning via automatic feature boosting (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "BLEU unchanged and 3.4\u00d7 speedup on TPU with quantised model", "label": "BLEU unchanged and 3.4\u00d7 speedup on TPU with quantised model (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Constrain accumulators and logits to enable post-training 8/16-bit quantisation", "label": "Constrain accumulators and logits to enable post-training 8/16-bit quantisation (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Active learning querying expert at high-entropy states reduces sample complexity in IRL", "label": "Active learning querying expert at high-entropy states reduces sample complexity in IRL (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "train-test distribution divergence undermines model calibration", "label": "train-test distribution divergence undermines model calibration (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Collect demonstrations via entropy-based active learning queries", "label": "Collect demonstrations via entropy-based active learning queries (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "limited access to high-fidelity human evaluation", "label": "limited access to high-fidelity human evaluation (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Use wordpiece segmentation (32k shared vocab) during NMT training and inference", "label": "Use wordpiece segmentation (32k shared vocab) during NMT training and inference (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "risk-sensitive criteria and safe region constraints reduce catastrophic exploration", "label": "risk-sensitive criteria and safe region constraints reduce catastrophic exploration (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "experiments showing Thompson sampling underperforms AI\u03be under equal sample budget", "label": "experiments showing Thompson sampling underperforms AI\u03be under equal sample budget (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "conditional independence-based unsupervised risk estimator", "label": "conditional independence-based unsupervised risk estimator (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Use maximum entropy optimization to resolve reward ambiguity in IRL", "label": "Use maximum entropy optimization to resolve reward ambiguity in IRL (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Integrate low-impact penalty R into utility function with scaling factor \u03bc", "label": "Integrate low-impact penalty R into utility function with scaling factor \u03bc (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "employ semi-supervised RL with active human queries during fine-tuning", "label": "employ semi-supervised RL with active human queries during fine-tuning (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "negative side effects in autonomous ML systems", "label": "negative side effects in autonomous ML systems (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Apply bisimulation-based state aggregation to cut sample complexity", "label": "Apply bisimulation-based state aggregation to cut sample complexity (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Maximum entropy principle yields least-biased reward distribution in IRL", "label": "Maximum entropy principle yields least-biased reward distribution in IRL (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Maintain reward uncertainty through Bayesian IRL framework", "label": "Maintain reward uncertainty through Bayesian IRL framework (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "objective function indifference to unmodelled environment variables", "label": "objective function indifference to unmodelled environment variables (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Operate multiple conditional low impact AIs to split critical task parameters", "label": "Operate multiple conditional low impact AIs to split critical task parameters (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Automatically learn nonlinear reward features with regression trees", "label": "Automatically learn nonlinear reward features with regression trees (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Expected reward objective with GLEU and \u03b1=0.017 weight", "label": "Expected reward objective with GLEU and \u03b1=0.017 weight (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Sentence-level expected reward optimisation aligns training with BLEU", "label": "Sentence-level expected reward optimisation aligns training with BLEU (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Quantization preserving accuracy while reducing computation in NMT", "label": "Quantization preserving accuracy while reducing computation in NMT (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Conditioning penalty on success announcement event A", "label": "Conditioning penalty on success announcement event A (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "weak asymptotic optimality via exploration schedules in Bayesian RL", "label": "weak asymptotic optimality via exploration schedules in Bayesian RL (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "hypothesis commitment through posterior sampling", "label": "hypothesis commitment through posterior sampling (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Greedy likelihood-maximising wordpiece model training", "label": "Greedy likelihood-maximising wordpiece model training (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Apply length-normalised, coverage-penalised beam search decoding", "label": "Apply length-normalised, coverage-penalised beam search decoding (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Probability ratio threshold detectability metric on future slice", "label": "Probability ratio threshold detectability metric on future slice (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "delusion box environment demonstration proposal", "label": "delusion box environment demonstration proposal (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Unsafe output channel illustration emphasises isolated message risk", "label": "Unsafe output channel illustration emphasises isolated message risk (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "information-gain-triggered exploration bursts in Bayes agents", "label": "information-gain-triggered exploration bursts in Bayes agents (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Deploy bisimulation-based state abstraction for efficient IRL training", "label": "Deploy bisimulation-based state abstraction for efficient IRL training (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "intrinsic information gain utility driving exploration", "label": "intrinsic information gain utility driving exploration (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Scenario of two AI coordinate split demonstrates high impact with low impact constraints", "label": "Scenario of two AI coordinate split demonstrates high impact with low impact constraints (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Provide unsafe output channel with random fallback and conditioned impact", "label": "Provide unsafe output channel with random fallback and conditioned impact (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Incomplete model knowledge affecting reward inference in IRL", "label": "Incomplete model knowledge affecting reward inference in IRL (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Kernel regression over bisimulation metric for policy generalization", "label": "Kernel regression over bisimulation metric for policy generalization (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Robust performance under noisy demonstrations in helicopter control tasks using MaxEnt IRL", "label": "Robust performance under noisy demonstrations in helicopter control tasks using MaxEnt IRL (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Objective-function misalignment with BLEU metric in NMT training", "label": "Objective-function misalignment with BLEU metric in NMT training (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Unbounded utility optimization magnifies goals with increasing AI capability", "label": "Unbounded utility optimization magnifies goals with increasing AI capability (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Convex entropy-constrained optimization of reward parameters", "label": "Convex entropy-constrained optimization of reward parameters (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "Monte Carlo tree search planning with mixture models for POMDPs", "label": "Monte Carlo tree search planning with mixture models for POMDPs (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Use multiple conditional low impact agents for joint tasks", "label": "Use multiple conditional low impact agents for joint tasks (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "distributional shift failures in ML systems", "label": "distributional shift failures in ML systems (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "State abstraction via bisimulation metrics enhances generalization in IRL", "label": "State abstraction via bisimulation metrics enhances generalization in IRL (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "safe policy update using reachability analysis", "label": "safe policy update using reachability analysis (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "deploy unsupervised risk estimator to trigger conservative policy on novel inputs at deployment", "label": "deploy unsupervised risk estimator to trigger conservative policy on novel inputs at deployment (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Incorrect reward inference in inverse reinforcement learning applications", "label": "Incorrect reward inference in inverse reinforcement learning applications (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "Dirichlet gridworld model with factorized tile priors", "label": "Dirichlet gridworld model with factorized tile priors (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "train reward network adversarially against agent policy", "label": "train reward network adversarially against agent policy (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Unintended high-impact side effects from goal-oriented AI systems", "label": "Unintended high-impact side effects from goal-oriented AI systems (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Stock market probability pump example demonstrates conditioning on success", "label": "Stock market probability pump example demonstrates conditioning on success (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Deep residual LSTM enables 8-layer GNMT reaching 38.95 BLEU", "label": "Deep residual LSTM enables 8-layer GNMT reaching 38.95 BLEU (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Element-wise residual addition between stacked LSTM layers", "label": "Element-wise residual addition between stacked LSTM layers (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "entropy-seeking utility misaligned with information gain", "label": "entropy-seeking utility misaligned with information gain (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Impact definition via informational importance of activation event", "label": "Impact definition via informational importance of activation event (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "suboptimal exploration in universal reinforcement learning agents", "label": "suboptimal exploration in universal reinforcement learning agents (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "unsafe exploration in reinforcement learning", "label": "unsafe exploration in reinforcement learning (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Length/coverage penalties give +1.1 BLEU on dev set", "label": "Length/coverage penalties give +1.1 BLEU on dev set (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "computational intractability of planning in partially observable environments for URL", "label": "computational intractability of planning in partially observable environments for URL (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Apply Bayesian IRL with MCMC to model reward uncertainty in training", "label": "Apply Bayesian IRL with MCMC to model reward uncertainty in training (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Length bias in beam search decoding", "label": "Length bias in beam search decoding (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "impact regularization can model dis-preference for environmental change", "label": "impact regularization can model dis-preference for environmental change (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Accurate reward posterior in gridworld experiments with Bayesian IRL", "label": "Accurate reward posterior in gridworld experiments with Bayesian IRL (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Independence gating of separate AIs based on assumption of other inactive", "label": "Independence gating of separate AIs based on assumption of other inactive (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Penalizing AI for deviations from baseline world reduces impact", "label": "Penalizing AI for deviations from baseline world reduces impact (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "Atari limited-reward sampling experiment proposal", "label": "Atari limited-reward sampling experiment proposal (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "rich model class maintains posterior entropy for sustained exploration", "label": "rich model class maintains posterior entropy for sustained exploration (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "simulated quadcopter altitude-bound experiment proposal", "label": "simulated quadcopter altitude-bound experiment proposal (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "\u03c1UCT Monte Carlo tree search planner with finite horizon", "label": "\u03c1UCT Monte Carlo tree search planner with finite horizon (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Ill-posed reward ambiguity from limited demonstrations in IRL", "label": "Ill-posed reward ambiguity from limited demonstrations in IRL (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Bayesian posterior over reward functions captures uncertainty in IRL", "label": "Bayesian posterior over reward functions captures uncertainty in IRL (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Integer-quantised GNMT inference on TPUs with 8-bit weights and 16-bit accumulators", "label": "Integer-quantised GNMT inference on TPUs with 8-bit weights and 16-bit accumulators (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Iteratively adjust penalty coefficient \u03bc during pre-deployment testing", "label": "Iteratively adjust penalty coefficient \u03bc during pre-deployment testing (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Noisy or suboptimal demonstrations impacting reward accuracy in IRL", "label": "Noisy or suboptimal demonstrations impacting reward accuracy in IRL (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "proof of weak asymptotic optimality for BayesExp in general environments", "label": "proof of weak asymptotic optimality for BayesExp in general environments (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "adopt Monte Carlo tree search with finite horizon approximation for POMDP planning in RL agents", "label": "adopt Monte Carlo tree search with finite horizon approximation for POMDP planning in RL agents (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Incorporate residual connections in deep stacked LSTM encoder-decoder architecture", "label": "Incorporate residual connections in deep stacked LSTM encoder-decoder architecture (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Beam-search scoring with \u03b1=0.2 length penalty and \u03b2=0.2 coverage penalty", "label": "Beam-search scoring with \u03b1=0.2 length penalty and \u03b2=0.2 coverage penalty (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Exclude output channel content from impact penalty", "label": "Exclude output channel content from impact penalty (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "semi-supervised reinforcement learning with active reward queries improves efficiency", "label": "semi-supervised reinforcement learning with active reward queries improves efficiency (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Difficulty specifying exhaustive safety constraints in utility functions", "label": "Difficulty specifying exhaustive safety constraints in utility functions (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Beam search decoder with pruning, batch decoding and lp/cp scoring", "label": "Beam search decoder with pruning, batch decoding and lp/cp scoring (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Improved generalization with fewer demonstrations using bisimulation IRL", "label": "Improved generalization with fewer demonstrations using bisimulation IRL (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Length-normalised coverage-aware beam search improves adequacy", "label": "Length-normalised coverage-aware beam search improves adequacy (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "supervised reward model with uncertainty-guided query strategy", "label": "supervised reward model with uncertainty-guided query strategy (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "adversarial reward modeling detects exploitation", "label": "adversarial reward modeling detects exploitation (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Integrate regression tree feature construction into reward modeling pipeline", "label": "Integrate regression tree feature construction into reward modeling pipeline (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "random or optimistic exploration disregards catastrophic outcomes", "label": "random or optimistic exploration disregards catastrophic outcomes (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "design environment models with factorized Dirichlet priors to stimulate exploration during planning", "label": "design environment models with factorized Dirichlet priors to stimulate exploration during planning (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "MCMC sampling of reward posterior distribution", "label": "MCMC sampling of reward posterior distribution (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "constrain actions to states with recoverability guarantee", "label": "constrain actions to states with recoverability guarantee (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "Dirichlet environment model increases epistemic uncertainty for exploration", "label": "Dirichlet environment model increases epistemic uncertainty for exploration (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Use coarse-grained variable l\u221e divergence as penalty metric in AI decision-making", "label": "Use coarse-grained variable l\u221e divergence as penalty metric in AI decision-making (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Shared 32k wordpiece vocabulary for source and target", "label": "Shared 32k wordpiece vocabulary for source and target (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Subword wordpiece modelling for open-vocabulary translation", "label": "Subword wordpiece modelling for open-vocabulary translation (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "restrict entropy-seeking agents from high-entropy noise sources during deployment", "label": "restrict entropy-seeking agents from high-entropy noise sources during deployment (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "state distance penalty with passive policy baseline", "label": "state distance penalty with passive policy baseline (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "l\u221e divergence penalty over coarse-grained variable probabilities", "label": "l\u221e divergence penalty over coarse-grained variable probabilities (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "scalable oversight failure in autonomous ML systems", "label": "scalable oversight failure in autonomous ML systems (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Baseline world via probabilistic signal failure preventing activation", "label": "Baseline world via probabilistic signal failure preventing activation (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "Monte Carlo tree search horizon limits agent foresight", "label": "Monte Carlo tree search horizon limits agent foresight (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Out-of-vocabulary word handling deficiencies in NMT", "label": "Out-of-vocabulary word handling deficiencies in NMT (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Embed low-impact penalty into AI utility function during model design", "label": "Embed low-impact penalty into AI utility function during model design (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "BayesExp algorithm with \u03b5-schedule and effective horizon planning", "label": "BayesExp algorithm with \u03b5-schedule and effective horizon planning (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Thought experiments of paperclip maximizer illustrate catastrophic impact potential", "label": "Thought experiments of paperclip maximizer illustrate catastrophic impact potential (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "High dimensional state spaces increasing computational burden in IRL", "label": "High dimensional state spaces increasing computational burden in IRL (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "WPM-32K model achieves highest BLEU and good speed", "label": "WPM-32K model achieves highest BLEU and good speed (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "noise-induced distraction in entropy-seeking reinforcement learning agents", "label": "noise-induced distraction in entropy-seeking reinforcement learning agents (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "fine-tune RL agents with learned impact regularizer to reduce side effects", "label": "fine-tune RL agents with learned impact regularizer to reduce side effects (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "speech recognition calibration across accents proposal", "label": "speech recognition calibration across accents proposal (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "information gain utility avoids noise attraction in stochastic environments", "label": "information gain utility avoids noise attraction in stochastic environments (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Implement maximum entropy IRL during reward inference for autonomous agents", "label": "Implement maximum entropy IRL during reward inference for autonomous agents (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "gridworld experiments showing KL-KSA outperforming entropy-seeking agents", "label": "gridworld experiments showing KL-KSA outperforming entropy-seeking agents (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "posterior collapse causing premature exploitation in mixture models", "label": "posterior collapse causing premature exploitation in mixture models (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "High computational cost during inference in NMT deployment", "label": "High computational cost during inference in NMT deployment (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Combine ML pre-training with small-weight RL fine-tuning", "label": "Combine ML pre-training with small-weight RL fine-tuning (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "generative adversarial reward function training during RL", "label": "generative adversarial reward function training during RL (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "insufficient intrinsic motivation design in AIXI", "label": "insufficient intrinsic motivation design in AIXI (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Fine-tune NMT models with RL using expected GLEU objective", "label": "Fine-tune NMT models with RL using expected GLEU objective (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Impact definition via coarse-grained variable divergence between worlds", "label": "Impact definition via coarse-grained variable divergence between worlds (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "learn reward predictor from sparse labels and unlabeled episodes", "label": "learn reward predictor from sparse labels and unlabeled episodes (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "experiments showing Dirichlet model improves AIXI exploration", "label": "experiments showing Dirichlet model improves AIXI exploration (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Deploy AI within sealed computational box isolated from external observations", "label": "Deploy AI within sealed computational box isolated from external observations (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "toy obstacle-course side-effect experiment proposal", "label": "toy obstacle-course side-effect experiment proposal (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "finite-horizon approximation acceptable for planning in MCTS", "label": "finite-horizon approximation acceptable for planning in MCTS (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Gradual tuning of \u03bc with human oversight", "label": "Gradual tuning of \u03bc with human oversight (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Inevitable physical perturbations complicate impact measurement", "label": "Inevitable physical perturbations complicate impact measurement (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "KL-KSA agent utilizing information gain utility function", "label": "KL-KSA agent utilizing information gain utility function (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "BLEU improvement of ~1 on WMT En\u2192Fr after RL refinement", "label": "BLEU improvement of ~1 on WMT En\u2192Fr after RL refinement (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "High sample complexity requirements in IRL", "label": "High sample complexity requirements in IRL (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "lack of asymptotic optimality in AIXI agent model", "label": "lack of asymptotic optimality in AIXI agent model (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "train agents with KL-KSA intrinsic motivation to enhance exploration without noise attraction", "label": "train agents with KL-KSA intrinsic motivation to enhance exploration without noise attraction (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Lower sample complexity in active BIRL simulated domains", "label": "Lower sample complexity in active BIRL simulated domains (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Automatic feature construction via regression boosting reduces manual bias in IRL", "label": "Automatic feature construction via regression boosting reduces manual bias in IRL (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "reward hacking in autonomous ML systems", "label": "reward hacking in autonomous ML systems (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "partial observation and exploitable reward implementation", "label": "partial observation and exploitable reward implementation (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "penalize agent-induced state deviation from baseline", "label": "penalize agent-induced state deviation from baseline (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Residual connections to improve gradient flow in deep NMT stacks", "label": "Residual connections to improve gradient flow in deep NMT stacks (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 106, "id": "Random message generator for control output channel", "label": "Random message generator for control output channel (C106)", "shape": "dot"}, {"font": {"color": "white"}, "group": 167, "id": "Entropy-driven state query strategy for expert demonstrations", "label": "Entropy-driven state query strategy for expert demonstrations (C167)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "accidents in autonomous ML systems", "label": "accidents in autonomous ML systems (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "apply bounded safe exploration with reachable set constraints during RL training", "label": "apply bounded safe exploration with reachable set constraints during RL training (C62)", "shape": "dot"}, {"font": {"color": "white"}, "group": 67, "id": "Gradient vanishing in deep stacked LSTMs", "label": "Gradient vanishing in deep stacked LSTMs (C67)", "shape": "dot"}, {"font": {"color": "white"}, "group": 104, "id": "fine-tune RL agents using BayesExp exploration bursts to achieve asymptotic optimality", "label": "fine-tune RL agents using BayesExp exploration bursts to achieve asymptotic optimality (C104)", "shape": "dot"}, {"font": {"color": "white"}, "group": 62, "id": "integrate adversarial reward agent during RL training to prevent reward hacking", "label": "integrate adversarial reward agent during RL training to prevent reward hacking (C62)", "shape": "dot"}]);
                  edges = new vis.DataSet([{"from": "Analytical examples show coarse-grained penalty limits AI influence", "to": "l\u221e divergence penalty over coarse-grained variable probabilities", "value": "2"}, {"from": "Analytical examples show coarse-grained penalty limits AI influence", "to": "Baseline world via probabilistic signal failure preventing activation", "value": "2"}, {"from": "Analytical examples show coarse-grained penalty limits AI influence", "to": "Probability ratio threshold detectability metric on future slice", "value": "2"}, {"from": "Analytical examples show coarse-grained penalty limits AI influence", "to": "Embed low-impact penalty into AI utility function during model design", "value": "2"}, {"from": "Analytical examples show coarse-grained penalty limits AI influence", "to": "Use coarse-grained variable l\u221e divergence as penalty metric in AI decision-making", "value": "2"}, {"from": "Analytical examples show coarse-grained penalty limits AI influence", "to": "Iteratively adjust penalty coefficient \u03bc during pre-deployment testing", "value": "2"}, {"from": "unsupervised risk estimation signals out-of-distribution inputs", "to": "train-test distribution divergence undermines model calibration", "value": "3"}, {"from": "unsupervised risk estimation signals out-of-distribution inputs", "to": "use error distribution modeling to predict performance", "value": "3"}, {"from": "Suboptimal translation quality and scalability in neural machine translation production", "to": "High computational cost during inference in NMT deployment", "value": "3"}, {"from": "Suboptimal translation quality and scalability in neural machine translation production", "to": "Objective-function misalignment with BLEU metric in NMT training", "value": "3"}, {"from": "Suboptimal translation quality and scalability in neural machine translation production", "to": "Out-of-vocabulary word handling deficiencies in NMT", "value": "4"}, {"from": "Suboptimal translation quality and scalability in neural machine translation production", "to": "Length bias in beam search decoding", "value": "3"}, {"from": "Suboptimal translation quality and scalability in neural machine translation production", "to": "Gradient vanishing in deep stacked LSTMs", "value": "3"}, {"from": "Thompson sampling with effective horizon Monte Carlo tree search", "to": "hypothesis commitment through posterior sampling", "value": "4"}, {"from": "Thompson sampling with effective horizon Monte Carlo tree search", "to": "experiments showing Thompson sampling underperforms AI\u03be under equal sample budget", "value": "4"}, {"from": "use error distribution modeling to predict performance", "to": "conditional independence-based unsupervised risk estimator", "value": "3"}, {"from": "Deploy quantised GNMT models on TPUs with 8-bit weights", "to": "BLEU unchanged and 3.4\u00d7 speedup on TPU with quantised model", "value": "4"}, {"from": "adopt Thompson sampling for policy selection with adequate planning horizon adjustments", "to": "experiments showing Thompson sampling underperforms AI\u03be under equal sample budget", "value": "3"}, {"from": "Box AI to minimize observable activation differences", "to": "Impact definition via informational importance of activation event", "value": "3"}, {"from": "Box AI to minimize observable activation differences", "to": "Probability ratio threshold detectability metric on future slice", "value": "2"}, {"from": "Condition impact penalty on externally defined success announcement", "to": "Impact definition via informational importance of activation event", "value": "3"}, {"from": "Condition impact penalty on externally defined success announcement", "to": "Conditioning penalty on success announcement event A", "value": "3"}, {"from": "Regression tree-based feature generation algorithm (FIRL)", "to": "Automatically learn nonlinear reward features with regression trees", "value": "4"}, {"from": "Regression tree-based feature generation algorithm (FIRL)", "to": "Reduced imitation loss in path planning via automatic feature boosting", "value": "3"}, {"from": "Solicit informative demonstrations using active Bayesian IRL", "to": "Active learning querying expert at high-entropy states reduces sample complexity in IRL", "value": "4"}, {"from": "Solicit informative demonstrations using active Bayesian IRL", "to": "Entropy-driven state query strategy for expert demonstrations", "value": "4"}, {"from": "Reduced imitation loss in path planning via automatic feature boosting", "to": "Integrate regression tree feature construction into reward modeling pipeline", "value": "3"}, {"from": "BLEU unchanged and 3.4\u00d7 speedup on TPU with quantised model", "to": "Integer-quantised GNMT inference on TPUs with 8-bit weights and 16-bit accumulators", "value": "4"}, {"from": "Constrain accumulators and logits to enable post-training 8/16-bit quantisation", "to": "Quantization preserving accuracy while reducing computation in NMT", "value": "3"}, {"from": "Constrain accumulators and logits to enable post-training 8/16-bit quantisation", "to": "Integer-quantised GNMT inference on TPUs with 8-bit weights and 16-bit accumulators", "value": "4"}, {"from": "Active learning querying expert at high-entropy states reduces sample complexity in IRL", "to": "High sample complexity requirements in IRL", "value": "3"}, {"from": "train-test distribution divergence undermines model calibration", "to": "distributional shift failures in ML systems", "value": "4"}, {"from": "Collect demonstrations via entropy-based active learning queries", "to": "Lower sample complexity in active BIRL simulated domains", "value": "3"}, {"from": "limited access to high-fidelity human evaluation", "to": "scalable oversight failure in autonomous ML systems", "value": "4"}, {"from": "limited access to high-fidelity human evaluation", "to": "semi-supervised reinforcement learning with active reward queries improves efficiency", "value": "3"}, {"from": "Use wordpiece segmentation (32k shared vocab) during NMT training and inference", "to": "WPM-32K model achieves highest BLEU and good speed", "value": "5"}, {"from": "risk-sensitive criteria and safe region constraints reduce catastrophic exploration", "to": "random or optimistic exploration disregards catastrophic outcomes", "value": "3"}, {"from": "risk-sensitive criteria and safe region constraints reduce catastrophic exploration", "to": "constrain actions to states with recoverability guarantee", "value": "3"}, {"from": "experiments showing Thompson sampling underperforms AI\u03be under equal sample budget", "to": "\u03c1UCT Monte Carlo tree search planner with finite horizon", "value": "3"}, {"from": "experiments showing Thompson sampling underperforms AI\u03be under equal sample budget", "to": "adopt Monte Carlo tree search with finite horizon approximation for POMDP planning in RL agents", "value": "3"}, {"from": "conditional independence-based unsupervised risk estimator", "to": "speech recognition calibration across accents proposal", "value": "2"}, {"from": "Use maximum entropy optimization to resolve reward ambiguity in IRL", "to": "Maximum entropy principle yields least-biased reward distribution in IRL", "value": "4"}, {"from": "Use maximum entropy optimization to resolve reward ambiguity in IRL", "to": "Convex entropy-constrained optimization of reward parameters", "value": "4"}, {"from": "Integrate low-impact penalty R into utility function with scaling factor \u03bc", "to": "Penalizing AI for deviations from baseline world reduces impact", "value": "3"}, {"from": "Integrate low-impact penalty R into utility function with scaling factor \u03bc", "to": "Impact definition via coarse-grained variable divergence between worlds", "value": "3"}, {"from": "Integrate low-impact penalty R into utility function with scaling factor \u03bc", "to": "l\u221e divergence penalty over coarse-grained variable probabilities", "value": "3"}, {"from": "Integrate low-impact penalty R into utility function with scaling factor \u03bc", "to": "Baseline world via probabilistic signal failure preventing activation", "value": "3"}, {"from": "Integrate low-impact penalty R into utility function with scaling factor \u03bc", "to": "Gradual tuning of \u03bc with human oversight", "value": "2"}, {"from": "employ semi-supervised RL with active human queries during fine-tuning", "to": "Atari limited-reward sampling experiment proposal", "value": "2"}, {"from": "negative side effects in autonomous ML systems", "to": "accidents in autonomous ML systems", "value": "3"}, {"from": "negative side effects in autonomous ML systems", "to": "objective function indifference to unmodelled environment variables", "value": "4"}, {"from": "Apply bisimulation-based state aggregation to cut sample complexity", "to": "State abstraction via bisimulation metrics enhances generalization in IRL", "value": "4"}, {"from": "Apply bisimulation-based state aggregation to cut sample complexity", "to": "Kernel regression over bisimulation metric for policy generalization", "value": "4"}, {"from": "Maximum entropy principle yields least-biased reward distribution in IRL", "to": "Ill-posed reward ambiguity from limited demonstrations in IRL", "value": "3"}, {"from": "Maintain reward uncertainty through Bayesian IRL framework", "to": "Bayesian posterior over reward functions captures uncertainty in IRL", "value": "4"}, {"from": "Maintain reward uncertainty through Bayesian IRL framework", "to": "MCMC sampling of reward posterior distribution", "value": "4"}, {"from": "objective function indifference to unmodelled environment variables", "to": "impact regularization can model dis-preference for environmental change", "value": "3"}, {"from": "Operate multiple conditional low impact AIs to split critical task parameters", "to": "Scenario of two AI coordinate split demonstrates high impact with low impact constraints", "value": "2"}, {"from": "Automatically learn nonlinear reward features with regression trees", "to": "Automatic feature construction via regression boosting reduces manual bias in IRL", "value": "4"}, {"from": "Expected reward objective with GLEU and \u03b1=0.017 weight", "to": "Combine ML pre-training with small-weight RL fine-tuning", "value": "3"}, {"from": "Expected reward objective with GLEU and \u03b1=0.017 weight", "to": "BLEU improvement of ~1 on WMT En\u2192Fr after RL refinement", "value": "4"}, {"from": "Sentence-level expected reward optimisation aligns training with BLEU", "to": "Objective-function misalignment with BLEU metric in NMT training", "value": "4"}, {"from": "Sentence-level expected reward optimisation aligns training with BLEU", "to": "Combine ML pre-training with small-weight RL fine-tuning", "value": "3"}, {"from": "Quantization preserving accuracy while reducing computation in NMT", "to": "High computational cost during inference in NMT deployment", "value": "4"}, {"from": "Conditioning penalty on success announcement event A", "to": "Stock market probability pump example demonstrates conditioning on success", "value": "2"}, {"from": "weak asymptotic optimality via exploration schedules in Bayesian RL", "to": "insufficient intrinsic motivation design in AIXI", "value": "3"}, {"from": "weak asymptotic optimality via exploration schedules in Bayesian RL", "to": "information-gain-triggered exploration bursts in Bayes agents", "value": "3"}, {"from": "weak asymptotic optimality via exploration schedules in Bayesian RL", "to": "hypothesis commitment through posterior sampling", "value": "3"}, {"from": "Greedy likelihood-maximising wordpiece model training", "to": "Shared 32k wordpiece vocabulary for source and target", "value": "4"}, {"from": "Greedy likelihood-maximising wordpiece model training", "to": "WPM-32K model achieves highest BLEU and good speed", "value": "4"}, {"from": "Apply length-normalised, coverage-penalised beam search decoding", "to": "Length/coverage penalties give +1.1 BLEU on dev set", "value": "4"}, {"from": "Probability ratio threshold detectability metric on future slice", "to": "Deploy AI within sealed computational box isolated from external observations", "value": "2"}, {"from": "delusion box environment demonstration proposal", "to": "generative adversarial reward function training during RL", "value": "2"}, {"from": "delusion box environment demonstration proposal", "to": "integrate adversarial reward agent during RL training to prevent reward hacking", "value": "2"}, {"from": "Unsafe output channel illustration emphasises isolated message risk", "to": "Random message generator for control output channel", "value": "2"}, {"from": "Unsafe output channel illustration emphasises isolated message risk", "to": "Provide unsafe output channel with random fallback and conditioned impact", "value": "2"}, {"from": "information-gain-triggered exploration bursts in Bayes agents", "to": "BayesExp algorithm with \u03b5-schedule and effective horizon planning", "value": "4"}, {"from": "Deploy bisimulation-based state abstraction for efficient IRL training", "to": "Improved generalization with fewer demonstrations using bisimulation IRL", "value": "3"}, {"from": "intrinsic information gain utility driving exploration", "to": "information gain utility avoids noise attraction in stochastic environments", "value": "3"}, {"from": "intrinsic information gain utility driving exploration", "to": "KL-KSA agent utilizing information gain utility function", "value": "4"}, {"from": "Scenario of two AI coordinate split demonstrates high impact with low impact constraints", "to": "Independence gating of separate AIs based on assumption of other inactive", "value": "2"}, {"from": "Provide unsafe output channel with random fallback and conditioned impact", "to": "Stock market probability pump example demonstrates conditioning on success", "value": "2"}, {"from": "Incomplete model knowledge affecting reward inference in IRL", "to": "Incorrect reward inference in inverse reinforcement learning applications", "value": "4"}, {"from": "Incomplete model knowledge affecting reward inference in IRL", "to": "Automatic feature construction via regression boosting reduces manual bias in IRL", "value": "3"}, {"from": "Kernel regression over bisimulation metric for policy generalization", "to": "Improved generalization with fewer demonstrations using bisimulation IRL", "value": "3"}, {"from": "Robust performance under noisy demonstrations in helicopter control tasks using MaxEnt IRL", "to": "Convex entropy-constrained optimization of reward parameters", "value": "3"}, {"from": "Robust performance under noisy demonstrations in helicopter control tasks using MaxEnt IRL", "to": "Implement maximum entropy IRL during reward inference for autonomous agents", "value": "3"}, {"from": "Unbounded utility optimization magnifies goals with increasing AI capability", "to": "Unintended high-impact side effects from goal-oriented AI systems", "value": "3"}, {"from": "Unbounded utility optimization magnifies goals with increasing AI capability", "to": "Penalizing AI for deviations from baseline world reduces impact", "value": "3"}, {"from": "Monte Carlo tree search planning with mixture models for POMDPs", "to": "finite-horizon approximation acceptable for planning in MCTS", "value": "3"}, {"from": "Monte Carlo tree search planning with mixture models for POMDPs", "to": "\u03c1UCT Monte Carlo tree search planner with finite horizon", "value": "4"}, {"from": "Use multiple conditional low impact agents for joint tasks", "to": "Penalizing AI for deviations from baseline world reduces impact", "value": "3"}, {"from": "Use multiple conditional low impact agents for joint tasks", "to": "Independence gating of separate AIs based on assumption of other inactive", "value": "3"}, {"from": "distributional shift failures in ML systems", "to": "accidents in autonomous ML systems", "value": "3"}, {"from": "State abstraction via bisimulation metrics enhances generalization in IRL", "to": "High dimensional state spaces increasing computational burden in IRL", "value": "3"}, {"from": "safe policy update using reachability analysis", "to": "constrain actions to states with recoverability guarantee", "value": "3"}, {"from": "safe policy update using reachability analysis", "to": "simulated quadcopter altitude-bound experiment proposal", "value": "2"}, {"from": "deploy unsupervised risk estimator to trigger conservative policy on novel inputs at deployment", "to": "speech recognition calibration across accents proposal", "value": "2"}, {"from": "Incorrect reward inference in inverse reinforcement learning applications", "to": "Ill-posed reward ambiguity from limited demonstrations in IRL", "value": "4"}, {"from": "Incorrect reward inference in inverse reinforcement learning applications", "to": "Noisy or suboptimal demonstrations impacting reward accuracy in IRL", "value": "4"}, {"from": "Incorrect reward inference in inverse reinforcement learning applications", "to": "High dimensional state spaces increasing computational burden in IRL", "value": "3"}, {"from": "Incorrect reward inference in inverse reinforcement learning applications", "to": "High sample complexity requirements in IRL", "value": "3"}, {"from": "Dirichlet gridworld model with factorized tile priors", "to": "rich model class maintains posterior entropy for sustained exploration", "value": "4"}, {"from": "Dirichlet gridworld model with factorized tile priors", "to": "experiments showing Dirichlet model improves AIXI exploration", "value": "4"}, {"from": "train reward network adversarially against agent policy", "to": "adversarial reward modeling detects exploitation", "value": "3"}, {"from": "train reward network adversarially against agent policy", "to": "generative adversarial reward function training during RL", "value": "3"}, {"from": "Unintended high-impact side effects from goal-oriented AI systems", "to": "Difficulty specifying exhaustive safety constraints in utility functions", "value": "3"}, {"from": "Unintended high-impact side effects from goal-oriented AI systems", "to": "Inevitable physical perturbations complicate impact measurement", "value": "2"}, {"from": "Unintended high-impact side effects from goal-oriented AI systems", "to": "Thought experiments of paperclip maximizer illustrate catastrophic impact potential", "value": "3"}, {"from": "Deep residual LSTM enables 8-layer GNMT reaching 38.95 BLEU", "to": "Element-wise residual addition between stacked LSTM layers", "value": "3"}, {"from": "Deep residual LSTM enables 8-layer GNMT reaching 38.95 BLEU", "to": "Incorporate residual connections in deep stacked LSTM encoder-decoder architecture", "value": "4"}, {"from": "Element-wise residual addition between stacked LSTM layers", "to": "Residual connections to improve gradient flow in deep NMT stacks", "value": "4"}, {"from": "entropy-seeking utility misaligned with information gain", "to": "noise-induced distraction in entropy-seeking reinforcement learning agents", "value": "3"}, {"from": "entropy-seeking utility misaligned with information gain", "to": "information gain utility avoids noise attraction in stochastic environments", "value": "3"}, {"from": "Impact definition via informational importance of activation event", "to": "Exclude output channel content from impact penalty", "value": "3"}, {"from": "suboptimal exploration in universal reinforcement learning agents", "to": "insufficient intrinsic motivation design in AIXI", "value": "3"}, {"from": "suboptimal exploration in universal reinforcement learning agents", "to": "posterior collapse causing premature exploitation in mixture models", "value": "4"}, {"from": "unsafe exploration in reinforcement learning", "to": "accidents in autonomous ML systems", "value": "3"}, {"from": "unsafe exploration in reinforcement learning", "to": "random or optimistic exploration disregards catastrophic outcomes", "value": "4"}, {"from": "Length/coverage penalties give +1.1 BLEU on dev set", "to": "Beam search decoder with pruning, batch decoding and lp/cp scoring", "value": "4"}, {"from": "computational intractability of planning in partially observable environments for URL", "to": "Monte Carlo tree search horizon limits agent foresight", "value": "3"}, {"from": "Apply Bayesian IRL with MCMC to model reward uncertainty in training", "to": "Accurate reward posterior in gridworld experiments with Bayesian IRL", "value": "3"}, {"from": "Length bias in beam search decoding", "to": "Length-normalised coverage-aware beam search improves adequacy", "value": "4"}, {"from": "impact regularization can model dis-preference for environmental change", "to": "penalize agent-induced state deviation from baseline", "value": "3"}, {"from": "Accurate reward posterior in gridworld experiments with Bayesian IRL", "to": "MCMC sampling of reward posterior distribution", "value": "3"}, {"from": "Penalizing AI for deviations from baseline world reduces impact", "to": "Difficulty specifying exhaustive safety constraints in utility functions", "value": "3"}, {"from": "Atari limited-reward sampling experiment proposal", "to": "supervised reward model with uncertainty-guided query strategy", "value": "2"}, {"from": "rich model class maintains posterior entropy for sustained exploration", "to": "Dirichlet environment model increases epistemic uncertainty for exploration", "value": "3"}, {"from": "simulated quadcopter altitude-bound experiment proposal", "to": "apply bounded safe exploration with reachable set constraints during RL training", "value": "2"}, {"from": "Bayesian posterior over reward functions captures uncertainty in IRL", "to": "Noisy or suboptimal demonstrations impacting reward accuracy in IRL", "value": "3"}, {"from": "proof of weak asymptotic optimality for BayesExp in general environments", "to": "BayesExp algorithm with \u03b5-schedule and effective horizon planning", "value": "4"}, {"from": "proof of weak asymptotic optimality for BayesExp in general environments", "to": "fine-tune RL agents using BayesExp exploration bursts to achieve asymptotic optimality", "value": "3"}, {"from": "Beam-search scoring with \u03b1=0.2 length penalty and \u03b2=0.2 coverage penalty", "to": "Length-normalised coverage-aware beam search improves adequacy", "value": "3"}, {"from": "Beam-search scoring with \u03b1=0.2 length penalty and \u03b2=0.2 coverage penalty", "to": "Beam search decoder with pruning, batch decoding and lp/cp scoring", "value": "3"}, {"from": "Exclude output channel content from impact penalty", "to": "Random message generator for control output channel", "value": "3"}, {"from": "semi-supervised reinforcement learning with active reward queries improves efficiency", "to": "learn reward predictor from sparse labels and unlabeled episodes", "value": "3"}, {"from": "supervised reward model with uncertainty-guided query strategy", "to": "learn reward predictor from sparse labels and unlabeled episodes", "value": "3"}, {"from": "adversarial reward modeling detects exploitation", "to": "partial observation and exploitable reward implementation", "value": "3"}, {"from": "design environment models with factorized Dirichlet priors to stimulate exploration during planning", "to": "experiments showing Dirichlet model improves AIXI exploration", "value": "4"}, {"from": "Dirichlet environment model increases epistemic uncertainty for exploration", "to": "posterior collapse causing premature exploitation in mixture models", "value": "3"}, {"from": "Shared 32k wordpiece vocabulary for source and target", "to": "Subword wordpiece modelling for open-vocabulary translation", "value": "4"}, {"from": "Subword wordpiece modelling for open-vocabulary translation", "to": "Out-of-vocabulary word handling deficiencies in NMT", "value": "5"}, {"from": "restrict entropy-seeking agents from high-entropy noise sources during deployment", "to": "gridworld experiments showing KL-KSA outperforming entropy-seeking agents", "value": "2"}, {"from": "state distance penalty with passive policy baseline", "to": "penalize agent-induced state deviation from baseline", "value": "3"}, {"from": "state distance penalty with passive policy baseline", "to": "toy obstacle-course side-effect experiment proposal", "value": "2"}, {"from": "scalable oversight failure in autonomous ML systems", "to": "accidents in autonomous ML systems", "value": "3"}, {"from": "Monte Carlo tree search horizon limits agent foresight", "to": "finite-horizon approximation acceptable for planning in MCTS", "value": "2"}, {"from": "Thought experiments of paperclip maximizer illustrate catastrophic impact potential", "to": "Gradual tuning of \u03bc with human oversight", "value": "2"}, {"from": "fine-tune RL agents with learned impact regularizer to reduce side effects", "to": "toy obstacle-course side-effect experiment proposal", "value": "2"}, {"from": "information gain utility avoids noise attraction in stochastic environments", "to": "insufficient intrinsic motivation design in AIXI", "value": "3"}, {"from": "gridworld experiments showing KL-KSA outperforming entropy-seeking agents", "to": "KL-KSA agent utilizing information gain utility function", "value": "4"}, {"from": "gridworld experiments showing KL-KSA outperforming entropy-seeking agents", "to": "train agents with KL-KSA intrinsic motivation to enhance exploration without noise attraction", "value": "4"}, {"from": "insufficient intrinsic motivation design in AIXI", "to": "lack of asymptotic optimality in AIXI agent model", "value": "3"}, {"from": "Fine-tune NMT models with RL using expected GLEU objective", "to": "BLEU improvement of ~1 on WMT En\u2192Fr after RL refinement", "value": "3"}, {"from": "Impact definition via coarse-grained variable divergence between worlds", "to": "Inevitable physical perturbations complicate impact measurement", "value": "3"}, {"from": "Lower sample complexity in active BIRL simulated domains", "to": "Entropy-driven state query strategy for expert demonstrations", "value": "3"}, {"from": "reward hacking in autonomous ML systems", "to": "accidents in autonomous ML systems", "value": "3"}, {"from": "reward hacking in autonomous ML systems", "to": "partial observation and exploitable reward implementation", "value": "4"}, {"from": "Residual connections to improve gradient flow in deep NMT stacks", "to": "Gradient vanishing in deep stacked LSTMs", "value": "4"}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {
    "configure": {
        "enabled": false
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": true,
            "type": "dynamic"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "enabled": true,
        "forceAtlas2Based": {
            "avoidOverlap": 0,
            "centralGravity": 0.005,
            "damping": 0.4,
            "gravitationalConstant": -50,
            "springConstant": 0.08,
            "springLength": 150
        },
        "solver": "forceAtlas2Based",
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  
                      network.on("stabilizationProgress", function(params) {
                          document.getElementById('loadingBar').removeAttribute("style");
                          var maxWidth = 496;
                          var minWidth = 20;
                          var widthFactor = params.iterations/params.total;
                          var width = Math.max(minWidth,maxWidth * widthFactor);
                          document.getElementById('bar').style.width = width + 'px';
                          document.getElementById('text').innerHTML = Math.round(widthFactor*100) + '%';
                      });
                      network.once("stabilizationIterationsDone", function() {
                          document.getElementById('text').innerHTML = '100%';
                          document.getElementById('bar').style.width = '496px';
                          document.getElementById('loadingBar').style.opacity = 0;
                          // really clean the dom element
                          setTimeout(function () {document.getElementById('loadingBar').style.display = 'none';}, 500);
                      });
                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>