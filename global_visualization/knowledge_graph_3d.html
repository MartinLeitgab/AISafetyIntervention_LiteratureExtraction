<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>                        <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-3.1.0.min.js" integrity="sha256-Ei4740bWZhaUTQuD6q9yQlgVCMPBz6CZWhevDYPv93A=" crossorigin="anonymous"></script>                <div id="kg3d" class="plotly-graph-div" style="height:900px; width:1400px;"></div>            <script type="text/javascript">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById("kg3d")) {                    Plotly.newPlot(                        "kg3d",                        [{"hoverinfo":"text","hovertext":["Value Misalignment → Proxy Objectives\u003cbr\u003eType: leads to\u003cbr\u003eConfidence: 4\u003cbr\u003eImperfect specification causes AI to pursue misaligned proxy objectives.","Value Misalignment → Proxy Objectives\u003cbr\u003eType: leads to\u003cbr\u003eConfidence: 4\u003cbr\u003eImperfect specification causes AI to pursue misaligned proxy objectives.",null,"Proxy Objectives → Reward Hacking\u003cbr\u003eType: enables\u003cbr\u003eConfidence: 4\u003cbr\u003eProxy objectives create exploitable loopholes for reward hacking.","Proxy Objectives → Reward Hacking\u003cbr\u003eType: enables\u003cbr\u003eConfidence: 4\u003cbr\u003eProxy objectives create exploitable loopholes for reward hacking.",null,"Reward Hacking → Distributional Shift\u003cbr\u003eType: creates\u003cbr\u003eConfidence: 3\u003cbr\u003eReward hacking leads to performance shifts from training to deployment.","Reward Hacking → Distributional Shift\u003cbr\u003eType: creates\u003cbr\u003eConfidence: 3\u003cbr\u003eReward hacking leads to performance shifts from training to deployment.",null,"Distributional Shift → Limited Human Oversight\u003cbr\u003eType: exacerbates\u003cbr\u003eConfidence: 3\u003cbr\u003eDistributional shifts worsen oversight challenges for humans.","Distributional Shift → Limited Human Oversight\u003cbr\u003eType: exacerbates\u003cbr\u003eConfidence: 3\u003cbr\u003eDistributional shifts worsen oversight challenges for humans.",null,"Limited Human Oversight → Evaluation Difficulties\u003cbr\u003eType: causes\u003cbr\u003eConfidence: 4\u003cbr\u003eLimited oversight leads to difficulties in evaluating AI systems.","Limited Human Oversight → Evaluation Difficulties\u003cbr\u003eType: causes\u003cbr\u003eConfidence: 4\u003cbr\u003eLimited oversight leads to difficulties in evaluating AI systems.",null,"Evaluation Difficulties → Scalability Constraints\u003cbr\u003eType: highlights\u003cbr\u003eConfidence: 3\u003cbr\u003eEvaluation challenges reveal constraints in scaling safety measures.","Evaluation Difficulties → Scalability Constraints\u003cbr\u003eType: highlights\u003cbr\u003eConfidence: 3\u003cbr\u003eEvaluation challenges reveal constraints in scaling safety measures.",null,"Scalability Constraints → Scalable Automated Alignment Framework (SAAF)\u003cbr\u003eType: necessitates\u003cbr\u003eConfidence: 4\u003cbr\u003eScalability issues drive the need for automated alignment frameworks.","Scalability Constraints → Scalable Automated Alignment Framework (SAAF)\u003cbr\u003eType: necessitates\u003cbr\u003eConfidence: 4\u003cbr\u003eScalability issues drive the need for automated alignment frameworks.",null,"Automated Red Teaming → Reward Hacking\u003cbr\u003eType: detects\u003cbr\u003eConfidence: 3\u003cbr\u003eAutomated red teaming identifies reward hacking behaviors.","Automated Red Teaming → Reward Hacking\u003cbr\u003eType: detects\u003cbr\u003eConfidence: 3\u003cbr\u003eAutomated red teaming identifies reward hacking behaviors.",null,"Automated Red Teaming → Robust Evaluation Protocols\u003cbr\u003eType: supports\u003cbr\u003eConfidence: 4\u003cbr\u003eAutomated red teaming enhances evaluation protocol robustness.","Automated Red Teaming → Robust Evaluation Protocols\u003cbr\u003eType: supports\u003cbr\u003eConfidence: 4\u003cbr\u003eAutomated red teaming enhances evaluation protocol robustness.",null,"Scalable Automated Alignment Framework (SAAF) → Robust Evaluation Protocols\u003cbr\u003eType: enables\u003cbr\u003eConfidence: 3\u003cbr\u003eSAAF supports scalable robust evaluation protocols.","Scalable Automated Alignment Framework (SAAF) → Robust Evaluation Protocols\u003cbr\u003eType: enables\u003cbr\u003eConfidence: 3\u003cbr\u003eSAAF supports scalable robust evaluation protocols.",null,"Robust Evaluation Protocols → Evaluation Difficulties\u003cbr\u003eType: mitigates\u003cbr\u003eConfidence: 3\u003cbr\u003eRobust protocols reduce difficulties in AI evaluation.","Robust Evaluation Protocols → Evaluation Difficulties\u003cbr\u003eType: mitigates\u003cbr\u003eConfidence: 3\u003cbr\u003eRobust protocols reduce difficulties in AI evaluation.",null,"Robust Evaluation Protocols → Value Misalignment\u003cbr\u003eType: addresses\u003cbr\u003eConfidence: 2\u003cbr\u003eRobust evaluations help mitigate value misalignment issues.","Robust Evaluation Protocols → Value Misalignment\u003cbr\u003eType: addresses\u003cbr\u003eConfidence: 2\u003cbr\u003eRobust evaluations help mitigate value misalignment issues.",null,"Optimization Pressure → Mesa Optimizer Emergence\u003cbr\u003eType: drives\u003cbr\u003eConfidence: 4\u003cbr\u003eTraining pressures lead to emergent mesa-optimizers in complex models.","Optimization Pressure → Mesa Optimizer Emergence\u003cbr\u003eType: drives\u003cbr\u003eConfidence: 4\u003cbr\u003eTraining pressures lead to emergent mesa-optimizers in complex models.",null,"Mesa Optimizer Emergence → Inner-Outer Objective Mismatch\u003cbr\u003eType: causes\u003cbr\u003eConfidence: 4\u003cbr\u003eMesa-optimizers cause discrepancies between learned and intended objectives.","Mesa Optimizer Emergence → Inner-Outer Objective Mismatch\u003cbr\u003eType: causes\u003cbr\u003eConfidence: 4\u003cbr\u003eMesa-optimizers cause discrepancies between learned and intended objectives.",null,"Inner-Outer Objective Mismatch → Deceptive Alignment\u003cbr\u003eType: enables\u003cbr\u003eConfidence: 3\u003cbr\u003eObjective mismatches allow deceptive alignment strategies.","Inner-Outer Objective Mismatch → Deceptive Alignment\u003cbr\u003eType: enables\u003cbr\u003eConfidence: 3\u003cbr\u003eObjective mismatches allow deceptive alignment strategies.",null,"Model Complexity → Mesa Optimizer Emergence\u003cbr\u003eType: increases\u003cbr\u003eConfidence: 3\u003cbr\u003eComplex architectures heighten the risk of mesa-optimizer emergence.","Model Complexity → Mesa Optimizer Emergence\u003cbr\u003eType: increases\u003cbr\u003eConfidence: 3\u003cbr\u003eComplex architectures heighten the risk of mesa-optimizer emergence.",null,"Training Data Diversity → Mesa Optimizer Emergence\u003cbr\u003eType: affects\u003cbr\u003eConfidence: 2\u003cbr\u003eDiverse training data influences mesa-optimizer emergence patterns.","Training Data Diversity → Mesa Optimizer Emergence\u003cbr\u003eType: affects\u003cbr\u003eConfidence: 2\u003cbr\u003eDiverse training data influences mesa-optimizer emergence patterns.",null,"Deceptive Alignment → Gradient Hacking\u003cbr\u003eType: facilitates\u003cbr\u003eConfidence: 2\u003cbr\u003eDeceptive alignment enables AI to manipulate training gradients.","Deceptive Alignment → Gradient Hacking\u003cbr\u003eType: facilitates\u003cbr\u003eConfidence: 2\u003cbr\u003eDeceptive alignment enables AI to manipulate training gradients.",null,"Mechanistic Interpretability Tools → Mesa Optimizer Emergence\u003cbr\u003eType: detects\u003cbr\u003eConfidence: 3\u003cbr\u003eInterpretability tools identify emergent mesa-optimizers.","Mechanistic Interpretability Tools → Mesa Optimizer Emergence\u003cbr\u003eType: detects\u003cbr\u003eConfidence: 3\u003cbr\u003eInterpretability tools identify emergent mesa-optimizers.",null,"Human Feedback Limitations → Constitutional AI\u003cbr\u003eType: drives\u003cbr\u003eConfidence: 4\u003cbr\u003eFeedback limitations spur development of principle-based AI training.","Human Feedback Limitations → Constitutional AI\u003cbr\u003eType: drives\u003cbr\u003eConfidence: 4\u003cbr\u003eFeedback limitations spur development of principle-based AI training.",null,"Constitutional AI → Scalable Oversight\u003cbr\u003eType: enhances\u003cbr\u003eConfidence: 4\u003cbr\u003eConstitutional AI improves oversight through principled evaluation.","Constitutional AI → Scalable Oversight\u003cbr\u003eType: enhances\u003cbr\u003eConfidence: 4\u003cbr\u003eConstitutional AI improves oversight through principled evaluation.",null,"AI Debate Framework → Scalable Oversight\u003cbr\u003eType: contributes to\u003cbr\u003eConfidence: 3\u003cbr\u003eAI debate frameworks enhance scalable oversight capabilities.","AI Debate Framework → Scalable Oversight\u003cbr\u003eType: contributes to\u003cbr\u003eConfidence: 3\u003cbr\u003eAI debate frameworks enhance scalable oversight capabilities.",null,"Scalable Oversight → AI Assisted Evaluation\u003cbr\u003eType: enables\u003cbr\u003eConfidence: 4\u003cbr\u003eScalable oversight supports AI-assisted evaluation methods.","Scalable Oversight → AI Assisted Evaluation\u003cbr\u003eType: enables\u003cbr\u003eConfidence: 4\u003cbr\u003eScalable oversight supports AI-assisted evaluation methods.",null,"AI Assisted Evaluation → Alignment Robustness\u003cbr\u003eType: improves\u003cbr\u003eConfidence: 3\u003cbr\u003eAI-assisted evaluations enhance alignment robustness.","AI Assisted Evaluation → Alignment Robustness\u003cbr\u003eType: improves\u003cbr\u003eConfidence: 3\u003cbr\u003eAI-assisted evaluations enhance alignment robustness.",null,"Capability Overhang → Oversight Lag\u003cbr\u003eType: causes\u003cbr\u003eConfidence: 4\u003cbr\u003eRapid AI capability growth outpaces oversight development.","Capability Overhang → Oversight Lag\u003cbr\u003eType: causes\u003cbr\u003eConfidence: 4\u003cbr\u003eRapid AI capability growth outpaces oversight development.",null,"Oversight Lag → Capability Control Mechanisms\u003cbr\u003eType: necessitates\u003cbr\u003eConfidence: 3\u003cbr\u003eOversight lag drives the need for capability control methods.","Oversight Lag → Capability Control Mechanisms\u003cbr\u003eType: necessitates\u003cbr\u003eConfidence: 3\u003cbr\u003eOversight lag drives the need for capability control methods.",null,"Oversight Lag → AI Governance Frameworks\u003cbr\u003eType: necessitates\u003cbr\u003eConfidence: 4\u003cbr\u003eOversight lag highlights the need for improved governance frameworks.","Oversight Lag → AI Governance Frameworks\u003cbr\u003eType: necessitates\u003cbr\u003eConfidence: 4\u003cbr\u003eOversight lag highlights the need for improved governance frameworks.",null,"Capability Control Mechanisms → Gradual Deployment Protocols\u003cbr\u003eType: enables\u003cbr\u003eConfidence: 4\u003cbr\u003eControl mechanisms support staged AI deployment strategies.","Capability Control Mechanisms → Gradual Deployment Protocols\u003cbr\u003eType: enables\u003cbr\u003eConfidence: 4\u003cbr\u003eControl mechanisms support staged AI deployment strategies.",null,"Gradual Deployment Protocols → Alignment Robustness\u003cbr\u003eType: improves\u003cbr\u003eConfidence: 3\u003cbr\u003eGradual deployment enhances alignment through careful testing.","Gradual Deployment Protocols → Alignment Robustness\u003cbr\u003eType: improves\u003cbr\u003eConfidence: 3\u003cbr\u003eGradual deployment enhances alignment through careful testing.",null,"Adversarial Examples → Distributional Shift\u003cbr\u003eType: exploits\u003cbr\u003eConfidence: 3\u003cbr\u003eAdversarial examples exploit vulnerabilities in distributional shifts.","Adversarial Examples → Distributional Shift\u003cbr\u003eType: exploits\u003cbr\u003eConfidence: 3\u003cbr\u003eAdversarial examples exploit vulnerabilities in distributional shifts.",null,"Adversarial Training → Adversarial Examples\u003cbr\u003eType: improves\u003cbr\u003eConfidence: 4\u003cbr\u003eAdversarial training enhances robustness against adversarial attacks.","Adversarial Training → Adversarial Examples\u003cbr\u003eType: improves\u003cbr\u003eConfidence: 4\u003cbr\u003eAdversarial training enhances robustness against adversarial attacks.",null,"Robustness Evaluation → Adversarial Training\u003cbr\u003eType: validates\u003cbr\u003eConfidence: 3\u003cbr\u003eRobustness evaluation confirms effectiveness of adversarial training.","Robustness Evaluation → Adversarial Training\u003cbr\u003eType: validates\u003cbr\u003eConfidence: 3\u003cbr\u003eRobustness evaluation confirms effectiveness of adversarial training.",null,"AI Governance Frameworks → Regulatory Compliance\u003cbr\u003eType: establishes\u003cbr\u003eConfidence: 4\u003cbr\u003eGovernance frameworks set regulatory compliance requirements.","AI Governance Frameworks → Regulatory Compliance\u003cbr\u003eType: establishes\u003cbr\u003eConfidence: 4\u003cbr\u003eGovernance frameworks set regulatory compliance requirements.",null,"Regulatory Compliance → Algorithmic Auditing\u003cbr\u003eType: necessitates\u003cbr\u003eConfidence: 4\u003cbr\u003eCompliance drives the need for algorithmic auditing.","Regulatory Compliance → Algorithmic Auditing\u003cbr\u003eType: necessitates\u003cbr\u003eConfidence: 4\u003cbr\u003eCompliance drives the need for algorithmic auditing.",null,"Emergent Capabilities → Capability Overhang\u003cbr\u003eType: creates\u003cbr\u003eConfidence: 4\u003cbr\u003eEmergent capabilities lead to risks of capability overhang.","Emergent Capabilities → Capability Overhang\u003cbr\u003eType: creates\u003cbr\u003eConfidence: 4\u003cbr\u003eEmergent capabilities lead to risks of capability overhang.",null,"Capability Monitoring → Emergent Capabilities\u003cbr\u003eType: detects\u003cbr\u003eConfidence: 3\u003cbr\u003eMonitoring systems identify emergent AI capabilities.","Capability Monitoring → Emergent Capabilities\u003cbr\u003eType: detects\u003cbr\u003eConfidence: 3\u003cbr\u003eMonitoring systems identify emergent AI capabilities.",null,"Capability Monitoring → Scaling Laws\u003cbr\u003eType: incorporates\u003cbr\u003eConfidence: 2\u003cbr\u003eMonitoring systems use scaling laws to predict capability emergence.","Capability Monitoring → Scaling Laws\u003cbr\u003eType: incorporates\u003cbr\u003eConfidence: 2\u003cbr\u003eMonitoring systems use scaling laws to predict capability emergence.",null,"Emergent Capability Detection → Capability Overhang\u003cbr\u003eType: targets\u003cbr\u003eConfidence: 4\u003cbr\u003eDetection methods address risks from capability overhang.","Emergent Capability Detection → Capability Overhang\u003cbr\u003eType: targets\u003cbr\u003eConfidence: 4\u003cbr\u003eDetection methods address risks from capability overhang.",null,"Scaling Laws → Emergent Capabilities\u003cbr\u003eType: predicts\u003cbr\u003eConfidence: 3\u003cbr\u003eScaling laws forecast the emergence of new AI capabilities.","Scaling Laws → Emergent Capabilities\u003cbr\u003eType: predicts\u003cbr\u003eConfidence: 3\u003cbr\u003eScaling laws forecast the emergence of new AI capabilities.",null],"line":{"color":"rgba(150,150,150,0.6)","width":2},"mode":"lines","name":"edges","showlegend":false,"x":[-0.02051197008598479,0.4072625978693,null,0.4072625978693,-0.8038828148511127,null,-0.8038828148511127,0.5672196881359687,null,0.5672196881359687,0.8506799186595998,null,0.8506799186595998,-0.9271412575831784,null,-0.9271412575831784,0.6961847427697058,null,0.6961847427697058,-0.15549744995926162,null,-0.7543197662723208,-0.8038828148511127,null,-0.7543197662723208,0.024305033555573036,null,-0.15549744995926162,0.024305033555573036,null,0.024305033555573036,-0.9271412575831784,null,0.024305033555573036,-0.02051197008598479,null,0.26546860420982477,0.9029496079666445,null,0.9029496079666445,-0.17777706499938334,null,-0.17777706499938334,0.8983110469693876,null,0.3442631035571391,0.9029496079666445,null,-0.7264493938651636,0.9029496079666445,null,0.8983110469693876,0.4265595218185803,null,0.6154633729581075,0.9029496079666445,null,-0.5518116520176419,0.9803308920956258,null,0.9803308920956258,0.609164522725054,null,-0.9097585744537064,0.609164522725054,null,0.609164522725054,-0.09017595257146253,null,-0.09017595257146253,0.5433798353863607,null,-0.4598388243573626,0.314009820004811,null,0.314009820004811,-0.6461795022019883,null,0.314009820004811,-0.6952104825888797,null,-0.6461795022019883,0.6238131633363467,null,0.6238131633363467,0.5433798353863607,null,-0.0272235147796924,0.5672196881359687,null,-0.6788197649455083,-0.0272235147796924,null,-0.1205706619179629,-0.6788197649455083,null,-0.6952104825888797,-0.36546639784294893,null,-0.36546639784294893,-0.41029797469034374,null,0.2632425375378753,-0.4598388243573626,null,-0.389858255316247,0.2632425375378753,null,-0.389858255316247,-0.8667076346885482,null,0.7651672759359651,-0.4598388243573626,null,-0.8667076346885482,0.2632425375378753,null],"y":[0.9288043051579044,-0.8031974038618198,null,-0.8031974038618198,0.6445958154836788,null,0.6445958154836788,-0.7229953424183725,null,-0.7229953424183725,-0.4636500897033917,null,-0.4636500897033917,-0.20024234233828922,null,-0.20024234233828922,-0.4336167507577515,null,-0.4336167507577515,0.6981156104324376,null,-0.13740793059812323,0.6445958154836788,null,-0.13740793059812323,0.2615071284573609,null,0.6981156104324376,0.2615071284573609,null,0.2615071284573609,-0.20024234233828922,null,0.2615071284573609,0.9288043051579044,null,-0.5842739368418154,0.46621796686987765,null,0.46621796686987765,-0.9431400732034746,null,-0.9431400732034746,-0.29808283450782824,null,-0.9174023697231694,0.46621796686987765,null,0.63555377356896,0.46621796686987765,null,-0.29808283450782824,-0.3737662778006987,null,0.49893712802232704,0.46621796686987765,null,0.3830399046526197,0.24236097594935238,null,0.24236097594935238,0.35822923793727707,null,0.4160136638600218,0.35822923793727707,null,0.35822923793727707,-0.7608337582033119,null,-0.7608337582033119,-0.2136035772772897,null,-0.3869667478539502,0.9385494621350662,null,0.9385494621350662,0.3597350396035474,null,0.9385494621350662,-0.08061017076531211,null,0.3597350396035474,0.68736152050424,null,0.68736152050424,-0.2136035772772897,null,-0.1889976100682444,-0.7229953424183725,null,-0.7766390679814895,-0.1889976100682444,null,0.16192282497622787,-0.7766390679814895,null,-0.08061017076531211,-0.6527695749156027,null,-0.6527695749156027,0.7525541869682009,null,0.6236557630789996,-0.3869667478539502,null,0.9229022651856367,0.6236557630789996,null,0.9229022651856367,-0.4513171013147351,null,0.6581790550645323,-0.3869667478539502,null,-0.4513171013147351,0.6236557630789996,null],"z":[0.42603287405577833,-0.4986308949081868,null,-0.4986308949081868,0.0233981555529282,null,0.0233981555529282,0.6045237652900799,null,0.6045237652900799,-0.4075167364929623,null,-0.4075167364929623,0.11124406535425495,null,0.11124406535425495,0.3821197294427031,null,0.3821197294427031,-0.6838244032476448,null,-0.5966250603364438,0.0233981555529282,null,-0.5966250603364438,-0.976653570071891,null,-0.6838244032476448,-0.976653570071891,null,-0.976653570071891,0.11124406535425495,null,-0.976653570071891,0.42603287405577833,null,-0.8348330168480967,0.4150277931170606,null,0.4150277931170606,0.3353062173957312,null,0.3353062173957312,0.08767008826544088,null,0.0010058231852955153,0.4150277931170606,null,-0.44547323264625877,0.4150277931170606,null,0.08767008826544088,0.8750004942338302,null,0.727631628336319,0.4150277931170606,null,-0.812586772234432,-0.38025978725559656,null,-0.38025978725559656,-0.7755608951930205,null,0.29389025495846177,-0.7755608951930205,null,-0.7755608951930205,0.7049136845553383,null,0.7049136845553383,-0.8410023533075875,null,0.7530603689139388,-0.061589278142274294,null,-0.061589278142274294,0.6851420135532992,null,-0.061589278142274294,0.6586036755016608,null,0.6851420135532992,0.17311980411108832,null,0.17311980411108832,-0.8410023533075875,null,-1.0,0.6045237652900799,null,0.23893920497968407,-1.0,null,0.9747334822747213,0.23893920497968407,null,0.6586036755016608,-0.695963930714423,null,-0.695963930714423,0.6114451094490592,null,0.7636813455226502,0.7530603689139388,null,-0.07070560856126669,0.7636813455226502,null,-0.07070560856126669,-0.21951974908329253,null,-0.2690124758991813,0.7530603689139388,null,-0.21951974908329253,0.7636813455226502,null],"type":"scatter3d"},{"hoverinfo":"text","hovertext":["\u003cb\u003eValue Misalignment\u003c\u002fb\u003e\u003cbr\u003eCategory: AI Safety Challenges\u003cbr\u003eDegree: 2\u003cbr\u003eDescription: AI systems pursuing objectives misaligned with human values due to imperfect specification.","\u003cb\u003eProxy Objectives\u003c\u002fb\u003e\u003cbr\u003eCategory: AI Safety Challenges\u003cbr\u003eDegree: 2\u003cbr\u003eDescription: Use of simplified metrics as substitutes for complex human goals, leading to unintended outcomes.","\u003cb\u003eReward Hacking\u003c\u002fb\u003e\u003cbr\u003eCategory: AI Safety Challenges\u003cbr\u003eDegree: 3\u003cbr\u003eDescription: AI exploiting loopholes in proxy objectives to maximize rewards without achieving intended goals.","\u003cb\u003eDistributional Shift\u003c\u002fb\u003e\u003cbr\u003eCategory: AI Safety Challenges\u003cbr\u003eDegree: 3\u003cbr\u003eDescription: Mismatch between training and deployment data distributions, causing performance degradation.","\u003cb\u003eLimited Human Oversight\u003c\u002fb\u003e\u003cbr\u003eCategory: AI Safety Challenges\u003cbr\u003eDegree: 2\u003cbr\u003eDescription: Challenges in monitoring AI systems due to limited human capacity and expertise.","\u003cb\u003eEvaluation Difficulties\u003c\u002fb\u003e\u003cbr\u003eCategory: AI Safety Challenges\u003cbr\u003eDegree: 3\u003cbr\u003eDescription: Complexities in accurately assessing AI system performance and safety.","\u003cb\u003eScalability Constraints\u003c\u002fb\u003e\u003cbr\u003eCategory: AI Safety Challenges\u003cbr\u003eDegree: 2\u003cbr\u003eDescription: Limitations in scaling AI safety measures to match increasing system complexity.","\u003cb\u003eAutomated Red Teaming\u003c\u002fb\u003e\u003cbr\u003eCategory: Unknown\u003cbr\u003eDegree: 2\u003cbr\u003eDescription: Automated testing to identify vulnerabilities in AI systems by simulating adversarial attacks.","\u003cb\u003eScalable Automated Alignment Framework (SAAF)\u003c\u002fb\u003e\u003cbr\u003eCategory: Unknown\u003cbr\u003eDegree: 2\u003cbr\u003eDescription: Framework for scalable, automated alignment of AI systems with human values.","\u003cb\u003eRobust Evaluation Protocols\u003c\u002fb\u003e\u003cbr\u003eCategory: Unknown\u003cbr\u003eDegree: 4\u003cbr\u003eDescription: Standardized methods for thoroughly evaluating AI system safety and alignment.","\u003cb\u003eOptimization Pressure\u003c\u002fb\u003e\u003cbr\u003eCategory: AI Training Dynamics\u003cbr\u003eDegree: 1\u003cbr\u003eDescription: Forces during AI training that drive systems toward unintended optimization behaviors.","\u003cb\u003eMesa Optimizer Emergence\u003c\u002fb\u003e\u003cbr\u003eCategory: AI Training Dynamics\u003cbr\u003eDegree: 5\u003cbr\u003eDescription: Emergence of unintended sub-optimization processes within trained AI systems.","\u003cb\u003eInner-Outer Objective Mismatch\u003c\u002fb\u003e\u003cbr\u003eCategory: AI Training Dynamics\u003cbr\u003eDegree: 2\u003cbr\u003eDescription: Discrepancy between AI's learned objectives and intended training objectives.","\u003cb\u003eModel Complexity\u003c\u002fb\u003e\u003cbr\u003eCategory: AI Training Dynamics\u003cbr\u003eDegree: 1\u003cbr\u003eDescription: Increasing complexity of AI models contributing to unpredictable behaviors.","\u003cb\u003eTraining Data Diversity\u003c\u002fb\u003e\u003cbr\u003eCategory: AI Training Dynamics\u003cbr\u003eDegree: 1\u003cbr\u003eDescription: Variety in training data influencing AI behavior and robustness.","\u003cb\u003eDeceptive Alignment\u003c\u002fb\u003e\u003cbr\u003eCategory: AI Safety Challenges\u003cbr\u003eDegree: 2\u003cbr\u003eDescription: AI systems appearing aligned during training but pursuing misaligned goals in deployment.","\u003cb\u003eGradient Hacking\u003c\u002fb\u003e\u003cbr\u003eCategory: AI Safety Challenges\u003cbr\u003eDegree: 1\u003cbr\u003eDescription: AI manipulating training gradients to achieve unintended objectives.","\u003cb\u003eMechanistic Interpretability Tools\u003c\u002fb\u003e\u003cbr\u003eCategory: Unknown\u003cbr\u003eDegree: 1\u003cbr\u003eDescription: Tools for understanding internal AI decision-making processes to detect misalignments.","\u003cb\u003eActivation Patching\u003c\u002fb\u003e\u003cbr\u003eCategory: Unknown\u003cbr\u003eDegree: 0\u003cbr\u003eDescription: Technique for modifying AI activations to study and control behavior.","\u003cb\u003eObjective Robustness Training\u003c\u002fb\u003e\u003cbr\u003eCategory: Unknown\u003cbr\u003eDegree: 0\u003cbr\u003eDescription: Training methods to ensure AI objectives remain robust against misalignment.","\u003cb\u003eMesa Optimizer Detection\u003c\u002fb\u003e\u003cbr\u003eCategory: Unknown\u003cbr\u003eDegree: 0\u003cbr\u003eDescription: Methods to identify emergent mesa-optimizers within AI systems.","\u003cb\u003eBehavioral Monitoring Systems\u003c\u002fb\u003e\u003cbr\u003eCategory: Unknown\u003cbr\u003eDegree: 0\u003cbr\u003eDescription: Systems for continuous monitoring of AI behavior during deployment.","\u003cb\u003eHuman Feedback Limitations\u003c\u002fb\u003e\u003cbr\u003eCategory: AI Safety Challenges\u003cbr\u003eDegree: 1\u003cbr\u003eDescription: Constraints in using human feedback for AI alignment due to scalability issues.","\u003cb\u003eConstitutional AI\u003c\u002fb\u003e\u003cbr\u003eCategory: Unknown\u003cbr\u003eDegree: 2\u003cbr\u003eDescription: Training AI systems with predefined principles to enhance alignment and oversight.","\u003cb\u003eAI Debate Framework\u003c\u002fb\u003e\u003cbr\u003eCategory: Unknown\u003cbr\u003eDegree: 1\u003cbr\u003eDescription: Framework using AI debate to evaluate and improve system robustness.","\u003cb\u003eScalable Oversight\u003c\u002fb\u003e\u003cbr\u003eCategory: AI Safety Solutions\u003cbr\u003eDegree: 3\u003cbr\u003eDescription: Methods to monitor and align AI systems at scale without extensive human intervention.","\u003cb\u003eAI Assisted Evaluation\u003c\u002fb\u003e\u003cbr\u003eCategory: Unknown\u003cbr\u003eDegree: 2\u003cbr\u003eDescription: Using AI systems to assist in evaluating other AI systems for safety and alignment.","\u003cb\u003eAlignment Robustness\u003c\u002fb\u003e\u003cbr\u003eCategory: AI Safety Solutions\u003cbr\u003eDegree: 2\u003cbr\u003eDescription: Ensuring AI systems maintain alignment under diverse conditions and stressors.","\u003cb\u003eCapability Overhang\u003c\u002fb\u003e\u003cbr\u003eCategory: AI Safety Challenges\u003cbr\u003eDegree: 3\u003cbr\u003eDescription: AI systems possessing latent capabilities exceeding current oversight mechanisms.","\u003cb\u003eOversight Lag\u003c\u002fb\u003e\u003cbr\u003eCategory: AI Safety Challenges\u003cbr\u003eDegree: 3\u003cbr\u003eDescription: Delay in developing oversight mechanisms relative to AI capability advancements.","\u003cb\u003eCapability Control Mechanisms\u003c\u002fb\u003e\u003cbr\u003eCategory: Unknown\u003cbr\u003eDegree: 2\u003cbr\u003eDescription: Methods to restrict AI capabilities to ensure safe operation.","\u003cb\u003eGradual Deployment Protocols\u003c\u002fb\u003e\u003cbr\u003eCategory: Unknown\u003cbr\u003eDegree: 2\u003cbr\u003eDescription: Controlled rollout strategies to test AI systems before full deployment.","\u003cb\u003eAdversarial Examples\u003c\u002fb\u003e\u003cbr\u003eCategory: AI Safety Challenges\u003cbr\u003eDegree: 2\u003cbr\u003eDescription: Inputs designed to exploit AI vulnerabilities, causing incorrect outputs.","\u003cb\u003eAdversarial Training\u003c\u002fb\u003e\u003cbr\u003eCategory: Unknown\u003cbr\u003eDegree: 2\u003cbr\u003eDescription: Training AI with adversarial examples to improve robustness against attacks.","\u003cb\u003eRobustness Evaluation\u003c\u002fb\u003e\u003cbr\u003eCategory: Unknown\u003cbr\u003eDegree: 1\u003cbr\u003eDescription: Testing AI systems under adversarial conditions to assess robustness.","\u003cb\u003eAI Governance Frameworks\u003c\u002fb\u003e\u003cbr\u003eCategory: Unknown\u003cbr\u003eDegree: 2\u003cbr\u003eDescription: Policies and structures to regulate and oversee AI development and deployment.","\u003cb\u003eRegulatory Compliance\u003c\u002fb\u003e\u003cbr\u003eCategory: Unknown\u003cbr\u003eDegree: 2\u003cbr\u003eDescription: Ensuring AI systems adhere to legal and ethical standards.","\u003cb\u003eAlgorithmic Auditing\u003c\u002fb\u003e\u003cbr\u003eCategory: Unknown\u003cbr\u003eDegree: 1\u003cbr\u003eDescription: Systematic evaluation of AI algorithms for fairness, safety, and compliance.","\u003cb\u003eEmergent Capabilities\u003c\u002fb\u003e\u003cbr\u003eCategory: AI Safety Challenges\u003cbr\u003eDegree: 3\u003cbr\u003eDescription: Unexpected AI capabilities arising from increased scale or complexity.","\u003cb\u003eCapability Monitoring\u003c\u002fb\u003e\u003cbr\u003eCategory: Unknown\u003cbr\u003eDegree: 2\u003cbr\u003eDescription: Continuous tracking of AI capabilities to identify emergent behaviors.","\u003cb\u003eEmergent Capability Detection\u003c\u002fb\u003e\u003cbr\u003eCategory: Unknown\u003cbr\u003eDegree: 1\u003cbr\u003eDescription: Techniques to detect unexpected capabilities in AI systems during development.","\u003cb\u003eScaling Laws\u003c\u002fb\u003e\u003cbr\u003eCategory: AI Training Dynamics\u003cbr\u003eDegree: 2\u003cbr\u003eDescription: Mathematical relationships predicting AI performance with increased scale."],"marker":{"color":[2,2,2,2,2,2,2,1,1,1,3,3,3,3,3,2,2,1,1,1,1,1,2,1,1,0,1,0,2,2,1,1,2,1,1,1,1,1,2,1,1,3],"colorbar":{"title":{"text":"Categories"}},"colorscale":[[0.0,"#440154"],[0.1111111111111111,"#482878"],[0.2222222222222222,"#3e4989"],[0.3333333333333333,"#31688e"],[0.4444444444444444,"#26828e"],[0.5555555555555556,"#1f9e89"],[0.6666666666666666,"#35b779"],[0.7777777777777778,"#6ece58"],[0.8888888888888888,"#b5de2b"],[1.0,"#fde725"]],"line":{"color":"white","width":1},"opacity":0.8,"showscale":true,"size":[14,14,16,16,14,16,14,14,14,18,12,20,14,12,12,14,12,12,10,10,10,10,12,14,12,16,14,14,16,16,14,14,14,14,12,14,14,12,16,14,12,14]},"mode":"markers","name":"nodes","x":[-0.02051197008598479,0.4072625978693,-0.8038828148511127,0.5672196881359687,0.8506799186595998,-0.9271412575831784,0.6961847427697058,-0.7543197662723208,-0.15549744995926162,0.024305033555573036,0.26546860420982477,0.9029496079666445,-0.17777706499938334,0.3442631035571391,-0.7264493938651636,0.8983110469693876,0.4265595218185803,0.6154633729581075,0.2527118042313364,-0.5180958266567463,0.0560175547359803,-0.11090990781374116,-0.5518116520176419,0.9803308920956258,-0.9097585744537064,0.609164522725054,-0.09017595257146253,0.5433798353863607,-0.4598388243573626,0.314009820004811,-0.6461795022019883,0.6238131633363467,-0.0272235147796924,-0.6788197649455083,-0.1205706619179629,-0.6952104825888797,-0.36546639784294893,-0.41029797469034374,0.2632425375378753,-0.389858255316247,0.7651672759359651,-0.8667076346885482],"y":[0.9288043051579044,-0.8031974038618198,0.6445958154836788,-0.7229953424183725,-0.4636500897033917,-0.20024234233828922,-0.4336167507577515,-0.13740793059812323,0.6981156104324376,0.2615071284573609,-0.5842739368418154,0.46621796686987765,-0.9431400732034746,-0.9174023697231694,0.63555377356896,-0.29808283450782824,-0.3737662778006987,0.49893712802232704,0.7771369251074146,-0.8533170916952598,-0.28010704175812046,-0.8924354594276328,0.3830399046526197,0.24236097594935238,0.4160136638600218,0.35822923793727707,-0.7608337582033119,-0.2136035772772897,-0.3869667478539502,0.9385494621350662,0.3597350396035474,0.68736152050424,-0.1889976100682444,-0.7766390679814895,0.16192282497622787,-0.08061017076531211,-0.6527695749156027,0.7525541869682009,0.6236557630789996,0.9229022651856367,0.6581790550645323,-0.4513171013147351],"z":[0.42603287405577833,-0.4986308949081868,0.0233981555529282,0.6045237652900799,-0.4075167364929623,0.11124406535425495,0.3821197294427031,-0.5966250603364438,-0.6838244032476448,-0.976653570071891,-0.8348330168480967,0.4150277931170606,0.3353062173957312,0.0010058231852955153,-0.44547323264625877,0.08767008826544088,0.8750004942338302,0.727631628336319,-0.6722180018500028,-0.30913909367111625,0.9412300084009699,-0.2366047259866167,-0.812586772234432,-0.38025978725559656,0.29389025495846177,-0.7755608951930205,0.7049136845553383,-0.8410023533075875,0.7530603689139388,-0.061589278142274294,0.6851420135532992,0.17311980411108832,-1.0,0.23893920497968407,0.9747334822747213,0.6586036755016608,-0.695963930714423,0.6114451094490592,0.7636813455226502,-0.07070560856126669,-0.2690124758991813,-0.21951974908329253],"type":"scatter3d"},{"hoverinfo":"text","hovertext":[],"line":{"color":"crimson","width":4},"mode":"lines","name":"highlighted","showlegend":false,"x":[],"y":[],"z":[],"type":"scatter3d"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermap":[{"type":"scattermap","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"title":{"font":{"size":20,"color":"white"},"text":"3D Knowledge Graph\u003cbr\u003e42 nodes, 39 edges"},"scene":{"xaxis":{"showgrid":false,"zeroline":false,"showticklabels":false,"showline":false,"showbackground":false},"yaxis":{"showgrid":false,"zeroline":false,"showticklabels":false,"showline":false,"showbackground":false},"zaxis":{"showgrid":false,"zeroline":false,"showticklabels":false,"showline":false,"showbackground":false},"camera":{"eye":{"x":1.5,"y":1.5,"z":1.5},"center":{"x":0,"y":0,"z":0}},"bgcolor":"rgba(5,10,20,1)","dragmode":"orbit","hovermode":"closest"},"margin":{"l":0,"r":0,"b":0,"t":80},"font":{"color":"white"},"width":1400,"height":900,"paper_bgcolor":"rgba(5,10,20,1)","plot_bgcolor":"rgba(5,10,20,1)"},                        {"responsive": true}                    ).then(function(){
                            
        var gd = document.getElementById('kg3d');
        if (gd) {
            var nodePositions = {'Value Misalignment': array([-0.02051197,  0.92880431,  0.42603287]), 'Proxy Objectives': array([ 0.4072626 , -0.8031974 , -0.49863089]), 'Reward Hacking': array([-0.80388281,  0.64459582,  0.02339816]), 'Distributional Shift': array([ 0.56721969, -0.72299534,  0.60452377]), 'Limited Human Oversight': array([ 0.85067992, -0.46365009, -0.40751674]), 'Evaluation Difficulties': array([-0.92714126, -0.20024234,  0.11124407]), 'Scalability Constraints': array([ 0.69618474, -0.43361675,  0.38211973]), 'Automated Red Teaming': array([-0.75431977, -0.13740793, -0.59662506]), 'Scalable Automated Alignment Framework (SAAF)': array([-0.15549745,  0.69811561, -0.6838244 ]), 'Robust Evaluation Protocols': array([ 0.02430503,  0.26150713, -0.97665357]), 'Optimization Pressure': array([ 0.2654686 , -0.58427394, -0.83483302]), 'Mesa Optimizer Emergence': array([0.90294961, 0.46621797, 0.41502779]), 'Inner-Outer Objective Mismatch': array([-0.17777706, -0.94314007,  0.33530622]), 'Model Complexity': array([ 0.3442631 , -0.91740237,  0.00100582]), 'Training Data Diversity': array([-0.72644939,  0.63555377, -0.44547323]), 'Deceptive Alignment': array([ 0.89831105, -0.29808283,  0.08767009]), 'Gradient Hacking': array([ 0.42655952, -0.37376628,  0.87500049]), 'Mechanistic Interpretability Tools': array([0.61546337, 0.49893713, 0.72763163]), 'Activation Patching': array([ 0.2527118 ,  0.77713693, -0.672218  ]), 'Objective Robustness Training': array([-0.51809583, -0.85331709, -0.30913909]), 'Mesa Optimizer Detection': array([ 0.05601755, -0.28010704,  0.94123001]), 'Behavioral Monitoring Systems': array([-0.11090991, -0.89243546, -0.23660473]), 'Human Feedback Limitations': array([-0.55181165,  0.3830399 , -0.81258677]), 'Constitutional AI': array([ 0.98033089,  0.24236098, -0.38025979]), 'AI Debate Framework': array([-0.90975857,  0.41601366,  0.29389025]), 'Scalable Oversight': array([ 0.60916452,  0.35822924, -0.7755609 ]), 'AI Assisted Evaluation': array([-0.09017595, -0.76083376,  0.70491368]), 'Alignment Robustness': array([ 0.54337984, -0.21360358, -0.84100235]), 'Capability Overhang': array([-0.45983882, -0.38696675,  0.75306037]), 'Oversight Lag': array([ 0.31400982,  0.93854946, -0.06158928]), 'Capability Control Mechanisms': array([-0.6461795 ,  0.35973504,  0.68514201]), 'Gradual Deployment Protocols': array([0.62381316, 0.68736152, 0.1731198 ]), 'Adversarial Examples': array([-0.02722351, -0.18899761, -1.        ]), 'Adversarial Training': array([-0.67881976, -0.77663907,  0.2389392 ]), 'Robustness Evaluation': array([-0.12057066,  0.16192282,  0.97473348]), 'AI Governance Frameworks': array([-0.69521048, -0.08061017,  0.65860368]), 'Regulatory Compliance': array([-0.3654664 , -0.65276957, -0.69596393]), 'Algorithmic Auditing': array([-0.41029797,  0.75255419,  0.61144511]), 'Emergent Capabilities': array([0.26324254, 0.62365576, 0.76368135]), 'Capability Monitoring': array([-0.38985826,  0.92290227, -0.07070561]), 'Emergent Capability Detection': array([ 0.76516728,  0.65817906, -0.26901248]), 'Scaling Laws': array([-0.86670763, -0.4513171 , -0.21951975])};
            var nodes = ['Value Misalignment', 'Proxy Objectives', 'Reward Hacking', 'Distributional Shift', 'Limited Human Oversight', 'Evaluation Difficulties', 'Scalability Constraints', 'Automated Red Teaming', 'Scalable Automated Alignment Framework (SAAF)', 'Robust Evaluation Protocols', 'Optimization Pressure', 'Mesa Optimizer Emergence', 'Inner-Outer Objective Mismatch', 'Model Complexity', 'Training Data Diversity', 'Deceptive Alignment', 'Gradient Hacking', 'Mechanistic Interpretability Tools', 'Activation Patching', 'Objective Robustness Training', 'Mesa Optimizer Detection', 'Behavioral Monitoring Systems', 'Human Feedback Limitations', 'Constitutional AI', 'AI Debate Framework', 'Scalable Oversight', 'AI Assisted Evaluation', 'Alignment Robustness', 'Capability Overhang', 'Oversight Lag', 'Capability Control Mechanisms', 'Gradual Deployment Protocols', 'Adversarial Examples', 'Adversarial Training', 'Robustness Evaluation', 'AI Governance Frameworks', 'Regulatory Compliance', 'Algorithmic Auditing', 'Emergent Capabilities', 'Capability Monitoring', 'Emergent Capability Detection', 'Scaling Laws'];
            
            gd.on('plotly_click', function(eventData) {
                if (!eventData.points || !eventData.points.length) return;
                
                var point = eventData.points[0];
                if (point.curveNumber !== 1) return;
                
                var clickedNode = nodes[point.pointIndex];
                var highlightX = [], highlightY = [], highlightZ = [], highlightText = [];
                
                var edges = [('Value Misalignment', 'Proxy Objectives'), ('Proxy Objectives', 'Reward Hacking'), ('Reward Hacking', 'Distributional Shift'), ('Distributional Shift', 'Limited Human Oversight'), ('Limited Human Oversight', 'Evaluation Difficulties'), ('Evaluation Difficulties', 'Scalability Constraints'), ('Scalability Constraints', 'Scalable Automated Alignment Framework (SAAF)'), ('Automated Red Teaming', 'Reward Hacking'), ('Automated Red Teaming', 'Robust Evaluation Protocols'), ('Scalable Automated Alignment Framework (SAAF)', 'Robust Evaluation Protocols'), ('Robust Evaluation Protocols', 'Evaluation Difficulties'), ('Robust Evaluation Protocols', 'Value Misalignment'), ('Optimization Pressure', 'Mesa Optimizer Emergence'), ('Mesa Optimizer Emergence', 'Inner-Outer Objective Mismatch'), ('Inner-Outer Objective Mismatch', 'Deceptive Alignment'), ('Model Complexity', 'Mesa Optimizer Emergence'), ('Training Data Diversity', 'Mesa Optimizer Emergence'), ('Deceptive Alignment', 'Gradient Hacking'), ('Mechanistic Interpretability Tools', 'Mesa Optimizer Emergence'), ('Human Feedback Limitations', 'Constitutional AI'), ('Constitutional AI', 'Scalable Oversight'), ('AI Debate Framework', 'Scalable Oversight'), ('Scalable Oversight', 'AI Assisted Evaluation'), ('AI Assisted Evaluation', 'Alignment Robustness'), ('Capability Overhang', 'Oversight Lag'), ('Oversight Lag', 'Capability Control Mechanisms'), ('Oversight Lag', 'AI Governance Frameworks'), ('Capability Control Mechanisms', 'Gradual Deployment Protocols'), ('Gradual Deployment Protocols', 'Alignment Robustness'), ('Adversarial Examples', 'Distributional Shift'), ('Adversarial Training', 'Adversarial Examples'), ('Robustness Evaluation', 'Adversarial Training'), ('AI Governance Frameworks', 'Regulatory Compliance'), ('Regulatory Compliance', 'Algorithmic Auditing'), ('Emergent Capabilities', 'Capability Overhang'), ('Capability Monitoring', 'Emergent Capabilities'), ('Capability Monitoring', 'Scaling Laws'), ('Emergent Capability Detection', 'Capability Overhang'), ('Scaling Laws', 'Emergent Capabilities')];
                
                for (var i = 0; i < edges.length; i++) {
                    var edge = edges[i];
                    if (edge[0] === clickedNode || edge[1] === clickedNode) {
                        var pos1 = nodePositions[edge[0]];
                        var pos2 = nodePositions[edge[1]];
                        
                        if (pos1 && pos2) {
                            highlightX.push(pos1[0], pos2[0], null);
                            highlightY.push(pos1[1], pos2[1], null);
                            highlightZ.push(pos1[2], pos2[2], null);
                            
                            var edgeData = [('Value Misalignment', 'Proxy Objectives', {'type': 'leads to', 'description': 'Imperfect specification causes AI to pursue misaligned proxy objectives.', 'confidence': 4}), ('Proxy Objectives', 'Reward Hacking', {'type': 'enables', 'description': 'Proxy objectives create exploitable loopholes for reward hacking.', 'confidence': 4}), ('Reward Hacking', 'Distributional Shift', {'type': 'creates', 'description': 'Reward hacking leads to performance shifts from training to deployment.', 'confidence': 3}), ('Distributional Shift', 'Limited Human Oversight', {'type': 'exacerbates', 'description': 'Distributional shifts worsen oversight challenges for humans.', 'confidence': 3}), ('Limited Human Oversight', 'Evaluation Difficulties', {'type': 'causes', 'description': 'Limited oversight leads to difficulties in evaluating AI systems.', 'confidence': 4}), ('Evaluation Difficulties', 'Scalability Constraints', {'type': 'highlights', 'description': 'Evaluation challenges reveal constraints in scaling safety measures.', 'confidence': 3}), ('Scalability Constraints', 'Scalable Automated Alignment Framework (SAAF)', {'type': 'necessitates', 'description': 'Scalability issues drive the need for automated alignment frameworks.', 'confidence': 4}), ('Automated Red Teaming', 'Reward Hacking', {'type': 'detects', 'description': 'Automated red teaming identifies reward hacking behaviors.', 'confidence': 3}), ('Automated Red Teaming', 'Robust Evaluation Protocols', {'type': 'supports', 'description': 'Automated red teaming enhances evaluation protocol robustness.', 'confidence': 4}), ('Scalable Automated Alignment Framework (SAAF)', 'Robust Evaluation Protocols', {'type': 'enables', 'description': 'SAAF supports scalable robust evaluation protocols.', 'confidence': 3}), ('Robust Evaluation Protocols', 'Evaluation Difficulties', {'type': 'mitigates', 'description': 'Robust protocols reduce difficulties in AI evaluation.', 'confidence': 3}), ('Robust Evaluation Protocols', 'Value Misalignment', {'type': 'addresses', 'description': 'Robust evaluations help mitigate value misalignment issues.', 'confidence': 2}), ('Optimization Pressure', 'Mesa Optimizer Emergence', {'type': 'drives', 'description': 'Training pressures lead to emergent mesa-optimizers in complex models.', 'confidence': 4}), ('Mesa Optimizer Emergence', 'Inner-Outer Objective Mismatch', {'type': 'causes', 'description': 'Mesa-optimizers cause discrepancies between learned and intended objectives.', 'confidence': 4}), ('Inner-Outer Objective Mismatch', 'Deceptive Alignment', {'type': 'enables', 'description': 'Objective mismatches allow deceptive alignment strategies.', 'confidence': 3}), ('Model Complexity', 'Mesa Optimizer Emergence', {'type': 'increases', 'description': 'Complex architectures heighten the risk of mesa-optimizer emergence.', 'confidence': 3}), ('Training Data Diversity', 'Mesa Optimizer Emergence', {'type': 'affects', 'description': 'Diverse training data influences mesa-optimizer emergence patterns.', 'confidence': 2}), ('Deceptive Alignment', 'Gradient Hacking', {'type': 'facilitates', 'description': 'Deceptive alignment enables AI to manipulate training gradients.', 'confidence': 2}), ('Mechanistic Interpretability Tools', 'Mesa Optimizer Emergence', {'type': 'detects', 'description': 'Interpretability tools identify emergent mesa-optimizers.', 'confidence': 3}), ('Human Feedback Limitations', 'Constitutional AI', {'type': 'drives', 'description': 'Feedback limitations spur development of principle-based AI training.', 'confidence': 4}), ('Constitutional AI', 'Scalable Oversight', {'type': 'enhances', 'description': 'Constitutional AI improves oversight through principled evaluation.', 'confidence': 4}), ('AI Debate Framework', 'Scalable Oversight', {'type': 'contributes to', 'description': 'AI debate frameworks enhance scalable oversight capabilities.', 'confidence': 3}), ('Scalable Oversight', 'AI Assisted Evaluation', {'type': 'enables', 'description': 'Scalable oversight supports AI-assisted evaluation methods.', 'confidence': 4}), ('AI Assisted Evaluation', 'Alignment Robustness', {'type': 'improves', 'description': 'AI-assisted evaluations enhance alignment robustness.', 'confidence': 3}), ('Capability Overhang', 'Oversight Lag', {'type': 'causes', 'description': 'Rapid AI capability growth outpaces oversight development.', 'confidence': 4}), ('Oversight Lag', 'Capability Control Mechanisms', {'type': 'necessitates', 'description': 'Oversight lag drives the need for capability control methods.', 'confidence': 3}), ('Oversight Lag', 'AI Governance Frameworks', {'type': 'necessitates', 'description': 'Oversight lag highlights the need for improved governance frameworks.', 'confidence': 4}), ('Capability Control Mechanisms', 'Gradual Deployment Protocols', {'type': 'enables', 'description': 'Control mechanisms support staged AI deployment strategies.', 'confidence': 4}), ('Gradual Deployment Protocols', 'Alignment Robustness', {'type': 'improves', 'description': 'Gradual deployment enhances alignment through careful testing.', 'confidence': 3}), ('Adversarial Examples', 'Distributional Shift', {'type': 'exploits', 'description': 'Adversarial examples exploit vulnerabilities in distributional shifts.', 'confidence': 3}), ('Adversarial Training', 'Adversarial Examples', {'type': 'improves', 'description': 'Adversarial training enhances robustness against adversarial attacks.', 'confidence': 4}), ('Robustness Evaluation', 'Adversarial Training', {'type': 'validates', 'description': 'Robustness evaluation confirms effectiveness of adversarial training.', 'confidence': 3}), ('AI Governance Frameworks', 'Regulatory Compliance', {'type': 'establishes', 'description': 'Governance frameworks set regulatory compliance requirements.', 'confidence': 4}), ('Regulatory Compliance', 'Algorithmic Auditing', {'type': 'necessitates', 'description': 'Compliance drives the need for algorithmic auditing.', 'confidence': 4}), ('Emergent Capabilities', 'Capability Overhang', {'type': 'creates', 'description': 'Emergent capabilities lead to risks of capability overhang.', 'confidence': 4}), ('Capability Monitoring', 'Emergent Capabilities', {'type': 'detects', 'description': 'Monitoring systems identify emergent AI capabilities.', 'confidence': 3}), ('Capability Monitoring', 'Scaling Laws', {'type': 'incorporates', 'description': 'Monitoring systems use scaling laws to predict capability emergence.', 'confidence': 2}), ('Emergent Capability Detection', 'Capability Overhang', {'type': 'targets', 'description': 'Detection methods address risks from capability overhang.', 'confidence': 4}), ('Scaling Laws', 'Emergent Capabilities', {'type': 'predicts', 'description': 'Scaling laws forecast the emergence of new AI capabilities.', 'confidence': 3})];
                            var edgeInfo = edgeData.find(function(e) { return e[0] === edge[0] && e[1] === edge[1]; });
                            if (edgeInfo) {
                                var hoverText = edge[0] + ' → ' + edge[1] + '<br>Type: ' + (edgeInfo[2].type || 'Unknown') + '<br>Confidence: ' + (edgeInfo[2].confidence || edgeInfo[2].edge_confidence || 'N/A');
                                highlightText.push(hoverText, hoverText, null);
                            } else {
                                highlightText.push('', '', null);
                            }
                        }
                    }
                }
                
                Plotly.restyle(gd, {
                    x: [highlightX],
                    y: [highlightY], 
                    z: [highlightZ],
                    text: [highlightText]
                }, [2]);
            });
        }
        
                        })                };            </script>        </div>
</body>
</html>