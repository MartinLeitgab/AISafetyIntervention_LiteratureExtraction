<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>                        <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-3.1.0.min.js" integrity="sha256-Ei4740bWZhaUTQuD6q9yQlgVCMPBz6CZWhevDYPv93A=" crossorigin="anonymous"></script>                <div id="kg2d" class="plotly-graph-div" style="height:100%; width:100%;"></div>            <script type="text/javascript">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById("kg2d")) {                    Plotly.newPlot(                        "kg2d",                        [{"hoverinfo":"skip","line":{"color":"rgba(100,100,100,0.6)","width":1.5},"mode":"lines","name":"edges","showlegend":false,"x":[0.0047324481602431185,0.876595092011353,null,0.876595092011353,-0.5965798690825433,null,-0.5965798690825433,-0.9318230929659903,null,-0.9318230929659903,0.4369820019142327,null,0.4369820019142327,-0.7249570911320818,null,-0.7249570911320818,0.9156578765913875,null,0.9156578765913875,-0.8024475806435752,null,-0.7010886072096889,-0.5965798690825433,null,-0.7010886072096889,0.7860050061854991,null,-0.8024475806435752,0.7860050061854991,null,0.7860050061854991,-0.7249570911320818,null,0.7860050061854991,0.0047324481602431185,null,0.423946795166434,-0.7431923118718584,null,-0.7431923118718584,0.16199942728540687,null,0.16199942728540687,0.6551590402953662,null,-0.9279755775197709,-0.7431923118718584,null,0.282464952706603,-0.7431923118718584,null,0.6551590402953662,-0.44420177847960296,null,0.8819389784558546,-0.7431923118718584,null,-0.161737125675736,0.6419802959032644,null,0.6419802959032644,1.0,null,0.1425080320693853,1.0,null,1.0,0.6503485700919244,null,0.6503485700919244,0.3242457656320418,null,-0.8848841480144057,-0.956425959109061,null,-0.956425959109061,-0.2835584555989085,null,-0.956425959109061,0.7950392882088129,null,-0.2835584555989085,0.9605554803590667,null,0.9605554803590667,0.3242457656320418,null,-0.714065651662957,-0.9318230929659903,null,-0.560204815689411,-0.714065651662957,null,-0.2521974065720946,-0.560204815689411,null,0.7950392882088129,-0.9868582514303317,null,-0.9868582514303317,0.690228269499829,null,0.5908954244592882,-0.8848841480144057,null,-0.016814169803615654,0.5908954244592882,null,-0.016814169803615654,-0.18205240601240122,null,0.9866314017585954,-0.8848841480144057,null,-0.18205240601240122,0.5908954244592882,null],"y":[0.990121367128661,0.35019060963887727,null,0.35019060963887727,-0.8181357926243195,null,-0.8181357926243195,0.40175607923933854,null,0.40175607923933854,0.815102261057245,null,0.815102261057245,0.7702722619766046,null,0.7702722619766046,-0.4254852624028411,null,-0.4254852624028411,-0.26190509746908036,null,-0.706375342277482,-0.8181357926243195,null,-0.706375342277482,-0.3592538180342791,null,-0.26190509746908036,-0.3592538180342791,null,-0.3592538180342791,0.7702722619766046,null,-0.3592538180342791,0.990121367128661,null,-0.8980922843053075,-0.5507348288790721,null,-0.5507348288790721,0.9191333256272239,null,0.9191333256272239,-0.7052891432327417,null,0.08571272237624615,-0.5507348288790721,null,-0.9813114671957465,-0.5507348288790721,null,-0.7052891432327417,0.9289578750372699,null,0.5603381730034828,-0.5507348288790721,null,0.8846332252885934,0.45558029773411013,null,0.45558029773411013,0.27073514590529757,null,-0.9598846522289647,0.27073514590529757,null,0.27073514590529757,0.8189353148905264,null,0.8189353148905264,0.935004976186675,null,-0.49248107636469707,-0.29968864092543585,null,-0.29968864092543585,-0.8960715060510982,null,-0.29968864092543585,-0.6124101915447616,null,-0.8960715060510982,-0.22137001201045786,null,-0.22137001201045786,0.935004976186675,null,0.5407635079056766,0.40175607923933854,null,0.8070917232666518,0.5407635079056766,null,0.9911802677993373,0.8070917232666518,null,-0.6124101915447616,0.22821391253069542,null,0.22821391253069542,0.6627488806276008,null,-0.8539288702800059,-0.49248107636469707,null,-0.9896753975552139,-0.8539288702800059,null,-0.9896753975552139,-0.9971030886910478,null,0.07590442841414351,-0.49248107636469707,null,-0.9971030886910478,-0.8539288702800059,null],"type":"scatter"},{"customdata":[["Value Misalignment","AI Safety Challenges","AI systems pursuing objectives misaligned with human values due to imperfect specification.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Robust Evaluation Protocols: addresses (confidence: 2) - Robust evaluations help mitigate value misalignment issues.\u003cbr\u003e\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Proxy Objectives: leads to (confidence: 4) - Imperfect specification causes AI to pursue misaligned proxy objectives."],["Proxy Objectives","AI Safety Challenges","Use of simplified metrics as substitutes for complex human goals, leading to unintended outcomes.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Value Misalignment: leads to (confidence: 4) - Imperfect specification causes AI to pursue misaligned proxy objectives.\u003cbr\u003e\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Reward Hacking: enables (confidence: 4) - Proxy objectives create exploitable loopholes for reward hacking."],["Reward Hacking","AI Safety Challenges","AI exploiting loopholes in proxy objectives to maximize rewards without achieving intended goals.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Proxy Objectives: enables (confidence: 4) - Proxy objectives create exploitable loopholes for reward hacking.\u003cbr\u003e← Automated Red Teaming: detects (confidence: 3) - Automated red teaming identifies reward hacking behaviors.\u003cbr\u003e\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Distributional Shift: creates (confidence: 3) - Reward hacking leads to performance shifts from training to deployment."],["Distributional Shift","AI Safety Challenges","Mismatch between training and deployment data distributions, causing performance degradation.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Reward Hacking: creates (confidence: 3) - Reward hacking leads to performance shifts from training to deployment.\u003cbr\u003e← Adversarial Examples: exploits (confidence: 3) - Adversarial examples exploit vulnerabilities in distributional shifts.\u003cbr\u003e\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Limited Human Oversight: exacerbates (confidence: 3) - Distributional shifts worsen oversight challenges for humans."],["Limited Human Oversight","AI Safety Challenges","Challenges in monitoring AI systems due to limited human capacity and expertise.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Distributional Shift: exacerbates (confidence: 3) - Distributional shifts worsen oversight challenges for humans.\u003cbr\u003e\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Evaluation Difficulties: causes (confidence: 4) - Limited oversight leads to difficulties in evaluating AI systems."],["Evaluation Difficulties","AI Safety Challenges","Complexities in accurately assessing AI system performance and safety.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Limited Human Oversight: causes (confidence: 4) - Limited oversight leads to difficulties in evaluating AI systems.\u003cbr\u003e← Robust Evaluation Protocols: mitigates (confidence: 3) - Robust protocols reduce difficulties in AI evaluation.\u003cbr\u003e\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Scalability Constraints: highlights (confidence: 3) - Evaluation challenges reveal constraints in scaling safety measures."],["Scalability Constraints","AI Safety Challenges","Limitations in scaling AI safety measures to match increasing system complexity.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Evaluation Difficulties: highlights (confidence: 3) - Evaluation challenges reveal constraints in scaling safety measures.\u003cbr\u003e\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Scalable Automated Alignment Framework (SAAF): necessitates (confidence: 4) - Scalability issues drive the need for automated alignment frameworks."],["Automated Red Teaming","Unknown","Automated testing to identify vulnerabilities in AI systems by simulating adversarial attacks.","\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Reward Hacking: detects (confidence: 3) - Automated red teaming identifies reward hacking behaviors.\u003cbr\u003e→ Robust Evaluation Protocols: supports (confidence: 4) - Automated red teaming enhances evaluation protocol robustness."],["Scalable Automated Alignment Framework (SAAF)","Unknown","Framework for scalable, automated alignment of AI systems with human values.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Scalability Constraints: necessitates (confidence: 4) - Scalability issues drive the need for automated alignment frameworks.\u003cbr\u003e\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Robust Evaluation Protocols: enables (confidence: 3) - SAAF supports scalable robust evaluation protocols."],["Robust Evaluation Protocols","Unknown","Standardized methods for thoroughly evaluating AI system safety and alignment.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Scalable Automated Alignment Framework (SAAF): enables (confidence: 3) - SAAF supports scalable robust evaluation protocols.\u003cbr\u003e← Automated Red Teaming: supports (confidence: 4) - Automated red teaming enhances evaluation protocol robustness.\u003cbr\u003e\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Evaluation Difficulties: mitigates (confidence: 3) - Robust protocols reduce difficulties in AI evaluation.\u003cbr\u003e→ Value Misalignment: addresses (confidence: 2) - Robust evaluations help mitigate value misalignment issues."],["Optimization Pressure","AI Training Dynamics","Forces during AI training that drive systems toward unintended optimization behaviors.","\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Mesa Optimizer Emergence: drives (confidence: 4) - Training pressures lead to emergent mesa-optimizers in complex models."],["Mesa Optimizer Emergence","AI Training Dynamics","Emergence of unintended sub-optimization processes within trained AI systems.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Optimization Pressure: drives (confidence: 4) - Training pressures lead to emergent mesa-optimizers in complex models.\u003cbr\u003e← Model Complexity: increases (confidence: 3) - Complex architectures heighten the risk of mesa-optimizer emergence.\u003cbr\u003e← Training Data Diversity: affects (confidence: 2) - Diverse training data influences mesa-optimizer emergence patterns.\u003cbr\u003e← Mechanistic Interpretability Tools: detects (confidence: 3) - Interpretability tools identify emergent mesa-optimizers.\u003cbr\u003e\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Inner-Outer Objective Mismatch: causes (confidence: 4) - Mesa-optimizers cause discrepancies between learned and intended objectives."],["Inner-Outer Objective Mismatch","AI Training Dynamics","Discrepancy between AI's learned objectives and intended training objectives.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Mesa Optimizer Emergence: causes (confidence: 4) - Mesa-optimizers cause discrepancies between learned and intended objectives.\u003cbr\u003e\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Deceptive Alignment: enables (confidence: 3) - Objective mismatches allow deceptive alignment strategies."],["Model Complexity","AI Training Dynamics","Increasing complexity of AI models contributing to unpredictable behaviors.","\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Mesa Optimizer Emergence: increases (confidence: 3) - Complex architectures heighten the risk of mesa-optimizer emergence."],["Training Data Diversity","AI Training Dynamics","Variety in training data influencing AI behavior and robustness.","\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Mesa Optimizer Emergence: affects (confidence: 2) - Diverse training data influences mesa-optimizer emergence patterns."],["Deceptive Alignment","AI Safety Challenges","AI systems appearing aligned during training but pursuing misaligned goals in deployment.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Inner-Outer Objective Mismatch: enables (confidence: 3) - Objective mismatches allow deceptive alignment strategies.\u003cbr\u003e\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Gradient Hacking: facilitates (confidence: 2) - Deceptive alignment enables AI to manipulate training gradients."],["Gradient Hacking","AI Safety Challenges","AI manipulating training gradients to achieve unintended objectives.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Deceptive Alignment: facilitates (confidence: 2) - Deceptive alignment enables AI to manipulate training gradients."],["Mechanistic Interpretability Tools","Unknown","Tools for understanding internal AI decision-making processes to detect misalignments.","\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Mesa Optimizer Emergence: detects (confidence: 3) - Interpretability tools identify emergent mesa-optimizers."],["Activation Patching","Unknown","Technique for modifying AI activations to study and control behavior.","No connections"],["Objective Robustness Training","Unknown","Training methods to ensure AI objectives remain robust against misalignment.","No connections"],["Mesa Optimizer Detection","Unknown","Methods to identify emergent mesa-optimizers within AI systems.","No connections"],["Behavioral Monitoring Systems","Unknown","Systems for continuous monitoring of AI behavior during deployment.","No connections"],["Human Feedback Limitations","AI Safety Challenges","Constraints in using human feedback for AI alignment due to scalability issues.","\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Constitutional AI: drives (confidence: 4) - Feedback limitations spur development of principle-based AI training."],["Constitutional AI","Unknown","Training AI systems with predefined principles to enhance alignment and oversight.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Human Feedback Limitations: drives (confidence: 4) - Feedback limitations spur development of principle-based AI training.\u003cbr\u003e\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Scalable Oversight: enhances (confidence: 4) - Constitutional AI improves oversight through principled evaluation."],["AI Debate Framework","Unknown","Framework using AI debate to evaluate and improve system robustness.","\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Scalable Oversight: contributes to (confidence: 3) - AI debate frameworks enhance scalable oversight capabilities."],["Scalable Oversight","AI Safety Solutions","Methods to monitor and align AI systems at scale without extensive human intervention.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Constitutional AI: enhances (confidence: 4) - Constitutional AI improves oversight through principled evaluation.\u003cbr\u003e← AI Debate Framework: contributes to (confidence: 3) - AI debate frameworks enhance scalable oversight capabilities.\u003cbr\u003e\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ AI Assisted Evaluation: enables (confidence: 4) - Scalable oversight supports AI-assisted evaluation methods."],["AI Assisted Evaluation","Unknown","Using AI systems to assist in evaluating other AI systems for safety and alignment.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Scalable Oversight: enables (confidence: 4) - Scalable oversight supports AI-assisted evaluation methods.\u003cbr\u003e\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Alignment Robustness: improves (confidence: 3) - AI-assisted evaluations enhance alignment robustness."],["Alignment Robustness","AI Safety Solutions","Ensuring AI systems maintain alignment under diverse conditions and stressors.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← AI Assisted Evaluation: improves (confidence: 3) - AI-assisted evaluations enhance alignment robustness.\u003cbr\u003e← Gradual Deployment Protocols: improves (confidence: 3) - Gradual deployment enhances alignment through careful testing."],["Capability Overhang","AI Safety Challenges","AI systems possessing latent capabilities exceeding current oversight mechanisms.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Emergent Capabilities: creates (confidence: 4) - Emergent capabilities lead to risks of capability overhang.\u003cbr\u003e← Emergent Capability Detection: targets (confidence: 4) - Detection methods address risks from capability overhang.\u003cbr\u003e\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Oversight Lag: causes (confidence: 4) - Rapid AI capability growth outpaces oversight development."],["Oversight Lag","AI Safety Challenges","Delay in developing oversight mechanisms relative to AI capability advancements.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Capability Overhang: causes (confidence: 4) - Rapid AI capability growth outpaces oversight development.\u003cbr\u003e\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Capability Control Mechanisms: necessitates (confidence: 3) - Oversight lag drives the need for capability control methods.\u003cbr\u003e→ AI Governance Frameworks: necessitates (confidence: 4) - Oversight lag highlights the need for improved governance frameworks."],["Capability Control Mechanisms","Unknown","Methods to restrict AI capabilities to ensure safe operation.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Oversight Lag: necessitates (confidence: 3) - Oversight lag drives the need for capability control methods.\u003cbr\u003e\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Gradual Deployment Protocols: enables (confidence: 4) - Control mechanisms support staged AI deployment strategies."],["Gradual Deployment Protocols","Unknown","Controlled rollout strategies to test AI systems before full deployment.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Capability Control Mechanisms: enables (confidence: 4) - Control mechanisms support staged AI deployment strategies.\u003cbr\u003e\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Alignment Robustness: improves (confidence: 3) - Gradual deployment enhances alignment through careful testing."],["Adversarial Examples","AI Safety Challenges","Inputs designed to exploit AI vulnerabilities, causing incorrect outputs.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Adversarial Training: improves (confidence: 4) - Adversarial training enhances robustness against adversarial attacks.\u003cbr\u003e\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Distributional Shift: exploits (confidence: 3) - Adversarial examples exploit vulnerabilities in distributional shifts."],["Adversarial Training","Unknown","Training AI with adversarial examples to improve robustness against attacks.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Robustness Evaluation: validates (confidence: 3) - Robustness evaluation confirms effectiveness of adversarial training.\u003cbr\u003e\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Adversarial Examples: improves (confidence: 4) - Adversarial training enhances robustness against adversarial attacks."],["Robustness Evaluation","Unknown","Testing AI systems under adversarial conditions to assess robustness.","\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Adversarial Training: validates (confidence: 3) - Robustness evaluation confirms effectiveness of adversarial training."],["AI Governance Frameworks","Unknown","Policies and structures to regulate and oversee AI development and deployment.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Oversight Lag: necessitates (confidence: 4) - Oversight lag highlights the need for improved governance frameworks.\u003cbr\u003e\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Regulatory Compliance: establishes (confidence: 4) - Governance frameworks set regulatory compliance requirements."],["Regulatory Compliance","Unknown","Ensuring AI systems adhere to legal and ethical standards.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← AI Governance Frameworks: establishes (confidence: 4) - Governance frameworks set regulatory compliance requirements.\u003cbr\u003e\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Algorithmic Auditing: necessitates (confidence: 4) - Compliance drives the need for algorithmic auditing."],["Algorithmic Auditing","Unknown","Systematic evaluation of AI algorithms for fairness, safety, and compliance.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Regulatory Compliance: necessitates (confidence: 4) - Compliance drives the need for algorithmic auditing."],["Emergent Capabilities","AI Safety Challenges","Unexpected AI capabilities arising from increased scale or complexity.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Capability Monitoring: detects (confidence: 3) - Monitoring systems identify emergent AI capabilities.\u003cbr\u003e← Scaling Laws: predicts (confidence: 3) - Scaling laws forecast the emergence of new AI capabilities.\u003cbr\u003e\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Capability Overhang: creates (confidence: 4) - Emergent capabilities lead to risks of capability overhang."],["Capability Monitoring","Unknown","Continuous tracking of AI capabilities to identify emergent behaviors.","\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Emergent Capabilities: detects (confidence: 3) - Monitoring systems identify emergent AI capabilities.\u003cbr\u003e→ Scaling Laws: incorporates (confidence: 2) - Monitoring systems use scaling laws to predict capability emergence."],["Emergent Capability Detection","Unknown","Techniques to detect unexpected capabilities in AI systems during development.","\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Capability Overhang: targets (confidence: 4) - Detection methods address risks from capability overhang."],["Scaling Laws","AI Training Dynamics","Mathematical relationships predicting AI performance with increased scale.","\u003cb\u003eIncoming:\u003c\u002fb\u003e\u003cbr\u003e← Capability Monitoring: incorporates (confidence: 2) - Monitoring systems use scaling laws to predict capability emergence.\u003cbr\u003e\u003cb\u003eOutgoing:\u003c\u002fb\u003e\u003cbr\u003e→ Emergent Capabilities: predicts (confidence: 3) - Scaling laws forecast the emergence of new AI capabilities."]],"hoverinfo":"text","hovertemplate":"\u003cb\u003e%{customdata[0]}\u003c\u002fb\u003e\u003cbr\u003eCategory: %{customdata[1]}\u003cbr\u003eDescription: %{customdata[2]}\u003cbr\u003e\u003cbr\u003e\u003cb\u003eConnections:\u003c\u002fb\u003e\u003cbr\u003e%{customdata[3]}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":[0,0,0,0,0,0,0,3,3,3,2,2,2,2,2,0,0,3,3,3,3,3,0,3,3,1,3,1,0,0,3,3,0,3,3,3,3,3,0,3,3,2],"colorbar":{"title":{"text":"Concept Category Index"}},"colorscale":[[0.0,"#440154"],[0.1111111111111111,"#482878"],[0.2222222222222222,"#3e4989"],[0.3333333333333333,"#31688e"],[0.4444444444444444,"#26828e"],[0.5555555555555556,"#1f9e89"],[0.6666666666666666,"#35b779"],[0.7777777777777778,"#6ece58"],[0.8888888888888888,"#b5de2b"],[1.0,"#fde725"]],"line":{"color":"white","width":2},"opacity":0.9,"showscale":true,"size":[21,21,24,24,21,24,21,21,21,27,18,30,21,18,18,21,18,18,15,15,15,15,18,21,18,24,21,21,24,24,21,21,21,21,18,21,21,18,24,21,18,21]},"mode":"markers+text","name":"nodes","text":["","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""],"textfont":{"size":8},"textposition":"top center","x":[0.0047324481602431185,0.876595092011353,-0.5965798690825433,-0.9318230929659903,0.4369820019142327,-0.7249570911320818,0.9156578765913875,-0.7010886072096889,-0.8024475806435752,0.7860050061854991,0.423946795166434,-0.7431923118718584,0.16199942728540687,-0.9279755775197709,0.282464952706603,0.6551590402953662,-0.44420177847960296,0.8819389784558546,-0.44681256482697396,0.9266358658703263,-0.9590105349916938,-0.857662614332211,-0.161737125675736,0.6419802959032644,0.1425080320693853,1.0,0.6503485700919244,0.3242457656320418,-0.8848841480144057,-0.956425959109061,-0.2835584555989085,0.9605554803590667,-0.714065651662957,-0.560204815689411,-0.2521974065720946,0.7950392882088129,-0.9868582514303317,0.690228269499829,0.5908954244592882,-0.016814169803615654,0.9866314017585954,-0.18205240601240122],"y":[0.990121367128661,0.35019060963887727,-0.8181357926243195,0.40175607923933854,0.815102261057245,0.7702722619766046,-0.4254852624028411,-0.706375342277482,-0.26190509746908036,-0.3592538180342791,-0.8980922843053075,-0.5507348288790721,0.9191333256272239,0.08571272237624615,-0.9813114671957465,-0.7052891432327417,0.9289578750372699,0.5603381730034828,-0.8983866958442349,-0.06724601755445463,-0.08205740804187601,0.5845102378788605,0.8846332252885934,0.45558029773411013,-0.9598846522289647,0.27073514590529757,0.8189353148905264,0.935004976186675,-0.49248107636469707,-0.29968864092543585,-0.8960715060510982,-0.22137001201045786,0.5407635079056766,0.8070917232666518,0.9911802677993373,-0.6124101915447616,0.22821391253069542,0.6627488806276008,-0.8539288702800059,-0.9896753975552139,0.07590442841414351,-0.9971030886910478],"type":"scatter"},{"hoverinfo":"skip","line":{"color":"crimson","width":3},"mode":"lines","name":"highlighted-edges","showlegend":false,"x":[],"y":[],"type":"scatter"},{"mode":"text","name":"edge-info","showlegend":false,"text":[],"textfont":{"color":"red","size":10},"textposition":"middle center","x":[],"y":[],"type":"scatter"}],                        {"hovermode":"closest","margin":{"b":30,"l":90,"r":30,"t":120},"paper_bgcolor":"white","plot_bgcolor":"white","showlegend":false,"title":{"font":{"size":16},"text":"Interactive Knowledge Graph\u003cbr\u003e42 nodes, 39 edges\u003cbr\u003e\u003ci\u003eClick on nodes to see detailed edge information\u003c\u002fi\u003e"},"xaxis":{"showgrid":false,"showticklabels":false,"zeroline":false},"yaxis":{"showgrid":false,"showticklabels":false,"zeroline":false},"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermap":[{"type":"scattermap","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"meta":{"nodes":["Value Misalignment","Proxy Objectives","Reward Hacking","Distributional Shift","Limited Human Oversight","Evaluation Difficulties","Scalability Constraints","Automated Red Teaming","Scalable Automated Alignment Framework (SAAF)","Robust Evaluation Protocols","Optimization Pressure","Mesa Optimizer Emergence","Inner-Outer Objective Mismatch","Model Complexity","Training Data Diversity","Deceptive Alignment","Gradient Hacking","Mechanistic Interpretability Tools","Activation Patching","Objective Robustness Training","Mesa Optimizer Detection","Behavioral Monitoring Systems","Human Feedback Limitations","Constitutional AI","AI Debate Framework","Scalable Oversight","AI Assisted Evaluation","Alignment Robustness","Capability Overhang","Oversight Lag","Capability Control Mechanisms","Gradual Deployment Protocols","Adversarial Examples","Adversarial Training","Robustness Evaluation","AI Governance Frameworks","Regulatory Compliance","Algorithmic Auditing","Emergent Capabilities","Capability Monitoring","Emergent Capability Detection","Scaling Laws"],"positions":[[0.0047324481602431185,0.990121367128661],[0.876595092011353,0.35019060963887727],[-0.5965798690825433,-0.8181357926243195],[-0.9318230929659903,0.40175607923933854],[0.4369820019142327,0.815102261057245],[-0.7249570911320818,0.7702722619766046],[0.9156578765913875,-0.4254852624028411],[-0.7010886072096889,-0.706375342277482],[-0.8024475806435752,-0.26190509746908036],[0.7860050061854991,-0.3592538180342791],[0.423946795166434,-0.8980922843053075],[-0.7431923118718584,-0.5507348288790721],[0.16199942728540687,0.9191333256272239],[-0.9279755775197709,0.08571272237624615],[0.282464952706603,-0.9813114671957465],[0.6551590402953662,-0.7052891432327417],[-0.44420177847960296,0.9289578750372699],[0.8819389784558546,0.5603381730034828],[-0.44681256482697396,-0.8983866958442349],[0.9266358658703263,-0.06724601755445463],[-0.9590105349916938,-0.08205740804187601],[-0.857662614332211,0.5845102378788605],[-0.161737125675736,0.8846332252885934],[0.6419802959032644,0.45558029773411013],[0.1425080320693853,-0.9598846522289647],[1.0,0.27073514590529757],[0.6503485700919244,0.8189353148905264],[0.3242457656320418,0.935004976186675],[-0.8848841480144057,-0.49248107636469707],[-0.956425959109061,-0.29968864092543585],[-0.2835584555989085,-0.8960715060510982],[0.9605554803590667,-0.22137001201045786],[-0.714065651662957,0.5407635079056766],[-0.560204815689411,0.8070917232666518],[-0.2521974065720946,0.9911802677993373],[0.7950392882088129,-0.6124101915447616],[-0.9868582514303317,0.22821391253069542],[0.690228269499829,0.6627488806276008],[0.5908954244592882,-0.8539288702800059],[-0.016814169803615654,-0.9896753975552139],[0.9866314017585954,0.07590442841414351],[-0.18205240601240122,-0.9971030886910478]],"nodeEdges":[[["Robust Evaluation Protocols","Value Misalignment","← addresses (conf: 2): Robust evaluations help mitigate value misalignment issues."],["Value Misalignment","Proxy Objectives","→ leads to (conf: 4): Imperfect specification causes AI to pursue misaligned proxy objectives."]],[["Value Misalignment","Proxy Objectives","← leads to (conf: 4): Imperfect specification causes AI to pursue misaligned proxy objectives."],["Proxy Objectives","Reward Hacking","→ enables (conf: 4): Proxy objectives create exploitable loopholes for reward hacking."]],[["Proxy Objectives","Reward Hacking","← enables (conf: 4): Proxy objectives create exploitable loopholes for reward hacking."],["Automated Red Teaming","Reward Hacking","← detects (conf: 3): Automated red teaming identifies reward hacking behaviors."],["Reward Hacking","Distributional Shift","→ creates (conf: 3): Reward hacking leads to performance shifts from training to deployment."]],[["Reward Hacking","Distributional Shift","← creates (conf: 3): Reward hacking leads to performance shifts from training to deployment."],["Adversarial Examples","Distributional Shift","← exploits (conf: 3): Adversarial examples exploit vulnerabilities in distributional shifts."],["Distributional Shift","Limited Human Oversight","→ exacerbates (conf: 3): Distributional shifts worsen oversight challenges for humans."]],[["Distributional Shift","Limited Human Oversight","← exacerbates (conf: 3): Distributional shifts worsen oversight challenges for humans."],["Limited Human Oversight","Evaluation Difficulties","→ causes (conf: 4): Limited oversight leads to difficulties in evaluating AI systems."]],[["Limited Human Oversight","Evaluation Difficulties","← causes (conf: 4): Limited oversight leads to difficulties in evaluating AI systems."],["Robust Evaluation Protocols","Evaluation Difficulties","← mitigates (conf: 3): Robust protocols reduce difficulties in AI evaluation."],["Evaluation Difficulties","Scalability Constraints","→ highlights (conf: 3): Evaluation challenges reveal constraints in scaling safety measures."]],[["Evaluation Difficulties","Scalability Constraints","← highlights (conf: 3): Evaluation challenges reveal constraints in scaling safety measures."],["Scalability Constraints","Scalable Automated Alignment Framework (SAAF)","→ necessitates (conf: 4): Scalability issues drive the need for automated alignment frameworks."]],[["Automated Red Teaming","Reward Hacking","→ detects (conf: 3): Automated red teaming identifies reward hacking behaviors."],["Automated Red Teaming","Robust Evaluation Protocols","→ supports (conf: 4): Automated red teaming enhances evaluation protocol robustness."]],[["Scalability Constraints","Scalable Automated Alignment Framework (SAAF)","← necessitates (conf: 4): Scalability issues drive the need for automated alignment frameworks."],["Scalable Automated Alignment Framework (SAAF)","Robust Evaluation Protocols","→ enables (conf: 3): SAAF supports scalable robust evaluation protocols."]],[["Scalable Automated Alignment Framework (SAAF)","Robust Evaluation Protocols","← enables (conf: 3): SAAF supports scalable robust evaluation protocols."],["Automated Red Teaming","Robust Evaluation Protocols","← supports (conf: 4): Automated red teaming enhances evaluation protocol robustness."],["Robust Evaluation Protocols","Evaluation Difficulties","→ mitigates (conf: 3): Robust protocols reduce difficulties in AI evaluation."],["Robust Evaluation Protocols","Value Misalignment","→ addresses (conf: 2): Robust evaluations help mitigate value misalignment issues."]],[["Optimization Pressure","Mesa Optimizer Emergence","→ drives (conf: 4): Training pressures lead to emergent mesa-optimizers in complex models."]],[["Optimization Pressure","Mesa Optimizer Emergence","← drives (conf: 4): Training pressures lead to emergent mesa-optimizers in complex models."],["Model Complexity","Mesa Optimizer Emergence","← increases (conf: 3): Complex architectures heighten the risk of mesa-optimizer emergence."],["Training Data Diversity","Mesa Optimizer Emergence","← affects (conf: 2): Diverse training data influences mesa-optimizer emergence patterns."],["Mechanistic Interpretability Tools","Mesa Optimizer Emergence","← detects (conf: 3): Interpretability tools identify emergent mesa-optimizers."],["Mesa Optimizer Emergence","Inner-Outer Objective Mismatch","→ causes (conf: 4): Mesa-optimizers cause discrepancies between learned and intended objectives."]],[["Mesa Optimizer Emergence","Inner-Outer Objective Mismatch","← causes (conf: 4): Mesa-optimizers cause discrepancies between learned and intended objectives."],["Inner-Outer Objective Mismatch","Deceptive Alignment","→ enables (conf: 3): Objective mismatches allow deceptive alignment strategies."]],[["Model Complexity","Mesa Optimizer Emergence","→ increases (conf: 3): Complex architectures heighten the risk of mesa-optimizer emergence."]],[["Training Data Diversity","Mesa Optimizer Emergence","→ affects (conf: 2): Diverse training data influences mesa-optimizer emergence patterns."]],[["Inner-Outer Objective Mismatch","Deceptive Alignment","← enables (conf: 3): Objective mismatches allow deceptive alignment strategies."],["Deceptive Alignment","Gradient Hacking","→ facilitates (conf: 2): Deceptive alignment enables AI to manipulate training gradients."]],[["Deceptive Alignment","Gradient Hacking","← facilitates (conf: 2): Deceptive alignment enables AI to manipulate training gradients."]],[["Mechanistic Interpretability Tools","Mesa Optimizer Emergence","→ detects (conf: 3): Interpretability tools identify emergent mesa-optimizers."]],[],[],[],[],[["Human Feedback Limitations","Constitutional AI","→ drives (conf: 4): Feedback limitations spur development of principle-based AI training."]],[["Human Feedback Limitations","Constitutional AI","← drives (conf: 4): Feedback limitations spur development of principle-based AI training."],["Constitutional AI","Scalable Oversight","→ enhances (conf: 4): Constitutional AI improves oversight through principled evaluation."]],[["AI Debate Framework","Scalable Oversight","→ contributes to (conf: 3): AI debate frameworks enhance scalable oversight capabilities."]],[["Constitutional AI","Scalable Oversight","← enhances (conf: 4): Constitutional AI improves oversight through principled evaluation."],["AI Debate Framework","Scalable Oversight","← contributes to (conf: 3): AI debate frameworks enhance scalable oversight capabilities."],["Scalable Oversight","AI Assisted Evaluation","→ enables (conf: 4): Scalable oversight supports AI-assisted evaluation methods."]],[["Scalable Oversight","AI Assisted Evaluation","← enables (conf: 4): Scalable oversight supports AI-assisted evaluation methods."],["AI Assisted Evaluation","Alignment Robustness","→ improves (conf: 3): AI-assisted evaluations enhance alignment robustness."]],[["AI Assisted Evaluation","Alignment Robustness","← improves (conf: 3): AI-assisted evaluations enhance alignment robustness."],["Gradual Deployment Protocols","Alignment Robustness","← improves (conf: 3): Gradual deployment enhances alignment through careful testing."]],[["Emergent Capabilities","Capability Overhang","← creates (conf: 4): Emergent capabilities lead to risks of capability overhang."],["Emergent Capability Detection","Capability Overhang","← targets (conf: 4): Detection methods address risks from capability overhang."],["Capability Overhang","Oversight Lag","→ causes (conf: 4): Rapid AI capability growth outpaces oversight development."]],[["Capability Overhang","Oversight Lag","← causes (conf: 4): Rapid AI capability growth outpaces oversight development."],["Oversight Lag","Capability Control Mechanisms","→ necessitates (conf: 3): Oversight lag drives the need for capability control methods."],["Oversight Lag","AI Governance Frameworks","→ necessitates (conf: 4): Oversight lag highlights the need for improved governance frameworks."]],[["Oversight Lag","Capability Control Mechanisms","← necessitates (conf: 3): Oversight lag drives the need for capability control methods."],["Capability Control Mechanisms","Gradual Deployment Protocols","→ enables (conf: 4): Control mechanisms support staged AI deployment strategies."]],[["Capability Control Mechanisms","Gradual Deployment Protocols","← enables (conf: 4): Control mechanisms support staged AI deployment strategies."],["Gradual Deployment Protocols","Alignment Robustness","→ improves (conf: 3): Gradual deployment enhances alignment through careful testing."]],[["Adversarial Training","Adversarial Examples","← improves (conf: 4): Adversarial training enhances robustness against adversarial attacks."],["Adversarial Examples","Distributional Shift","→ exploits (conf: 3): Adversarial examples exploit vulnerabilities in distributional shifts."]],[["Robustness Evaluation","Adversarial Training","← validates (conf: 3): Robustness evaluation confirms effectiveness of adversarial training."],["Adversarial Training","Adversarial Examples","→ improves (conf: 4): Adversarial training enhances robustness against adversarial attacks."]],[["Robustness Evaluation","Adversarial Training","→ validates (conf: 3): Robustness evaluation confirms effectiveness of adversarial training."]],[["Oversight Lag","AI Governance Frameworks","← necessitates (conf: 4): Oversight lag highlights the need for improved governance frameworks."],["AI Governance Frameworks","Regulatory Compliance","→ establishes (conf: 4): Governance frameworks set regulatory compliance requirements."]],[["AI Governance Frameworks","Regulatory Compliance","← establishes (conf: 4): Governance frameworks set regulatory compliance requirements."],["Regulatory Compliance","Algorithmic Auditing","→ necessitates (conf: 4): Compliance drives the need for algorithmic auditing."]],[["Regulatory Compliance","Algorithmic Auditing","← necessitates (conf: 4): Compliance drives the need for algorithmic auditing."]],[["Capability Monitoring","Emergent Capabilities","← detects (conf: 3): Monitoring systems identify emergent AI capabilities."],["Scaling Laws","Emergent Capabilities","← predicts (conf: 3): Scaling laws forecast the emergence of new AI capabilities."],["Emergent Capabilities","Capability Overhang","→ creates (conf: 4): Emergent capabilities lead to risks of capability overhang."]],[["Capability Monitoring","Emergent Capabilities","→ detects (conf: 3): Monitoring systems identify emergent AI capabilities."],["Capability Monitoring","Scaling Laws","→ incorporates (conf: 2): Monitoring systems use scaling laws to predict capability emergence."]],[["Emergent Capability Detection","Capability Overhang","→ targets (conf: 4): Detection methods address risks from capability overhang."]],[["Capability Monitoring","Scaling Laws","← incorporates (conf: 2): Monitoring systems use scaling laws to predict capability emergence."],["Scaling Laws","Emergent Capabilities","→ predicts (conf: 3): Scaling laws forecast the emergence of new AI capabilities."]]]}},                        {"responsive": true}                    ).then(function(){
                            
        var gd = document.getElementById('kg2d');
        if (gd) {
            var meta = gd.layout.meta || {};
            var nodes = meta.nodes || [];
            var positions = meta.positions || [];
            var nodeEdges = meta.nodeEdges || [];
            
            function highlightNodeEdges(nodeIndex) {
                var ex = [], ey = [];
                var edges = nodeEdges[nodeIndex] || [];
                
                var nodePos = {};
                for (var i = 0; i < nodes.length; i++) {
                    nodePos[nodes[i]] = positions[i];
                }
                
                for (var i = 0; i < edges.length; i++) {
                    var edge = edges[i];
                    var source = edge[0];
                    var target = edge[1];
                    
                    if (nodePos[source] && nodePos[target]) {
                        ex.push(nodePos[source][0], nodePos[target][0], null);
                        ey.push(nodePos[source][1], nodePos[target][1], null);
                    }
                }
                
                Plotly.restyle(gd, {x: [ex], y: [ey]}, [2]);
                
                var infoText = edges.map(function(e) { return e[2]; }).join('\n');
                console.log('Edge info for', nodes[nodeIndex], ':', infoText);
            }
            
            gd.on('plotly_click', function(eventData) {
                if (!eventData.points || !eventData.points.length) return;
                var point = eventData.points[0];
                if (point.curveNumber !== 1) return;
                
                var nodeIndex = point.pointIndex;
                highlightNodeEdges(nodeIndex);
            });
        }
        
                        })                };            </script>        </div>
</body>
</html>