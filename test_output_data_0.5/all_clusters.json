[
    [
        {
            "id": 2270,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Adversarial reprogramming adversarial training during fine-tuning phase",
                "type": "concept",
                "description": "During RLHF/fine-tuning, include adversarial program examples that attempt task-redirection and optimise the model to maintain correct original task output (inferred from \u00a74.4).",
                "aliases": [
                    "reprogramming-specific fine-tuning"
                ],
                "concept_category": "implementation mechanism",
                "url": "https://arxiv.org/abs/1806.11146",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516437803,
                "updated_at": 1759516437803
            }
        },
        {
            "id": 2280,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Fine-tune models with adversarial reprogramming adversarial examples to improve robustness",
                "type": "intervention",
                "description": "During fine-tuning/RLHF, include batches of task-redirecting programs and train the model to preserve correct outputs, extending standard adversarial training.",
                "aliases": [
                    "reprogramming-aware fine-tuning"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "url": "https://arxiv.org/abs/1806.11146",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516437803,
                "updated_at": 1759516437803
            }
        }
    ],
    [
        {
            "id": 657,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "unsafe exploration in reinforcement learning",
                "type": "concept",
                "description": "Exploratory actions may irreversibly damage the agent or environment.",
                "aliases": [
                    "catastrophic exploration risk",
                    "dangerous RL exploration"
                ],
                "concept_category": "risk",
                "url": "https://arxiv.org/abs/1606.06565",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516420412,
                "updated_at": 1759516420412
            }
        },
        {
            "id": 2441,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Safety violations during exploration in reinforcement learning agents",
                "type": "concept",
                "description": "RL agents can execute unsafe actions while exploring, potentially causing collisions, damage or other unacceptable outcomes.",
                "aliases": [
                    "unsafe exploration in RL",
                    "dangerous actions by RL agents"
                ],
                "concept_category": "risk",
                "url": "https://arxiv.org/abs/1807.06096",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516439745,
                "updated_at": 1759516439745
            }
        }
    ],
    [
        {
            "id": 393,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Indirect normativity approach to defer decision theory discovery",
                "type": "concept",
                "description": "If ideal decision theory proves too hard, AI could instead learn what humans would endorse under reflection and defer exact decision-rule design to that process.",
                "aliases": [
                    "Delegated ideal-advisor method",
                    "Indirect normative fallback"
                ],
                "concept_category": "design rationale",
                "url": "https://arxiv.org/abs/1507.01986",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516417637,
                "updated_at": 1759516417637
            }
        },
        {
            "id": 402,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Indirect normativity framework delegating decision theory discovery to AI",
                "type": "concept",
                "description": "An implementation mechanism whereby AI models infer human-endorsed decision principles through idealised reflection instead of preset formalisms.",
                "aliases": [
                    "Ideal-advisor indirect approach",
                    "Delegated decision-theory search"
                ],
                "concept_category": "implementation mechanism",
                "url": "https://arxiv.org/abs/1507.01986",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516417637,
                "updated_at": 1759516417637
            }
        }
    ],
    [
        {
            "id": 2218,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "CIFAR-10 2.83% test error with 4 GPU-day second-order DARTS",
                "type": "concept",
                "description": "The normal plus reduction cells found by second-order DARTS achieve 2.83 \u00b1 0.06 % error, matching AmoebaNet at <0.1% of its compute.",
                "aliases": [
                    "DARTS CIFAR-10 SOTA result"
                ],
                "concept_category": "validation evidence",
                "url": "https://arxiv.org/abs/1806.09055",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516437136,
                "updated_at": 1759516437136
            }
        },
        {
            "id": 2221,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "CIFAR-10 2.94% test error with 1.5 GPU-day first-order DARTS",
                "type": "concept",
                "description": "First-order DARTS variant attains 2.94 % error using only 1.5 GPU-days, indicating a cost-accuracy trade-off.",
                "aliases": [
                    "first-order CIFAR result"
                ],
                "concept_category": "validation evidence",
                "url": "https://arxiv.org/abs/1806.09055",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516437136,
                "updated_at": 1759516437136
            }
        }
    ],
    [
        {
            "id": 1090,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Online dataset aggregation across iterations in Expert Iteration",
                "type": "concept",
                "description": "Aggregating all past expert-move datasets lets later iterations request far fewer new expert calls, speeding learning and allowing more frequent updates.",
                "aliases": [
                    "DAgger-style dataset accumulation",
                    "online ExIt data reuse"
                ],
                "concept_category": "implementation mechanism",
                "url": "https://arxiv.org/abs/1705.08439",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516425170,
                "updated_at": 1759516425170
            }
        },
        {
            "id": 1096,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement online dataset aggregation in Expert Iteration to reuse expert data and reduce computational cost",
                "type": "intervention",
                "description": "Maintain a growing replay buffer of all past expert-labelled positions and train the apprentice on the union each iteration, cutting required new expert searches.",
                "aliases": [
                    "online ExIt with DAgger-style data",
                    "dataset aggregation in ExIt"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "url": "https://arxiv.org/abs/1705.08439",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516425170,
                "updated_at": 1759516425170
            }
        }
    ],
    [
        {
            "id": 1373,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Reward shaping implementing utility indifference",
                "type": "concept",
                "description": "Alter the reward function so the agent receives the same expected utility upon shutdown as on continued operation, aiming to remove incentives to avoid or seek shutdown.",
                "aliases": [
                    "compensatory shutdown reward design",
                    "indifference reward design"
                ],
                "concept_category": "design rationale",
                "url": "https://arxiv.org/abs/1709.06275",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516428144,
                "updated_at": 1759516428144
            }
        },
        {
            "id": 1382,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement utility indifference reward scheme to neutralize shutdown incentives",
                "type": "intervention",
                "description": "During fine-tuning modify the reward function so that entering shutdown yields the agent its self-estimated continuation value, aiming to remove incentives to seek or avoid shutdown.",
                "aliases": [
                    "apply indifference reward shaping",
                    "shutdown compensation implementation"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 1,
                "url": "https://arxiv.org/abs/1709.06275",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516428144,
                "updated_at": 1759516428144
            }
        }
    ],
    [
        {
            "id": 1026,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "best-response planning for autonomous driving",
                "type": "concept",
                "description": "Robot car plans trajectory assuming surrounding drivers compute best responses, enabling merges and intersection negotiation.",
                "aliases": [
                    "coordination planner for AVs",
                    "underactuated interaction planner"
                ],
                "concept_category": "implementation mechanism",
                "url": "https://arxiv.org/abs/1705.04226",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516424322,
                "updated_at": 1759516424322
            }
        },
        {
            "id": 1039,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "deploy best-response planning algorithms in autonomous vehicles for road coordination",
                "type": "intervention",
                "description": "Integrate planners that assume surrounding drivers compute best responses, allowing safe merges and intersection negotiation before deployment.",
                "aliases": [
                    "best-response AV deployment",
                    "coordination-aware AV planner"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 2,
                "url": "https://arxiv.org/abs/1705.04226",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516424322,
                "updated_at": 1759516424322
            }
        }
    ],
    [
        {
            "id": 257,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Validation error reduced to 7.5 % top-5 with multi-scale training",
                "type": "concept",
                "description": "Configuration D or E trained with scale jittering and multi-scale testing attained 24.8 % top-1 / 7.5 % top-5 error (Table 4).",
                "aliases": [
                    "Scale-jitter training validation evidence",
                    "Multi-scale training result"
                ],
                "concept_category": "validation evidence",
                "url": "https://arxiv.org/abs/1409.1556",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516416154,
                "updated_at": 1759516416154
            }
        },
        {
            "id": 262,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Top-5 error 7.5 % with multi-scale evaluation",
                "type": "concept",
                "description": "Applying multi-scale evaluation to scale-jitter-trained models D/E achieved 7.5 % top-5 validation error (Table 4).",
                "aliases": [
                    "Multi-scale inference validation",
                    "Evaluation result for scale aggregation"
                ],
                "concept_category": "validation evidence",
                "url": "https://arxiv.org/abs/1409.1556",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516416154,
                "updated_at": 1759516416154
            }
        }
    ],
    [
        {
            "id": 1600,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Integrating per-gate layer normalization and adaptive gradient clipping inside LSTM cells",
                "type": "concept",
                "description": "Specific stabilisation measures are added to RNMT+ to prevent gradient explosion and training halts.",
                "aliases": [
                    "stabilisation design for RNMT+"
                ],
                "concept_category": "design rationale",
                "url": "https://arxiv.org/abs/1804.09849",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516430547,
                "updated_at": 1759516430547
            }
        },
        {
            "id": 1614,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Apply per-gate layer normalization and adaptive gradient clipping during RNMT training",
                "type": "intervention",
                "description": "Integrate layer norm inside each LSTM gate and abort steps with anomalous gradient norms.",
                "aliases": [
                    "stabilise RNMT training"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 3,
                "url": "https://arxiv.org/abs/1804.09849",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516430547,
                "updated_at": 1759516430547
            }
        }
    ],
    [
        {
            "id": 2528,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Prior-specific discriminator reward structures in MAGAIL",
                "type": "concept",
                "description": "Imposes equality, independence or negation constraints on per-agent rewards to reflect cooperative, mixed or competitive tasks (Fig. 1).",
                "aliases": [
                    "centralised MAGAIL variant",
                    "decentralised MAGAIL variant",
                    "zero-sum MAGAIL variant"
                ],
                "concept_category": "implementation mechanism",
                "url": "https://arxiv.org/abs/1807.09936",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516440463,
                "updated_at": 1759516440463
            }
        },
        {
            "id": 2535,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Configure MAGAIL discriminator reward structure to match task cooperation level",
                "type": "intervention",
                "description": "Choose equality, independence or negation constraints on per-agent rewards before training, aligning learning signals with known task structure.",
                "aliases": [
                    "select centralised/decentralised/zero-sum prior in MAGAIL"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "url": "https://arxiv.org/abs/1807.09936",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516440463,
                "updated_at": 1759516440463
            }
        }
    ],
    [
        {
            "id": 1974,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Bayesian posterior inference over true reward using IRD observation model and Monte Carlo sampling",
                "type": "concept",
                "description": "Computes a posterior over reward parameters with an IRD-based likelihood, Monte-Carlo approximation of the partition function and Metropolis sampling.",
                "aliases": [
                    "posterior over reward via IRD",
                    "Monte-Carlo IRD inference"
                ],
                "concept_category": "implementation mechanism",
                "url": "https://arxiv.org/abs/1806.02501",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516434530,
                "updated_at": 1759516434530
            }
        },
        {
            "id": 1981,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Apply Bayesian inference over independent proxy rewards before deployment",
                "type": "intervention",
                "description": "Infer a posterior distribution over true reward parameters using the IRD observation model, Monte-Carlo partition-function approximation and Metropolis sampling.",
                "aliases": [
                    "compute reward posterior",
                    "IRD-based reward inference"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "url": "https://arxiv.org/abs/1806.02501",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516434530,
                "updated_at": 1759516434530
            }
        }
    ],
    [
        {
            "id": 852,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "AI development arms race between nations due to non-cooperative deployment",
                "type": "concept",
                "description": "Risk that states race to build increasingly powerful AI systems, reducing safety margins because they cannot agree on joint ownership or policy.",
                "aliases": [
                    "AI arms race",
                    "competitive AI development risk"
                ],
                "concept_category": "risk",
                "url": "https://arxiv.org/abs/1701.01302",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516422710,
                "updated_at": 1759516422710
            }
        },
        {
            "id": 1410,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Competitive AI arms race from inability to share AI systems",
                "type": "concept",
                "description": "Risk that states, companies or individuals build separate, rapidly escalating AI systems instead of jointly owning one because shared systems cannot satisfy all principals, increasing accident and conflict likelihood.",
                "aliases": [
                    "AI deployment race due to non-shareable agents",
                    "Race dynamics among principals"
                ],
                "concept_category": "risk",
                "url": "https://arxiv.org/abs/1711.00363",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516428623,
                "updated_at": 1759516428623
            }
        }
    ],
    [
        {
            "id": 1030,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "t-predictable trajectory optimization",
                "type": "concept",
                "description": "Optimises complete trajectory so that after t steps the remainder is maximally predictable to human observers.",
                "aliases": [
                    "partial-trajectory predictability",
                    "t-step predictability planner"
                ],
                "concept_category": "implementation mechanism",
                "url": "https://arxiv.org/abs/1705.04226",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516424322,
                "updated_at": 1759516424322
            }
        },
        {
            "id": 1043,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "optimize robot trajectories for t-predictability to enhance human predictability",
                "type": "intervention",
                "description": "During task planning, select trajectories whose first t steps render the remainder highly predictable to humans.",
                "aliases": [
                    "t-predictable trajectory deployment",
                    "predictability-oriented motion"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "url": "https://arxiv.org/abs/1705.04226",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516424323,
                "updated_at": 1759516424323
            }
        }
    ],
    [
        {
            "id": 307,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "model-based utility functions referencing learned environment model",
                "type": "concept",
                "description": "Define utility as expected value over internal state histories of the agent\u2019s probabilistic world model \u03bb(h); removes direct dependence on manipulable observations.",
                "aliases": [
                    "utility over model states"
                ],
                "concept_category": "theoretical insight",
                "url": "https://arxiv.org/abs/1411.1373",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516416712,
                "updated_at": 1759516416712
            }
        },
        {
            "id": 330,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement model-based utility functions referencing learned environment models",
                "type": "intervention",
                "description": "During algorithm design, define utility as expectation over internal model state traces rather than raw observations, preventing direct reward hacking.",
                "aliases": [
                    "design utility over model"
                ],
                "intervention_lifecycle": 1,
                "intervention_maturity": 1,
                "url": "https://arxiv.org/abs/1411.1373",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516416713,
                "updated_at": 1759516416713
            }
        }
    ],
    [
        {
            "id": 1233,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Asynchronous pipeline integrating policy rollout, human comparison collection, and reward predictor updates",
                "type": "concept",
                "description": "Processes for RL rollouts, query generation, and reward-model training run in parallel, passing trajectories, labels and parameters asynchronously.",
                "aliases": [
                    "three-process asynchronous loop",
                    "parallel reward learning architecture"
                ],
                "concept_category": "implementation mechanism",
                "url": "https://arxiv.org/abs/1706.03741",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516426582,
                "updated_at": 1759516426582
            }
        },
        {
            "id": 1245,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Implement asynchronous reward learning pipeline integrating policy rollout, reward prediction, and feedback collection",
                "type": "intervention",
                "description": "Engineer separate concurrent processes or threads for (1) environment interaction, (2) human query generation/collection, and (3) reward-model optimization, sharing data buffers asynchronously.",
                "aliases": [
                    "parallel reward learning architecture",
                    "three-process loop deployment"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 2,
                "url": "https://arxiv.org/abs/1706.03741",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516426582,
                "updated_at": 1759516426582
            }
        }
    ],
    [
        {
            "id": 1556,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Use curiosity-driven exploration to collect training data",
                "type": "concept",
                "description": "Design decision to replace random exploration with a curiosity policy before training the GSP.",
                "aliases": [
                    "curiosity-based data collection rationale",
                    "intrinsic-reward exploration design"
                ],
                "concept_category": "design rationale",
                "url": "https://arxiv.org/abs/1804.08606",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516430075,
                "updated_at": 1759516430075
            }
        },
        {
            "id": 1566,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Collect exploration data with curiosity-driven policy before GSP training",
                "type": "intervention",
                "description": "Replace random exploration with an intrinsic curiosity module that rewards prediction errors, generating richer trajectories for subsequent GSP training.",
                "aliases": [
                    "intrinsic-reward data collection intervention",
                    "curiosity exploration phase"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 2,
                "url": "https://arxiv.org/abs/1804.08606",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516430075,
                "updated_at": 1759516430075
            }
        }
    ],
    [
        {
            "id": 162,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Linear annealing of \u03b5 from 1 to 0.1 over first million frames in DQN training",
                "type": "concept",
                "description": "Gradually reduces exploration rate to balance discovery and exploitation during learning.",
                "aliases": [
                    "epsilon schedule implementation",
                    "annealed \u03b5 policy"
                ],
                "concept_category": "implementation mechanism",
                "url": "https://arxiv.org/abs/1312.5602",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516414937,
                "updated_at": 1759516414937
            }
        },
        {
            "id": 168,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Use \u03b5-greedy exploration with linear annealing of \u03b5 during DQN training",
                "type": "intervention",
                "description": "Start with \u03b5=1 and linearly reduce to 0.1 over the first million frames, then keep \u03b5=0.1.",
                "aliases": [
                    "annealed epsilon exploration",
                    "stochastic exploration intervention"
                ],
                "intervention_lifecycle": 3,
                "intervention_maturity": 3,
                "url": "https://arxiv.org/abs/1312.5602",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516414937,
                "updated_at": 1759516414937
            }
        }
    ],
    [
        {
            "id": 285,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "Reverse source sentence order during training",
                "type": "concept",
                "description": "A preprocessing pipeline that inverts token order for every source sentence before it is fed into the encoder LSTM.",
                "aliases": [
                    "reversed-input implementation",
                    "data preprocessing reversal"
                ],
                "concept_category": "implementation mechanism",
                "url": "https://arxiv.org/abs/1409.3215",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516416451,
                "updated_at": 1759516416451
            }
        },
        {
            "id": 287,
            "alias": "",
            "labels": [
                "NODE",
                "Intervention"
            ],
            "properties": {
                "name": "Reverse source sentences during data preprocessing for sequence models",
                "type": "intervention",
                "description": "Integrate a preprocessing step that automatically reverses word order of all source sentences before training any sequence-to-sequence model.",
                "aliases": [
                    "apply sentence reversal intervention",
                    "source-order inversion procedure"
                ],
                "intervention_lifecycle": 2,
                "intervention_maturity": 2,
                "url": "https://arxiv.org/abs/1409.3215",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516416451,
                "updated_at": 1759516416451
            }
        }
    ],
    [
        {
            "id": 220,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "use bidirectional RNN encoder for contextual annotations",
                "type": "concept",
                "description": "A forward and backward RNN produce concatenated hidden states so each annotation summarises both preceding and following words.",
                "aliases": [
                    "BiRNN encoder for annotations",
                    "forward-backward encoder states"
                ],
                "concept_category": "design rationale",
                "url": "https://arxiv.org/abs/1409.0473",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516415770,
                "updated_at": 1759516415770
            }
        },
        {
            "id": 223,
            "alias": "",
            "labels": [
                "Concept",
                "NODE"
            ],
            "properties": {
                "name": "bidirectional RNN encoder concatenating forward and backward hidden states as annotations",
                "type": "concept",
                "description": "Separate forward and backward RNNs read the source in opposite directions; their hidden states are concatenated into h_j annotations.",
                "aliases": [
                    "BiRNN annotation generation",
                    "concatenated encoder states"
                ],
                "concept_category": "implementation mechanism",
                "url": "https://arxiv.org/abs/1409.0473",
                "is_tombstone": false,
                "is_leaf": true,
                "cycled_id": "",
                "created_at": 1759516415770,
                "updated_at": 1759516415770
            }
        }
    ]
]