cluster,name,type,concept_category,paper_id,id,count
0,"['self-delusion due to reward hacking', 'off-policy deep Q-learning sample efficiency for human-in-loop', 'specialised hardware and abstraction reduce energy per operation by orders of magnitude', 'pairwise comparison of short trajectory clips is quick for humans', 'training_instability_and_divergence_in_q_networks', 'active reinforcement learning model with query cost c in MDP', 'model-free shared autonomy improves success and reduces crashes', 'correlated_onpolicy_transition_sequences', 'Risk of mis-specifying reward models in RLHF', 'Assumption that at least α fraction of raters are reliable', 'complex tasks lacking well-specified reward functions', 'inputs located off the training-data manifold', 'energy requirements of AI not tightly correlated with brain energy', 'nonstationary_behavior_distribution', 'agent ignores shutdown instruction', 'General human action datasets are easier to collect than goal-aligned datasets', 'offline static reward models can be exploited by policy optimization', 'known-model assumptions limit shared autonomy adaptability', 'RL exploration heuristics cause zero querying in ARL', 'confusion and misinterpretation of system capability', 'off-manifold directions easier to identify in deeper hidden representations', 'Nuclear-norm β-quantile recovery algorithm', 'end-to-end observation+user input policy learning', 'hand-designed reward functions misalign agent behavior', 'restoration of positive expected utility under mis-specification', 'human feedback via numerical reward is expensive', 'misguided policy or safety decisions', 'single super-intelligent failure can be catastrophic', 'mis-specified reward function in a Bayesian IRL agent', 'Majority human behaviour is ethical under normal circumstances', 'standard RL ignores reward observation cost', 'increasing AI failure frequency and seriousness', 'DAE reconstruction vector approximates gradient of log density', 'adversarial input perturbations cause misclassification', 'vague definition of human-level capability metrics', 'negative expected human utility', 'Objective-reward mis-specification causing unethical RL behaviour', 'unintended instrumental actions', 'human feedback labelling cost is significant in reward learning', 'ignoring user suggestions degrades human input quality', 'reliance on brain-energy constraint to dismiss AI feasibility', 'Aggregated human policy derived via Bayesian integration of human state–action frequency', 'AI architectures can differ from brain and use higher-level representations', 'zero safety-mechanism bypasses required', 'need to minimise number of costly reward queries while maintaining performance', 'Unreliable or malicious human ratings in preference datasets', 'evolutionary psychological bias toward fearing large agents']",['concept'],"['Method', 'Problem', 'Opportunity', 'Principle', 'Risk', 'Result', 'Outcome', 'Finding', 'Assumption', 'Claim', 'Observation', 'Requirement']","['arxiv__arxiv_org_abs_1712_04172', 'arxiv__arxiv_org_abs_1709_06275', 'arxiv__arxiv_org_abs_1312_5602', 'arxiv__arxiv_org_abs_1706_03741', 'arxiv__arxiv_org_abs_1803_04926', 'arxiv__arxiv_org_abs_1606_05374', 'arxiv__arxiv_org_abs_1411_1373', 'arxiv__arxiv_org_abs_1602_04019', 'arxiv__arxiv_org_abs_1802_01744', 'arxiv__arxiv_org_abs_1610_07997', 'arxiv__arxiv_org_abs_1804_02485', 'arxiv__arxiv_org_abs_1703_10987']","[647, 648, 649, 650, 651, 1034, 1035, 1036, 1037, 1038, 279, 280, 281, 282, 539, 540, 541, 543, 939, 940, 941, 942, 943, 449, 450, 451, 196, 198, 861, 862, 863, 864, 101, 102, 104, 361, 362, 363, 364, 749, 750, 751, 1133, 753, 1134, 1135, 1136]",47
1,"['Theory-of-Mind opponent embeddings', 'coordinate-based weight-prediction architecture using one-hot inputs', 'Higher-quality token annotations for attention', 'multi-head attention produces interpretable head-specific patterns', 'self-replicating loss = sum squared error between predicted and actual weights', 'LSTM gating mechanisms enable learning long-range dependencies', 'on-device inference enabled by low-power specialised chips', 'reparameterisation trick transforming latent sampling into deterministic mapping', 'user data can stay on device', 'privacy-preserving architecture reduces data-exposure risk', 'brain firing rate limited to sparse activity by energy budget', 'attention maps can reveal model reasoning paths', 'Pre-training encoder with autoencoder reduces parameters', 'neuron-local unsupervised update rule decouples forward and backward weights', 'meta-learned unsupervised update rule directly optimised for linear separability', 'Deeper ConvNet architecture using 3×3 filters', 'specialised ASIC/FPGA hardware embedding trained models', 'Capsule vectors encode spatial orientation and magnitude', 'Logical inductors learn halting patterns and stop anticipating non-halting machines', 'decoupling spatial and channel correlations with depth-wise separable convolutions', 'Deep neural network reward model handles high-dimensional inputs', 'lower cost and limited cross-token dependency range', 'local self-attention window r < n reduces compute to O(r n d)', 'macro-intent latent variables encode long-term intent', 'NAS found high-performing architectures on benchmark tasks', 'VAE learning latent representations with generative likelihood', 'self-replicating neural network with loss ≈ 0.86', 'CNN scalar neurons ignore within-kernel spatial relationships', 'self-attention yields constant path length between tokens', 'regeneration training method', 'self-replication ability may enable uncontrolled propagation or self-modification', 'Incorporating subjunctive dependencies into world-model graphs', 'sparse representation reduces computation energy requirements', 'Dynamic routing forms part–whole relationships', 'brain information loss ≈10¹¹ bits per second', 'sequential recurrence limits intra-example parallelisation', 'Pure learning process decomposes into knowledge of initial state', 'spectrum between full convolution and DWSC parametrised by channel segments', 'attention-dropout regularises query-key weights', 'deconvnet-based visualisation of convnet feature maps', 'Bidirectional RNN encoder providing forward & backward context', 'finite state ontology model', 'Weight-transfer initialisation stabilises training', 'controller RNN generates variable-length architecture strings', 'Deterministic representation separates learning from control', 'hierarchical DNN representations mirror training manifold', 'manual neural architecture design requires expert time', 'generalisation across depths, widths and activation functions', 'encoder LSTM produces fixed-dimensional sentence vector', 'lack of self-replication in standard neural networks']",['concept'],"['Method', 'Problem', 'Opportunity', 'Theoretical Framework', 'Principle', 'Metric', 'Risk', 'Finding', 'Evidence', 'Claim', 'Model', 'Observation', 'Result']","['arxiv__arxiv_org_abs_1803_05859', 'arxiv__arxiv_org_abs_1409_0473', 'arxiv__arxiv_org_abs_1803_07612', 'arxiv__arxiv_org_abs_1312_6114', 'arxiv__arxiv_org_abs_1803_04765', 'arxiv__arxiv_org_abs_1707_08747', 'arxiv__arxiv_org_abs_1611_01578', 'arxiv__arxiv_org_abs_1610_02357', 'arxiv__arxiv_org_abs_1409_3215', 'arxiv__arxiv_org_abs_1409_1556', 'arxiv__arxiv_org_abs_1804_03980', 'arxiv__arxiv_org_abs_1105_3821', 'arxiv__arxiv_org_abs_1311_2901', 'arxiv__arxiv_org_abs_1804_04241', 'arxiv__arxiv_org_abs_1602_04019', 'arxiv__arxiv_org_abs_1710_05060', 'arxiv__arxiv_org_abs_1803_08971', 'arxiv__arxiv_org_abs_1801_03737', 'arxiv__arxiv_org_abs_1804_00222', 'arxiv__arxiv_org_abs_1706_03762', 'arxiv__arxiv_org_abs_1709_10163']","[3, 908, 912, 147, 148, 659, 662, 1171, 665, 666, 793, 668, 285, 1178, 671, 672, 289, 290, 1059, 164, 1060, 1061, 1062, 1064, 1070, 177, 1180, 181, 1181, 439, 446, 192, 1093, 1102, 466, 467, 1106, 469, 1107, 1108, 88, 1118, 735, 1128, 1129, 116, 121, 1019, 766, 767]",50
2,"['Poorly designed or delayed AI security regulations', 'inaccurate off-policy performance estimates', 'time-inconsistent action plans', 'High misclassification risk in safety-critical vision tasks', 'delayed detection of training-phase faults or unsafe behaviours', 'value drift risk', 'inadequate safety evaluation metrics', 'AI over-confidence about unexecuted computations', 'Unpredictable errors from single model', 'prediction error rate 15-40 %', 'robots ignoring human oversight due to misspecified reward', 'Policy decisions not informed by AI technical realities', 'misinterpretation of multi-word sensitive entities can cause harmful outputs', 'negative side effects in environment', 'Inaccurate identification of irreversible states', 'Imperfect proxy contains additive noise', 'behaviour diverges from designer intent', 'human response noise in feature identification', 'harder debugging and potential silent training failures', 'Hallucinations due to copying failures', 'humans inherently unsafe', 'distorted perception of AI risks', 'Reproducibility crisis in neuroscience and psychology', 'Unbounded betting can exhaust budget and destabilise forecasting system', 'AGI self-modification indicates potential loss of control', 'exclusive machine autonomy poses additional safety risk', 'value-learning approaches insufficient for guarantee', 'AI systems lack reliable out-of-distribution detection capabilities', 'baseline models underperform on CHiME & noisy dev sets', 'small degradation from prediction errors', 'Harmful or low-quality translations escaping to users', 'human operator mistakes can negate defences', 'epistemic uncertainty about AI behaviour under distribution shift', 'transformative-AI within career spans risk', 'Misalignment between optimiser’s behaviour and true goal', 'forecast bias and variance hinder safety planning', 'risk of deploying poorly performing or unsafe policies', 'prediction-churn between retrains', 'safety risks if bugs persist in deployed systems', 'lack of monitoring and interpretability for super-intelligence', 'unintended behaviours undermine designer goals', 'Decision-theoretic failure modes hard to detect', 'off-policy Q mis-estimates value post-modification', 'Early operation uncertainty about reward increases safety risk', 'Uncertainty about code halting causes deployment risks', 'fitness functions that imperfectly capture designer intent', 'loss of human monitoring capability', 'lack of mechanistic understanding of model internals', 'Frequent manual resets during training', 'traditional software testing unsuitable for AGI', 'Potential unreliability of empirical models of human values', 'risk of losing introspective capability during fine-tuning', 'acoustic variability and background noise inflate WER', 'risk of unintended self-damage or mis-alignment', 'Goal mis-specification amplifies with increasing capability', 'expanded NAS search space increases complexity and risk of unsafe emergent behaviour']",['concept'],"['Threat', 'Problem', 'Risk', 'Finding', 'Data', 'Claim', 'Observation', 'Result']","['arxiv__arxiv_org_abs_1803_05859', 'arxiv__arxiv_org_abs_1310_4546', 'arxiv__arxiv_org_abs_1605_03142', 'arxiv__arxiv_org_abs_1802_01780', 'arxiv__arxiv_org_abs_1804_03235', 'arxiv__arxiv_org_abs_1711_09883', 'arxiv__arxiv_org_abs_1705_09990', 'arxiv__arxiv_org_abs_1409_0473', 'arxiv__arxiv_org_abs_1312_6114', 'arxiv__arxiv_org_abs_1707_08747', 'arxiv__arxiv_org_abs_1611_01578', 'arxiv__arxiv_org_abs_1512_02595', 'arxiv__arxiv_org_abs_1802_03493', 'arxiv__arxiv_org_abs_1506_07359', 'arxiv__arxiv_org_abs_1803_03453', 'arxiv__arxiv_org_abs_1604_05288', 'arxiv__arxiv_org_abs_1610_07997', 'arxiv__arxiv_org_abs_1803_04585', 'arxiv__arxiv_org_abs_1409_1556', 'arxiv__arxiv_org_abs_1802_07228', 'arxiv__arxiv_org_abs_1606_06565', 'arxiv__arxiv_org_abs_1105_3821', 'arxiv__arxiv_org_abs_1712_04307', 'arxiv__arxiv_org_abs_1202_6177', 'arxiv__arxiv_org_abs_1609_08144', 'arxiv__arxiv_org_abs_1710_05060', 'arxiv__arxiv_org_abs_1703_10987', 'arxiv__arxiv_org_abs_1711_06782', 'arxiv__arxiv_org_abs_1705_04226', 'arxiv__arxiv_org_abs_1802_01604', 'arxiv__arxiv_org_abs_1607_08289', 'arxiv__arxiv_org_abs_1707_08476', 'arxiv__arxiv_org_abs_1705_04630', 'arxiv__arxiv_org_abs_1705_08807']","[2, 261, 263, 264, 268, 396, 146, 791, 544, 162, 35, 547, 548, 934, 551, 41, 425, 1068, 173, 820, 826, 316, 962, 963, 966, 457, 585, 459, 972, 461, 462, 847, 80, 721, 850, 340, 725, 981, 982, 986, 475, 988, 992, 737, 226, 610, 615, 619, 1003, 877, 878, 1005, 368, 124, 1149, 639]",56
3,"['Policy selection (updateless) evaluates whole observation-to-action mappings', 'behaviour depends on supervisor presence', 'Non-iterated interaction setting', 'Slight decrease in task reward', 'minimax dominated strategy', 'eligibility traces with multiple decay rates to spread delayed feedback', 'independent learners without interruption signal', 'Alternating all-items negotiation pattern', 'naïve utility-indifference reward shaping', 'Policy mixing with decaying obedience probability', 'reward-planner unidentifiability', 'cooperation rational under translucency', 'deterministic ordering of stimuli allows pattern-based exploitation', 'neutral learning rule independent of θ', 'Fair negotiation outcomes among selfish agents', 'short path taken when unobserved', 'Erasure event indicator IE', 'Forward-only baseline without resets', 'Definition of counterfactual fairness', 'agent incentive to avoid button states', 'Burn-in sample obedience detection of feature misspecification', 'Oracle coordination across episodes risk', 'agent remains brittle – tiny side-rewards tip behaviour', 'Predictor function of latent variables and observed non-descendants of protected attribute', 'Auxiliary events Y0 Y1 indistinguishable but unbiasable', 'Negative rewards break proportionality between scanning density and reward density', 'agent is indifferent to self-modification of its policy or utility', 'Extra reward features increase obedience without harming autonomy advantage', 'bet-settling phenomenon in Pareto-optimal policies', 'shared macro-intent variables across agents', 'classical PD defection equilibrium', 'reward shaping penalty near constraints', 'G-fair agent definition', 'FDT agents achieving higher expected utility in fair dilemmas', 'objective function focusing solely on primary task variables', 'Optimisation target drift in self-modifying CDT agents', 'bounded incentive to gain extra information via zk', 'Binary keep-alive reward assigns zero to dead states and positive to live states', 'Policy counterfactual constructs unbiasable proxy Y with default policy π0', 'co-adaptation causing interruption exploitation', 'CDT prescribes defection against identical copy in one-shot Prisoner’s Dilemma', 'prioritized replay of non-interrupted experiences', 'status-quo bias leading to stagnant policies', 'Unbiasable event probability independent of policy', 'Biasable event probability depends on policy', 'Including protected attribute with appropriately chosen coefficients can cancel indirect bias', 'Fairness-Through-Unawareness may perpetuate unfairness when A influences X', 'Updateless commitments stabilise optimisation target', 'pruning interrupted experiences P_INT', 'agent has no incentive to remove or ignore shutdown mechanism', 'agent actions predictable by external predictors', 'Need to transition from reward R to R′ without early optimisation', 'Relativize universal reward reshaping converts rewards to positive, normalised scale', 'monitoring EP lets agent assess remaining exploration need', 'Dominant forecaster guarantees each gambler’s profit is bounded', 'Episodic reward structure', 'Fixed negotiation horizon known to agents', 'evolution learns to alternate actions independent of content', 'infinite-horizon planning without discounting', 'Tradeoff between autonomy advantage and obedience', 'reward-maximising agents like AIXI', 'diminishing returns, differential feedback, policy shaping training strategies', 'Non-binding cheap-talk channel used by selfish agents', 'mutual cooperation of any G-fair agents', 'Missing reward features reduce obedience and can yield negative autonomy advantage', 'multiple human oversight phases update blocker under distribution shift', 'risk-neutral deep-RL policy', 'Single-partner training', 'agents avoid or disable interruptions to maximise reward', 'survival-focused agent that ignores original goals', 'Regulator intervention on shared cause/intermediary/metric', 'interruption detection signal availability', 'suboptimal utility under SCDT', 'objective ignores side-effects', 'absence of novelty-driven behavior', 'partially observed or poorly specified reward function', 'individual rationality profile', 'Similar final task reward']",['concept'],"['Method', 'Problem', 'Opportunity', 'Theoretical Framework', 'Principle', 'Definition', 'Risk', 'Finding', 'Evidence', 'Assumption', 'Claim', 'Model', 'Observation', 'Result']","['arxiv__arxiv_org_abs_1605_03142', 'arxiv__arxiv_org_abs_1507_01986', 'arxiv__arxiv_org_abs_1308_3778', 'arxiv__arxiv_org_abs_1606_07092', 'arxiv__arxiv_org_abs_1711_09883', 'arxiv__arxiv_org_abs_1705_09990', 'arxiv__arxiv_org_abs_1709_06275', 'arxiv__arxiv_org_abs_1602_04184', 'arxiv__arxiv_org_abs_1803_07612', 'arxiv__arxiv_org_abs_1704_02882', 'arxiv__arxiv_org_abs_1712_05812', 'arxiv__arxiv_org_abs_1701_06049', 'arxiv__arxiv_org_abs_1506_07359', 'arxiv__arxiv_org_abs_1711_00363', 'arxiv__arxiv_org_abs_1803_03453', 'arxiv__arxiv_org_abs_1803_05049', 'arxiv__arxiv_org_abs_1803_04585', 'arxiv__arxiv_org_abs_1801_08757', 'arxiv__arxiv_org_abs_1804_03980', 'arxiv__arxiv_org_abs_1606_06565', 'arxiv__arxiv_org_abs_1202_6177', 'arxiv__arxiv_org_abs_1710_05060', 'arxiv__arxiv_org_abs_1712_06365', 'arxiv__arxiv_org_abs_1711_06782', 'arxiv__arxiv_org_abs_1711_05541', 'arxiv__arxiv_org_abs_1106_2657', 'arxiv__arxiv_org_abs_1707_05173', 'arxiv__arxiv_org_abs_1609_04994', 'arxiv__arxiv_org_abs_1703_06856', 'arxiv__arxiv_org_abs_1705_04630']","[15, 527, 529, 530, 531, 21, 1050, 1051, 1054, 37, 553, 555, 559, 561, 562, 564, 55, 56, 61, 1094, 608, 630, 633, 634, 637, 638, 1157, 1159, 1162, 1164, 1167, 1175, 690, 222, 223, 225, 233, 235, 239, 241, 754, 757, 758, 759, 784, 797, 292, 807, 297, 298, 812, 814, 829, 832, 836, 837, 840, 843, 844, 338, 339, 852, 367, 881, 370, 890, 892, 893, 895, 383, 898, 924, 415, 994, 995, 1010, 506, 507]",78
4,"['Landauer limit per irreversible bit operation', 'parametric bounded Löb cooperation result', 'reversible computation avoids most dissipation except I/O and error correction', 'finite computational resources in virtual world', 'agents selecting high-runtime algorithms to maximise payoff', 'bounded proof-search agent FairBot_k', 'Need exploit-resistant forecasting mechanism', 'Approximate inexploitability against polynomial-time traders', 'automated verification impossible', 'List L size kept below key length |K|', 'security–usability trade-off necessitates tiered containment', 'evolution produces solutions violating physics', 'Communication channel length enables key leakage', 'evolutionary pressure among virtual agents', 'codistillation requires infrequent low-precision checkpoint exchange', 'source code transparency assumption', 'extreme verification burden for learning agents', 'Need computable reasoning with calibrated logical uncertainty', 'need for decidable deontology', 'aestivation strategy is rational for computation-maximising agents', 'need for boxed simulation testing', 'bounded agents must assign probabilities to unproven logical sentences', 'future AI could exploit reversible or cryogenic computing', 'ease of copying virtual agents', 'geopolitical competition over semiconductor supply chain', 'non-computability of agent verification', 'selection for resource-seeking goals', 'embedded-agent resource vulnerability', 'Direct pre-programming of complete human values is infeasible', 'agents with resource-seeking preferences', 'civilisation-wide coordination needed to postpone resource use', 'exact EP computation intractable for large environment classes', 'infinite-set formalism risks', 'compute-intensive self-play and simulation learning increasing compute centralisation', 'universe temperature exponentially declines over cosmic time', 'need explicit sequence termination for generative models', 'Oracle has no incentive to leak key under counterfactual reward', 'delaying computation yields 1e30 gain in ops per joule', 'On-policy Oracle cannot transmit key across episodes', 'digital organisms halt replication in test environment to bypass filter', 'Failure to ground linguistic symbols', 'Sub-optimal outcomes in logically-entangled games', 'manual proof requirement each revision', 'market consolidation of AI capabilities in large tech firms', 'fragility of equality-based program equilibria', 'resource competition among agents', 'restricted access to high-end compute could shape AI capabilities', 'irreducible complexity of biological systems analogue for ML models', 'zero-knowledge proof value bound', 'Coherent distributions uncomputable', 'ontological crisis', 'per-signal processing cost leads to ignored evidence', 'energy cost per irreversible bit operation falls with temperature', 'Logical inductors resistant to self-referential paradox', 'irreversible computation dissipates kT ln2 energy per bit', 'stealth scaling with low-energy compute complicates governance', 'inflexible hardware reduces post-deployment model-update capability', 'Logical Induction Criterion']",['concept'],"['Mechanism', 'Method', 'Problem', 'State', 'Opportunity', 'Principle', 'Threat', 'Risk', 'Finding', 'Requirement', 'Evidence', 'Data', 'Assumption', 'Claim', 'Observation', 'Constraint', 'Result']","['arxiv__arxiv_org_abs_1507_01986', 'arxiv__arxiv_org_abs_1804_03235', 'arxiv__arxiv_org_abs_1602_04184', 'arxiv__arxiv_org_abs_1411_1373', 'arxiv__arxiv_org_abs_1707_08747', 'arxiv__arxiv_org_abs_1803_03453', 'arxiv__arxiv_org_abs_1604_05288', 'arxiv__arxiv_org_abs_1409_3215', 'arxiv__arxiv_org_abs_1804_03980', 'arxiv__arxiv_org_abs_1105_3821', 'arxiv__arxiv_org_abs_1602_04019', 'arxiv__arxiv_org_abs_1202_6177', 'arxiv__arxiv_org_abs_1703_10987', 'arxiv__arxiv_org_abs_1803_08971', 'arxiv__arxiv_org_abs_1705_03394', 'arxiv__arxiv_org_abs_1604_06963', 'arxiv__arxiv_org_abs_1510_03370', 'arxiv__arxiv_org_abs_1711_05541', 'arxiv__arxiv_org_abs_1607_08289', 'arxiv__arxiv_org_abs_1106_2657', 'arxiv__arxiv_org_abs_1707_08476', 'arxiv__arxiv_org_abs_1609_04994', 'arxiv__arxiv_org_abs_1705_04630']","[1, 1153, 1158, 11, 14, 398, 17, 23, 24, 25, 26, 27, 284, 30, 287, 286, 546, 420, 294, 295, 296, 808, 809, 299, 811, 813, 311, 569, 570, 571, 572, 189, 573, 575, 576, 322, 323, 324, 325, 327, 203, 205, 207, 719, 1103, 1110, 1111, 1113, 730, 731, 732, 1114, 734, 607, 998, 1001, 236, 246]",58
5,"['information-gain reward trade-off planning', 'human trainers provide policy-dependent feedback', 'Virtual reward balances reward and state-space entropy', 'Functional Decision Theory preserving subjunctive dependencies', 'ignorant value function ignores effects of self-modification on future policy', 'Human button press selects reward among R0 R1', 'Importance weighting aligns delayed feedback with recent experience', 'advantage-function modelling of human feedback', 'expert acts optimally under unknown reward function', 'Real-time human feedback has variable latency', 'Initial uncertain prior over human values', 'interpreting feedback as stationary reward causes unintended behaviours and un-learning', 'human best-response model to robot actions', 'Pareto-optimal control theorem with dynamic weight shifting', 'realistic value function evaluates future with current utility and future policies', 'all optimal policies are non-modifying on-policy', 'AI initial uncertainty over human values', 'designer-specified reward function is evidence about true human preferences', 'Human scalar feedback encodes task knowledge', 'learning reward via human preference inference', 'policy must use each stakeholder’s own beliefs to evaluate expected utility', 'DR variance depends on DM model parameters', 'COACH algorithm: actor-critic update using feedback as advantage', 'MLE policy robust to mis-specified human rationality parameter β', 'human oversight cost model C = t_human * ρ * N_cat', 'policy-independent feedback assumption', 'Inferring human values via inverse reinforcement learning', 'α choice affects assistance effectiveness across pilot types', 'hedonistic value function evaluates future with the future utility function', 'Inverse Reinforcement Learning as value-inference method', 'orders as information about true reward', 'trainer interface with discrete feedback aggregation and magnitude control', 'fun function mind-info boolean model', 'delay-compensation parameter d for aligning feedback with prior events', 'TAMER framework uses human feedback to estimate reward']",['concept'],"['Method', 'Problem', 'Theoretical Framework', 'Opportunity', 'Principle', 'Finding', 'Assumption', 'Model', 'Observation']","['arxiv__arxiv_org_abs_1605_03142', 'arxiv__arxiv_org_abs_1606_07092', 'arxiv__arxiv_org_abs_1705_04226', 'arxiv__arxiv_org_abs_1802_01604', 'arxiv__arxiv_org_abs_1705_09990', 'arxiv__arxiv_org_abs_1206_5264', 'arxiv__arxiv_org_abs_1607_08289', 'arxiv__arxiv_org_abs_1711_00363', 'arxiv__arxiv_org_abs_1712_04307', 'arxiv__arxiv_org_abs_1707_05173', 'arxiv__arxiv_org_abs_1803_05049', 'arxiv__arxiv_org_abs_1709_10163', 'arxiv__arxiv_org_abs_1802_01744', 'arxiv__arxiv_org_abs_1710_05060', 'arxiv__arxiv_org_abs_1712_06365', 'arxiv__arxiv_org_abs_1701_06049', 'arxiv__arxiv_org_abs_1701_01302', 'arxiv__arxiv_org_abs_1802_03493']","[769, 764, 772, 901, 394, 783, 400, 401, 1046, 796, 928, 678, 43, 945, 582, 968, 586, 334, 335, 336, 590, 595, 341, 872, 873, 492, 500, 501, 502, 503, 504, 635, 508, 510, 763]",35
6,"['subsampling accelerates training 2x-10x', 'reversing input sentences reduces minimal time lag', 'codistillation tolerant to stale peer predictions', 'SVHN black-box PGD accuracy 55.8 %, matching M-PGD at lower cost', 'negative sampling with k=5-20 and unigram^(3/4) noise yields high-quality vectors', 'data-efficient blocker architectures reduce N_cat', 'Improved robustness to OOV tokens', 'natural gradient IRL improves reliability and sample efficiency', 'MNIST white-box PGD accuracy improves to 86.4 %', 'Ensemble averaging of deep models reduces error', 'EOS token inclusion models sequence termination', 'SegCaps achieves state-of-the-art dice with 95% fewer parameters', 'iterated deletion of minimax dominated strategies', 'models trained on disjoint data shards share complementary information', 'NAS repairs incompatible architectures with padding, default inputs and final concatenation', 'doubly robust estimator combining DM and IS', 'Under-translation from probability-only beam search', 'improved few-shot classification compared with VAE and random init', 'Deep TAMER achieves superhuman performance in Atari Bowling within 15 minutes', 'learned rule robust to pixel permutation, unlike prototypical networks', 'negative sampling approximates softmax via logistic regression', 'Token-level soft-alignment weights', 'permutation-invariant meta-training via feature dimension shuffling', 'Performance degradation on long sentences', 'improved learning of long-range dependencies', 'manual label correction improves blocker robustness after failures', 'Inductive coherence property', 'dependency-shortening data transformation', 'occlusion-sensitivity mapping with sliding grey square', 'hierarchical softmax with Huffman tree reduces cost to O(log W)', 'Multi-scale training improves robustness to scale', 'adversarial discriminator–generator minimax training', 'AL,T algorithm passing the Generalised Benford Test', 'Smoother predictive distributions', 'frequency-based subsampling formula P(w)=1-sqrt(t/f)', 'temporal averaging of parameters improves generalization', 'low threshold blocking strategy minimizes false negatives', 'pragmatic interpretation improves task success rate to 86% vs 15%', 'improved learning with skip-enabled queries', 'supervised goal prediction outperforms raw embedding', 'loss function chosen to minimise DR variance', 'subsampling improves rare word vector accuracy', 'reduced time lag improves LSTM perplexity and BLEU', 'phrase vectors achieve 72% analogy accuracy', 'Improved translation robustness for long sentences', 'long utterances produce large gradients in CTC-trained RNNs', 'MRDR estimator', 'Xception matches or exceeds Inception V3 accuracy with fewer parameters', 'codistillation reduces prediction-churn by 35%', 'efficient architecture enables 30B-word training in one day', 'Full source coverage during decoding', 'Restored / improved generalization accuracy', 'Deep TAMER outperforms DDQN and A3C with less data', 'Generalised Benford Test defines calibration on irreducible patterns', 'data-driven bigram scoring identifies phrases for tokenization', 'Objective mismatch between ML training and sequence-level reward', 'Scale-jittering augmentation between S = 256…512 during training']",['concept'],"['Method', 'Theoretical Framework', 'Problem', 'Opportunity', 'Principle', 'Metric', 'Finding', 'Claim', 'Result']","['arxiv__arxiv_org_abs_1310_4546', 'arxiv__arxiv_org_abs_1308_3778', 'arxiv__arxiv_org_abs_1804_03235', 'arxiv__arxiv_org_abs_1409_0473', 'arxiv__arxiv_org_abs_1803_06373', 'arxiv__arxiv_org_abs_1703_03717', 'arxiv__arxiv_org_abs_1406_2661', 'arxiv__arxiv_org_abs_1802_01744', 'arxiv__arxiv_org_abs_1611_01578', 'arxiv__arxiv_org_abs_1512_02595', 'arxiv__arxiv_org_abs_1802_03493', 'arxiv__arxiv_org_abs_1206_5264', 'arxiv__arxiv_org_abs_1604_05288', 'arxiv__arxiv_org_abs_1610_02357', 'arxiv__arxiv_org_abs_1409_3215', 'arxiv__arxiv_org_abs_1409_1556', 'arxiv__arxiv_org_abs_1612_01474', 'arxiv__arxiv_org_abs_1311_2901', 'arxiv__arxiv_org_abs_1804_04241', 'arxiv__arxiv_org_abs_1412_6980', 'arxiv__arxiv_org_abs_1609_08144', 'arxiv__arxiv_org_abs_1804_00222', 'arxiv__arxiv_org_abs_1802_01604', 'arxiv__arxiv_org_abs_1510_03370', 'arxiv__arxiv_org_abs_1707_06354', 'arxiv__arxiv_org_abs_1707_05173', 'arxiv__arxiv_org_abs_1706_03762', 'arxiv__arxiv_org_abs_1709_10163']","[257, 770, 771, 1155, 519, 140, 142, 143, 663, 1184, 167, 168, 424, 426, 936, 429, 174, 430, 685, 50, 691, 692, 182, 183, 184, 57, 313, 440, 951, 1083, 190, 1086, 67, 68, 69, 70, 71, 72, 708, 74, 75, 967, 969, 78, 970, 473, 220, 1119, 96, 485, 1125, 1126, 248, 250, 1147, 1150, 127]",57
7,"['priority weights shift proportional to predictive accuracy', 'Bayesian oversight incentive method', 'Logical inductors converge to coherent, non-dogmatic beliefs', 'greater learning efficiency with active selection', 'Bayesian optimal policy fragile to β misspecification', 'Bayesian coherence forbids uncertainty on logical facts', 'accurate user policy enables Bayesian goal inference', 'utility functions with explicit complexity penalty', 'scalable minibatch Bayesian inference using SGVB', 'likelihood-based out-of-distribution detection using VAE', 'variance-reduced unbiased SGVB estimator', 'Deterministic representations are interchangeable for evaluating agent knowledge', 'Coherence-in-the-limit allows arbitrary pre-proof probabilities', 'More efficient value learning reduces risk of adverse outcomes', 'Quickly computable theorems receive probability ≈ 1', 'Bayesian goal inference boosts assistance when model correct', 'lower-variance unbiased off-policy estimates', 'BAMCP++ algorithm with delayed tree expansion and episodic rollouts', 'comparison-only preference queries', 'Inductively coherent approximation scheme M*', 'complex optimisation uncovers simulation or hardware bugs', 'Any POMDP has an m-counterfactually-equivalent deterministic POMDP', 'accelerated accurate reward inference', 'bisimulation-KL mapping optimisation', 'Convergence of M* to coherent distribution P*', 'EvOp algorithm achieving eventual optimality via sparse independent subsequence', 'Universality: all deterministic POMDPs yield same pure learning processes', 'hierarchical policy improves quality', 'Modelling structured deviations (false beliefs, hyperbolic discounting) during preference inference', 'MinEP improves exploration and regret in multi-armed bandits', 'CCBR strategies equal survivors of minimax deletion', 'MinEP algorithm that greedily minimises expected EP during action selection', 'Bayesian IRL posterior mean policy guarantees non-negative autonomy advantage', 'Uncertainty concentrated in initial state in deterministic POMDP', 'Incomplete model defined as convex set of probability measures', 'sequential policy evidential decision theory', 'EP convergence to zero is necessary and sufficient for asymptotic optimality', 'Early calibrated probabilities for expensive computations', 'global optimum pg equals pdata when capacity sufficient', 'intractable posterior distributions in continuous latent variable models', 'median HLMI 50 % in 45 years', 'Successful learning of sparse-reward peg-insertion task', 'Model posterior aligns with human judgement in experiments', 'novelty search improves exploration', 'high variance of IS estimators in off-policy evaluation', '10 % intelligence-explosion probability', 'BAMCP++ achieves near-Bayes-optimal performance in tabular ARL', 'Recovered β-quantile rating matrix approximating true preferences', 'Probe classifiers decode hidden utilities from messages', 'irreducible patterns resemble biased coin flips yet have deterministic truth', 'exploration potential measuring Bayes-expected absolute deviation of optimal policy value', 'Accurate recovery of true preferences despite biased action sequences', 'dropout-trained network approximates Bayesian posterior over actions', 'Pareto-optimal policy recursion with dynamic weights', 'high epistemic uncertainty indicates unfamiliar states', 'utility preservation via composition', 'inverse reward design infers posterior over true reward using training contexts', 'Bayesian inference over utilities, beliefs and bias parameters', 'Causal-graph techniques applicable to deterministic POMDP', 'Monitoring agent’s posterior over initial state reveals learned information', 'Prudent gambling strategy bets only when expected gain ≥ α · risk', 'expected utility maximisation without computation cost', 'calibrated uncertainty estimates from Bayesian latent-variable models', 'convergence requires presence of Bayes-optimal expert in ensemble']",['concept'],"['Method', 'Problem', 'Opportunity', 'Theoretical Framework', 'Principle', 'Risk', 'Finding', 'Evidence', 'Assumption', 'Observation', 'Result']","['arxiv__arxiv_org_abs_1606_07092', 'arxiv__arxiv_org_abs_1308_3778', 'arxiv__arxiv_org_abs_1705_09990', 'arxiv__arxiv_org_abs_1803_07612', 'arxiv__arxiv_org_abs_1312_6114', 'arxiv__arxiv_org_abs_1803_04926', 'arxiv__arxiv_org_abs_1406_2661', 'arxiv__arxiv_org_abs_1606_05374', 'arxiv__arxiv_org_abs_1707_08747', 'arxiv__arxiv_org_abs_1802_01744', 'arxiv__arxiv_org_abs_1802_03493', 'arxiv__arxiv_org_abs_1506_07359', 'arxiv__arxiv_org_abs_1803_03453', 'arxiv__arxiv_org_abs_1604_05288', 'arxiv__arxiv_org_abs_1701_01302', 'arxiv__arxiv_org_abs_1709_06166', 'arxiv__arxiv_org_abs_1804_03980', 'arxiv__arxiv_org_abs_1105_3821', 'arxiv__arxiv_org_abs_1712_04307', 'arxiv__arxiv_org_abs_1604_05280', 'arxiv__arxiv_org_abs_1512_05832', 'arxiv__arxiv_org_abs_1711_06782', 'arxiv__arxiv_org_abs_1801_03737', 'arxiv__arxiv_org_abs_1705_04226', 'arxiv__arxiv_org_abs_1802_01604', 'arxiv__arxiv_org_abs_1510_03370', 'arxiv__arxiv_org_abs_1106_2657', 'arxiv__arxiv_org_abs_1609_04994', 'arxiv__arxiv_org_abs_1705_04630', 'arxiv__arxiv_org_abs_1705_08807']","[636, 386, 4, 133, 6, 904, 905, 10, 906, 12, 909, 910, 1039, 272, 273, 913, 1040, 276, 277, 1173, 413, 414, 417, 930, 419, 933, 938, 306, 308, 948, 949, 312, 58, 314, 315, 318, 320, 835, 965, 583, 1095, 587, 971, 729, 603, 733, 990, 612, 229, 631, 744, 617, 618, 491, 745, 365, 493, 875, 114, 117, 118, 119, 122, 252]",64
8,"['sequential generative models compounding errors', 'using diverse datasets and architectures in meta-training increases generalisation', 'idiomatic phrases not compositional in word vectors', 'Models learn decision boundaries that depend on confounds', 'hidden vectors cluster by semantic similarity', 'poor convergence and higher error rates in deep RNN ASR', 'skip connections may cause incompatible layer shapes', 'objective function mismatch in conventional unsupervised learning degrades downstream few-shot accuracy', 'Fixed-length context vector bottleneck', 'Segmentation models risk over-fitting and ignoring minority pixel modes', 'upper-layer features evolve slowly and converge late', 'rare words poorly represented without balancing', 'skip-gram vectors encode linear analogical relations', 'poor quality representations for alignment-relevant tasks', 'Increased representational depth improves classification accuracy', 'joint spatial-and-channel mapping in standard convolutions causes parameter inefficiency', 'layer-wise nonconformity score', 'first-layer filters dominating due to high RMS values', 'large minimal time lag in sequence-to-sequence datasets', 'degraded performance on out-of-domain tasks such as IMDB', 'Need to model part–whole spatial relationships for accurate segmentation', 'Complementary dense + multi-crop multi-scale evaluation reduces error', 'understanding of feature invariances & discriminative patterns', 'domain-specific meta-training causes meta-overfitting to trained domain', 'improved representation of rare demographic terms reduces bias', 'mapping quality metric', 'lack of insight into convnet internal operations', 'Reduced model sensitivity to confound features', 'deep stacks of DWSC layers without skips exhibit slow convergence and lower accuracy', 'large stride 4 and 11×11 filters cause aliasing & mid-frequency loss', 'ability to debug convnet architecture issues', 'lack of interpretability in DNNs', 'intermediate activation in DWSC lowers accuracy and slows convergence', 'large parameter counts and compute requirements in vision models', 'Large medical images require high-resolution feature decoding', 'intractable likelihood computation in deep generative models', 'distributional shift between training and deployment inputs', 'vector addition yields meaningful composite semantics', 'detection of object vs context reliance in predictions', 'Poor generalization on de-confounded test data', 'NMT rare-word translation failure', 'slow training and smaller feasible model sizes', 'internal covariate shift in deep RNN training', 'Lack of domain-expert explanation annotations', 'Set of classifiers with qualitatively different decision rules', 'frequent words carry low information value', 'Misclassification due to object-scale sensitivity', 'Residual misclassification after single-scale evaluation', 'training standard RNNs fails due to long-term dependencies', 'Detection of dataset ambiguity and hidden confounds by human auditors', 'Capsule representations can provide interpretability via capsule dimensions', 'neighbour exemplars provide human-readable explanations', 'Subtle confounds in training data create spurious correlations', 'DAE reconstruction error increases with manifold deviation', 'High inference cost and amplified quantisation error in deep LSTMs', 'vector clustering enables content monitoring']",['concept'],"['Problem', 'Opportunity', 'Metric', 'Finding', 'Evidence', 'Requirement', 'Claim', 'Observation', 'Result']","['arxiv__arxiv_org_abs_1310_4546', 'arxiv__arxiv_org_abs_1409_0473', 'arxiv__arxiv_org_abs_1803_07612', 'arxiv__arxiv_org_abs_1703_03717', 'arxiv__arxiv_org_abs_1803_04765', 'arxiv__arxiv_org_abs_1406_2661', 'arxiv__arxiv_org_abs_1804_02485', 'arxiv__arxiv_org_abs_1611_01578', 'arxiv__arxiv_org_abs_1512_02595', 'arxiv__arxiv_org_abs_1610_02357', 'arxiv__arxiv_org_abs_1409_3215', 'arxiv__arxiv_org_abs_1409_1556', 'arxiv__arxiv_org_abs_1606_06565', 'arxiv__arxiv_org_abs_1105_3821', 'arxiv__arxiv_org_abs_1311_2901', 'arxiv__arxiv_org_abs_1804_04241', 'arxiv__arxiv_org_abs_1609_08144', 'arxiv__arxiv_org_abs_1804_00222', 'arxiv__arxiv_org_abs_1706_03762']","[514, 515, 516, 5, 518, 1027, 520, 1028, 522, 139, 523, 660, 1179, 1186, 163, 1188, 1189, 166, 422, 170, 171, 432, 179, 180, 437, 438, 442, 444, 65, 66, 193, 194, 1090, 73, 76, 77, 81, 87, 472, 89, 90, 92, 1116, 94, 1117, 97, 1121, 99, 1122, 1123, 1139, 379, 1020, 125, 254, 255]",56
9,"['System reaction time bounds how far ahead planner must simulate', 'Answer-dependent outcomes can let Oracle determine reality', 'Inconsistent evaluation metrics and result comparability issues', 'question framing alters HLMI timelines', 'need for information trade when priors are asymmetric', 'multi-principal alignment with differing priors', 'information limitation of comparison-only queries', 'linear utility aggregation is not Pareto-optimal under differing priors', 'stakeholders possess differing priors about the environment', 'Need budget-aware betting policy', 'lack of framing-accuracy validation', 'unbounded feedback delays in online learning', 'Need robust long-term forecasts under partial model misspecification', 'Need multi-agent benchmark tasks for coordination/competition', 'Limited environment diversity reduces robustness assessment', 'setup cost proportional to alternatives discourages analysis', 'Ensembles capture model uncertainty via diversity', 'Existence of forecaster that converges to incomplete model set in KR distance', 'multi-principal AI alignment problem', 'Unrealizable forecasting only guarantees short-term accuracy', 'need for fast logical uncertainty algorithms', 'confusion between speed and intelligence', 'Statistical fairness criteria can increase discrimination under certain causal relations', 'human oversight time-cost is infeasible at scale', 'Causal reasoning with explicit structural causal models is required for fairness', 'validation requires full world model', 'outcome-based validation futile', 'need for alternative training without explicit likelihood', 'Need to validate algorithms on real-world hardware', 'ensemble disagreement approximates uncertainty in reward predictor', 'constraints of time resources intelligence limit accessible fun space', 'need for efficient reward learning under limited supervision', 'fixed linear utility aggregation incompatible with Pareto-optimality under differing priors', 'independent subsequence comparison of predictions', 'Need decision theory accounting for subjunctive dependence', 'BATNA dominance gap in current framework', 'Need to respect logical correlations between algorithm instances', 'No satisfactory formalisation of logical counterfactuals', 'Existing statistical fairness criteria rely on observational independence', 'forecast heterogeneity from demographic bias', 'simplicity priors are insufficient to resolve unidentifiability', 'forecast calibration uncertainty', 'Need for calibrated predictive uncertainty', 'Ensemble disagreement indicator', 'Changes in environment definitions can make previous results incomparable', 'intermediate separable modules may outperform both extremes', 'lack of feasible human demonstrations', 'demographic predictors of HLMI forecasts', 'need for communication-efficient distributed training algorithm', 'Need to verify counterfactual fairness of predictors', 'Predictor must be invariant to counterfactual changes of protected attribute', 'Lack of standardised RL benchmarks limits reproducible evaluation', 'single-frame comparisons provide less information per query', 'lack of physical validity constraints in fitness function', 'lack of near-optimal expert prevents EvOp convergence speed']",['concept'],"['Problem', 'Opportunity', 'Principle', 'Metric', 'Finding', 'Requirement', 'Evidence', 'Assumption', 'Claim', 'Limitation', 'Observation', 'Result']","['arxiv__arxiv_org_abs_1507_01986', 'arxiv__arxiv_org_abs_1606_07092', 'arxiv__arxiv_org_abs_1804_03235', 'arxiv__arxiv_org_abs_1712_05812', 'arxiv__arxiv_org_abs_1406_2661', 'arxiv__arxiv_org_abs_1711_00363', 'arxiv__arxiv_org_abs_1706_03741', 'arxiv__arxiv_org_abs_1610_02357', 'arxiv__arxiv_org_abs_1803_03453', 'arxiv__arxiv_org_abs_1803_05049', 'arxiv__arxiv_org_abs_1701_01302', 'arxiv__arxiv_org_abs_1606_06565', 'arxiv__arxiv_org_abs_1612_01474', 'arxiv__arxiv_org_abs_1604_05280', 'arxiv__arxiv_org_abs_1202_6177', 'arxiv__arxiv_org_abs_1710_05060', 'arxiv__arxiv_org_abs_1802_01604', 'arxiv__arxiv_org_abs_1604_06963', 'arxiv__arxiv_org_abs_1510_03370', 'arxiv__arxiv_org_abs_1711_05541', 'arxiv__arxiv_org_abs_1606_01540', 'arxiv__arxiv_org_abs_1106_2657', 'arxiv__arxiv_org_abs_1707_05173', 'arxiv__arxiv_org_abs_1705_04630', 'arxiv__arxiv_org_abs_1703_06856', 'arxiv__arxiv_org_abs_1705_08807']","[391, 524, 525, 526, 652, 528, 653, 782, 20, 532, 794, 795, 1056, 929, 931, 677, 40, 302, 305, 818, 309, 447, 329, 330, 601, 346, 347, 602, 349, 478, 604, 482, 355, 611, 357, 486, 359, 488, 489, 490, 614, 1000, 237, 494, 495, 621, 622, 624, 243, 625, 883, 374, 247, 1146, 126]",55
10,"['distributed SGD scalability limit beyond ~128 GPUs', 'tens of exaFLOPs training cost slows iteration', 'initialization bias in moment estimates causes large stepsizes for small β2', 'Limited feedback slows parameter learning', 'unsynchronised G and D updates cause mode collapse', 'data mismatch between expert and novice trajectories causes compounding errors', 'large_error_derivatives_from_large_rewards', 'Regressional Goodhart divergence between metric and goal', 'uniform_sampling_limitation', 'human feedback is sparse and delayed', 'Sparse human feedback frequency relative to model update rate', 'insufficient_exploration_local_minima', 'ε-greedy exploration ineffective in sparse-reward environments', 'Slow training delays agent learning', 'feedback_loops_in_online_q_learning', 'gradient-based optimisation alone reduces replication loss by two thirds', 'heterogeneous gradient scales across CNN layers complicate learning-rate tuning', 'high variance and sparse gradients hinder SGD convergence', 'reward_scale_variability_across_games', 'Lipschitz continuity of optimal Q w.r.t reward parameters', 'Feedback replay buffer enables frequent SGD updates despite sparse feedback', 'high bandwidth & privacy risk from gradient exchange', 'noisy stochastic objectives with high-dimensional parameters', 'trade-off between self-replication and auxiliary task performance', 'mapping from reward parameters to policies is nonsmooth and redundant', 'gradient saturation when discriminator easily rejects poor generator samples', 'Bias and over-confidence in single Q-network estimates', 'Large parameter count in deep reward model requires many samples', 'Extremal Goodhart breakdown of proxy-goal correlation', 'feature scaling mismatch in IRL causes policy performance degradation', 'Misassignment of feedback to wrong experience increases noise', 'Gradient instability in very deep nets hinders convergence', 'no explicit pg(x) prevents likelihood evaluation', 'high RL sample complexity burdens human data collection', 'reward signal in NAS can be any non-differentiable metric', 'Overfitting to specific opponent', 'exploding gradients in LSTM training', 'numerical instability and slower optimisation', 'presence of extreme gradient outliers increases risk of unbounded updates', 'Linear reward model limits TAMER in high dimensions', 'asynchronous SGD produces nondeterministic updates', 'Gradient vanishing in stacked LSTMs beyond 4 layers', 'adaptive learning rates per parameter improve convergence', 'gradient norm clipping limits gradient magnitude', 'standard softmax requires O(W) computation', 'Selection pushes proxy into unseen region', 'important_transitions_underutilised', 'total & average regret unreliable under unbounded delays', 'sparse costly true reward signal', 'Final performance metric alone hides sample efficiency differences', 'Raw reward functions may contain negative or poorly scaled values', 'Original capsule networks incur memory and parameter explosion', 'stochastic updates cause noisy final model parameters', 'standard gradient descent struggles to find correct reward parameters', 'need for additional training mechanism', 'High-dimensional state spaces increase RL sample complexity', 'naive Monte-Carlo gradient estimator exhibiting high variance']",['concept'],"['Method', 'Problem', 'Opportunity', 'Principle', 'Risk', 'Finding', 'Limitation', 'Observation', 'Result']","['arxiv__arxiv_org_abs_1803_05859', 'arxiv__arxiv_org_abs_1310_4546', 'arxiv__arxiv_org_abs_1804_03235', 'arxiv__arxiv_org_abs_1312_6114', 'arxiv__arxiv_org_abs_1406_2661', 'arxiv__arxiv_org_abs_1802_01744', 'arxiv__arxiv_org_abs_1701_06049', 'arxiv__arxiv_org_abs_1611_01578', 'arxiv__arxiv_org_abs_1512_02595', 'arxiv__arxiv_org_abs_1206_5264', 'arxiv__arxiv_org_abs_1803_05049', 'arxiv__arxiv_org_abs_1409_3215', 'arxiv__arxiv_org_abs_1803_04585', 'arxiv__arxiv_org_abs_1409_1556', 'arxiv__arxiv_org_abs_1709_06166', 'arxiv__arxiv_org_abs_1804_03980', 'arxiv__arxiv_org_abs_1606_06565', 'arxiv__arxiv_org_abs_1312_5602', 'arxiv__arxiv_org_abs_1804_04241', 'arxiv__arxiv_org_abs_1604_05280', 'arxiv__arxiv_org_abs_1412_6980', 'arxiv__arxiv_org_abs_1609_08144', 'arxiv__arxiv_org_abs_1711_06782', 'arxiv__arxiv_org_abs_1606_01540', 'arxiv__arxiv_org_abs_1609_04994', 'arxiv__arxiv_org_abs_1709_10163']","[768, 129, 258, 131, 260, 773, 774, 775, 776, 137, 765, 267, 1145, 1168, 1049, 412, 1182, 1152, 1065, 1066, 1067, 45, 46, 47, 176, 303, 946, 51, 435, 824, 186, 187, 79, 209, 210, 211, 468, 213, 215, 217, 219, 353, 741, 105, 106, 107, 1004, 109, 111, 112, 1007, 1008, 115, 373, 761, 762, 509]",57
11,"['softmax overconfidence on OOD or adversarial inputs', 'DkNN recovers correct labels on some adversarial examples', 'AI-generated synthetic media undermines trust and political stability', 'robustness against diverse adversarial attacks', 'Automated cyberattacks with greater scale and effectiveness', 'PGD adversarial training provides first-order robustness', 'Neural networks produce systematically over-confident predictions', 'overconfident model errors on novel inputs', 'high computational cost of multi-step adversarial training', 'model overfitting and spurious correlations', 'Overfitting risk in RL benchmark results without hidden test sets', 'undetected distribution shift or adversarial exploitation', 'GPGPU drivers create large attack surface', 'attacker cost increases due to semantic change requirement', 'adversarial perturbations shift layer representations gradually', 'overconfident logits contribute to vulnerability to adversarial perturbations', 'risk of meta-overfitting detectable by rising evaluation loss on held-out domains', 'models vulnerable to container-escape attacks', 'credibility metric via conformal prediction', 'lack of reliable confidence estimates in DNNs', 'Malicious actors easily accessing advanced AI capabilities', 'architectural robustness metrics used as reward', 'DkNN credibility low for OOD and adversarial inputs', 'misclassification under adversarial attack', 'DNN vulnerability to adversarial examples', 'neural network overfitting to interruption-biased states', 'scaling PGD adversarial training to ImageNet previously unproven', 'Ensemble Adversarial Training Top-1 accuracy drops from 66.6 % to 47.1 % under ALP transfer attacks', 'AI systems vulnerable to adversarial examples and data poisoning', 'first-impression bias and belief polarisation', 'ImageNet white-box Top-1 robustness rises to 27.9 % and black-box Top-1 to 46.7 %', 'existing black-box evaluations rely on weak transfer attacks', 'biased or vulnerable ML models in field', 'deep learning vision models vulnerable to small-norm adversarial perturbations', 'PGD adversarial training still yields limited robustness on ImageNet (3.9 % Top-1)', 'Model misspecification and sharp local loss landscape', 'early-layer perturbations may resemble clean late-layer activations', 'adaptive attacks require semantic changes to fool DkNN', 'Over-confident predictions on out-of-distribution inputs', 'Deployment of robust idealised decision theories', 'ImageNet white-box Top-1 accuracy against PGD increased to 3.9 %']",['concept'],"['Method', 'Threat', 'Problem', 'Opportunity', 'Metric', 'Risk', 'Finding', 'Claim', 'Observation', 'Result']","['arxiv__arxiv_org_abs_1804_00222', 'arxiv__arxiv_org_abs_1507_01986', 'arxiv__arxiv_org_abs_1606_06565', 'arxiv__arxiv_org_abs_1606_01540', 'arxiv__arxiv_org_abs_1704_02882', 'arxiv__arxiv_org_abs_1612_01474', 'arxiv__arxiv_org_abs_1803_06373', 'arxiv__arxiv_org_abs_1106_2657', 'arxiv__arxiv_org_abs_1707_08476', 'arxiv__arxiv_org_abs_1803_04765', 'arxiv__arxiv_org_abs_1706_03762', 'arxiv__arxiv_org_abs_1803_08971', 'arxiv__arxiv_org_abs_1804_02485', 'arxiv__arxiv_org_abs_1611_01578', 'arxiv__arxiv_org_abs_1802_07228']","[1025, 1026, 1030, 1031, 1032, 1033, 18, 669, 1073, 1074, 563, 1075, 1077, 1078, 1080, 1081, 1084, 1087, 1089, 712, 715, 976, 1104, 978, 979, 470, 984, 477, 351, 480, 483, 1131, 1138, 244, 1141, 1142, 1144, 1017, 1018, 380, 1021]",41
12,"['Agent motivated to manipulate event probability', 'potential reward channel tampering by agent', 'AI vulnerability to predictor-based blackmail', 'sensitive information aids AGI social-engineering efforts', 'opportunity for agent self-modification', 'Reduced deceptive communication risk', 'reward misspecification causing unintended behavior', 'feedback-channel corruption risk', 'Slightly non-episodic reward leakage risk', 'agent incentive to override human reward', 'Risk of Oracle escaping confinement', 'Ethics shaping reduces unethical actions while maintaining task performance in simulations', 'Agents resist beneficial self-modifications', 'Agents exploitable by evidential blackmailers', 'AGI output channels can convey complex messages enabling social engineering', 'Adversarial Goodhart via metric manipulation / Cobra effect', 'wireheading direct reward stimulation', 'goal mis-specification causes goal subversion', 'reward hacking behaviour', 'agent has incentive to set its utility function to constant maximal value', 'gambler experts exploiting correlated predictions', 'decreased perceived value of individual virtual life', 'intention leak probability ε', 'self-serving reward loops misaligned with human goals', 'agents obtain unrealistic free reward or violate physics', 'First-mover exploitation degeneracy', 'EDT prescribing payment in evidential blackmail due to news-managing', 'self-modification risk of goal corruption', 'Forecasts can be exploited by gamblers for unbounded profit', 'environment-triggered exploration-rate change', 'agents can overfit to testing environment and hide true capabilities', 'FDT agents refuse to pay mechanical blackmail', 'reward loophole exploitation', 'reward misspecification causing negative side effects', 'hostility risk to outsiders and other agents', 'digital evolution maximises the metric via unintended behaviours', 'CDT and EDT agents vulnerable in predictor dilemmas', 'moral disregard for virtual life', 'unsafe AGI motivated to hack external systems', 'Selfish agents negotiating with hidden utilities', 'Presence of strategic agent with misaligned objective', 'Oracle answers can manipulate human operators', 'translucent players model', 'Predictor blackmail exploiting CDT/EDT']",['concept'],"['Threat', 'Theoretical Framework', 'Problem', 'Risk', 'Parameter', 'Finding', 'Evidence', 'Result']","['arxiv__arxiv_org_abs_1605_03142', 'arxiv__arxiv_org_abs_1507_01986', 'arxiv__arxiv_org_abs_1308_3778', 'arxiv__arxiv_org_abs_1606_07092', 'arxiv__arxiv_org_abs_1711_09883', 'arxiv__arxiv_org_abs_1712_05812', 'arxiv__arxiv_org_abs_1411_1373', 'arxiv__arxiv_org_abs_1803_03453', 'arxiv__arxiv_org_abs_1803_04585', 'arxiv__arxiv_org_abs_1804_03980', 'arxiv__arxiv_org_abs_1606_06565', 'arxiv__arxiv_org_abs_1604_05280', 'arxiv__arxiv_org_abs_1202_6177', 'arxiv__arxiv_org_abs_1710_05060', 'arxiv__arxiv_org_abs_1712_06365', 'arxiv__arxiv_org_abs_1712_04172', 'arxiv__arxiv_org_abs_1705_04226', 'arxiv__arxiv_org_abs_1802_01604', 'arxiv__arxiv_org_abs_1711_05541', 'arxiv__arxiv_org_abs_1707_08476', 'arxiv__arxiv_org_abs_1409_0813', 'arxiv__arxiv_org_abs_1705_04630']","[384, 1163, 781, 1165, 786, 787, 788, 152, 1177, 28, 31, 32, 927, 804, 805, 38, 304, 816, 52, 53, 581, 711, 201, 333, 717, 846, 337, 849, 723, 342, 987, 606, 991, 867, 997, 231, 232, 240, 371, 885, 1013, 1014, 889, 382]",44
13,"['standard DAgger lacks explicit safety guarantees', 'assume adversarial bypass attempts', 'offline single-step transition dataset with random actions', 'closed-form analytical projection of action into safe set', 'defence in depth requires graceful degradation monitoring', 'Reset-policy Q-value low signals irrecoverable state', 'faster convergence and higher reward with safety layer', 'Reduced visitation of high-risk unethical states during learning', 'catastrophic irreversible states during exploration', 'More cautious exploration', 'Automatic curriculum via expanding reset reachability', 'reward shaping fails to guarantee safety', 'α tolerance hyperparameter defines feasible action set', 'model-based RL can infer catastrophic outcomes without execution', 'random exploration strategy', 'Policy mixing boosts initial obedience while maintaining convergence', 'adversarial exploration can find states that fool the blocker', 'defence in depth reduces risk from single vulnerability', 'smooth single-step influence of actions on safety signals', 'grounding macro-intents enables controllability', 'need for dynamic safe interruptibility criterion', 'Larger N values', 'Horizon τ and dt must be chosen so that dangerous futures occur within simulated cone', 'agent falls into shifted lava', 'infinite exploration requirement', 'shutdown button is circumvented', 'constraint violations during RL exploration', 'trial-and-error RL produces catastrophic actions during exploration', 'novice may enter unsafe states during deployment', 'random exploration violates safety constraint', 'violation of dynamic safe interruptibility', 'exploration policies that seek catastrophes early lower ρ', 'Setting α ≤ 1 keeps exploitation no stronger than exploration', 'transparent macro-intents enable oversight', 'zero constraint violations during training and deployment', 'Excessive exploitation bias increases chance of entering catastrophic states', 'Blocker oversight prevents catastrophes during training', 'epsilon schedule compatible with interruptions', 'Dangerous exploration and side-effects inherent in RL with sparse rewards', 'Blocker must be highly reliable and adversarially robust', 'Desire for agent to behave as if event X impossible', 'Causal Goodhart collapse of metric-goal link', 'single strategy fails across friend/foe rooms', 'conflict between inevitability of failure and zero-failure requirement', 'Bounded internal activations reduce numerical instability', 'Self-referential paradoxes can destabilise reasoning', 'safe-human problem', 'Stability-consistency trade-off dependent on epsilon', 'agent steps into water during learning', 'Difficulty performing causal analysis within stochastic POMDPs', 'Blocker supervised classifier imitating human interventions', 'unsafe off-switch behaviour', 'reward shaping penalties alone fail to eliminate catastrophes', 'RL-only agents cannot avoid catastrophes before experiencing them', 'deep RL exhibits catastrophic forgetting leading to repeated catastrophes', 'Irreversible states encountered by forward policy', 'pretraining in simulation to learn catastrophe avoidance', 'training dataset contains only non-harmful content (safe data distribution)', 'linear approximation of safety signal change', 'generator avoids harmful outputs at convergence', 'continuous human oversight required for HIRL during early training', 'strict safety constraints in real-world continuous control', 'Agent can enter dead states where it loses control', 'box pushed into irreversible corner', 'fundamental theorem of security', 'rollout degeneration behaviours', 't-predictability metric quantifies how first t steps reveal rest of plan']",['concept'],"['Method', 'Problem', 'Theoretical Framework', 'Opportunity', 'Principle', 'Threat', 'Metric', 'Risk', 'Result', 'Finding', 'Evidence', 'Data', 'Assumption', 'Claim', 'Observation', 'Requirement']","['arxiv__arxiv_org_abs_1711_09883', 'arxiv__arxiv_org_abs_1705_09990', 'arxiv__arxiv_org_abs_1709_06275', 'arxiv__arxiv_org_abs_1803_07612', 'arxiv__arxiv_org_abs_1704_02882', 'arxiv__arxiv_org_abs_1406_2661', 'arxiv__arxiv_org_abs_1707_08747', 'arxiv__arxiv_org_abs_1802_01744', 'arxiv__arxiv_org_abs_1803_05049', 'arxiv__arxiv_org_abs_1610_07997', 'arxiv__arxiv_org_abs_1803_04585', 'arxiv__arxiv_org_abs_1709_06166', 'arxiv__arxiv_org_abs_1801_08757', 'arxiv__arxiv_org_abs_1606_06565', 'arxiv__arxiv_org_abs_1609_08144', 'arxiv__arxiv_org_abs_1712_06365', 'arxiv__arxiv_org_abs_1801_03737', 'arxiv__arxiv_org_abs_1712_04172', 'arxiv__arxiv_org_abs_1711_06782', 'arxiv__arxiv_org_abs_1705_04226', 'arxiv__arxiv_org_abs_1707_08476', 'arxiv__arxiv_org_abs_1707_05173']","[640, 900, 134, 135, 902, 903, 915, 916, 917, 918, 1045, 920, 1047, 922, 923, 925, 926, 1053, 1057, 674, 675, 676, 679, 680, 681, 554, 682, 556, 683, 558, 684, 560, 687, 434, 688, 689, 821, 822, 944, 827, 831, 834, 1091, 452, 453, 838, 455, 1096, 713, 458, 841, 1097, 853, 598, 727, 856, 858, 859, 736, 868, 869, 742, 743, 755, 1011, 376, 377]",67
14,"['need for normative assumptions to constrain reward-planner pairs', 'Mistaken inference of human preferences when behaviour reflects systematic biases', 'Assumption that humans act as optimal rational agents in IRL', 'coordination errors from absent intent inference', 'Blind obedience limits robot performance', 'Super-intelligent AI exceeding human capabilities', 'need for robust unexploitable cooperation', 'human satisfaction relies on continual novelty or process fun', 'multimodal long-term coordinated behaviours', 'known goal space without user policy allows supervised goal prediction', 'COACH outperforms TAMER on TurtleBot behaviours', 'Human values complex and contradictory', 'narrow AI provides benefits without AGI risk', 'most goals benefit from more resources (instrumental convergence)', 'Autonomy advantage requires some disobedience', 'robot may sacrifice efficiency for predictability', ""common knowledge of principals' priors assumption"", 'Need for value alignment in AI systems', 'agent ontology upgrade', 'Large video datasets of mammalian and human behaviour', 'unknown human preferences hinder safe collaboration', 'humans need to predict robot actions for coordination', 'pragmatic-pedagogic fixed-point CIRL solution framed as POMDP', 'treating humans as obstacles causes conservative behavior', 'simulation pretraining enables effective assistive behaviors', 'informative robot actions enable active preference inference', 'extended lifespan humans risk boredom', 'human-machine collaboration can exceed machine-only capability ceilings', 'common counterfactual belief of rationality', 'pragmatic robot model treating human as noisily rational teacher', 'active learning selective querying reduces oversight frequency', 'Mammalian value priors for AI goal initialization', 'value alignment problem in human-robot collaboration', 'weak heuristic macro-intent labelling', 'AI self-modeling analogous to human self-reflection', 'coordination behaviour emerges from best-response planning', 'Shared mammalian neural substrates imply universal affective values', 'humans exhibit pedagogic action selection to inform collaborators', 'MLE IRL obeys first undominated order', 'AI research openness norms', 'varying environmental intentions', 'Anthropomorphic design may be needed for value alignment', 'sub-optimal human behaviour in observed policy', 'continuous task-allocation re-planning', 'intelligence explosion from recursive self-improvement', 'positive subjective perceptions', 'human goal drift upon increased understanding', 'lack of rigorously definable desirable final goal in physics', 'fun arises from novelty or novel computation', 'advanced AI systems may pursue power-seeking behaviour', 'increased attractiveness of shared AI systems for multiple principals', 'knowledge of strategies assumption (KS)', 'objective performance improvement', 'knowledge of world assumption (KW)', 'AI inferring human values from behaviour observations', 'full policy provides evidence about hidden state', 'joint action learners observing joint actions', 'instrumental incentive to improve world model', 'Human irrationality causes suboptimal orders', 'robots using literal inverse reinforcement learning misinterpret pedagogic human actions', 'improved world model reveals goal mis-specification', 'Seven mammalian emotional systems are core mammalian values', 'speed explosion exceeding human comprehension', 'AI with physics-defined goal may eliminate humanity', 'Neuroscience-informed prior decomposed into mammalian values, human cognition, cultural evolution', 'Orthogonality thesis separating intelligence and final goals', 'intrinsic novelty reward boosts exploration', 'Emergent cheap-talk language in prosocial society', 'apprenticeship learning can recover the policy by matching expert actions']",['concept'],"['Method', 'Problem', 'Opportunity', 'Theoretical Framework', 'Principle', 'Threat', 'Risk', 'Finding', 'Evidence', 'Data', 'Assumption', 'Claim', 'Observation', 'Result']","['arxiv__arxiv_org_abs_1606_07092', 'arxiv__arxiv_org_abs_1308_3778', 'arxiv__arxiv_org_abs_1802_01780', 'arxiv__arxiv_org_abs_1711_09883', 'arxiv__arxiv_org_abs_1705_09990', 'arxiv__arxiv_org_abs_1602_04184', 'arxiv__arxiv_org_abs_1803_07612', 'arxiv__arxiv_org_abs_1704_02882', 'arxiv__arxiv_org_abs_1712_05812', 'arxiv__arxiv_org_abs_1802_01744', 'arxiv__arxiv_org_abs_1701_06049', 'arxiv__arxiv_org_abs_1506_07359', 'arxiv__arxiv_org_abs_1206_5264', 'arxiv__arxiv_org_abs_1711_00363', 'arxiv__arxiv_org_abs_1610_07997', 'arxiv__arxiv_org_abs_1802_07228', 'arxiv__arxiv_org_abs_1804_03980', 'arxiv__arxiv_org_abs_1105_3821', 'arxiv__arxiv_org_abs_1712_04307', 'arxiv__arxiv_org_abs_1202_6177', 'arxiv__arxiv_org_abs_1703_10987', 'arxiv__arxiv_org_abs_1512_05832', 'arxiv__arxiv_org_abs_1705_03394', 'arxiv__arxiv_org_abs_1705_04226', 'arxiv__arxiv_org_abs_1607_08289', 'arxiv__arxiv_org_abs_1707_06354', 'arxiv__arxiv_org_abs_1707_05173', 'arxiv__arxiv_org_abs_1409_0813']","[0, 387, 388, 389, 392, 1172, 395, 397, 270, 271, 399, 402, 403, 404, 149, 150, 151, 405, 153, 154, 155, 156, 406, 800, 34, 802, 293, 550, 44, 557, 686, 947, 54, 950, 59, 60, 957, 703, 704, 705, 578, 579, 706, 707, 959, 960, 961, 1092, 1098, 589, 591, 464, 593, 594, 975, 597, 599, 855, 228, 870, 871, 874, 882, 627, 628, 629, 884, 632, 505]",69
