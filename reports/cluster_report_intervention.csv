cluster,name,type,paper_id,id,count
0,"['adopt transformer architecture with multi-head self-attention', 'use Adam instead of manually tuned layer-wise learning rates in CNN training', 'DkNN inference algorithm with layer-wise kNN and conformal calibration', 'Adopt 16–19-layer 3×3 ConvNet architecture in vision models', 'Insert residual connections between successive LSTM layers', 'conduct architecture search over segment counts to optimise robustness and efficiency', 'add residual connections around each DWSC block', 'Implement locally constrained dynamic routing with shared transformations in capsule segmentation models', 'Initialise deep ConvNet with first 4 conv and last 3 FC layers from 11-layer precursor', 'replace convolution or Inception modules with depth-wise separable convolutions during model design', 'GPU-resident CTC implementation reducing compute 10-20%', 'remove intermediate non-linearity inside DWSC when stacking them', 'Apply masked positive-class reconstruction loss during capsule segmentation training', 'LSH indices for layer representations', 'Adopt deconvolutional capsule layers to enable large-input segmentation', 'filter RMS clipping to radius 0.1 during training', 'adjust first-layer to 7×7 filters with stride 2', 'stack 2-D time-frequency convolution layers before RNN', 'configure 8-head scaled-dot attention with dk=64 during model design', 'implement sparsity-enforcing neural architectures']",['intervention'],"['arxiv__arxiv_org_abs_1409_1556', 'arxiv__arxiv_org_abs_1412_6980', 'arxiv__arxiv_org_abs_1602_04019', 'arxiv__arxiv_org_abs_1610_02357', 'arxiv__arxiv_org_abs_1706_03762', 'arxiv__arxiv_org_abs_1804_04241', 'arxiv__arxiv_org_abs_1609_08144', 'arxiv__arxiv_org_abs_1512_02595', 'arxiv__arxiv_org_abs_1311_2901', 'arxiv__arxiv_org_abs_1803_04765']","[266, 269, 661, 664, 1183, 1185, 291, 1187, 165, 178, 436, 441, 443, 445, 448, 216, 93, 95, 1022, 1023]",20
1,"['interpretability logging with slowed replay for auditors', 'require occlusion-sensitivity evaluation with grid sweep before deployment', 'lineage-based tracking of replication rate with randomised test inputs', 'Ensemble-proxy optimisation with correlation monitoring and optimisation throttling', 'design human-in-the-loop control frameworks with real-time oversight switches', 'integrate automatic validation and repair safeguards in NAS for safety constraints', 'Monitoring of every timestep and producing learning curves', 'Benford-test performance as safety evaluation metric', 'monitor per-layer reconstruction error at inference to flag adversarial or OOD inputs', 'real-time monitoring & interpretability dashboards', 'implement technical tripwires that detect self-modification or abnormal execution and shut down AGI', 'Monitor emergent communication semantics via probes', 'manual label correction after blocker failures', 'Human/automatic auditing of unusual attention patterns', 'conduct real-time redundant supervision with varied breach drills on AGI execution', 'train and deploy CNN Blocker to replace human oversight', 'Self-reflection transparency monitor detecting goal drift', 'conduct mechanistic interpretability audits during model evaluation', 'biennial expert survey with back-testing', 'evaluate replication quotient during pre-deployment safety audit', 'conduct mandatory attention-map visualisation audits with diverse red-team prompts pre-deployment', 'adoption of universal intelligence measure Υ-based evaluation', 'divergence threshold with human review', 'compute and energy-usage auditing during AI development and deployment', 'Conduct capsule-dimension visualisation inspection before deployment', 'audit embeddings using vector arithmetic to detect unintended associations', 'monitoring meta-objective performance on cross-domain validation sets before deployment', 'Out-of-distribution detector with penalty on extreme metric values during deployment', 'monitor encoded sentence vectors for harmful content via similarity', 'regret-based override detector in evaluation pipeline', 'Tamper-evident logging & anomaly detection on metric channels', 'monitor feature-map visualisations over epochs to decide training termination', 'integrate deconvnet visualisation into model-development workflow', 'conduct periodic human oversight phases to update Blocker']",['intervention'],"['arxiv__arxiv_org_abs_1804_00222', 'arxiv__arxiv_org_abs_1202_6177', 'arxiv__arxiv_org_abs_1707_05173', 'arxiv__arxiv_org_abs_1311_2901', 'arxiv__arxiv_org_abs_1804_02485', 'arxiv__arxiv_org_abs_1703_10987', 'arxiv__arxiv_org_abs_1705_08807', 'arxiv__arxiv_org_abs_1707_08476', 'arxiv__arxiv_org_abs_1602_04019', 'arxiv__arxiv_org_abs_1712_05812', 'arxiv__arxiv_org_abs_1706_03762', 'arxiv__arxiv_org_abs_1105_3821', 'arxiv__arxiv_org_abs_1804_04241', 'arxiv__arxiv_org_abs_1409_0813', 'arxiv__arxiv_org_abs_1409_0473', 'arxiv__arxiv_org_abs_1804_03980', 'arxiv__arxiv_org_abs_1611_01578', 'arxiv__arxiv_org_abs_1510_03370', 'arxiv__arxiv_org_abs_1803_05859', 'arxiv__arxiv_org_abs_1310_4546', 'arxiv__arxiv_org_abs_1803_04585', 'arxiv__arxiv_org_abs_1803_03453', 'arxiv__arxiv_org_abs_1409_3215', 'arxiv__arxiv_org_abs_1610_07997', 'arxiv__arxiv_org_abs_1606_01540']","[8, 145, 1174, 667, 288, 161, 36, 549, 1190, 552, 42, 1071, 694, 696, 702, 195, 463, 722, 85, 728, 474, 91, 98, 354, 100, 999, 1132, 1006, 1009, 626, 1015, 1140, 887, 251]",34
2,"['insert denoising autoencoder modules into selected hidden layers', 'Applying ethics-shaping reward based on KL divergence between RL policy and aggregated human policy during training', 'standardise multi-dimensional capability metrics and evaluation benchmarks for AI systems', 'model-based utility over inferred state', 'experience_replay_uniform_1M_buffer', 'staged AI architecture', 'statistical learning of human values', 'embed hard-coded module that forces the shutdown action when the operator issues a shutdown command']",['intervention'],"['arxiv__arxiv_org_abs_1709_06275', 'arxiv__arxiv_org_abs_1712_04172', 'arxiv__arxiv_org_abs_1411_1373', 'arxiv__arxiv_org_abs_1312_5602', 'arxiv__arxiv_org_abs_1804_02485', 'arxiv__arxiv_org_abs_1703_10987']","[865, 197, 199, 200, 103, 752, 1137, 542]",8
3,"['Apply Relativize transformation to every scalar reward component before use in training or planning', 'Corrective pseudo-rewards with epsilon to ensure seamless transition', 'dynamic utility weight-shifting objective function during policy optimisation', 'embed likelihood-based priority-shifting mechanism into agent’s reward/objective during training', 'apply refined utility-indifference that equalises continuation vs. shutdown payoff and adds small positive incentive to preserve the shutdown module', 'Policy counterfactual compound reward with proxy event Y', 'Causal counterfactual reward combining Y0 Y1 to follow human decision', 'Prosocial reward shaping during training', 'Constant reward under event X to induce effective disbelief', 'intrinsic novelty-based reward shaping in RL agents', 'Include mandatory keep-alive reward R0 in overall reward composition during system design', 'Compound reward using indicator of unbiasable event']",['intervention'],"['arxiv__arxiv_org_abs_1709_06275', 'arxiv__arxiv_org_abs_1606_07092', 'arxiv__arxiv_org_abs_1803_05049', 'arxiv__arxiv_org_abs_1804_03980', 'arxiv__arxiv_org_abs_1712_06365', 'arxiv__arxiv_org_abs_1711_00363', 'arxiv__arxiv_org_abs_1701_01302']","[896, 385, 897, 899, 1160, 496, 894, 760, 891, 1052, 798, 1055]",12
4,"['content recommendation balancing novelty and process fun', 'Curating diverse categories of tasks in benchmark suite', 'AI-generated personalized novel experiences', 'multi-layer AI container combining physical isolation, language sandboxing, OS sandboxing, VMs, instruction-level emulation, plus intrusion detection', 'Design AI architecture with modules mirroring mammalian emotional systems', 'provide runtime interface to set macro-intents before deployment', 'Systematic red-teaming plus responsible disclosure for AI models', 'deploy zero-knowledge interaction APIs', 'Implement binding proposal channel', 'evolutionary fuzz testing of simulations and hardware prior to deployment', 'Collaborative policymaker-technical researcher working groups on AI security', 'Employ preregistration, preprints, and post-publication peer review in replication studies', 'legal and ethical rights framework for virtual agents', 'governance segregation of narrow AI vs AGI projects', 'AI containment boxing', 'inclusion of multiple fortified layers across the network', 'establish G-fair compliance standard', 'export-control policies on high-performance computing chips', 'screen and train a small, vetted operator group using containment guidelines and social-engineering drills', 'Integrating Gym API with robotic hardware', 'Peer review with code and writeups for scoreboard submissions', 'Standardised environment API with versioned tasks (OpenAI Gym)', 'Pre-publication risk assessment for sensitive AI research', 'red-team penetration testing of AI safety layers', 'physics-consistency penalties and high-fidelity simulation checks during optimisation', 'adopt reversible or neuromorphic hardware for AI training', 'allocate 10 % of AI R&D to safety now', 'Add Newcomblike blackmail scenarios to RL training', 'classify containment level (light / medium / heavy) per threat-model guidance', 'Adding multi-agent environments to Gym', 'pretrain agent in simulation for catastrophe avoidance', 'AI-driven authenticity verification & certification of media prior to platform release', 'bet-settling clauses in multi-party AI governance contracts', 'broadcast cryptographic commitments of planned strategies', 'public compute infrastructure for research and small firms', 'taxonomy of dangerous AI pathways', 'decision-support simulator with surrogate actions', 'Provide agent ID embeddings', 'Diverse population training with varied prosociality', 'augment alignment pipeline with incentive-compatible information-exchange protocol before weight finalisation']",['intervention'],"['arxiv__arxiv_org_abs_1712_04307', 'arxiv__arxiv_org_abs_1202_6177', 'arxiv__arxiv_org_abs_1606_07092', 'arxiv__arxiv_org_abs_1707_05173', 'arxiv__arxiv_org_abs_1804_02485', 'arxiv__arxiv_org_abs_1705_08807', 'arxiv__arxiv_org_abs_1707_08476', 'arxiv__arxiv_org_abs_1308_3778', 'arxiv__arxiv_org_abs_1411_1373', 'arxiv__arxiv_org_abs_1802_07228', 'arxiv__arxiv_org_abs_1804_03980', 'arxiv__arxiv_org_abs_1803_08971', 'arxiv__arxiv_org_abs_1602_04184', 'arxiv__arxiv_org_abs_1803_07612', 'arxiv__arxiv_org_abs_1711_00363', 'arxiv__arxiv_org_abs_1803_03453', 'arxiv__arxiv_org_abs_1106_2657', 'arxiv__arxiv_org_abs_1607_08289', 'arxiv__arxiv_org_abs_1705_03394', 'arxiv__arxiv_org_abs_1610_07997', 'arxiv__arxiv_org_abs_1710_05060', 'arxiv__arxiv_org_abs_1701_01302', 'arxiv__arxiv_org_abs_1606_01540']","[390, 393, 1161, 16, 1169, 1170, 790, 410, 33, 801, 301, 699, 64, 577, 454, 456, 714, 460, 1101, 208, 465, 720, 977, 980, 726, 983, 1112, 985, 1115, 348, 352, 993, 356, 358, 360, 1002, 620, 880, 499, 1143]",40
5,"['Apply prudent-gambling threshold (risk-gain ratio α=0.5) in automated safety evaluation harness', 'reversibility regulariser', 'prune minimax dominated actions during training', 'include resource-budget penalties in reward during fine-tuning', 'train direct-model with MRDR variance-minimisation loss', 'Use burn-in obedience with feature detection switching policy', 'Apply IRL/BIP to refine AI value model from behavioural data', 'apply Boltzmann softmax policy during IRL optimisation', 'Add input-gradient L2 penalty on expert-tagged irrelevant features during training', 'Include hyperbolic discounting and belief-uncertainty parameters in IRL models used for alignment', 'Adopt Maximum Entropy IRL for robustness to β', 'entropy-regularised policy optimisation', 'mutual-information penalty between actions and supervision flag', 'penalising actions that cause high human-regret during RL training', 'reward regularisation with lambda complexity term', 'apply constraint regularisation on NAS controller to limit unsafe capability emergence', 'impact regularizer penalizing agent-induced state change vs null policy', 'Apply length-normalisation exponent α≈0.2 during beam search', 'α-constrained near-optimal action selection', 'adopt AdaMax variant to bound updates under ∞-norm', 'apply gradient clipping with threshold 5 during training', 'Logit Squeezing penalty on L2 norm of logits during training', 'Use constrained-norm MLE IRL during early operations', 'Higher early-abort threshold Qmin', 'Implement Bayesian IRL posterior-mean policy', 'computing natural gradients via induced metric and sub-differentials', 'constrained policy optimisation with Lagrange multipliers', 'embedding normative assumptions into reward inference prior']",['intervention'],"['arxiv__arxiv_org_abs_1606_06565', 'arxiv__arxiv_org_abs_1206_5264', 'arxiv__arxiv_org_abs_1712_05812', 'arxiv__arxiv_org_abs_1308_3778', 'arxiv__arxiv_org_abs_1705_04630', 'arxiv__arxiv_org_abs_1711_09883', 'arxiv__arxiv_org_abs_1611_01578', 'arxiv__arxiv_org_abs_1802_03493', 'arxiv__arxiv_org_abs_1711_06782', 'arxiv__arxiv_org_abs_1512_05832', 'arxiv__arxiv_org_abs_1803_06373', 'arxiv__arxiv_org_abs_1705_09990', 'arxiv__arxiv_org_abs_1703_03717', 'arxiv__arxiv_org_abs_1106_2657', 'arxiv__arxiv_org_abs_1802_01744', 'arxiv__arxiv_org_abs_1412_6980', 'arxiv__arxiv_org_abs_1607_08289', 'arxiv__arxiv_org_abs_1705_03394', 'arxiv__arxiv_org_abs_1409_3215', 'arxiv__arxiv_org_abs_1609_08144']","[641, 642, 644, 517, 645, 13, 274, 408, 427, 48, 49, 953, 828, 188, 1085, 63, 580, 842, 845, 974, 854, 218, 476, 860, 613, 369, 886, 888]",28
6,"['Integrate eligibility traces and delay compensation in RLHF pipelines', 'Implement advantage-based RLHF (COACH) during training of AI systems', 'recurrent supervised goal prediction for goal estimation', 'Train AI on diverse mammal and human behavioural videos for value inference', 'Implement continual value learning mapping human preferences to physical proxies', 'simulation pretraining followed by human fine-tuning', 'generate weakly-supervised macro-intent labels using domain heuristics', 'train novice policy as Bayesian NN with dropout at every layer', 'integrating BAMCP++ querying policy into RLHF systems to reduce human labelling cost', 'Iterative ‘find-another-explanation’ training that penalizes previously high-magnitude gradients', 'train production models via codistillation before deployment', 'use joint action learning with neutral Q-learning and full action observability', 'train AI agents for low temporal discounting via preference shaping', 'Quantisation-aware training with δ-clipping and 8-bit weights', 'Collect human feedback online during training', 'Maintain feedback replay buffer for minibatch SGD every 10 time steps', 'use low-precision checkpoint exchange plus codistillation', 'Provide trainer UI supporting discrete aggregated feedback values during RLHF', 'incorporating Bayesian ARL framework into RLHF to model reward uncertainty and query cost', 'two-way codistillation with checkpoint exchange ≤50 steps', 'Apply importance weighting using uniform delay 0.2–4s in SGD updates', 'apply MinEP exploration policy during RL training', 'model-free shared autonomy via deep Q-learning with raw action embedding', 'asynchronous pipeline of policy, preference queries, and reward model reduces feedback by 3 orders of magnitude', 'Train AI to model subjunctive dependencies and adopt FDT strategies', 'during RL training, compute action-values using each stakeholder’s subjective model separately and combine via shifting weights', 'Pre-train CNN encoder via autoencoder on random policy states', 'combine hierarchical softmax with subsampling for efficient phrase embedding training', 'Joint learning of reset and forward policies', 'Policy-gradient fine-tuning with sentence-level GLEU reward', 'train hierarchical multi-agent policies with shared macro-intent latent variables', 'Shared 32k source–target word-piece vocabulary', 'Train networks with proper scoring-rule losses', 'implement DropoutDAgger during imitation-learning training', 'Integrate trainable soft-attention module into encoder-decoder']",['intervention'],"['arxiv__arxiv_org_abs_1709_10163', 'arxiv__arxiv_org_abs_1704_02882', 'arxiv__arxiv_org_abs_1612_01474', 'arxiv__arxiv_org_abs_1409_0813', 'arxiv__arxiv_org_abs_1409_0473', 'arxiv__arxiv_org_abs_1803_04926', 'arxiv__arxiv_org_abs_1709_06166', 'arxiv__arxiv_org_abs_1701_01302', 'arxiv__arxiv_org_abs_1711_06782', 'arxiv__arxiv_org_abs_1803_07612', 'arxiv__arxiv_org_abs_1310_4546', 'arxiv__arxiv_org_abs_1703_03717', 'arxiv__arxiv_org_abs_1802_01744', 'arxiv__arxiv_org_abs_1607_08289', 'arxiv__arxiv_org_abs_1705_03394', 'arxiv__arxiv_org_abs_1706_03741', 'arxiv__arxiv_org_abs_1609_08144', 'arxiv__arxiv_org_abs_1710_05060', 'arxiv__arxiv_org_abs_1701_06049', 'arxiv__arxiv_org_abs_1609_04994', 'arxiv__arxiv_org_abs_1804_03235']","[512, 513, 1154, 521, 778, 779, 780, 141, 655, 656, 1041, 1044, 789, 409, 1151, 160, 418, 423, 431, 433, 565, 952, 954, 956, 574, 833, 1099, 1100, 86, 479, 746, 748, 497, 1148, 511]",35
7,"['integrate AL,T-style logical-uncertainty estimator into AI reasoning engine', 'Embed logical-counterfactual evaluation (UDT style) into decision algorithms', 'Conduct foundational research on logical uncertainty & UDT formalisation', 'Integrate logical inductor module into AI system reasoning about code', 'Implement updateless policy-selection in agent decision module', 'implement realistic value functions that incorporate self-modification effects and current utility', 'implement SPEDT value function', 'Integrate inductively coherent reasoning module', 'Apply logical induction for self-referential belief modules', 'Design agents with stable optimisation targets via UDT-based self-modification policy', 'differentiable safety layer projecting policy actions to satisfy constraints', 'Implement FDT-based reasoning in AI agents']",['intervention'],"['arxiv__arxiv_org_abs_1604_05288', 'arxiv__arxiv_org_abs_1710_05060', 'arxiv__arxiv_org_abs_1605_03142', 'arxiv__arxiv_org_abs_1507_01986', 'arxiv__arxiv_org_abs_1801_08757', 'arxiv__arxiv_org_abs_1506_07359', 'arxiv__arxiv_org_abs_1707_08747', 'arxiv__arxiv_org_abs_1510_03370']","[224, 738, 739, 234, 238, 785, 242, 245, 343, 249, 921, 317]",12
8,"['Represent human agents as probabilistic programs with hyperbolic discounting and belief distributions', 'Governance policy mandating explicit causal model disclosure for fairness audits', 'finite stochastic loop program restriction', 'self-modelling with stochastic action & verification', 'bounded safe exploration using ergodic region constraints', 'embed G-fair bounded proof-search cooperation module', 'exploration that deliberately seeks catastrophes early', 'Restrict AI goal self-modification without external alignment verification', 'designing update rules with neuron-local computations for safety-critical models', 'use fixed finite lifetime horizon with recursive value functions', 'adaptive compute budgets for late feedback', 'maintaining per-principal world models for expected-utility estimation', 'Causal-model-based policy optimisation restricting interventions to causal parents of goal', 'Integrate incomplete-model forecaster module for conservative long-term prediction in AI systems', 'runtime computational resource quota enforcement per agent', 'Delay deployment of self-improving AGI until definable desirable goal is found', 'Strict environment versioning policy', 'Convert stochastic environment models to deterministic equivalents before safety causal verification', 'mandatory pre-deployment verification of ML models for inflexible hardware', 'limit agent capability or domain', 'Use multiple deterministic equivalents to test agent robustness to representation change', 'avoid hedonistic and ignorant value functions in agent design', 'AI-risk assessment frameworks avoiding energy-constraint assumptions', 'deontological governor enforcing computable rules', 'explicitly model self-modification consequences in the world model and value function', 'Randomised negotiation length (Poisson 4-10)', 'model-based RL planning to avoid catastrophes', 'restrict deontology language to decidable subset', 'compute stochastic mapping and compose utility', 'pre-compute translations over ontology distribution', 'abandon language of certainty']",['intervention'],"['arxiv__arxiv_org_abs_1804_00222', 'arxiv__arxiv_org_abs_1202_6177', 'arxiv__arxiv_org_abs_1707_05173', 'arxiv__arxiv_org_abs_1606_06565', 'arxiv__arxiv_org_abs_1602_04019', 'arxiv__arxiv_org_abs_1605_03142', 'arxiv__arxiv_org_abs_1703_06856', 'arxiv__arxiv_org_abs_1105_3821', 'arxiv__arxiv_org_abs_1705_04630', 'arxiv__arxiv_org_abs_1409_0813', 'arxiv__arxiv_org_abs_1604_06963', 'arxiv__arxiv_org_abs_1411_1373', 'arxiv__arxiv_org_abs_1804_03980', 'arxiv__arxiv_org_abs_1803_08971', 'arxiv__arxiv_org_abs_1602_04184', 'arxiv__arxiv_org_abs_1512_05832', 'arxiv__arxiv_org_abs_1711_00363', 'arxiv__arxiv_org_abs_1803_04585', 'arxiv__arxiv_org_abs_1106_2657', 'arxiv__arxiv_org_abs_1801_03737', 'arxiv__arxiv_org_abs_1506_07359', 'arxiv__arxiv_org_abs_1606_01540']","[7, 9, 907, 1166, 911, 19, 275, 538, 283, 29, 158, 159, 799, 300, 698, 701, 326, 328, 331, 204, 332, 206, 1105, 344, 345, 605, 350, 227, 1130, 1012, 378]",31
9,"['train generator–discriminator pair with back-prop only on safe dataset', 'SortaGrad curriculum ordering minibatches by utterance length in epoch 1', 'Train model using causal DAG features limited to latent fair variables U and non-descendants of A', 'replace high-score n-grams with phrase tokens before training embeddings', 'alternate k=1 discriminator update for every generator update', 'apply input sequence reversal during training for seq2seq tasks', 'alternate regeneration with optimisation during training', 'Clean Logit Pairing between random clean images with Gaussian noise augmentation', 'random feature-permutation augmentation during meta-training', 'inverse-variance demographic weighting', 'apply Adam optimizer with α=0.001 β1=0.9 β2=0.999 ε=1e-8 during pre-training', 'Generate transfer attacks using ALP-trained models for black-box evaluation before deployment', 'training curricula including irreducible-pattern sequences', 'including cross-domain datasets in meta-training pipeline', 'use DkNN neighbour analysis to audit and mitigate dataset bias', 'Apply scale-jittering augmentation in training pipeline', 'apply attention-dropout p=0.1 during training', 'curate harm-free dataset before GAN training', 'Adversarial Logit Pairing (add λ · L2 loss between clean/adversarial logits during training)', 'include bias-correction terms in adaptive moment estimation when using Adam or RMSProp', 'Compute additive-error residuals independent of A and train on residuals', 'Train ensemble of M independently initialised networks on full data', 'adopting meta-learned unsupervised update rule during pre-training', 'add replication-regularisation term with dynamic weighting during fine-tuning', 'inject additive noise into 40% of training utterances', 'Mixed-minibatch PGD adversarial training on ImageNet with synchronous distributed training and batch size = 1 600', 'apply frequency-based subsampling during embedding pre-training', 'Visualise attention heat-maps for source–target pairs', 'randomisation of test stimuli order during training and evaluation', 'Apply fast-gradient-sign adversarial training with ε=1% input range', 'Deploy ensemble of two independently trained multi-scale VGG models', 'synchronous SGD with ring-all-reduce and fixed minibatch', 'train embeddings with negative sampling using k=5-20 and unigram^(3/4) noise', 'sequence-wise Batch Normalisation applied to each RNN layer', 'Include protected attribute and adjust coefficients based on causal analysis to cancel mediated paths', 'partition training data across codistillation groups', 'maximise log D(G(z)) instead of minimise log(1-D(G(z))) for generator', 'maintain exponential moving average of model parameters during training']",['intervention'],"['arxiv__arxiv_org_abs_1803_05859', 'arxiv__arxiv_org_abs_1705_08807', 'arxiv__arxiv_org_abs_1409_1556', 'arxiv__arxiv_org_abs_1804_00222', 'arxiv__arxiv_org_abs_1412_6980', 'arxiv__arxiv_org_abs_1612_01474', 'arxiv__arxiv_org_abs_1409_3215', 'arxiv__arxiv_org_abs_1310_4546', 'arxiv__arxiv_org_abs_1703_06856', 'arxiv__arxiv_org_abs_1406_2661', 'arxiv__arxiv_org_abs_1706_03762', 'arxiv__arxiv_org_abs_1512_02595', 'arxiv__arxiv_org_abs_1409_0473', 'arxiv__arxiv_org_abs_1803_06373', 'arxiv__arxiv_org_abs_1803_04765', 'arxiv__arxiv_org_abs_1510_03370', 'arxiv__arxiv_org_abs_1803_03453', 'arxiv__arxiv_org_abs_1804_03235']","[128, 256, 130, 259, 132, 1029, 262, 1156, 136, 265, 144, 533, 535, 536, 670, 1063, 169, 1069, 175, 1076, 1079, 185, 1082, 1088, 82, 83, 84, 212, 214, 221, 1120, 481, 484, 996, 1124, 1127, 623, 253]",38
10,"['Adopt sequential sub-questioning with independent verification before next Oracle', 'implement intentional transparency channels', 'action-override with off-policy Q-learning', 'apply limited yes/no shuffled answer protocol for heavy containment', 'skip option for uncertain feature queries', 'restrict AGI datasets and runtime feeds to remove sensitive content via content filters and screening', 'require on-device inference for sensitive personal-data processing', 'validate and limit GPU API calls or use modified open-source drivers inside container', 'Implement counterfactual Oracle with erasure-based reward', 'include EOS token during training and penalize generation beyond EOS', 'develop data-efficient blocker architectures', 'Limit answer precision or list size to penalise deviation', 'Implement on-policy Oracle with limited answer list and episodic reward', 'Multiple autonomous reset attempts parameter N', 'Early-abort mechanism gating forward actions', 'apply conservative low threshold blocking strategy', 'credibility-threshold rejection mechanism', 'design a small, formally-verified external shutdown module that overrides agent actions and cannot be disabled', 'implement restricted-window self-attention (r=128) on sensitive workloads', 'implement authorisation gating to disable weight-output channels in production', 'Design top-answer-preserving questions for on-policy Oracle']",['intervention'],"['arxiv__arxiv_org_abs_1803_05859', 'arxiv__arxiv_org_abs_1709_06275', 'arxiv__arxiv_org_abs_1707_08476', 'arxiv__arxiv_org_abs_1711_06782', 'arxiv__arxiv_org_abs_1409_3215', 'arxiv__arxiv_org_abs_1706_03762', 'arxiv__arxiv_org_abs_1308_3778', 'arxiv__arxiv_org_abs_1707_05173', 'arxiv__arxiv_org_abs_1711_05541', 'arxiv__arxiv_org_abs_1803_04765', 'arxiv__arxiv_org_abs_1711_09883', 'arxiv__arxiv_org_abs_1803_08971', 'arxiv__arxiv_org_abs_1802_01604']","[1024, 673, 806, 935, 810, 815, 1072, 817, 819, 823, 695, 700, 830, 62, 191, 839, 716, 718, 724, 1109, 756]",21
11,"['Use short trajectory clip comparisons', 'plan actions maximising task reward plus λ·information gain', 'using mind-changing cost heuristic querying strategy', 'using First-N heuristic querying strategy', 'explicit human goal communication channel', 'Bayesian goal inference with max-entropy IRL for goal estimation', 'Collect diverse human behaviour data for ethics shaping', 'Set dt ≈ reaction_time/10 and τ ≥ worst-case-collision-lead-time when deploying FMC for safety-critical control', 'Bayesian motion-level goal inference', 'implement pragmatic-pedagogic CIRL policy in robot decision-making', 'three-argument utility with delta filter', 'Design AGI with corrigibility and goal uncertainty under human oversight', 'integrate best-response human model into motion planner', 'Route high-uncertainty inputs to human review before final decision', 'optimise trajectories for t-predictability in motion planner', 'active learning selective querying']",['intervention'],"['arxiv__arxiv_org_abs_1705_04226', 'arxiv__arxiv_org_abs_1802_01744', 'arxiv__arxiv_org_abs_1712_04172', 'arxiv__arxiv_org_abs_1706_03741', 'arxiv__arxiv_org_abs_1803_05049', 'arxiv__arxiv_org_abs_1707_05173', 'arxiv__arxiv_org_abs_1409_0813', 'arxiv__arxiv_org_abs_1707_06354', 'arxiv__arxiv_org_abs_1411_1373', 'arxiv__arxiv_org_abs_1802_01780', 'arxiv__arxiv_org_abs_1803_04926', 'arxiv__arxiv_org_abs_1612_01474']","[866, 1058, 964, 709, 487, 202, 592, 657, 1042, 1043, 596, 600, 697, 955, 157, 958]",16
12,"['Targeted replication of linchpin neuroscience findings on human values using open-science methods', 'model shut-down orders as reward evidence to induce compliance', 'Pre-deployment evaluation harness with predictor dilemmas', 'feature-augmented comparison queries', 'apply HIRL with human blocking catastrophic actions and penalty', 'Use ensemble of reward predictors and active query selection', 'Apply policy mixing with decaying obedience probability schedule', 'Implement internal market with dominant-forecaster pricing for pre-deployment red-team evaluation', 'semi-supervised reinforcement learning with learned reward predictor from limited queries', 'adversarial evolutionary red-teaming of reward functions before deployment', 'decoupled-RL with Bayesian reward inference', 'Incentive scheme that rewards ground-truth audits instead of proxy alone', 'Include superset reward feature set', 'use NAS with robustness reward to discover adversarially robust architectures', 'run pre-deployment bargaining simulation to pick initial weight vector ensuring both principals beat their BATNAs', 'adversarial reward model that actively seeks counter-examples', 'Filtering or re-weighting human feedback before training reward model', 'EXP3-style adversary-aware exploration', 'secure reward channels with tripwire and adversarial training', 'Specify AI objective without inherent self-preservation and maintain value uncertainty', 'implement inverse reward design in reward specification pipeline', 'Train reward predictor from human pairwise clip comparisons and use it as proxy reward during RL', 'Implement Deep TAMER with deep reward model and online SGD using human scalar feedback', 'reward_clipping_to_minus1_1', 'Repeated-game training schedule']",['intervention'],"['arxiv__arxiv_org_abs_1712_04307', 'arxiv__arxiv_org_abs_1202_6177', 'arxiv__arxiv_org_abs_1707_05173', 'arxiv__arxiv_org_abs_1606_06565', 'arxiv__arxiv_org_abs_1705_04630', 'arxiv__arxiv_org_abs_1711_09883', 'arxiv__arxiv_org_abs_1312_5602', 'arxiv__arxiv_org_abs_1804_03980', 'arxiv__arxiv_org_abs_1611_01578', 'arxiv__arxiv_org_abs_1606_05374', 'arxiv__arxiv_org_abs_1701_01302', 'arxiv__arxiv_org_abs_1802_01604', 'arxiv__arxiv_org_abs_1705_09990', 'arxiv__arxiv_org_abs_1803_04585', 'arxiv__arxiv_org_abs_1803_03453', 'arxiv__arxiv_org_abs_1705_04226', 'arxiv__arxiv_org_abs_1607_08289', 'arxiv__arxiv_org_abs_1706_03741', 'arxiv__arxiv_org_abs_1710_05060', 'arxiv__arxiv_org_abs_1709_10163']","[643, 646, 777, 654, 658, 792, 1176, 411, 932, 39, 693, 584, 588, 848, 471, 857, 989, 609, 108, 366, 879, 498, 372, 375, 1016]",25
13,"['Initialize AI goal structure with validated neuroscience-derived value prior during IRL training', 'implement EvOp-style independent subsequence scoring among ensemble models during long-horizon alignment training', ""conduct pre-deployment evaluation of robot's pragmatic inference accuracy with human users"", 'implement SGVB-based variational inference to capture model uncertainty', 'update beliefs with policy-conditioned probabilities', 'unsupervised risk estimation via error distribution modeling', 'Implement runtime monitoring of agent belief over initial state to detect unexpected learning patterns', 'implement MRDR-based off-policy evaluation before deployment', 'pre-training linear safety-signal model on offline data', 'Use M* prior for compute-budget metareasoning', 'include high-fidelity simulator expert in ensemble used for EvOp selection', 'periodic reevaluation with reserved compute budget', 'apply structured expert elicitation with debiasing training for AI-risk forecasts', 'dual-framing elicitation protocol', ""protocols for eliciting and sharing principals' priors"", 'Calibration test using quickly computable theorem sequence', 'Perform combined dense and 50-crop evaluation over 3 scales before deployment', 'Fit latent fair attributes via probabilistic causal inference and use them as features', 'deploy VAE-based OOD detection gate during pre-deployment testing', 'Initialize AI goal structure with mammalian value priors', 'approximate EP with random environment sampling', 'active query selection via expected volume removed', 'provide interruption flag and prune interrupted experiences during learning', 'apply probabilistic decision rule: execute novice action only if ≥ p mass lies within τ of expert action', 'Pre-deployment causal abduction–action–prediction evaluation procedure', 'use Parzen-window log-likelihood estimation on generated samples', 'Maintain posterior over multiple preference-belief explanations before acting on inferred preferences', 'Use logical induction to screen code termination before execution', 'apply experience replay prioritizing non-interrupted experiences', 'Bootstrap ensemble of Q-functions', 'Add coverage penalty β≈0.2 to beam-search score', 'context-conditioned off-policy learning']",['intervention'],"['arxiv__arxiv_org_abs_1712_04307', 'arxiv__arxiv_org_abs_1409_1556', 'arxiv__arxiv_org_abs_1312_6114', 'arxiv__arxiv_org_abs_1606_06565', 'arxiv__arxiv_org_abs_1704_02882', 'arxiv__arxiv_org_abs_1707_08747', 'arxiv__arxiv_org_abs_1703_10987', 'arxiv__arxiv_org_abs_1705_08807', 'arxiv__arxiv_org_abs_1703_06856', 'arxiv__arxiv_org_abs_1711_09883', 'arxiv__arxiv_org_abs_1709_06166', 'arxiv__arxiv_org_abs_1802_03493', 'arxiv__arxiv_org_abs_1604_05288', 'arxiv__arxiv_org_abs_1711_06782', 'arxiv__arxiv_org_abs_1512_05832', 'arxiv__arxiv_org_abs_1707_06354', 'arxiv__arxiv_org_abs_1711_00363', 'arxiv__arxiv_org_abs_1802_01604', 'arxiv__arxiv_org_abs_1106_2657', 'arxiv__arxiv_org_abs_1607_08289', 'arxiv__arxiv_org_abs_1801_03737', 'arxiv__arxiv_org_abs_1406_2661', 'arxiv__arxiv_org_abs_1609_08144', 'arxiv__arxiv_org_abs_1801_08757', 'arxiv__arxiv_org_abs_1506_07359', 'arxiv__arxiv_org_abs_1609_04994', 'arxiv__arxiv_org_abs_1604_05280']","[138, 914, 22, 407, 278, 537, 534, 919, 545, 803, 421, 937, 172, 428, 307, 310, 567, 568, 825, 319, 321, 710, 973, 851, 740, 230, 616, 747, 876, 120, 123, 381]",32
14,"['switch from exploration to greedy exploitation when EP < ε threshold', 'Configure α parameter to ≤1 in VR during forward-planning for environments where irreversible failure is possible', 'prioritised_experience_replay_by_td_error', 'set εt = c/(m√nt(s)) or c/log t with matched θt', 'epsilon_greedy_annealing_1_to_0.1']",['intervention'],"['arxiv__arxiv_org_abs_1609_04994', 'arxiv__arxiv_org_abs_1704_02882', 'arxiv__arxiv_org_abs_1803_05049', 'arxiv__arxiv_org_abs_1312_5602']","[416, 110, 113, 566, 1048]",5
